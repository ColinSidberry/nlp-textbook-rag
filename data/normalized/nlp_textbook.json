[
  {
    "id": "be0ebac954e4195f",
    "source": "nlp_textbook",
    "chapter": "Words and Tokens 2 User: I need some help, that much seems certain. ELIZA: WHAT WOULD IT MEAN TO YOU IF YOU GOT SOME HELP",
    "filename": "words-and-tokens.txt",
    "content": "rights reserved. Draft of August 24, 2025.\n\nCHAPTER\n\n Words and Tokens\n2 User: I need some help, that much seems certain.\n ELIZA: WHAT WOULD IT MEAN TO YOU IF YOU GOT SOME HELP\n User: Perhaps I could learn to get along with my mother.\n ELIZA: TELL ME MORE ABOUT YOUR FAMILY\n User: My mother takes care of me.\n ELIZA: WHO ELSE IN YOU FAMILY TAKES CARE OF YOU\n User: My father.\n ELIZA: YOUR FATHER\n User: You are like my father in some ways.\n Weizenbaum (1966)\n ELIZA The dialogue above is from ELIZA, an early natural language processing system\n that could carry on a limited conversation with a user by imitating the responses of\n a Rogerian psychotherapist (Weizenbaum, 1966). ELIZA is a surprisingly simple\n program that uses pattern matching on words to recognize phrases like ‚ÄúI need X‚Äù\n and change the words into suitable outputs like ‚ÄúWhat would it mean to you if you\n got X?‚Äù. ELIZA‚Äôs mimicry of human conversation, while very crude by modern\n standards, was remarkably successful: many people who interacted with ELIZA\n came to believe that it really understood them. As a result, this work led researchers\n to first think about the impacts of chatbots on their users (Weizenbaum, 1976).\n Of course modern chatbots don‚Äôt use the simple pattern-based mimicry that\n ELIZA pioneered. Yet the pattern-based approach to words instantiated in ELIZA\n tokenization is still relevant today in the context of tokenization, the task of separating out or\n tokenizing words and word parts from running text. Tokenization, the first step in\n modern NLP, includes pattern-based approaches that date back to ELIZA.\n To understand tokenization we first need to ask: What is a word? Is um a word?\n What about New York? Is the nature of words similar across languages? Some\n languages, like Vietnamese or Cantonese, have very short words while others, like\n Turkish, have very long words. We also need to think about how to represent words\n in terms of characters. We‚Äôll introduce Unicode, the modern system for representing characters, and the UTF-8 text encoding. And we‚Äôll introduce the morpheme,\n the meaningful subpart of words (like the morpheme -er in the word longer)\n The standard way to tokenize text is to use the input characters to guide us.\n So once we‚Äôve understand the possible subparts of words, we‚Äôll introduce the stan-\nBPE dard Byte-Pair Encoding (BPE) algorithm that automatically breaks up input text\n into tokens. This algorithm uses simple statistics of letter sequences to induce a\n vocabulary of subword tokens. All tokenization systems also depend on regular\n regular\n expressions expressions as a processing step. The regular expression is a language for formally\n specifying and manipulating text strings, an important tool in all modern NLP systems. We‚Äôll introduce regular expressions and show examples of their use\n Finally, we‚Äôll introduce a metric called edit distance that measures how similar\n two words or strings are based on the number of edits (insertions, deletions, substitutions) it takes to change one string into the other. Edit distance plays a role in NLP\n whenever we need compare two words or strings, for example in the crucial word\n error rate metric for automatic speech recognition.\n2 C HAPTER 2 ‚Ä¢ W ORDS AND T OKENS\n\n How many words are in the following sentence?\n They picnicked by the pool, then lay back on the grass and\n looked at the stars.\n This sentence has 16 words if we don‚Äôt count punctuation as words, 18 if we\n count punctuation. Whether we treat period (‚Äú.‚Äù), comma (‚Äú,‚Äù), and so on as words\n depends on the task. Punctuation is critical for finding boundaries of things (commas, periods, colons) and for identifying some aspects of meaning (question marks,\n exclamation marks, quotation marks). Large language models generally count punctuation as separate words.\n Spoken language introduces other complications with regard to defining words.\n utterance What about this utterance from a spoken conversation? (Utterance is the technical\n linguistic term for the spoken correlate of a sentence).\n I do uh main- mainly business data processing\n disfluency This utterance has two kinds of disfluencies. The broken-off word main- is\n fragment called a fragment. Words like uh and um are called fillers or filled pauses. Should\n filled pause we consider these to be words? Again, it depends on the application. If we are\n building a speech transcription system, we might want to eventually strip out the\n disfluencies. But we also sometimes keep disfluencies around. Disfluencies like uh\n or um are actually helpful in speech recognition in predicting the upcoming word,\n because they may signal that the speaker is restarting the clause or idea, and so for\n speech recognition they are treated as regular words. Because different people use\n different disfluencies they can also be a cue to speaker identification. In fact Clark\n and Fox Tree (2002) showed that uh and um have different meanings in English.\n What do you think they are?\n Perhaps most important, in thinking about what is a word, we need to distinguish\n word type two ways of talking about words that will be useful throughout the book. Word types\n are the number of distinct words in a corpus; if the set of words in the vocabulary\n word instance is V , the number of types is the vocabulary size |V |. Word instances are the total\n number N of running words.1 If we ignore punctuation, the picnic sentence has 14\n types and 16 instances:\n They picnicked by the pool, then lay back on the grass and\n looked at the stars.\n We still have decisions to make! For example, should we consider a capitalized\n string (like They) and one that is uncapitalized (like they) to be the same word type?\n The answer is that it depends on the task! They and they might be lumped together as\n the same type in some tasks where we care less about the formatting, while for other\n tasks, capitalization is a useful feature and is retained. Sometimes we keep around\n two versions of a particular NLP model, one with capitalization and one without\n capitalization.\n So far we have been talking about orthographic words: words based on our\n English writing system. But there are many other possible ways to define words.\n For example, while orthographically I‚Äôm is one word, grammatically it functions as\n two words: the subject pronoun I and the verb ‚Äôm, short for am.\n 1 In earlier tradition, and occasionally still, you might see word instances referred to as word tokens, but\n we now try to reserve the word token instead to mean the output of subword tokenization algorithms.\n 2.1 ‚Ä¢ W ORDS 3\n\n Corpus Types = |V | Instances = N\n Shakespeare 31 thousand 884 thousand\n Brown corpus 38 thousand 1 million\n Switchboard telephone conversations 20 thousand 2.4 million\n COCA 2 million 440 million\n Google n-grams 13 million 1 trillion\n corpora. The largest, the Google n-grams corpus, contains 13 million types, but this count\n only includes types appearing 40 or more times, so the true number would be much larger.\n\n The distinctions get even harder to make once we start to think about other languages. For example the writing systems of languages like Chinese, Japanese, and\n Thai simply don‚Äôt have orthographic words at all! That is, they don‚Äôt use spaces to\n mark potential word-boundaries. In Chinese, for example, words are composed of\nhanzi characters (called hanzi in Chinese). Each character generally represents a single\n unit of meaning (called a morpheme, introduced below) and is pronounceable as a\n single syllable. Words are about 2.4 characters long on average. But since Chinese\n has no orthographic words, deciding what counts as a word in Chinese is complex.\n For example, consider the following sentence:\n (2.1) ÂßöÊòéËøõÂÖ•ÊÄªÂÜ≥Ëµõ yaÃÅo mƒ±ÃÅng jƒ±ÃÄn ruÃÄ zoÃång jueÃÅ saÃÄi\n ‚ÄúYao Ming reaches the finals‚Äù\n As Chen et al. (2017) point out, this could be treated as 3 words (a definition of\n words called the ‚ÄòChinese Treebank‚Äô definition, in which Chinese names (family\n name followed by personal names) are treated as a single word):\n (2.2) ÂßöÊòé ËøõÂÖ• ÊÄªÂÜ≥Ëµõ\n YaoMing reaches finals\n But the same sentence could be treated as 5 words (‚ÄòPeking University‚Äô standard),\n in which names are separated into their own units and some adjectives appear as\n distinct words:\n (2.3) Âßö Êòé ËøõÂÖ• ÊÄª ÂÜ≥Ëµõ\n Yao Ming reaches overall finals\n Finally, it is possible in Chinese simply to ignore words altogether and use characters\n as the basic elements, treating the sentence as a series of 7 characters, which works\n pretty well for Chinese since characters are at a reasonable semantic level for most\n applications (Li et al., 2019):\n (2.4) Âßö Êòé Ëøõ ÂÖ• ÊÄª ÂÜ≥ Ëµõ\n Yao Ming enter enter overall decision game\n But that method doesn‚Äôt work for Japanese and Thai, where the individual character\n is too small a unit.\n These issues with defining words makes it hard to use words as the basis for\n tokenizing text in NLP across languages.\n But there‚Äôs another problem with words. There are too many of them!!! How\n many words are there in English? When we speak about the number of words in\n the language, we are generally referring to word types. Fig. 2.1 shows the rough\n numbers of types and instances computed from some English corpora.\n You will notice that the larger the corpora we look at, the more word types we\n find! That suggests that there is not a clear answer to how many words there are;\n the answer keeps growing as we see more data! We can see this fact mathematically\n4 C HAPTER 2 ‚Ä¢ W ORDS AND T OKENS\n\n because the relationship between the number of types |V | and number of instances\n Herdan‚Äôs Law N is called Herdan‚Äôs Law (Herdan, 1960) or Heaps‚Äô Law (Heaps, 1978) after its\n Heaps‚Äô Law discoverers (in linguistics and information retrieval respectively). It is shown in\n Eq. 2.5, where k and Œ≤ are positive constants, and 0 < Œ≤ < 1.\n\n |V | = kN Œ≤ (2.5)\n\n The value of Œ≤ depends on the corpus size and the genre; numbers from 0.44 to 0.56\n or even higher have often been reported. Roughly we can say that the vocabulary\n size for a text goes up a little faster than the square root of its length in words.\n There are also variants of the law, which capture the fact that we can distinguish\nfunction words roughly two classes of words. One is function words, the grammatical words like\n English a and of, that tend not to grow indefinitely (a language tends to have a fixed\n content words number of these). The other is content words: nouns, adjectives and verbs that tend\n to have meanings about people and places and events. Nouns, and especially particular nouns like names and technical terms do tend to grow indefinitely. So models\n that are sensitive to this difference between function words and content words have\n one value of Œ≤ for the initial part of the corpus where all words are still appearing,\n and then a second Œ≤ afterwords for when only the content words are still appearing.\n Fig. 2.2 shows an example from Tria et al. (2018) showing two values of Œ≤ for Heaps\n law computed on the Gutenberg corpus of books.\n Entropy 2018, 20, 752 4 of 19\n\n Figure 2.2. Growth\n Figure Vocabulary size asofa distinct\n of the number function of text\n words length,oncomputed\n computed on the\n the Gutenberg Gutenberg\n corpus corpus\n of texts [15].\n The position\n of publicly of texts books.\n available in the corpus\n Figureis chosen at random.\n from Tria In this case g ' 0.44. Similar behaviours are\n et al. (2018).\n observed in many other systems.\n\n The fact that words grow without end leads to a problem for any computational\n 2.3. Zipf‚Äôs vs. Heaps‚Äô Laws\n model. No matter how big our vocabulary, we will never have a vocabulary that\n In this section\n captures all thewe comparewords\n possible the twothat\n lawsmight\n just observed,\n occur! Zipf‚Äôs\n That law for the\n means frequencies\n that of occurrence\n our computational\n of the elements in a system and Heaps‚Äô law for their temporal appearance. It has often been claimed that\n model will constantly see unknown words: words that it has never seen before.\n Heaps‚Äô and Zipf‚Äôs law are trivially related and that one can derive Heaps‚Äôs law once the Zipf‚Äôs is known.\n This is a huge problem for machine learning models.\n This is not true in general. It turns out to be true only under the specific hypothesis of random-sampling\n Because\n as follows. of these\n Suppose two problems\n the existence (first, that\n of a strict power-law many languages\n behaviour don‚Äôt have\n of the frequency-rank orthodistribution,\n f graphic\n ( R) ‚á† R words, and defining\n a , and construct them\n a sequence of post-hoc\n elements byisrandomly\n challenging and from\n sampling second, that distribution\n this Zipf the num-\n ( R). of\n f ber Through\n wordsthis procedure,\n grows onebound),\n without recovers alanguage\n Heaps‚Äô law with theand\n models functional\n other NLPform D [23,24]\n (t) ‚á† tg don‚Äôt\n models\n with g = 1/a. In order to do that we need to consider the correct expression\n tend to use words as their unit of processing. Instead, they use smaller units for f ( R ) that includes the\n called\n normalisation factor, whose expression can be derived through\n subwords that can be recombined to model new words that our model has never the following approximated integral:\n seen before. To think about defining Z Rmaxsubwords, we first need to talk about units that\n f ( RÃÉ)d RÃÉ = 1 . (3)\n are smaller than words; morphemes 1 and characters.\n\n Let us now distinguish the two cases. For a 6= 1 one has\n\n 1 a\n f ( R) = R a. (4)\n R1maxa 1\n\n while for a = 1 one obtains:\n f ( R) = R 1. (5)\n 2.2 ‚Ä¢ M ORPHEMES : PARTS OF W ORDS 5\n\n Words have parts. At the level of characters, this is obvious. The word cats is composed of four characters, ‚Äòc‚Äô, ‚Äòa‚Äô, ‚Äòt‚Äô, ‚Äòs‚Äô. But this is also true at a more subtle level:\n words have components that themselves have coherent meanings. These compomorphology nents are called morphemes, and the study of morphemes is called morphology. A\n morpheme morpheme is a minimal meaning-bearing unit in a language. So, for example, the\n word fox consists of one morpheme (the morpheme fox) while the word cats consists\n of two: the morpheme cat and the morpheme -s that indicates plural.\n Here‚Äôs a sentence in English segmented into morphemes with hyphens:\n (2.6) Doc work-ed care-ful-ly wash-ing the glass-es\n As we mentioned above, in Chinese, conveniently, the writing system is set up\n so that each character mainly describes a morpheme. Here‚Äôs a sentence in Mandarin\n Chinese with each morpheme character glossed, followed by the translation:\n (2.7) Ê¢Ö Âπ≤ Ëèú Áî® Ê∏Ö Ê∞¥ Ê≥° ËΩØ ÔºåÊçû Âá∫ Âêé ÔºåÊ≤• Âπ≤\n plum dry vegetable use clear water soak soft , remove out after , drip dry\n Âàá Á¢é\n chop fragment\n Soak the preserved vegetable in water until soft, remove, drain, and chop\n root We generally distinguish two broad classes of morphemes: roots‚Äîthe central\n affix morpheme of the word, supplying the main meaning‚Äîand affixes‚Äîadding ‚Äúadditional‚Äù meanings of various kinds. In the English example above, for the word\n worked, work is a root and -ed is an affix; similarly for glasses, glass is a root and\n -es an affix.\n Affixes themselves fall into two classes, or more correctly a continuum between\n inflectional\n morphemes two poles. At one end, inflectional morphemes are grammatical morphemes that\n tend to play a syntactic role, such as marking agreement. For example, English has\n the inflectional morpheme -s (or -es) for marking the plural on nouns and the inflectional morpheme -ed for marking the past tense on verbs. Inflectional morphemes\n tend to be productive and often obligatory and their meanings tend to be predictable.\n derivational\n morphemes Derivational morphemes are more idiosyncratic in their application and meaning.\n Usually they apply only to a specific subclass of words and result in a word of a different grammatical class than the root, often with a meaning hard to predict exactly.\n In the example above, the word care (a noun) can be combined with the derivational\n affix -full to produce an adjective (careful), and another derivational affix -ly to result\n in an adverb (carefully).\n clitic There is another class of morphemes: clitics. A clitic is a morpheme that acts\n syntactically like a word but is reduced in form and attached (phonologically and\n sometimes orthographically) to another word. For example the English morpheme\n ‚Äôve in the word I‚Äôve is a clitic; it has the grammatical meaning of the word have, but\n in form in cannot appear alone (you can‚Äôt just say the sentence ‚Äú‚Äôve‚Äù). The English\n possessive morpheme ‚Äôs in the phrase the teacher‚Äôs book is a clitic. French definite\n article l‚Äô in the word l‚Äôopera is a clitic, as are prepositions in Arabic like b ‚Äòby/with‚Äô\n and conjunctions like w ‚Äòand‚Äô.\n The study of how languages vary in their morphology, i.e., how words break\nmorphological\n typology up into their parts, is called morphological typology. While morphologies of languages can differ along many dimensions, two dimensions are particularly relevant\n for computational word tokenization.\n6 C HAPTER 2 ‚Ä¢ W ORDS AND T OKENS\n\n The first dimension is the number of morphemes per word. In some languages,\n like Vietnamese and Cantonese, each word on average has just over one morpheme.\n isolating We call languages at this end of the scale isolating languages. For example each\n word in the following Cantonese sentence has one morpheme (and one syllable):\n (2.8) keoi5 waa6 cyun4 gwok3 zeoi3 daai6 gaan1 uk1 hai6 ni1 gaan1\n he say entire country most big building house is this building\n ‚ÄúHe said the biggest house in the country was this one‚Äù\n Alternatively, in languages like Koryak, a Chukotko-Kamchatkan language spoken in the northern part of the Kamchatka peninsula in Russia, a single word may\n have very many morphemes, corresponding to a whole sentence in English (Arkadiev,\n synthetic 2020; Kurebito, 2017). We call languages toward this end of the scale synthetic lanpolysynthetic guages, and the very end of the scale polysynthetic languages.\n (2.9) t-@-nk‚Äôe-mejN-@-jetem@-nni-k\n 1SG.S-E-midnight-big-E-yurt.cover-E-sew-1SG.S[PFV]\n ‚ÄúI sewed a lot of yurt covers in the middle of a night.‚Äù\n (Koryak, Chukotko-Kamchatkan, Russia; Kurebito (2017, 844))\n Fig. 2.3 shows an early computation of morphemes per words on a few languages\n by the linguistic typologist Joseph Greenberg (1960).\n\n e sh ic\n es gli t a nd\n am i ish n\n E t ili kri l\n n )\n et\n n rs g l\n l d aku wah ans ee uit\n Vi Fa En O Y S S Gr (In\n\n Analytic Synthetic Polysynthetic\n Morphemes per Word\n\n The second dimension is the degree to which morphemes are easily segmentable,\n agglutinative ranging from agglutinative languages like Turkish, in which morphemes have relfusion atively clean boundaries, to fusion languages like Russian, in which a single affix\n may conflate multiple morphemes, like -om in the word stolom (table-SG-INSTR-\nDECL 1), which fuses the distinct morphological categories instrumental, singular,\n and first declension.\n The English -s suffix in She reads the article is an example of fusion, since the\n suffix means both third person singular but also means present tense, and there‚Äôs no\n way to divide up the meaning to different parts of the -s.\n Although we have loosely talked about these properties (analytic, polysynthetic,\n fusional, agglutinative) as if they are properties of languages, in fact languages can\n make use of different morphological systems so it would be more accurate to talk\n about these as general tendencies.\n Nonetheless, the fact morphemes can be hard to define, and that many languages\n can have complex morphemes that aren‚Äôt easy to break up into pieces makes it very\n difficult to use morphemes as a standard for tokenization cross-lingually.\n 2.3 ‚Ä¢ U NICODE 7\n\n Another option we could consider for tokenization is the level of the individual character. How do we even represent characters across languages and writing system?\n Unicode The Unicode standard is a method for representing text written using any character\n in any script of the languages of the world (including dead languages like Sumerian\n cuneiform, and invented languages like Klingon).\n Let‚Äôs start with a brief historical note about an English-specific subset of Unicode\n (technically called ‚ÄòBasic Latin‚Äô in Unicode, and commonly referred to as ASCII).\n Starting in the 1960s, the Latin characters used to write English (like the ones used\n ASCII in this sentence), were represented with a code called ASCII (American Standard\n Code for Information Interchange). ASCII represented each character with a single\n byte. A byte can represent 256 different characters, but ASCII only used 127 of\n them; the high-order bit of ASCII bytes is always set to 0. (Actually it only used 95\n of them and the rest were control codes for an obsolete machine called a teletype).\n Here‚Äôs a few ASCII characters with their representation in hex and decimal:\n\n Ch Hex Dec Ch Hex Dec Ch Hex Dec Ch Hex Dec\n < 3C 60 @ 40 64 ... \\ 5C 92 ` 60 96\n = 3D 61 A 41 65 ... [ 5D 93 a 61 97\n > 3E 62 B 42 66 ... ÀÜ 5E 94 b 62 98\n ? 3F 63 C 43 67 ... _ 5F 95 c 63 99\n in hexadecimal and decimal.\n\n But ASCII is of course insufficient since there are lots of other characters in the\n world‚Äôs writing systems! Even for scripts that use Latin characters, there are many\n more than the 95 in ASCII. For example, this Spanish phrase (meaning ‚ÄúSir, replied\n Sancho‚Äù) has two non-ASCII characters, nÃÉ and oÃÅ:\n (2.10) SenÃÉor- respondioÃÅ Sancho-\nDevanagari And lots of languages aren‚Äôt based on Latin characters at all! The Devanagari\n script is used for 120 languages (including Hindi, Marathi, Nepali, Sindhi, and Sanskrit). Here‚Äôs a Devanagari example from the Hindi text of the Universal Declaration\n of Human Rights:\n\n Chinese has about 100,000 Chinese characters in Unicode (including overlapping and non-overlapping variants used in Chinese, Japanese, Korean, and Vietnamese, collectively referred to as CJKV).\n All in all there are more than 150,000 characters and 168 different scripts supported in Unicode 16.0. Even though many scripts from around the world have\n yet to be added to Unicode, there are so many there, from scripts used by modern languages (Chinese, Arabic, Hindi, Cherokee, Ethiopic, Khmer, N‚ÄôKo, Turkish,\n Spanish) to scripts of ancient languages (Cuneiform, Ugaritic, Egyptian Hieroglyph,\n Pahlavi), as well as mathematical symbols, emojis, currency symbols, and more.\n\n 2.3.1 Code Points\n code point How does it work? Unicode assigns a unique id, called a code point, for each one\n8 C HAPTER 2 ‚Ä¢ W ORDS AND T OKENS\n\n of these 150,000 characters.\n The code point is an abstract representation of the character, and each code point\n is represented by a number, traditionally written in hexadecimal, from number 0\n through 0x10FFFF (which is 1,114,111 decimal). Having over a million code points\n means there is a lot of room for new characters. It is traditional to represent these\n code points with the prefix ‚ÄúU+‚Äù (which just means ‚Äúthe following is a Unicode hex\n representation of a code point‚Äù). So the code point for the character a is U+0061\n which is the same as 0x0061. (Note that Unicode was designed to be backwards\n compatible with ASCII, which means that the first 127 code points, including the\n code for a, are identical with ASCII.) Here are some sample code points; some (but\n not all) come with descriptions:\n U+0061 a LATIN SMALL LETTER A\n U+0062 b LATIN SMALL LETTER B\n U+0063 c LATIN SMALL LETTER C\n U+00F9 uÃÄ LATIN SMALL LETTER U WITH GRAVE\n U+00FA uÃÅ LATIN SMALL LETTER U WITH ACUTE\n U+00FB uÃÇ LATIN SMALL LETTER U WITH CIRCUMFLEX\n U+00FC uÃà LATIN SMALL LETTER U WITH DIAERESIS\n U+8FDB 5/23/25,\n Ëøõ 5:26 PM x.htm\n U+8FDC Ëøú\n U+8FDD Ëøù\n üÄé\n U+8FDE Ëøû\n 5/23/25, 5:26 PM x.htm\n U+1F600 GRINNING FACE\n U+1F00E üÄé MAHJONG TILE EIGHT OF CHARACTERS\n glyph Note that a code point does not specifiy the glyph, the visual representation\n of a character. Glyphs are stored in fonts. The code point U+0061 is an abstract\n representation of a. There can be an indefinite number of visual representations,\n for example in different fonts like Times Roman (a) or Courier (a), or different font\n styles like boldface (a) or italic (a). But all of them are represented by the same code\n point U+0061.\n\n 2.3.2 UTF-8 Encoding\n While the code point (the unique id) is the abstract Unicode representation of the\n character, we don‚Äôt just stick that id in a text file.\n Instead, whenever we need to represent a character in a text string, we write an\n encoding encoding of the character. There are many different possible encoding methods, but\n the encoding method called UTF-8 is by far the most frequent (for example almost\n the entire web is encoded in UTF-8).\n Let‚Äôs talk about encodings. The Unicode representation of the word hello consists of the following sequence of 5 code points:\n U+0068 U+0065 U+006C U+006C U+006F\n We can imagine a very simple encoding method: just write the code point id in\n a file. Since there are more than 1 million characters, 16 bits (2 bytes) isn‚Äôt enough,\n so we‚Äôll need to use 4 bytes (32 bit) to capture the 21 bits we need to represent 1.1\n million characters. (We could fit it in 3 bytes but it‚Äôs inconvenient to use multiples\n of 3 for bytes.)\n With this 4-byte representation the word hello would be encoded as the following set of bytes:\n 2.3 ‚Ä¢ U NICODE 9\n\n 00 00 00 68 00 00 00 65 00 00 00 6C 00 00 00 6C 00 00 00 6F\n But we don‚Äôt use this encoding (which is technically called UTF-32) because it\n makes every file 4 times longer than it would have been in ASCII, making files really\n big and full of zeros. Also those zeros cause another problem: it turns out that having\n any byte that is completely zero messes things up for backwards compatibility for\n ASCII-based systems that historically used a 0 byte as an end-of-string marker.\n UTF-8 Instead, the most common encoding standard is UTF-8 (Unicode Transformation Format 8), which represents characters efficiently (using fewer bytes on average) by writing some characters using fewer bytes and some using more bytes.\nvariable-length\n encoding UTF-8 is thus a variable-length encoding.\n For some characters (the first 127 code points, i.e. the set of ASCII characters),\n UTF-8 encodes them as a single byte, so the UTF-8 encoding of hello is :\n 68 65 6C 6C 6F\n This conveniently means that files encoded in ASCII are also valid UTF-8 encodings!\n But UTF-8 is a variable length encoding, meaning that code points ‚â•128 are\n encoded as a sequence of two, three, or four bytes. Each of these bytes are between\n 128 and 255, so they won‚Äôt be confused with ASCII, and each byte indicates in the\n first few bits whether it‚Äôs a 2-byte, 3-byte, or 4-byte encoding.\n Code Points UTF-8 Encoding\n From - To Bit Value Byte 1 Byte 2 Byte 3 Byte 4\n U+0000-U+007F 0xxxxxxx xxxxxxxx\n U+0080-U+07FF 00000yyy yyxxxxxx 110yyyyy 10xxxxxx\n U+0800-U+FFFF zzzzyyyy yyxxxxxx 1110zzzz 10yyyyyy 10xxxxxx\n U+010000-U+10FFFF 000uuuuu zzzzyyyy yyxxxxxx 11110uuu 10uuzzzz 10yyyyyy 10xxxxxx\nin the From-To range, the bit value in column 2 is packed into 1, 2, 3, or 4 bytes. Figure adapted from Unicode\n\n Fig. 2.5 shows how this mapping occurs. For example these rules explain how\n the character nÃÉ, which has code point U+00F1, or bit sequence 00000000 11110001,\n (where blue indicates the sequence yyyyy and red the sequence xxxxxx) is encoded\n into to the two-byte bit sequence 11000011 10110001 or 0xC3B1. As a result of\n these rules, the first 127 characters (ASCII) are mapped to one byte, most remaining characters in European, Middle Eastern, and African scripts map to two bytes,\n most Chinese, Japanese, and Korean characters map to three bytes, and rarer CJKV\n characters and emojis and some symbols map to 4 bytes.\n UTF-8 has a number of advantages. It‚Äôs relatively efficient, using fewer bytes for\n commonly-encountered characters, it doesn‚Äôt use zero bytes (except when literally\n representing the NULL character which is U+0000), it‚Äôs backwards compatible with\n ASCII, and it‚Äôs self-synchronizing, meaning that if a file is corrupted, it‚Äôs always\n possible to find the start of the next or prior character just by moving up to 3 bytes\n left or right.\n Unicode and Python: Starting with Python 3, all Python strings are stored internally as Unicode, each string a sequence of Unicode code points. Thus string\n functions and regular expressions all apply natively to code points. For example,\n functions like len() of a string return its length in characters, i.e., code points, not\n its length in bytes.\n When reading or writing from a file, however, the code points need to be encoded\n and decoding using a method like UTF-8. That is, every file is encoded in some\n10 C HAPTER 2 ‚Ä¢ W ORDS AND T OKENS\n\n encoding. If it‚Äôs not UTF-8, it‚Äôs an older encoding method like ASCII or Latin-1\n (iso 8859 1). There is no such thing as a text file without an encoding. The encoding\n method is specified in Python when opening a file for reading and writing.\n\n tokenization Tokenization, the first stage of natural language processing, is the process of segtokens menting the running input text into tokens.\n We‚Äôve seen three candidates for tokens: words, morphemes and characters. But\n each has problems as a unit. Words and morphemes seem approximately at the right\n level for NLP processing, since they tend to have consistent meanings, but they are\n challenging to define formally. Characters are clearer to define, but seem too small\n a unit to choose for tokens.\n In this section we introduce what we do in practice for NLP: use a data-driven\n approach to define tokens that will generally result in units about the size of morphemes or words, but occasionally use units as small as characters.\n Why tokenize the input? One reason is that converting an input to a deterministic\n fixed set of units means that different algorithms and systems can agree on simple\n questions. For example, How long is this text? (How many units are in it?). Or:\n Is don‚Äôt or New York one token or two? Standardizing is thus essential for replicability in NLP experiments, and many algorithms that we introduce in this book\n (like the perplexity metric for language models) assume that all texts have a fixed\n tokenization.\n Tokenization algorithms that include smaller tokens for morphemes and letters\n also eliminate the problem of unknown words. What are these? As we will see\n in the next chapter, NLP algorithms often learn some facts about language from\n one corpus (a training corpus) and then use these facts to make decisions about a\n separate test corpus and its language. Thus if our training corpus contains, say the\n words low, new, and newer, but not lower, then if the word lower appears in our test\n corpus, our system will not know what to do with it.\n To deal with this unknown word problem, modern tokenizers automatically insubwords duce sets of tokens that include tokens smaller than words, called subwords. Subwords can be arbitrary substrings, or they can be meaning-bearing units like the\n morphemes -est or -er. In modern tokenization schemes, many tokens are words,\n but other tokens are frequently occurring morphemes or other subwords like -er.\n Every unseen word can thus be represented by some sequence of known subword\n units. For example, if we had happened not to ever see the word lower, when it appears we could segment it successfully into low and er which we had already seen.\n In the worst case, a really unusual word (perhaps an acronym like GRPO) could be\n tokenized as a sequence of individual letters if necessary.\n Two tokenization algorithms are widely used in modern language models: bytepair encoding (BPE) (Sennrich et al., 2016), and unigram language modeling\n BPE (ULM) (Kudo, 2018).2 In this section we introduce the byte-pair encoding or BPE\n algorithm (Sennrich et al., 2016; Gage, 1994); see Fig. 2.6.\n Like most tokenization schemes, the BPE algorithm has two parts: a trainer,\n and an encoder. In general in the token training phase we take a raw training corpus\n\n 2 The SentencePiece library includes implementations of both of these (Kudo and Richardson, 2018a),\n and people sometimes use the name SentencePiece to simply mean ULM tokenization.\n 2.4 ‚Ä¢ S UBWORD T OKENIZATION : B YTE -PAIR E NCODING 11\n\n(usually roughly pre-separated into words, for example by whitespace) and induce\na vocabulary, a set of tokens. Then a token encoder take a raw test sentence and\nencodes it into the tokens in the vocabulary that were learned in training.\n\n2.4.1 BPE training\nThe BPE training algorithm iteratively merges frequent neighboring tokens to create\nlonger and longer tokens. The algorithm begins with a vocabulary that is just the\nset of all individual characters. It then examines the training corpus, and finds the\ntwo characters that are most frequently adjacent. Imagine our original corpus is 10\ncharacters long, using a vocabulary of 5 characters, {A, B, C, D, E}:\n A B D C A B E C A B\n The most frequent neighboring pair of characters is ‚ÄúA B‚Äù so we merge those,\nadd a new merged token ‚ÄòAB‚Äô to the vocabulary, and replace every adjacent ‚ÄòA‚Äô ‚ÄòB‚Äô\nin the corpus with the new ‚ÄòAB‚Äô:\n AB D C AB E C AB\n Now we have a vocabulary of 6 possible tokens {A, B, C, D, E, AB}, and the\ncorpus has length 7. And now the most frequent pair of tokens is ‚ÄúC AB‚Äù, so we\nmerge those, leading to a vocabulary with 7 tokens {A, B, C, D, E, AB, CAB}, and the\ncorpus has length 5.\n AB D CAB F CAB\n The algorithm continues to count and merge, creating new longer and longer\ncharacter strings, until k merges have been done creating k novel tokens; k is thus a\nparameter of the algorithm. The resulting vocabulary consists of the original set of\ncharacters plus k new symbols. That‚Äôs the core of the algorithm.\n The only additional complication is that in practice, instead of running on the\nraw sequence of characters, the algorithm is usually run only inside words. That is,\nthe algorithm does not merge across word boundaries. To do this, the input corpus\nis often first separated at white space and punctuation (using the regular expressions\nthat we define later in the chapter). This gives a starting set of strings, each corresponding to the characters of a word, (with the white space usually attached to the\nstart of the word), together with the counts of the words. Then while counts come\nfrom a corpus, merges are only allowed within the strings.\n Let‚Äôs see how the full algorithm thus works on this tiny synthetic corpus, where\nwe‚Äôve explicitly marked the spaces between words:3\n(2.11) set new new renew reset renew\n First, we‚Äôll break up the corpus into words, with leading whitespace, together\nwith their counts; no merges will be allowed to go beyond these word boundaries.\nThe result looks like the following list of 4 words and a starting vocabulary of 7\ncharacters:\n corpus vocabulary\n 2 n e w , e, n, r, s, t, w\n 2 r e n e w\n 1 s e t\n 1 r e s e t\n3 Yes, we realize this isn‚Äôt a particularly likely or exciting sentence.\n12 C HAPTER 2 ‚Ä¢ W ORDS AND T OKENS\n\n The BPE training algorithm first counts all pairs of adjacent symbols: the most\n frequent is the pair n e because it occurs in new (frequency of 2) and renew (frequency of 2) for a total of 4 occurrences. We then merge these symbols, treating ne\n as one symbol, and count again:\n corpus vocabulary\n 2 ne w , e, n, r, s, t, w, ne\n 2 r e ne w\n 1 s e t\n 1 r e s e t\n Now the most frequent pair is ne w (total count=4), which we merge.\n corpus vocabulary\n 2 new , e, n, r, s, t, w, ne, new\n 2 r e new\n 1 s e t\n 1 r e s e t\n Next r (total count of 3) get merged to r, and then r e (total count 3) gets\n merged to re. The system has essentially induced that there is a word-initial prefix\n re-:\n corpus vocabulary\n 2 new , e, n, r, s, t, w, ne, new, r, re\n 2 re new\n 1 s e t\n 1 re s e t\n If we continue, the next merges are:\n merge current vocabulary\n ( , new) , e, n, r, s, t, w, ne, new, r, re, new\n ( re, new) , e, n, r, s, t, w, ne, new, r, re, new, renew\n (s, e) , e, n, r, s, t, w, ne, new, r, re, new, renew, se\n (se, t) , e, n, r, s, t, w, ne, new, r, re, new, renew, se, set\n\n function B YTE - PAIR ENCODING(strings C, number of merges k) returns vocab V\n\n V ‚Üê all unique characters in C # initial set of tokens is characters\n for i = 1 to k do # merge tokens k times\n tL , tR ‚Üê Most frequent pair of adjacent tokens in C\n tNEW ‚Üê tL + tR # make new token by concatenating\n V ‚Üê V + tNEW # update the vocabulary\n Replace each occurrence of tL , tR in C with tNEW # and update the corpus\n return V\n\n adapted from Bostrom and Durrett (2020).\n\n 2.4.2 BPE encoder\n Once we‚Äôve learned our vocabulary, the BPE encoder is used to tokenize a test\n sentence. The encoder just runs on the test data the merges we have learned from\n 2.4 ‚Ä¢ S UBWORD T OKENIZATION : B YTE -PAIR E NCODING 13\n\n the training data. It runs them greedily, in the order we learned them. (Thus the\n frequencies in the test data don‚Äôt play a role, just the frequencies in the training\n data). So first we segment each test sentence word into characters. Then we apply\n the first rule: replace every instance of n e in the test corpus with ne, and then the\n second rule: replace every instance of ne w in the test corpus with new, and so on.\n By the end of course many of the merges simple recreated words in the training\n set. But the merges also created knowledge of morphemes like the re- prefix (that\n might appear in perhaps unseen combinations like revisit or rearrange), or the\n morpheme new without an initial space (hence word-internal) that might appear at\n the start of sentences or in words unseen in training like anew.\n Of course in real settings BPE is run with tens of thousands of merges on a very\n large input corpus, to produce vocabulary sizes of 50,000, 100,000, or even 200,000\n tokens. The result is that most words can be represented as single tokens, and only\n the rarer words (and unknown words) will have to be represented by multiple tokens.\n At least for English. For multilingual systems, the tokens can be dominated by\n English, leaving fewer tokens for other languages, as we‚Äôll discuss below.\n\n 2.4.3 BPE in practice\n The example above just showed simple BPE learning from sequences of ASCII\n bytes. How does BPE work with Unicode input? We normally run BPE on the\n individual bytes of UTF-8-encoded text. That is, we take a Unicode representations\n of text as a series of code points, encode it in bytes using UTF-8, and we treat each of\n these individual bytes as the input to BPE. Thus BPE likely begins by rediscovering\n the 2-byte and common 3-byte sequences that UTF-8 uses to encode various code\n points. Again, running BPE only inside presegmented words helps avoid problems.\n Because there are only 256 possible values of a byte, there will be no unknown tokens, although it‚Äôs possible that BPE will learn some illegal UTF-8 sequences across\n character boundaries. These will be very rare, and can be eliminated with a filter.\n Let‚Äôs see some examples of the industrial application of the BPE tokenizer used\n in large systems like OpenAI GPT4o. This tokenizer has 200K tokens, which is a\n comparatively large number. We can use Tat Dat Duong‚Äôs Tiktokenizer visualizer\n (https://tiktokenizer.vercel.app/) to see the number of tokens in a given\n sentence. For example here‚Äôs the tokenization of a nonsense sentence we made up;\n the visualizer uses a center dot to indicate a space:\n\n The visualization shows colors to separate out words, but of course the true output of the tokenizer is simply a sequence of unique token ids. (In case you‚Äôre interested, they were the following 13 tokens: 11865, 8923, 11, 31211, 6177, 23919,\n 885, 220, 19427, 7633, 18887, 147065, 0)\n Notice that most words are their own token, usually including the leading space.\n Clitics like ‚Äôs are segmented off when they appear on proper nouns like Jane, but\n are counted as part of a word for frequent words like she‚Äôs. Numbers tend to be\n segmented into chunks of 3 digits. And some words (like anyhow) are segmented\n differently if they appear capitalized sentence-initially (two tokens, Any and how),\n then if they appear after a space, lower case (one token anyhow).\n Some of these are related to preprocessing steps. As we mentioned briefly above,\npretokenization language models usually create their tokens in a pretokenization stage that first segments the input using regular expressions, for example breaking the input at spaces\n and punctuation, stripping off clitics, and breaking numbers into sets of 3 digits.\n14 C HAPTER 2 ‚Ä¢ W ORDS AND T OKENS\n\n We‚Äôll see how to use regular expressions in Section 2.7.\n It‚Äôs possible to change this pretokenization to allow BPE tokens to span multiple\n SuperBPE words. For example the SuperBPE algorithm first induces regular BPE subword\n tokens by enforcing pretokenization. It then runs a second stage of BPE allowing\n merges across spaces and punctuation. The result is a large set of tokens that can be\n Preprint\n more efficient. See Fig. 2.7.\n\n merging across spaces. Figure from Liu et al. (2025).\n\n Many of the tokenizers used in practice for large language models are multilingual, trained on many languages. But because the training data for large language\n models is vastly dominated by English text, these multilingual BPE tokenizers tend\n to use most of the tokens for English, leaving fewer of them for other languages. The\n result is that they do a better job of tokenizing English, and the other languages tend\n to get their words split up into shorter tokens. For example let‚Äôs look at a Spanish\n sentence from a recipe for plantains, together with an English translation.\n The English has 18 tokens; each of the 14 words is a token (none of the words\n are split into Figure\n multiple tokens):\n 1: SuperBPE tokenizers encode text much more efficiently than BPE, and the\n gap grows with larger vocabulary size. Encoding efficiency (y-axis) is measured with\n bytes-per-token, the number of bytes encoded per token on average over a large corpus of text.\n In the above text with 40 bytes, SuperBPE uses 7 tokens and BPE uses 13, so the methods‚Äô\n efficiencies are 40/7 = 5.7 and 40/13 = 3.1 bytes-per-token, respectively. In the graph,\n the encoding efficiency of BPE plateaus early due to exhausting the valuable whitespacedelimited words in the training data. In fact, it is bounded above by the gray dotted line,\n which shows the maximum achievable encoding efficiency with BPE, if every whitespace-\nBy contrast, the original\n delimited word were 16 words\n in the in Spanish\n vocabulary. have\n On the other been\n hand, encoded\n SuperBPE into 33 tokens,\n has dramatically\n a much largerbetter encoding\n number. efficiency\n Notice that many\n that continues to improve\n basic words withave\n increased\n beenvocabulary\n broken\n it can continue to add common word sequences to treat as tokens to the vocabulary. The\n size,\n intoas pieces.\n For example different\n hondo, ‚Äòdeep‚Äô,\n gradient hasdifferent\n lines show been transition\n segmented intolearning\n points from h and ondo.\n subword Similarly for\n to superword\n tokens, which always gives an immediate improvement. SuperBPE also has better encoding\n jugo, ‚Äòjuice‚Äô,efficiency\n nuez,than ‚Äònut‚Äô andvariant\n a naive jenjibre ‚Äòginger‚Äô):\n of BPE that does not use whitespace pretokenization at all.\n performing well on these languages. Including multi-word tokens promises to be beneficial\n in several ways: it can lead to shorter token sequences, lowering the computational costs of\n LM training and inference, and may also offer representational advantages by segmenting\n text into more semantically cohesive units (Salehi et al., 2015; Otani et al., 2020; Hofmann\n et al., 2021).\n In this work, we introduce a superword tokenization algorithm that produces a vocabulary of\n Spanish isbothnot a particularly\n subword low-resource\n and ‚Äúsuperword‚Äù tokens, whichlanguage;\n we use to referthis oversegmenting\n to tokens that bridge more can be\n than one word. Our method, SuperBPE, introduces a pretokenization curriculum to the popueven more serious in lower\n lar byte-pair encoding resource languages,\n (BPE) algorithm (Sennrich etoften down\n al., 2016): to individual\n whitespace pretokenizationcharacters.\n is\n Oversegmenting initially used these\n into to enforce learning\n tiny of subword\n tokens can tokens\n cause only (as done in\n various conventionalfor\n problems BPE),the\n but downis disabled in a second stage, where the tokenizer transitions to learning superword tokens.\n stream processing\n Notably, of the language.\n SuperBPE Asmuch\n tokenizers scale willbetter\n become more size‚Äîwhile\n with vocabulary clear once BPEwe introduce\n quickly\n hits a point of diminishing returns and begins adding increasingly rare subwords to the\n transformer models\n vocabulary,in Chapter\n SuperBPE can 8, suchtofragmentation\n continue discover common word cansequences\n lead to poor\n to treat representaas single\n tokens and improve encoding efficiency (see Figure 1).\n tions of meaning, the need for longer contexts, and higher costs to train models\n In our main experiments, we pretrain English LMs at 8B scale from scratch. When fixing the\n (Rust et al., 2021; Ahia et al.,size,\n model size, vocabulary 2023).\n and training compute‚Äîvarying only the algorithm for learning\n the vocabulary‚Äîwe find that models trained with SuperBPE tokenizers consistently and\n significantly improve over counterparts trained with a BPE tokenizer, while also being 27‚Äì\n 33% more efficient at inference time. Our best SuperBPE model achieves an average +4.0%\n\n While data-based tokenization like BPE is the most common way of doing tokenization, there are also situations where we want to constrain our tokens to be words and\n not subwords. This might be useful if we are running parsing algorithms for English\n where the parser might need grammatical words as input. Or it can be useful for\n any linguistic application where we have some a prior definition of the token that we\n 2.5 ‚Ä¢ RULE - BASED TOKENIZATION 15\n\n are interested in studying. Or it can be useful for social science applications where\n orthographic words are useful domains of study.\n In rule-based tokenization, we pre-define a standard and implement rules to implement that kind of tokenization. Let‚Äôs explore this for English word tokenization.\n We have some desiderata for English. We often want to break off punctuation as a separate token; commas are a useful piece of information for parsers,\n and periods help indicate sentence boundaries. But we‚Äôll often want to keep the\n punctuation that occurs word internally, in examples like m.p.h., Ph.D., AT&T, and\n cap‚Äôn. Special characters and numbers will need to be kept in prices ($45.55) and\n dates (01/02/06); we don‚Äôt want to segment that price into separate tokens of ‚Äú45‚Äù\n and ‚Äú55‚Äù. And there are URLs (https://www.stanford.edu), Twitter hashtags\n (#nlproc), or email addresses (someone@cs.colorado.edu).\n Number expressions introduce complications; in addition to appearing at word\n boundaries, commas appear inside numbers in English, every three digits: 555,500.50.\n Tokenization differs by language; languages like Spanish, French, and German, for\n example, use a comma to mark the decimal point, and spaces (or sometimes periods)\n where English puts commas, for example, 555 500,50.\n clitic A rule-based tokenizer can also be used to expand clitic contractions that are\n marked by apostrophes, converting what‚Äôre to the two tokens what are, and we‚Äôre\n to we are. A clitic is a part of a word that can‚Äôt stand on its own, and can only occur when it is attached to another word. Such contractions occur in other alphabetic\n languages, including French pronouns (j‚Äôai and articles l‚Äôhomme).\n Depending on the application, tokenization algorithms may also tokenize multiword expressions like New York or rock ‚Äôn‚Äô roll as a single token, which requires a multiword expression dictionary of some sort. Rule-based tokenization is\n thus intimately tied up with named entity recognition, the task of detecting names,\n dates, and organizations (Chapter 17).\n One commonly used tokenization standard is known as the Penn Treebank to-\nPenn Treebank kenization standard, used for the parsed corpora (treebanks) released by the Lintokenization\n guistic Data Consortium (LDC), the source of many useful datasets. This standard\n separates out clitics (doesn‚Äôt becomes does plus n‚Äôt), keeps hyphenated words together, and separates out all punctuation (to save space we‚Äôre showing visible spaces\n ‚Äò ‚Äô between tokens, although newlines is a more common output):\n\n Input: \"The San Francisco-based restaurant,\" they said,\n \"doesn‚Äôt charge $10\".\n Output: \" The San Francisco-based restaurant , \" they said ,\n \" does n‚Äôt charge $ 10 \" .\n\n In practice, since tokenization is run before any other language processing, it\n needs to be very fast. For rule-based word tokenization we generally use deterministic algorithms based on regular expressions compiled into efficient finite state\n automata. For example, Fig. 2.8 shows a basic regular expression that can be used\n to tokenize English with the nltk.regexp tokenize function of the Python-based\n Natural Language Toolkit (NLTK) (Bird et al. 2009; https://www.nltk.org).\n Carefully designed deterministic algorithms can deal with the ambiguities that\n arise, such as the fact that the apostrophe needs to be tokenized differently when used\n as a genitive marker (as in the book‚Äôs cover), a quotative as in ‚ÄòThe other class‚Äô, she\n said, or in clitics like they‚Äôre.\n16 C HAPTER 2 ‚Ä¢ W ORDS AND T OKENS\n\n >>> text = 'That U.S.A. poster-print costs $12.40...'\n >>> pattern = r'''(?x) # set flag to allow verbose regexps\n ... (?:[A-Z]\\.)+ # abbreviations, e.g. U.S.A.\n ... | \\w+(?:-\\w+)* # words with optional internal hyphens\n ... | \\$?\\d+(?:\\.\\d+)?%? # currency, percentages, e.g. $12.40, 82%\n ... | \\.\\.\\. # ellipsis\n ... | [][.,;\"'?():_`-] # these are separate tokens; includes ], [\n ... '''\n >>> nltk.regexp_tokenize(text, pattern)\n ['That', 'U.S.A.', 'poster-print', 'costs', '$12.40', '...']\n natural language processing toolkit (Bird et al., 2009), commented for readability; the (?x)\n verbose flag tells Python to strip comments and whitespace. Figure from Chapter 3 of Bird\n et al. (2009).\n\n 2.5.1 Sentence Segmentation\n Rule-based segmentation is commonly used for another kind of tokenization prosentence\n segmentation cess: the sentence. Sentence segmentation is a step that is can be optionally applied\n in text processing. It is especially important when applying NLP algorithms to tasks\n of detecting structure, like parse structure.\n Sentence segmentation depends on the language and the genre. The most useful\n cues for segmenting a text into sentences in English written text tend to be punctuation, like periods, question marks, and exclamation points. Question marks and\n exclamation points are relatively unambiguous markers of sentence boundaries, and\n simple rules can segment sentences when they appear.\n The period character ‚Äú.‚Äù, on the other hand, is ambiguous between a sentence\n boundary marker and a marker of abbreviations like Dr. or Inc. The previous sentence that you just read showed an even more complex case of this ambiguity, in\n which the final period of Inc. marked both an abbreviation and the sentence boundary marker. For this reason, sentence tokenization and word tokenization can be\n addressed jointly.\n Many English sentence tokenization methods work by first deciding (often based\n on deterministic rules, but sometimes via machine learning) whether a period is part\n of the word or is a sentence-boundary marker. An abbreviation dictionary can help\n determine whether the period is part of a commonly used abbreviation; the dictionaries can be hand-built or machine-learned (Kiss and Strunk, 2006), as can the final\n sentence splitter. In the Stanford CoreNLP toolkit (Manning et al., 2014), for example sentence splitting is rule-based, a deterministic consequence of tokenization; a\n sentence ends when a sentence-ending punctuation (., !, or ?) is not already grouped\n with other characters into a token (such as for an abbreviation or number), optionally\n followed by additional final quotes or brackets.\n\n Words don‚Äôt appear out of nowhere. Any particular piece of text that we study\n is produced by one or more specific speakers or writers, in a specific dialect of a\n specific language, at a specific time, in a specific place, for a specific function.\n Perhaps the most important dimension of variation is the language. NLP algo-\n2.6 ‚Ä¢ C ORPORA 17\n\n rithms are most useful when they apply across many languages. The world has 7097\n languages at the time of this writing, according to the online Ethnologue catalog\n (Simons and Fennig, 2018). It is important to test algorithms on more than one language, and particularly on languages with different properties; by contrast there is\n an unfortunate current tendency for NLP algorithms to be developed or tested just on\n English (Bender, 2019). Even when algorithms are developed beyond English, they\n tend to be developed for the official languages of large industrialized nations (Chinese, Spanish, Japanese, German etc.), but we don‚Äôt want to limit tools to just these\n few languages. Furthermore, most languages also have multiple varieties, often spoken in different regions or by different social groups. Thus, for example, if we‚Äôre\n AAE processing text that uses features of African American English (AAE) or African\n American Vernacular English (AAVE)‚Äîthe variations of English that can be used\n by millions of people in African American communities (King 2020)‚Äîwe must use\n NLP tools that function with features of those varieties. Twitter posts might use features often used by speakers of African American English, such as constructions like\n MAE iont (I don‚Äôt in Mainstream American English (MAE)), or talmbout corresponding\n to MAE talking about, both examples that influence word segmentation (Blodgett\n et al. 2016, Jones 2015).\n It‚Äôs also quite common for speakers or writers to use multiple languages in a sincode switching gle utterance, a phenomenon called code switching. Code switching is enormously\n common across the world; here are examples showing Spanish and (transliterated)\n Hindi code switching with English (Solorio et al. 2014, Jurgens et al. 2017):\n (2.12) Por primera vez veo a @username actually being hateful! it was beautiful:)\n [For the first time I get to see @username actually being hateful! it was\n beautiful:) ]\n (2.13) dost tha or ra- hega ... dont wory ... but dherya rakhe\n [‚Äúhe was and will remain a friend ... don‚Äôt worry ... but have faith‚Äù]\n Another dimension of variation is the genre. The text that our algorithms must\n process might come from newswire, fiction or non-fiction books, scientific articles,\n Wikipedia, or religious texts. It might come from spoken genres like telephone\n conversations, business meetings, police body-worn cameras, medical interviews,\n or transcripts of television shows or movies. It might come from work situations\n like doctors‚Äô notes, legal text, or parliamentary or congressional proceedings.\n Text also reflects the demographic characteristics of the writer (or speaker): their\n age, gender, race, socioeconomic class can all influence the linguistic properties of\n the text we are processing.\n And finally, time matters too. Language changes over time, and for some languages we have good corpora of texts from different historical periods.\n Because language is so situated, when developing computational models for language processing from a corpus, it‚Äôs important to consider who produced the language, in what context, for what purpose. How can a user of a dataset know all these\n datasheet details? The best way is for the corpus creator to build a datasheet (Gebru et al.,\n 2020) or data statement (Bender et al., 2021) for each corpus. A datasheet specifies\n properties of a dataset like:\n Motivation: Why was the corpus collected, by whom, and who funded it?\n Situation: When and in what situation was the text written/spoken? For example,\n was there a task? Was the language originally spoken conversation, edited\n text, social media communication, monologue vs. dialogue?\n Language variety: What language (including dialect/region) was the corpus in?\n18 C HAPTER 2 ‚Ä¢ W ORDS AND T OKENS\n\n Speaker demographics: What was, e.g., the age or gender of the text‚Äôs authors?\n Collection process: How big is the data? If it is a subsample how was it sampled?\n Was the data collected with consent? How was the data pre-processed, and\n what metadata is available?\n Annotation process: What are the annotations, what are the demographics of the\n annotators, how were they trained, how was the data annotated?\n Distribution: Are there copyright or other intellectual property restrictions?\n\n One of the most useful tools for text processing in computer science is the regular\n regular\n expression expression (or regex), a language for specifying text strings. Regexes are used in\n every computer language, in text processing tools like Unix grep, and in editors\n like vim or Emacs. And they play an important role in the pre-tokenization step\n for tokenization algorithms like BPE. Formally, a regular expression is an algebraic\n notation for characterizing a set of strings. Practically, we can use a regex to search\n for a string in a text and to specify how to change the string, both of which are key\n to tokenization.\n string We use regular expressions to search for a pattern in a string which can be a\n single line or a longer text. For example, the Python function\n re.search(pattern,string)\n scans through the string and returns the first match inside it for the pattern. In the\n following examples we generally highlight the exact string that matches the regular\n expression and show only the first match. We‚Äôll use Python syntax, expressing the\n regex as a raw string delimited by double quotes: r\"regex\". Raw strings treat\n backslashes as literal characters, which will be important since many regex patterns\n we‚Äôll introduce use backslashes.\n Regular expressions come in different variants, so using an online regex tester\n can help make sure your regex does what you think it‚Äôs doing.\n\n 2.7.1 Character Disjunction: The Square Bracket\n The simplest kind of regular expression is a sequence of simple characters. The pattern r\"Buttercup\" matches the substring Buttercup in any string (like the string\n I‚Äôm called little Buttercup). But often we need to use special characters.\n For example, we might want to match either some character or another. For example, regular expressions are generally case sensitive: r\"s\" matches a lower case s\n but not an upper case S. To match both s and S we can use the character disjunccharacter\n disjunction tion operator, the square braces [ and ]. The string of characters inside the braces\n specifies a disjunction of characters to match. For example, Fig. 2.9 shows that the\n pattern r\"[mM]\" matches patterns containing either m or M.\n\n Pattern Match String\n r\"[mM]ary\" Mary or mary ‚ÄúMary Ann stopped by Mona‚Äôs‚Äù\n r\"[abc]\" ‚Äòa‚Äô, ‚Äòb‚Äô, or ‚Äòc‚Äô ‚ÄúIn uomini, in soldati‚Äù\n r\"[1234567890]\" any one digit ‚Äúplenty of 7 to 5‚Äù\n\n The regular expression r\"[1234567890]\" specifies any single digit. This can\n 2.7 ‚Ä¢ R EGULAR E XPRESSIONS 19\n\n get awkward (imagine typing r\"[ABCDEFGHIJKLMNOPQRSTUVWXYZ]\" to mean an\n uppercase letter) so the brackets can also be used with a dash (-) to specify any one\n range character in a range. The pattern r\"[2-5]\" specifies any one of the characters 2, 3,\n 4, or 5. The pattern r\"[b-g]\" specifies one of the characters b, c, d, e, f, or g. Some\n other examples are shown in Fig. 2.10.\n\n Regex Match Example Patterns Matched\n r\"[A-Z]\" an upper case letter ‚Äúwe should call it ‚ÄòDrenched Blossoms‚Äô ‚Äù\n r\"[a-z]\" a lower case letter ‚Äúmy beans were impatient to be hoed!‚Äù\n r\"[0-9]\" a single digit ‚ÄúChapter 1: Down the Rabbit Hole‚Äù\n\n The square braces can also be used to specify what a single character cannot be,\n by use of the caret ÀÜ. If the caret ÀÜ is the first symbol after the open square brace\n [, the resulting pattern is negated. For example, the pattern r\"[ÀÜa]\" matches any\n single character (including special characters) except a. This is only true when the\n caret is the first symbol after the open square brace. If it occurs anywhere else, it\n usually stands for a caret; Fig. 2.11 shows some examples.\n\n Regex Match (single characters) Example Patterns Matched\n r\"[ÀÜA-Z]\" not an upper case letter ‚ÄúOyfn pripetchik‚Äù\n r\"[ÀÜSs]\" neither ‚ÄòS‚Äô nor ‚Äòs‚Äô ‚ÄúI have no exquisite reason for‚Äôt‚Äù\n r\"[ÀÜ.]\" not a period ‚Äúour resident Djinn‚Äù\n r\"[eÀÜ]\" either ‚Äòe‚Äô or ‚ÄòÀÜ‚Äô ‚Äúlook up ÀÜ now‚Äù\n r\"aÀÜb\" the pattern ‚ÄòaÀÜb‚Äô ‚Äúlook up aÀÜ b now‚Äù\n\n 2.7.2 Counting, Optionality, and Wildcards\n How can we talk about optional elements, like an optional s if we want to match both\n koala and koalas? We can‚Äôt use the square brackets, because while they allow us to\n say ‚Äús or S‚Äù, they don‚Äôt allow us to say ‚Äús or nothing‚Äù. For this we use the question\n mark r\"?\", which means ‚Äúthe preceding character or nothing‚Äù,. So r\"colou?r\"\n matches both color and colour, and r\"koala?\" matches koala or koalas.\n There‚Äôs another way to talk about elements that may or may not occur. Consider\n the language of certain sheep, which consists of strings that look like the following:\n baa!\n baaa!\n baaaa!\n ...\n This sheep language consists of strings with a b, followed by at least two (and\n arbitrarily more) a‚Äôs, followed by an exclamation point. To represent this language,\n we‚Äôll use a useful operator that is represented by the asterisk or *, called the Kleene\n Kleene * * (generally pronounced ‚Äúcleany star‚Äù). The Kleene star means ‚Äúzero or more occurrences of the immediately previous character or regular expression‚Äù. So r\"a*\"\n means ‚Äúany string of zero or more as‚Äù.\n Could r\"ba*\" represent the sheep language? It will correctly match ba or\n baaaaaa, but there‚Äôs a problem! It will also match b, with no a, or ba with only one\n a. That‚Äôs because Kleene star means ‚Äúzero or more occurrences‚Äù. Instead, for the\n20 C HAPTER 2 ‚Ä¢ W ORDS AND T OKENS\n\n sheep language we‚Äôll want r\"baaa*\", meaning b followed by aa followed by zero\n or more additional as. More complex patterns can also be repeated. So r\"[ab]*\"\n means ‚Äúzero or more a‚Äôs or b‚Äôs‚Äù (not ‚Äúzero or more right square braces‚Äù). This will\n match strings like aaaa or ababab or bbbb, as well as the empty string. For specifying an integer (a string of digits) we can use r\"[0-9][0-9]*\". (Why isn‚Äôt it just\n r\"[0-9]*\"?)\n There is a slightly shorter way to specify ‚Äúat least one‚Äù of some character: the\n Kleene + Kleene +, which means ‚Äúone or more occurrences of the immediately preceding\n character or regular expression‚Äù. So r\"[0-9]+\" is the normal way to specify ‚Äúa\n sequence of digits‚Äù, and we could also specify the sheep language as r\"baa+!\".\n Besides the Kleene * and Kleene + we can also use explicit numbers as counters, by enclosing them in curly brackets. The operator r\"{3}\" means ‚Äúexactly 3\n occurrences of the previous character or expression‚Äù. So r\"ax{10}z\" will match a\n followed by exactly 10 x‚Äôs followed by z.\n period An important special character is the period (r\".\"), a wildcard expression that\n matches any single character (except a newline).\n The wildcard is often used together with the Kleene star to mean ‚Äúany string\n of characters‚Äù. For example, suppose we want to find any line in which a particular word, for example, rose, appears twice. We can specify this with the regular\n expression r\"rose.*rose\", meaning two roses, with a sequence of zero or more\n characters (of any kind) between them. Fig. 2.12 summarizes.\n\n Regex Match\n * zero or more occurrences of the previous char or expression\n + one or more occurrences of the previous char or expression\n ? zero or one occurrence of the previous char or expression\n {n} exactly n occurrences of the previous char or expression\n . any single char\n .* any string of zero or more chars\n\n 2.7.3 Anchors and Boundaries\n anchors Anchors are special characters that anchor regular expressions to particular places\n in a string. The most common anchors are the caret ÀÜ and the dollar sign $. The\n caret ÀÜ matches the start of a line. The pattern r\"ÀÜThe\" matches the word The only\n at the start of a line. Thus, the caret ÀÜ has three uses: to match the start of a line,\n to indicate a negation inside of square brackets, and just to mean a caret. (What are\n the contexts that allow the system to know which function a given caret is supposed\n to have?) The dollar sign $ matches the end of a line. So the pattern $ is a useful\n pattern for matching a space at the end of a line, and r\"ÀÜThe dog\\.$\" matches a\n line that contains only the phrase The dog. with a final period.\n Note that we have to use the backslash in the prior example since we want\n the . to mean ‚Äúperiod‚Äù and not the wildcard. By contrast, the regular expression\n r\"ÀÜThe dog.$\" would match The dog. but also The dog! and The dogo. As\n we‚Äôll discuss below, all the special characters we‚Äôve defined so far (* + ? . [\n ]) need to be backslashed when we mean to use them literally.\n There are other anchors: \\b matches a word boundary, and \\B matches a non\n word-boundary. Thus, r\"\\bthe\\b\" matches the word the but not the word other.\n A ‚Äúword‚Äù for the purposes of a regex is defined (based on words in programming\n 2.7 ‚Ä¢ R EGULAR E XPRESSIONS 21\n\n Regex Match\n ÀÜ start of line\n $ end of line\n \\b word boundary\n \\B non-word boundary\n\n languages) as a sequence of digits, underscores, or letters. Thus r\"\\b99\\b\" will\n match the string 99 in There are 99 bottles of beer on the wall (because\n 99 follows a space) but not 99 in There are 299 bottles of beer on the\n wall (since 99 follows a number). But it will match 99 in $99 (since 99 follows a\n dollar sign ($), which is not a digit, underscore, or letter).\n Note that all these anchors and boundary operators technically match the empty\n string, meaning that they don‚Äôt eat up any characters of the string. The carat in the\n pattern r\"ÀÜThe\" matches the start of \"The\" but doesn‚Äôt actually advance over the\n first character T. And the pattern r\"the\\b the matches the the; the \\b is aware\n of the fact that the space is a boundary, but it matches the empty string right before\n the space, not the space, so that the space character is available to be matched.\n\n 2.7.4 Disjunction, Grouping, and Precedence\n Suppose we need to search for texts about pets; perhaps we are particularly interested\n in cats and dogs. In such a case, we might want to search for either the string\n cat or the string dog. Since we can‚Äôt use the square brackets to search for ‚Äúcat or\n dog‚Äù (why wouldn‚Äôt r\"[catdog]\" do the right thing?), we need a new operator,\ndisjunction the disjunction operator, also called the pipe symbol |. The pattern r\"cat|dog\"\n matches either the string cat or the string dog.\n Sometimes we need to use this disjunction operator in the midst of a larger sequence. For example, suppose I want to search for mentions of pet fish. How can\n I specify both guppy and guppies? We cannot simply say r\"guppy|ies\", because\n that would match only the strings guppy and ies. This is because sequences like\nprecedence guppy take precedence over the disjunction operator |. To make the disjunction\n operator apply only to a specific pattern, we need to use the parenthesis operators (\n and ). Enclosing a pattern in parentheses makes it act like a single character for the\n purposes of neighboring operators like the pipe | and the Kleene*. So the pattern\n r\"gupp(y|ies)\" would specify that we meant the disjunction only to apply to the\n suffixes y and ies.\n The parenthesis operator ( is also useful when we are using counters like the\n Kleene*. Unlike the | operator, the Kleene* operator applies by default only to\n a single character, not to a whole sequence. Suppose we want to match repeated\n instances of a string. Perhaps we have a line that has column labels of the form\n Column 1 Column 2 Column 3. The expression r\"Column [0-9]+ *\" will not\n match any number of columns; instead, it will match a single column followed by\n any number of spaces! The star here applies only to the space that precedes it,\n not to the whole sequence. With the parentheses, we could write the expression\n r\"(Column [0-9]+ +)*\" to match the word Column, followed by a number and\n optional spaces, the whole pattern repeated zero or more times.\n This idea that one operator may take precedence over another, requiring us to\n sometimes use parentheses to specify what we mean, is formalized by the operator\n operator\nprecedence precedence hierarchy for regular expressions. The following table gives the order\n of operator precedence, from highest precedence to lowest precedence.\n22 C HAPTER 2 ‚Ä¢ W ORDS AND T OKENS\n\n Parenthesis ()\n Counters * + ? {}\n Sequences and anchors the ÀÜmy end$\n Disjunction |\n Thus, because counters have a higher precedence than sequences,\n r\"the*\" matches theeeee but not thethe. Because sequences have a higher precedence than disjunction, r\"the|any\" matches the or any but not thany or theny.\n Patterns can be ambiguous in another way. Consider the expression r\"[a-z]*\"\n when matching against the text once upon a time. Since r\"[a-z]*r\" matches zero\n or more letters, this expression could match nothing, or just the first letter o, on, onc,\n or once. In these cases regular expressions always match the largest string they can;\n greedy we say that patterns are greedy, expanding to cover as much of a string as they can.\n non-greedy There are, however, ways to enforce non-greedy matching, using another mean-\n *? ing of the ? qualifier. The operator *? is a Kleene star that matches as little text as\n +? possible. The operator +? is a Kleene plus that matches as little text as possible.\n\n 2.7.5 A Simple Example\n Suppose we wanted to write a regex to find cases of the English article the. A simple\n (but incorrect) pattern might be:\n r\"the\" (2.14)\n One problem is that this pattern will miss the word when it begins a sentence and\n hence is capitalized (i.e., The). This might lead us to the following pattern:\n r\"[tT]he\" (2.15)\n But we will still overgeneralize, incorrectly return texts with the embedded in other\n words (e.g., other or there). So we need to specify that we want instances with a\n word boundary on both sides:\n r\"\\b[tT]he\\b\" (2.16)\n The simple process we just went through was based on fixing two kinds of errors:\n false positives false positives, strings that we incorrectly matched like other or there, and false\n false negatives negatives, strings that we incorrectly missed, like The. Addressing these two kinds\n of errors comes up again and again in language processing. Reducing the overall\n error rate for an application thus involves two antagonistic efforts:\n ‚Ä¢ Increasing precision (minimizing false positives)\n ‚Ä¢ Increasing recall (minimizing false negatives)\n We‚Äôll come back to precision and recall with more precise definitions in Chapter 4.\n\n 2.7.6 More Operators\n Finally, certain special characters are referred to by special notation based on the\n newline backslash (\\) (see Fig. 2.15). The most common of these are the newline character\n \\n and the tab character \\t.\n How do we refer to characters that are special themselves (like ., *, -, [, and\n \\) when we mean them literally, not in their special usage? That is, if we are trying\n to match a period, or a star, or a bracket or paren? To get the literal meaning of a\n special character, we need to precede them with a backslash, (i.e., r\"\\.\", r\"\\*\",\n r\"\\[\", and r\"\\\\\").\n 2.7 ‚Ä¢ R EGULAR E XPRESSIONS 23\n\n Regex Expansion Match First Matches\n \\d [0-9] any digit Party of 5\n \\D [ÀÜ0-9] any non-digit Blue moon\n \\w [a-zA-Z0-9_] any alphanumeric/underscore Daiyu\n \\W [ÀÜ\\w] a non-alphanumeric !!!!\n \\s [ \\r\\t\\n\\f] whitespace (space, tab) in Concord\n \\S [ÀÜ\\s] Non-whitespace in Concord\n\n Regex Match First Patterns Matched\n \\* an asterisk ‚Äú*‚Äù ‚ÄúK*A*P*L*A*N‚Äù\n \\. a period ‚Äú.‚Äù ‚ÄúDr. Livingston, I presume‚Äù\n \\? a question mark ‚ÄúWhy don‚Äôt they come and lend a hand?‚Äù\n \\n a newline\n \\t a tab\n\n 2.7.7 Substitutions and Capture Groups\n substitution An important use of regular expressions is in substitutions, where we want to replace one string with another. Regular expression can help us specify the string to\n be replaced as well as the replacement. In Python we use the function re.sub()\n (similar functions exist in other languages and environments).\n re.sub(pattern, repl, string) takes three arguments: a pattern to search for, a\n replacement to replace it with, and a string in which to do the search and replacing\n We could for example change every instance of cherry to apricot in string:\n re.sub(r\"cherry\", r\"apricot\", string)\n Or we could convert to upper case all the instances of a particular name:\n re.sub(r\"janet\", r\"Janet\", string)\n More often, however, the substitution depends in a more complex way on the\n string that matched the pattern. For example, suppose we have a document in\n which all the dates are in US format (mm/dd/yyyy) and we want to change them\n into the format used in the EU and many other regions: (dd-mm-yyyy). The pattern r\"\\d{2}/\\d{2}/\\d{4}\" will match a date. But how do we specify in the\n replacement that we want to swap the date and month values?\ncapture group The tool in regular expression for this is the capture group. A capture group\n uses parentheses to capture (store) the values that we matched in the search, so we\n can reuse them in the replacement. We put a set of parentheses around the part of\n the pattern we want to capture, and it will get stored in a numbered group (groups\n are numbered from left to right). Then in the repl, we refer back to that group with\n a number command.\n Consider the following expression:\n re.sub(r\"(\\d{2})/(\\d{2})/(\\d{4})\", r\"\\2-\\1-\\3\", string)}\n We‚Äôve put parentheses ( and ) around the two month digits, the two day digits,\n and the four year digits, thus storing the first 2 digits in group 1, the second 2 digits\n in group 2, and the final digits in group 3. Then in the repl string, we use number\n operators \\1, \\2, and \\3, to refer back to the first, second, and third registers. The\n result would take a string like\n The date is 10/15/2011\n24 C HAPTER 2 ‚Ä¢ W ORDS AND T OKENS\n\n and convert it to\n The date is 15-10-2011\n Capture groups can be useful even if we are not doing substitutions. For example\n we can use them to find repetitions, something we often need in text processing. For\n example, to find a repeated word in a string, we can use this pattern which searches\n for a word, captures it in a group, and then refers back to it after whitespace:\n r\"\\b([A-Za-z]+)\\s+\\1\\b\"\n Parentheses thus have a double function in regular expressions; they are used to\n group terms for specifying the order in which operators should apply, and they are\n used to capture the match. Occasionally we need parentheses for grouping, but don‚Äôt\n non-capturing\n group want to capture the resulting pattern. In that case we use a non-capturing group,\n which is specified by putting the special commands ?: after the open parenthesis,\n in the form (?: pattern ). Non-capture groups are usually used when we are\n trying to capture only part of a long or complex pattern. Perhaps we are matching\n a sequence of dates (\\d\\d/\\d\\d/\\d\\d\\d\\d) separated by spaces and we want to\n extract only the 15th one. We need to use parenthesis in order to use the counting\n operator on the first 14, but we don‚Äôt want to store all the useless information. The\n following pattern only stores the 15th date in group 1:\n\n r\"(?:\\d\\d/\\d\\d/\\d\\d\\d\\d\\s+){14}(\\d\\d/\\d\\d/\\d\\d\\d\\d)\" (2.17)\n\n Substitutions and capture groups are also useful for implementing historically\n important chatbots like ELIZA (Weizenbaum, 1966). Recall that ELIZA simulates\n a Rogerian psychologist by carrying on conversations like the following:\n User2 : They‚Äôre always bugging us about something or other.\n ELIZA2 : CAN YOU THINK OF A SPECIFIC EXAMPLE\n User3 : Well, my boyfriend made me come here.\n ELIZA3 : YOUR BOYFRIEND MADE YOU COME HERE\n User4 : He says I‚Äôm depressed much of the time.\n ELIZA4 : I AM SORRY TO HEAR YOU ARE DEPRESSED\n\n ELIZA works by having a series or cascade of regex substitutions each of which\n matches and changes some part of the input lines. After the input is uppercased,\n substitutions change all instances of MY to YOUR, and I‚ÄôM to YOU ARE, and so on.\n That way when ELIZA repeats back part of the user utterance, it will seem to be\n referring correctly to the user. The next set of substitutions matches and replaces\n other patterns in the input, turning the input into a complete response. Here are\n some examples:\nre.sub(r\".* YOU ARE (DEPRESSED|SAD) .*\",r\"I AM SORRY TO HEAR YOU ARE \\1\",input)\nre.sub(r\".* YOU ARE (DEPRESSED|SAD) .*\",r\"WHY DO YOU THINK YOU ARE \\1\",input)\nre.sub(r\".* ALWAYS .*\",r\"CAN YOU THINK OF A SPECIFIC EXAMPLE\",input)\n\n 2.7.8 Lookahead Assertions\n Finally, there will be times when we need to predict the future: look ahead in the\n text to see if some pattern matches, but not yet advance the pointer we always keep\n to where we are in the text, so that we can then deal with the pattern if it occurs, but\n if it doesn‚Äôt we can check for something else instead.\n 2.8 ‚Ä¢ S IMPLE U NIX T OOLS FOR W ORD T OKENIZATION 25\n\n lookahead These lookahead assertions make use of the (? syntax that we saw in the previous section for non-capture groups. The operator (?= pattern) is true if pattern\n zero-width occurs, but is zero-width, i.e. the match pointer doesn‚Äôt advance, just as we saw\n with anchors and boundary markers like \\b. The operator (?! pattern) only returns true if a pattern does not match, but again is zero-width and doesn‚Äôt advance\n the pointer. Negative lookahead is commonly used when we are parsing some complex pattern but want to rule out a special case. For example suppose we want to\n capture the first word on the line, but only if it doesn‚Äôt start with the letter T. We can\n use negative lookahead to do this:\n\n r\"ÀÜ(?![tT])(\\w+)\\b\" (2.18)\n\n The first negative lookahead says that the line must not start with a t or T, but\n matches the empty string, not moving the match pointer. Then the capture group\n captures the first word.\n\n For English it is possible to do simple naive word tokenization and frequency computation in a single Unix command-line. As Church (1994) points out, this can be\n useful when we need quick information about a text corpus. We‚Äôll make use of some\n Unix commands: tr, used to systematically change particular characters in the input; sort, which sorts input lines in alphabetical order; and uniq, which collapses\n and counts adjacent identical lines.\n For example let‚Äôs begin with the ‚Äòcomplete words‚Äô of Shakespeare in one file,\n sh.txt. We can use tr to tokenize the words by changing every sequence of nonalphabetic characters to a newline (‚ÄôA-Za-z‚Äô means alphabetic and the -c option\n complements to non-alphabet, so together they mean to change every non-alphabetic\n character into a newline. The -s (‚Äòsqueeze‚Äô) option is used to replace the result\n of multiple consecutive changes into a single output, so a series of non-alphabetic\n characters in a row would all be ‚Äòsqueezed‚Äô into a single newline):\n tr -sc 'A-Za-z' '\\n' < sh.txt\n The output of this command will be:\n THE\n SONNETS\n by\n William\n Shakespeare\n From\n fairest\n creatures\n ...\n Now that there is one word per line, we can sort the lines, and pass them to uniq\n -c which will collapse and count them:\n tr -sc ‚ÄôA-Za-z‚Äô ‚Äô\\n‚Äô < sh.txt | sort | uniq -c\n with the following output:\n26 C HAPTER 2 ‚Ä¢ W ORDS AND T OKENS\n\n 1945 A\n 72 AARON\n 19 ABBESS\n 25 Aaron\n 6 Abate\n 1 Abates\n ...\n Alternatively, we can collapse all the upper case to lower case:\n tr -sc 'A-Za-z' '\\n' < sh.txt | tr A-Z a-z | sort | uniq -c\n whose output is\n 14725 a\n 97 aaron\n 1 abaissiez\n 10 abandon\n 2 abandoned\n 2 abase\n 1 abash\n 14 abate\n ...\n Now we can sort again to find the frequent words. The -n option to sort means\n to sort numerically rather than alphabetically, and the -r option means to sort in\n reverse order (highest-to-lowest):\n tr -sc 'A-Za-z' '\\n' < sh.txt | tr A-Z a-z | sort | uniq -c | sort -n -r\n The results show that the most frequent words in Shakespeare, as in any other\n corpus, are the short function words like articles, pronouns, prepositions:\n 27378 the\n 26084 and\n 22538 i\n 19771 to\n 17481 of\n 14725 a\n 13826 you\n ...\n Unix tools of this sort can be very handy in building quick word count statistics\n for any corpus in English. For anything more complex, we generally turn to the\n more sophisticated tokenization algorithms we‚Äôve discussed above.\n\n We often need a way to compare how similar two words or strings are. As we‚Äôll\n see in later chapters, this comes up most commonly in tasks like automatic speech\n recognition or machine translation, where we want to know how similar the sequence\n of words is to some reference sequence of words.\n Edit distance gives us a way to quantify these intuitions about string similarity.\n minimum edit More formally, the minimum edit distance between two strings is defined as the\n distance\n 2.9 ‚Ä¢ M INIMUM E DIT D ISTANCE 27\n\n minimum number of editing operations (operations like insertion, deletion, substitution) needed to transform one string into another. In this section we‚Äôll introduce edit\n distance for single words, but the algorithm applies equally to entire strings.\n The gap between intention and execution, for example, is 5 (delete an i, substitute e for n, substitute x for t, insert c, substitute u for n). It‚Äôs much easier to see\n alignment this by looking at the most important visualization for string distances, an alignment\n between the two strings, shown in Fig. 2.16. Given two sequences, an alignment is\n a correspondence between substrings of the two sequences. Thus, we say I aligns\n with the empty string, N with E, and so on. Beneath the aligned strings is another\n representation; a series of symbols expressing an operation list for converting the\n top string into the bottom string: d for deletion, s for substitution, i for insertion.\n\n INTE*NTION\n *EXECUTION\n d s s i s\n\n The final row gives the operation list for converting the top string into the bottom string: d for\n deletion, s for substitution, i for insertion.\n\n We can also assign a particular cost or weight to each of these operations. The\n Levenshtein distance between two sequences is the simplest weighting factor in\n which each of the three operations has a cost of 1 (Levenshtein, 1966)‚Äîwe assume\n that the substitution of a letter for itself, for example, t for t, has zero cost. The Levenshtein distance between intention and execution is 5. Levenshtein also proposed\n an alternative version of his metric in which each insertion or deletion has a cost of\n 1 and substitutions are not allowed. (This is equivalent to allowing substitution, but\n giving each substitution a cost of 2 since any substitution can be represented by one\n insertion and one deletion). Using this version, the Levenshtein distance between\n intention and execution is 8.\n\n 2.9.1 The Minimum Edit Distance Algorithm\n How do we find the minimum edit distance? We can think of this as a search task, in\n which we are searching for the shortest path‚Äîa sequence of edits‚Äîfrom one string\n to another.\n\n i n t e n t i o n\n\n del ins subst\n\n n t e n t i o n i n t e c n t i o n i n x e n t i o n\n\n The space of all possible edits is enormous, so we can‚Äôt search naively. However,\n lots of distinct edit paths will end up in the same state (string), so rather than recomputing all those paths, we could just remember the shortest path to a state each time\n dynamic\nprogramming we saw it. We can do this by using dynamic programming. Dynamic programming\n is the name for a class of algorithms, first introduced by Bellman (1957), that apply\n28 C HAPTER 2 ‚Ä¢ W ORDS AND T OKENS\n\n a table-driven method to solve problems by combining solutions to subproblems.\n Some of the most commonly used algorithms in natural language processing make\n use of dynamic programming, such as the Viterbi algorithm (Chapter 17) and the\n CKY algorithm for parsing (Chapter 18).\n The intuition of a dynamic programming problem is that a large problem can\n be solved by properly combining the solutions to various subproblems. Consider\n the shortest path of transformed words that represents the minimum edit distance\n between the strings intention and execution shown in Fig. 2.18.\n\n i n t e n t i o n\n delete i\n n t e n t i o n\n substitute n by e\n e t e n t i o n\n substitute t by x\n e x e n t i o n\n insert u\n e x e n u t i o n\n substitute n by c\n e x e c u t i o n\n\n Imagine some string (perhaps it is exention) that is in this optimal path (whatever\n it is). The intuition of dynamic programming is that if exention is in the optimal\n operation list, then the optimal sequence must also include the optimal path from\n intention to exention. Why? If there were a shorter path from intention to exention,\n then we could use it instead, resulting in a shorter overall path, and the optimal\n minimum edit\n sequence wouldn‚Äôt be optimal, thus leading to a contradiction.\n distance The minimum edit distance algorithm was named by Wagner and Fischer\n algorithm\n (1974) but independently discovered by many people (see the Historical Notes section of Chapter 17).\n Let‚Äôs first define the minimum edit distance between two strings. Given two\n strings, the source string X of length n, and target string Y of length m, we‚Äôll define\n D[i, j] as the edit distance between X[1..i] and Y [1.. j], i.e., the first i characters of X\n and the first j characters of Y . The edit distance between X and Y is thus D[n, m].\n We‚Äôll use dynamic programming to compute D[n, m] bottom up, combining solutions to subproblems. In the base case, with a source substring of length i but an\n empty target string, going from i characters to 0 requires i deletes. With a target\n substring of length j but an empty source going from 0 characters to j characters\n requires j inserts. Having computed D[i, j] for small i, j we then compute larger\n D[i, j] based on previously computed smaller values. The value of D[i, j] is computed by taking the minimum of the three possible paths through the matrix which\n arrive there:\n Ô£±\n Ô£≤ D[i ‚àí 1, j] + del-cost(source[i])\n D[i, j] = min D[i, j ‚àí 1] + ins-cost(target[ j]) (2.19)\n Ô£≥\n D[i ‚àí 1, j ‚àí 1] + sub-cost(source[i], target[ j])\n\n We mentioned above two versions of Levenshtein distance, one in which substitutions cost 1 and one in which substitutions cost 2 (i.e., are equivalent to an insertion\n plus a deletion). Let‚Äôs here use that second version of Levenshtein distance in which\n the insertions and deletions each have a cost of 1 (ins-cost(¬∑) = del-cost(¬∑) = 1), and\n substitutions have a cost of 2 (except substitution of identical letters has zero cost).\n 2.9 ‚Ä¢ M INIMUM E DIT D ISTANCE 29\n\nUnder this version of Levenshtein, the computation for D[i, j] becomes:\n Ô£±\n Ô£¥\n Ô£¥ D[i ‚àí 1, j] + 1\n Ô£≤\n D[i, j ‚àí 1] + 1 \u001a\n D[i, j] = min (2.20)\n Ô£¥\n Ô£¥ 2; if source[i] 6= target[ j]\n Ô£≥ D[i ‚àí 1, j ‚àí 1] +\n 0; if source[i] = target[ j]\nThe algorithm is summarized in Fig. 2.19; Fig. 2.20 shows the results of applying\nthe algorithm to the distance between intention and execution with the version of\nLevenshtein in Eq. 2.20.\n\n function M IN -E DIT-D ISTANCE(source, target) returns min-distance\n\n n ‚Üê L ENGTH(source)\n m ‚Üê L ENGTH(target)\n Create a distance matrix D[n+1,m+1]\n\n # Initialization: the zeroth row and column is the distance from the empty string\n D[0,0] = 0\n for each row i from 1 to n do\n D[i,0] ‚Üê D[i-1,0] + del-cost(source[i])\n for each column j from 1 to m do\n D[0,j] ‚Üê D[0, j-1] + ins-cost(target[j])\n\n # Recurrence relation:\n for each row i from 1 to n do\n for each column j from 1 to m do\n D[i, j] ‚Üê M IN( D[i‚àí1, j] + del-cost(source[i]),\n D[i‚àí1, j‚àí1] + sub-cost(source[i], target[j]),\n D[i, j‚àí1] + ins-cost(target[j]))\n # Termination\n return D[n,m]\n\nprogramming algorithms. The various costs can either be fixed (e.g., ‚àÄx, ins-cost(x) = 1)\nor can be specific to the letter (to model the fact that some letters are more likely to be inserted than others). We assume that there is no cost for substituting a letter for itself (i.e.,\nsub-cost(x, x) = 0).\n\nAlignment Knowing the minimum edit distance is useful for algorithms like finding potential spelling error corrections. But the edit distance algorithm is important\nin another way; with a small change, it can also provide the minimum cost alignment between two strings. Aligning two strings is useful throughout speech and\nlanguage processing. In speech recognition, minimum edit distance alignment is\nused to compute the word error rate (Chapter 15). Alignment plays a role in machine translation, in which sentences in a parallel corpus (a corpus with a text in two\nlanguages) need to be matched to each other.\n To extend the edit distance algorithm to produce an alignment, we can start by\nvisualizing an alignment as a path through the edit distance matrix. Figure 2.21\nshows this path with boldfaced cells. Each boldfaced cell represents an alignment\nof a pair of letters in the two strings. If two boldfaced cells occur in the same row,\nthere will be an insertion in going from the source to the target; two boldfaced cells\nin the same column indicate a deletion.\n30 C HAPTER 2 ‚Ä¢ W ORDS AND T OKENS\n\n Src\\Tar # e x e c u t i o n\n # 0 1 2 3 4 5 6 7 8 9\n i 1 2 3 4 5 6 7 6 7 8\n n 2 3 4 5 6 7 8 7 8 7\n t 3 4 5 6 7 8 7 8 9 8\n e 4 3 4 5 6 7 8 9 10 9\n n 5 4 5 6 7 8 9 10 11 10\n t 6 5 6 7 8 9 8 9 10 11\n i 7 6 7 8 9 10 9 8 9 10\n o 8 7 8 9 10 11 10 9 8 9\n n 9 8 9 10 11 12 11 10 9 8\n the algorithm of Fig. 2.19, using Levenshtein distance with cost of 1 for insertions or deletions, 2 for substitutions.\n\n computation proceeds in two steps. In the first step, we augment the minimum edit\n distance algorithm to store backpointers in each cell. The backpointer from a cell\n points to the previous cell (or cells) that we came from in entering the current cell.\n We‚Äôve shown a schematic of these backpointers in Fig. 2.21. Some cells have multiple backpointers because the minimum extension could have come from multiple\n backtrace previous cells. In the second step, we perform a backtrace. In a backtrace, we start\n from the last cell (at the final row and column), and follow the pointers back through\n the dynamic programming matrix. Each complete path between the final cell and the\n initial cell is a minimum distance alignment. Exercise 2.7 asks you to modify the\n minimum edit distance algorithm to store the pointers and compute the backtrace to\n output an alignment.\n\n # e x e c u t i o n\n # 0 ‚Üê1 ‚Üê2 ‚Üê3 ‚Üê4 ‚Üê5 ‚Üê6 ‚Üê7 ‚Üê8 ‚Üê9\n i ‚Üë 1 -‚Üê‚Üë 2 -‚Üê‚Üë 3 -‚Üê‚Üë 4 -‚Üê‚Üë 5 -‚Üê‚Üë 6 -‚Üê‚Üë 7 -6 ‚Üê7 ‚Üê8\n n ‚Üë 2 -‚Üê‚Üë 3 -‚Üê‚Üë 4 -‚Üê‚Üë 5 -‚Üê‚Üë 6 -‚Üê‚Üë 7 -‚Üê‚Üë 8 ‚Üë7 -‚Üê‚Üë 8 - 7\n t ‚Üë 3 -‚Üê‚Üë 4 -‚Üê‚Üë 5 -‚Üê‚Üë 6 -‚Üê‚Üë 7 -‚Üê‚Üë 8 -7 ‚Üê‚Üë 8 -‚Üê‚Üë 9 ‚Üë8\n e ‚Üë4 -3 ‚Üê4 -‚Üê 5 ‚Üê6 ‚Üê7 ‚Üê‚Üë 8 -‚Üê‚Üë 9 -‚Üê‚Üë 10 ‚Üë9\n n ‚Üë5 ‚Üë 4 -‚Üê‚Üë 5 -‚Üê‚Üë 6 -‚Üê‚Üë 7 -‚Üê‚Üë 8 -‚Üê‚Üë 9 -‚Üê‚Üë 10 -‚Üê‚Üë 11 -‚Üë 10\n t ‚Üë6 ‚Üë 5 -‚Üê‚Üë 6 -‚Üê‚Üë 7 -‚Üê‚Üë 8 -‚Üê‚Üë 9 -8 ‚Üê9 ‚Üê 10 ‚Üê‚Üë 11\n i ‚Üë7 ‚Üë 6 -‚Üê‚Üë 7 -‚Üê‚Üë 8 -‚Üê‚Üë 9 -‚Üê‚Üë 10 ‚Üë9 -8 ‚Üê 9 ‚Üê 10\n o ‚Üë8 ‚Üë 7 -‚Üê‚Üë 8 -‚Üê‚Üë 9 -‚Üê‚Üë 10 -‚Üê‚Üë 11 ‚Üë 10 ‚Üë9 -8 ‚Üê9\n n ‚Üë9 ‚Üë 8 -‚Üê‚Üë 9 -‚Üê‚Üë 10 -‚Üê‚Üë 11 -‚Üê‚Üë 12 ‚Üë 11 ‚Üë 10 ‚Üë9 -8\n cells we came from with up to three arrows. After the table is full we compute an alignment\n (minimum edit path) by using a backtrace, starting at the 8 in the lower-right corner and\n following the arrows back. The sequence of bold cells represents one possible minimum\n cost alignment between the two strings, again using Levenshtein distance with cost of 1 for\n insertions or deletions, 2 for substitutions. Diagram design after Gusfield (1997).\n\n While we worked our example with simple Levenshtein distance, the algorithm\n in Fig. 2.19 allows arbitrary weights on the operations. For spelling correction, for\n example, substitutions are more likely to happen between letters that are next to\n each other on the keyboard. The Viterbi algorithm is a probabilistic extension of\n minimum edit distance. Instead of computing the ‚Äúminimum edit distance‚Äù between\n 2.10 ‚Ä¢ S UMMARY 31\n\n two strings, Viterbi computes the ‚Äúmaximum probability alignment‚Äù of one string\n with another. We‚Äôll discuss this more in Chapter 17.\n\n This chapter introduced the fundamental concepts of tokens and tokenization in language processing. We discussed the linguistic levels of words, morphemes, and\n characters, introduced Unicode code points and the UTF-8 encoding, introduced\n the BPE algorithm for tokenization, and introduced the regular expression and the\n minimum edit distance algorithm for comparing strings. Here‚Äôs a summary of the\n main points we covered about these ideas:\n ‚Ä¢ Words and morphemes are useful units of representation, but difficult to define\n formally.\n ‚Ä¢ Unicode is a system for representing characters in the many scripts used to\n write the languages of the world.\n ‚Ä¢ Each character is represented internally with a unique id called a code point,\n and can be encoded in a file via encoding methods like UTF-8, which is a\n variable-length encoding.\n ‚Ä¢ Byte-Pair Encoding or BPE is the standard way to induce tokens in a datadriven way. It is the first step in most large language models.\n ‚Ä¢ BPE tokens are often roughly word or morpheme-sized, although they can be\n as small as single characters.\n ‚Ä¢ The regular expression language is a powerful tool for pattern-matching.\n ‚Ä¢ Basic operations in regular expressions include disjunction of symbols ([],\n |), counters (*, +, and {n,m}), anchors (ÀÜ, $), capture groups ((,)), and\n substitutions.\n ‚Ä¢ The minimum edit distance between two strings is the minimum number of\n operations it takes to edit one into the other. Minimum edit distance can be\n computed by dynamic programming, which also results in an alignment of\n the two strings.\n\nHistorical Notes\n For more on Herdan‚Äôs law and Heaps‚Äô Law, see Herdan (1960, p. 28), Heaps (1978),\n Egghe (2007) and Baayen (2001);\n Unicode drew on ASCII and ISO character encoding standards. Early drafts\n were worked out in discussions between engineers from Xerox and Apple. An early\n draft standard was published in 1988, with a more formal release of the Unicode\n Stanford in 1991. What became UTF-8 began with ISO drafts in 1989, with various\n extensions. The self-synchronizing aspects were famously outlined on a placemat in\n a New Jersey dinner in 1992 by Ken Thompson.\n Word tokenization and other text normalization algorithms have been applied\n since the beginning of the field. This include stemming, like the widely used stemmer of Lovins (1968), and applications to the digital humanities like those of by\n Packard (1973), who built an affix-stripping morphological parser for Ancient Greek.\n BPE, originally a text compression method proposed by Gage (1994), was applied\n32 C HAPTER 2 ‚Ä¢ W ORDS AND T OKENS\n\n to subword tokenization in the context of early neural machine translation by Sennrich et al. (2016). It was then taken up in OpenAI‚Äôs GPT-2 (Radford et al., 2019)\n as the default tokenization method, and also included in the open-source Sentence-\nPiece library (Kudo and Richardson, 2018b). There is a nice a public implementation, minbpe, https://github.com/karpathy/minbpe, by Andrej Karpathy,\n who also has a popular lecture introducing BPE (https://www.youtube.com/\n watch?v=zduSFxRajkE).\n Kleene 1951; 1956 first defined regular expressions and the finite automaton,\n based on the McCulloch-Pitts neuron. Ken Thompson was one of the first to build\n regular expressions compilers into editors for text searching (Thompson, 1968). His\n editor ed included a command ‚Äúg/regular expression/p‚Äù, or Global Regular Expression Print, which later became the Unix grep utility.\n NLTK is an essential tool that offers both useful Python libraries (https://\n www.nltk.org) and textbook descriptions (Bird et al., 2009) of many algorithms\n including text normalization and corpus interfaces.\n For more on edit distance, see Gusfield (1997). Our example measuring the edit\n distance from ‚Äòintention‚Äô to ‚Äòexecution‚Äô was adapted from Kruskal (1983). There are\n various publicly available packages to compute edit distance, including Unix diff\n and the NIST sclite program (NIST, 2005).\n In his autobiography Bellman (1984) explains how he originally came up with\n the term dynamic programming:\n ‚Äú...The 1950s were not good years for mathematical research. [the]\n Secretary of Defense ...had a pathological fear and hatred of the word,\n research... I decided therefore to use the word, ‚Äúprogramming‚Äù. I\n wanted to get across the idea that this was dynamic, this was multistage... I thought, let‚Äôs ... take a word that has an absolutely precise\n meaning, namely dynamic... it‚Äôs impossible to use the word, dynamic,\n in a pejorative sense. Try thinking of some combination that will possibly give it a pejorative meaning. It‚Äôs impossible. Thus, I thought\n dynamic programming was a good name. It was something not even a\n Congressman could object to.‚Äù\n\nExercises\n 1. the set of all alphabetic strings;\n 2. the set of all lower case alphabetic strings ending in a b;\n 3. the set of all strings from the alphabet a, b such that each a is immediately preceded by and immediately followed by a b;\n an alphabetic string separated from other words by whitespace, any relevant\n punctuation, line breaks, and so forth.\n 1. the set of all strings with two consecutive repeated words (e.g., ‚ÄúHumbert Humbert‚Äù and ‚Äúthe the‚Äù but not ‚Äúthe bug‚Äù or ‚Äúthe big bug‚Äù);\n 2. all strings that start at the beginning of the line with an integer and that\n end at the end of the line with a word;\n E XERCISES 33\n\n 3. all strings that have both the word grotto and the word raven in them\n (but not, e.g., words like grottos that merely contain the word grotto);\n 4. write a pattern that places the first word of an English sentence in a\n register. Deal with punctuation.\n on page 24. You might want to choose a different domain than a Rogerian psychologist, although keep in mind that you would need a domain in which your\n program can legitimately engage in a lot of simple repetition.\n cost 1) of ‚Äúleda‚Äù to ‚Äúdeal‚Äù. Show your work (using the edit distance grid).\n tance is to each. You may use any version of distance that you like.\n results to check your code.\n will need to store pointers and add a stage to compute the backtrace.\n34 Chapter 2 ‚Ä¢ Words and Tokens\n\nAhia, O., S. Kumar, H. Gonen, J. Kasai, D. Mortensen, Kiss, T. and J. Strunk. 2006. Unsupervised multilingual\n N. Smith, and Y. Tsvetkov. 2023. Do all languages cost sentence boundary detection. Computational Linguistics,\n the same? tokenization in the era of commercial language 32(4):485‚Äì525.\n models. EMNLP. Kleene, S. C. 1951. Representation of events in nerve nets\nArkadiev, P. M. 2020. Morphology in typology: Historical and finite automata. Technical Report RM-704, RAND\n retrospect, state of the art, and prospects. Oxford. Corporation. RAND Research Memorandum.\nBaayen, R. H. 2001. Word frequency distributions. Springer. Kleene, S. C. 1956. Representation of events in nerve nets\nBellman, R. 1957. Dynamic Programming. Princeton Uni- and finite automata. In C. Shannon and J. McCarthy, eds,\n versity Press. Automata Studies, 3‚Äì41. Princeton University Press.\nBellman, R. 1984. Eye of the Hurricane: an autobiography. Kruskal, J. B. 1983. An overview of sequence comparison.\n World Scientific Singapore. In D. Sankoff and J. B. Kruskal, eds, Time Warps, String\n Edits, and Macromolecules: The Theory and Practice of\nBender, E. M. 2019. The #BenderRule: On naming the lan- Sequence Comparison, 1‚Äì44. Addison-Wesley.\n guages we study and why it matters. Blog post.\n Kudo, T. 2018. Subword regularization: Improving neural\nBender, E. M., B. Friedman, and A. McMillan-Major. 2021. network translation models with multiple subword candi-\nA guide for writing data statements for natural lan- dates. ACL.\n guage processing. http://techpolicylab.uw.edu/\n data-statements/. Kudo, T. and J. Richardson. 2018a. SentencePiece: A simple\n and language independent subword tokenizer and detok-\nBird, S., E. Klein, and E. Loper. 2009. Natural Language enizer for neural text processing. EMNLP.\n Processing with Python. O‚ÄôReilly.\n Kudo, T. and J. Richardson. 2018b. SentencePiece: A simple\nBlodgett, S. L., L. Green, and B. O‚ÄôConnor. 2016. Demo- and language independent subword tokenizer and detokgraphic dialectal variation in social media: A case study enizer for neural text processing. EMNLP.\n of African-American English. EMNLP.\n Kurebito, M. 2017. Koryak. In M. Fortescue, M. Mithun,\nBostrom, K. and G. Durrett. 2020. Byte pair encoding is and N. Evans, eds, Oxford Handbook of Polysynthesis.\n suboptimal for language model pretraining. EMNLP. Oxford.\nChen, X., Z. Shi, X. Qiu, and X. Huang. 2017. Adversar- Levenshtein, V. I. 1966. Binary codes capable of correctial multi-criteria learning for Chinese word segmentation. ing deletions, insertions, and reversals. Cybernetics and\n ACL. Control Theory, 10(8):707‚Äì710. Original in Doklady\nChurch, K. W. 1994. Unix for Poets. Slides from 2nd EL- Akademii Nauk SSSR 163(4): 845‚Äì848 (1965).\n SNET Summer School and unpublished paper ms. Li, X., Y. Meng, X. Sun, Q. Han, A. Yuan, and J. Li. 2019.\nClark, H. H. and J. E. Fox Tree. 2002. Using uh and um in Is word segmentation necessary for deep learning of Chispontaneous speaking. Cognition, 84:73‚Äì111. nese representations? ACL.\nEgghe, L. 2007. Untangling Herdan‚Äôs law and Heaps‚Äô Liu, A., J. Hayase, V. Hofmann, S. Oh, N. A. Smith, and\n law: Mathematical and informetric arguments. JASIST, Y. Choi. 2025. SuperBPE: Space travel for language mod-\n58(5):702‚Äì709. els. ArXiv preprint.\nGage, P. 1994. A new algorithm for data compression. The Lovins, J. B. 1968. Development of a stemming algorithm.\n C Users Journal, 12(2):23‚Äì38. Mechanical Translation and Computational Linguistics,\nGebru, T., J. Morgenstern, B. Vecchione, J. W. Vaughan, 11(1‚Äì2):9‚Äì13.\n H. Wallach, H. DaumeÃÅ III, and K. Crawford. 2020. Manning, C. D., M. Surdeanu, J. Bauer, J. Finkel, S. Bethard,\n Datasheets for datasets. ArXiv. and D. McClosky. 2014. The Stanford CoreNLP natural\nGreenberg, J. H. 1960. A quantitative approach to the mor- language processing toolkit. ACL.\n phological typology of language. International journal of NIST. 2005. Speech recognition scoring toolkit (sctk) ver-\nAmerican linguistics, 26(3):178‚Äì194. sion 2.1. http://www.nist.gov/speech/tools/.\nGusfield, D. 1997. Algorithms on Strings, Trees, and Se- Packard, D. W. 1973. Computer-assisted morphological\n quences. Cambridge University Press. analysis of ancient Greek. COLING.\nHeaps, H. S. 1978. Information retrieval. Computational and Radford, A., J. Wu, R. Child, D. Luan, D. Amodei, and\n theoretical aspects. Academic Press. I. Sutskever. 2019. Language models are unsupervised\nHerdan, G. 1960. Type-token mathematics. Mouton. multitask learners. OpenAI tech report.\nJones, T. 2015. Toward a description of African American Rust, P., J. Pfeiffer, I. VulicÃÅ, S. Ruder, and I. Gurevych. 2021.\n Vernacular English dialect regions using ‚ÄúBlack Twitter‚Äù. How good is your tokenizer? on the monolingual perfor-\nAmerican Speech, 90(4):403‚Äì440. mance of multilingual language models. ACL.\nJurgens, D., Y. Tsvetkov, and D. Jurafsky. 2017. Incorpo- Sennrich, R., B. Haddow, and A. Birch. 2016. Neural marating dialectal variability for socially equitable language chine translation of rare words with subword units. ACL.\n identification. ACL. Simons, G. F. and C. D. Fennig. 2018. Ethnologue: Lan-\nKing, S. 2020. From African American Vernacular English guages of the world, 21st edition. SIL International.\n to African American Language: Rethinking the study of\n race and language in African Americans‚Äô speech. Annual\n Review of Linguistics, 6:285‚Äì300.\n Exercises 35\n\nSolorio, T., E. Blair, S. Maharjan, S. Bethard, M. Diab,\n M. Ghoneim, A. Hawwari, F. AlGhamdi, J. Hirschberg,\n A. Chang, and P. Fung. 2014. Overview for the first\n shared task on language identification in code-switched\n data. Workshop on Computational Approaches to Code\n Switching.\nThompson, K. 1968. Regular expression search algorithm.\n CACM, 11(6):419‚Äì422.\nTria, F., V. Loreto, and V. D. Servedio. 2018. Zipf‚Äôs, heaps‚Äô\n and taylor‚Äôs laws are determined by the expansion into the\n adjacent possible. Entropy, 20(10):752.\nWagner, R. A. and M. J. Fischer. 1974. The string-to-string\n correction problem. Journal of the ACM, 21:168‚Äì173.\nWeizenbaum, J. 1966. ELIZA ‚Äì A computer program for the\n study of natural language communication between man\n and machine. CACM, 9(1):36‚Äì45.\nWeizenbaum, J. 1976. Computer Power and Human Reason:\n From Judgement to Calculation. W.H. Freeman & Co.\n",
    "file_path": "/Users/colinsidberry/nlp-textbook-rag/data/extracted_text/words-and-tokens.txt",
    "file_size_kb": 103.41
  },
  {
    "id": "eaf18e46fa414c38",
    "source": "nlp_textbook",
    "chapter": "N-gram Language Models",
    "filename": "n-gram.txt",
    "content": "rights reserved. Draft of August 24, 2025.\n\nCHAPTER\n\n N-gram Language Models\n ‚ÄúYou are uniformly charming!‚Äù cried he, with a smile of associating and now\n and then I bowed and they perceived a chaise and four to wish for.\n Random sentence generated from a Jane Austen trigram model\n\n Predicting is difficult‚Äîespecially about the future, as the old quip goes. But how\n about predicting something that seems much easier, like the next word someone is\n going to say? What word, for example, is likely to follow\n The water of Walden Pond is so beautifully ...\n You might conclude that a likely word is blue, or green, or clear, but probably\n not refrigerator nor this. In this chapter we formalize this intuition by introlanguage model ducing n-gram language models or LMs. A language model is a machine learning\n LM model that predicts upcoming words. More formally, a language model assigns a\n probability to each possible next word, or equivalently gives a probability distribution over possible next words. Language models can also assign a probability to an\n entire sentence. Thus an LM could tell us that the following sequence has a much\n higher probability of appearing in a text:\n all of a sudden I notice three guys standing on the sidewalk\n\n than does this same set of words in a different order:\n on guys all I of notice sidewalk three a sudden standing the\n\n Why would we want to predict upcoming words? The main reason is that large\n language models are built just by training them to predict words!! As we‚Äôll see\n in chapters 5-10, large language models learn an enormous amount about language\n solely from being trained to predict upcoming words from neighboring words.\n This probabilistic knowledge can be very practical. Consider correcting grammar or spelling errors like Their are two midterms, in which There was mistyped\n as Their, or Everything has improve, in which improve should have been\n improved. The phrase There are is more probable than Their are, and has\n improved than has improve, so a language model can help users select the more\n grammatical variant.\n Or for a speech system to recognize that you said I will be back soonish\n and not I will be bassoon dish, it helps to know that back soonish is a more\n probable sequence. Language models can also help in augmentative and alterna-\nAAC tive communication (Trnka et al. 2007, Kane et al. 2017). People can use AAC\n systems if they are physically unable to speak or sign but can instead use eye gaze\n or other movements to select words from a menu. Word prediction can be used to\n suggest likely words for the menu.\n n-gram In this chapter we introduce the simplest kind of language model: the n-gram\n2 C HAPTER 3 ‚Ä¢ N- GRAM L ANGUAGE M ODELS\n\n language model. An n-gram is a sequence of n words: a 2-gram (which we‚Äôll call\n bigram) is a two-word sequence of words like The water, or water of, and a 3gram (a trigram) is a three-word sequence of words like The water of, or water\n of Walden. But we also (in a bit of terminological ambiguity) use the word ‚Äòngram‚Äô to mean a probabilistic model that can estimate the probability of a word given\n the n-1 previous words, and thereby also to assign probabilities to entire sequences.\n In later chapters we will introduce the much more powerful neural large language models, based on the transformer architecture of Chapter 8. But because\n n-grams have a remarkably simple and clear formalization, we use them to introduce some major concepts of large language modeling, including training and test\n sets, perplexity, sampling, and interpolation.\n\n Let‚Äôs begin with the task of computing P(w|h), the probability of a word w given\n some history h. Suppose the history h is ‚ÄúThe water of Walden Pond is so\n beautifully ‚Äù and we want to know the probability that the next word is blue:\n\n P(blue|The water of Walden Pond is so beautifully) (3.1)\n\n One way to estimate this probability is directly from relative frequency counts: take a\n very large corpus, count the number of times we see The water of Walden Pond\n is so beautifully, and count the number of times this is followed by blue. This\n would be answering the question ‚ÄúOut of the times we saw the history h, how many\n times was it followed by the word w‚Äù, as follows:\n\n P(blue|The water of Walden Pond is so beautifully) =\n C(The water of Walden Pond is so beautifully blue)\n (3.2)\n C(The water of Walden Pond is so beautifully)\n\n If we had a large enough corpus, we could compute these two counts and estimate\n the probability from Eq. 3.2. But even the entire web isn‚Äôt big enough to give us\n good estimates for counts of entire sentences. This is because language is creative;\n new sentences are invented all the time, and we can‚Äôt expect to get accurate counts\n for such large objects as entire sentences. For this reason, we‚Äôll need more clever\n ways to estimate the probability of a word w given a history h, or the probability of\n an entire word sequence W .\n Let‚Äôs start with some notation. First, throughout this chapter we‚Äôll continue to\n refer to words, although in practice we usually compute language models over tokens like the BPE tokens of page ??. To represent the probability of a particular\n random variable Xi taking on the value ‚Äúthe‚Äù, or P(Xi = ‚Äúthe‚Äù), we will use the\n simplification P(the). We‚Äôll represent a sequence of n words either as w1 . . . wn or\n w1:n . Thus the expression w1:n‚àí1 means the string w1 , w2 , ..., wn‚àí1 , but we‚Äôll also\n be using the equivalent notation w<n , which can be read as ‚Äúall the elements of w\n from w1 up to and including wn‚àí1 ‚Äù. For the joint probability of each word in a sequence having a particular value P(X1 = w1 , X2 = w2 , X3 = w3 , ..., Xn = wn ) we‚Äôll\n use P(w1 , w2 , ..., wn ).\n Now, how can we compute probabilities of entire sequences like P(w1 , w2 , ..., wn )?\n One thing we can do is decompose this probability using the chain rule of proba-\n3.1 ‚Ä¢ N-G RAMS 3\n\n bility:\n\n P(X1 ...Xn ) = P(X1 )P(X2 |X1 )P(X3 |X1:2 ) . . . P(Xn |X1:n‚àí1 )\n Yn\n = P(Xk |X1:k‚àí1 ) (3.3)\n k=1\n\n Applying the chain rule to words, we get\n\n P(w1:n ) = P(w1 )P(w2 |w1 )P(w3 |w1:2 ) . . . P(wn |w1:n‚àí1 )\n Yn\n = P(wk |w1:k‚àí1 ) (3.4)\n k=1\n\n The chain rule shows the link between computing the joint probability of a sequence\n and computing the conditional probability of a word given previous words. Equation 3.4 suggests that we could estimate the joint probability of an entire sequence of\n words by multiplying together a number of conditional probabilities. But using the\n chain rule doesn‚Äôt really seem to help us! We don‚Äôt know any way to compute the\n exact probability of a word given a long sequence of preceding words, P(wn |w1:n‚àí1 ).\n As we said above, we can‚Äôt just estimate by counting the number of times every word\n occurs following every long string in some corpus, because language is creative and\n any particular context might have never occurred before!\n\n 3.1.1 The Markov assumption\n The intuition of the n-gram model is that instead of computing the probability of a\n word given its entire history, we can approximate the history by just the last few\n words.\nbigram The bigram model, for example, approximates the probability of a word given\n all the previous words P(wn |w1:n‚àí1 ) by using only the conditional probability given\n the preceding word P(wn |wn‚àí1 ). In other words, instead of computing the probability\n\n P(blue|The water of Walden Pond is so beautifully) (3.5)\n\n we approximate it with the probability\n\n P(blue|beautifully) (3.6)\n\n When we use a bigram model to predict the conditional probability of the next word,\n we are thus making the following approximation:\n\n P(wn |w1:n‚àí1 ) ‚âà P(wn |wn‚àí1 ) (3.7)\n\n The assumption that the probability of a word depends only on the previous word is\nMarkov called a Markov assumption. Markov models are the class of probabilistic models\n that assume we can predict the probability of some future unit without looking too\n far into the past. We can generalize the bigram (which looks one word into the past)\nn-gram to the trigram (which looks two words into the past) and thus to the n-gram (which\n looks n ‚àí 1 words into the past).\n Let‚Äôs see a general equation for this n-gram approximation to the conditional\n probability of the next word in a sequence. We‚Äôll use N here to mean the n-gram\n4 C HAPTER 3 ‚Ä¢ N- GRAM L ANGUAGE M ODELS\n\n size, so N = 2 means bigrams and N = 3 means trigrams. Then we approximate the\n probability of a word given its entire context as follows:\n\n P(wn |w1:n‚àí1 ) ‚âà P(wn |wn‚àíN+1:n‚àí1 ) (3.8)\n\n Given the bigram assumption for the probability of an individual word, we can compute the probability of a complete word sequence by substituting Eq. 3.7 into Eq. 3.4:\n n\n Y\n P(w1:n ) ‚âà P(wk |wk‚àí1 ) (3.9)\n k=1\n\n 3.1.2 How to estimate probabilities\n maximum\n How do we estimate these bigram or n-gram probabilities? An intuitive way to\n likelihood estimate probabilities is called maximum likelihood estimation or MLE. We get\n estimation\n the MLE estimate for the parameters of an n-gram model by getting counts from\n normalize a corpus, and normalizing the counts so that they lie between 0 and 1. For probabilistic models, normalizing means dividing by some total count so that the resulting\n probabilities fall between 0 and 1 and sum to 1.\n For example, to compute a particular bigram probability of a word wn given a\n previous word wn‚àí1 , we‚Äôll compute the count of the bigram C(wn‚àí1 wn ) and normalize by the sum of all the bigrams that share the same first word wn‚àí1 :\n\n C(wn‚àí1 wn )\n P(wn |wn‚àí1 ) = P (3.10)\n w C(wn‚àí1 w)\n We can simplify this equation, since the sum of all bigram counts that start with\n a given word wn‚àí1 must be equal to the unigram count for that word wn‚àí1 (the reader\n should take a moment to be convinced of this):\n\n C(wn‚àí1 wn )\n P(wn |wn‚àí1 ) = (3.11)\n C(wn‚àí1 )\n Let‚Äôs work through an example using a mini-corpus of three sentences. We‚Äôll\n first need to augment each sentence with a special symbol <s> at the beginning\n of the sentence, to give us the bigram context of the first word. We‚Äôll also need a\n special end-symbol </s>.1\n <s> I am Sam </s>\n <s> Sam I am </s>\n <s> I do not like green eggs and ham </s>\n Here are the calculations for some of the bigram probabilities from this corpus\n P(I|<s>) = 23 = 0.67 P(Sam|<s>) = 31 = 0.33 P(am|I) = 23 = 0.67\n P(</s>|Sam) = 21 = 0.5 P(Sam|am) = 12 = 0.5 P(do|I) = 13 = 0.33\n For the general case of MLE n-gram parameter estimation:\n\n C(wn‚àíN+1:n‚àí1 wn )\n P(wn |wn‚àíN+1:n‚àí1 ) = (3.12)\n C(wn‚àíN+1:n‚àí1 )\n 1 We need the end-symbol to make the bigram grammar a true probability distribution. Without an endsymbol, instead of the sentence probabilities of all sentences summing to one, the sentence probabilities\n for all sentences of a given length would sum to one. This model would define an infinite set of probability\n distributions, with one distribution per sentence length. See Exercise 3.5.\n 3.1 ‚Ä¢ N-G RAMS 5\n\n Equation 3.12 (like Eq. 3.11) estimates the n-gram probability by dividing the\n observed frequency of a particular sequence by the observed frequency of a prefix.\n relative\nfrequency This ratio is called a relative frequency. We said above that this use of relative\n frequencies as a way to estimate probabilities is an example of maximum likelihood\n estimation or MLE. In MLE, the resulting parameter set maximizes the likelihood of\n the training set T given the model M (i.e., P(T |M)). For example, suppose the word\n Chinese occurs 400 times in a corpus of a million words. What is the probability\n that a random word selected from some other text of, say, a million words will be the\n word Chinese? The MLE of its probability is 1000000 or 0.0004. Now 0.0004 is not\n the best possible estimate of the probability of Chinese occurring in all situations; it\n might turn out that in some other corpus or context Chinese is a very unlikely word.\n But it is the probability that makes it most likely that Chinese will occur 400 times\n in a million-word corpus. We present ways to modify the MLE estimates slightly to\n get better probability estimates in Section 3.6.\n Let‚Äôs move on to some examples from a real but tiny corpus, drawn from the\n now-defunct Berkeley Restaurant Project, a dialogue system from the last century\n that answered questions about a database of restaurants in Berkeley, California (Jurafsky et al., 1994). Here are some sample user queries (text-normalized, by lower\n casing and with punctuation striped) (a sample of 9332 sentences is on the website):\n can you tell me about any good cantonese restaurants close by\n tell me about chez panisse\n i‚Äôm looking for a good place to eat breakfast\n when is caffe venezia open during the day\n\n values are zero. In fact, we have chosen the sample words to cohere with each other;\n a matrix selected from a random set of eight words would be even more sparse.\n\n i want to eat chinese food lunch spend\n i 5 827 0 9 0 0 0 2\n want 2 0 608 1 6 6 5 1\n to 2 0 4 686 2 0 6 211\n eat 0 0 2 0 16 2 42 0\n chinese 1 0 0 0 0 82 1 0\n food 15 0 15 0 1 4 0 0\n lunch 2 0 0 0 0 1 0 0\n spend 1 0 1 0 0 0 0 0\n the column label word following the row label word. Thus the cell in row i and column want\n means that want followed i 827 times in the corpus.\n\n in Fig. 3.1 by the appropriate unigram for its row, taken from the following set of\n unigram counts):\n\n i want to eat chinese food lunch spend\n 2533 927 2417 746 158 1093 341 278\n6 C HAPTER 3 ‚Ä¢ N- GRAM L ANGUAGE M ODELS\n\n i want to eat chinese food lunch spend\n i 0.002 0.33 0 0.0036 0 0 0 0.00079\n want 0.0022 0 0.66 0.0011 0.0065 0.0065 0.0054 0.0011\n to 0.00083 0 0.0017 0.28 0.00083 0 0.0025 0.087\n eat 0 0 0.0027 0 0.021 0.0027 0.056 0\n chinese 0.0063 0 0 0 0 0.52 0.0063 0\n food 0.014 0 0.014 0 0.00092 0.0037 0 0\n lunch 0.0059 0 0 0 0 0.0029 0 0\n spend 0.0036 0 0.0036 0 0 0 0 0\n of 9332 sentences. Zero probabilities are in gray.\n\n Here are a few other useful probabilities:\n P(i|<s>) = 0.25 P(english|want) = 0.0011\n P(food|english) = 0.5 P(</s>|food) = 0.68\n Now we can compute the probability of sentences like I want English food or\n I want Chinese food by simply multiplying the appropriate bigram probabilities together, as follows:\n P(<s> i want english food </s>)\n = P(i|<s>)P(want|i)P(english|want)\n P(food|english)P(</s>|food)\n = 0.25 √ó 0.33 √ó 0.0011 √ó 0.5 √ó 0.68\n = 0.000031\n We leave it as Exercise 3.2 to compute the probability of i want chinese food.\n What kinds of linguistic phenomena are captured in these bigram statistics?\n Some of the bigram probabilities above encode some facts that we think of as strictly\n syntactic in nature, like the fact that what comes after eat is usually a noun or an\n adjective, or that what comes after to is usually a verb. Others might be a fact about\n the personal assistant task, like the high probability of sentences beginning with\n the words I. And some might even be cultural rather than linguistic, like the higher\n probability that people are looking for Chinese versus English food.\n\n 3.1.3 Dealing with scale in large n-gram models\n In practice, language models can be very large, leading to practical issues.\n Log probabilities Language model probabilities are always stored and computed\n log\n probabilities in log space as log probabilities. This is because probabilities are (by definition)\n less than or equal to 1, and so the more probabilities we multiply together, the\n smaller the product becomes. Multiplying enough n-grams together would result\n in numerical underflow. Adding in log space is equivalent to multiplying in linear\n space, so we combine log probabilities by adding them. By adding log probabilities\n instead of multiplying probabilities, we get results that are not as small. We do all\n computation and storage in log space, and just convert back into probabilities if we\n need to report probabilities at the end by taking the exp of the logprob:\n p1 √ó p2 √ó p3 √ó p4 = exp(log p1 + log p2 + log p3 + log p4 ) (3.13)\n\n In practice throughout this book, we‚Äôll use log to mean natural log (ln) when the\n base is not specified.\n 3.2 ‚Ä¢ E VALUATING L ANGUAGE M ODELS : T RAINING AND T EST S ETS 7\n\n Longer context Although for pedagogical purposes we have only described bitrigram gram models, when there is sufficient training data we use trigram models, which\n 4-gram condition on the previous two words, or 4-gram or 5-gram models. For these larger\n 5-gram n-grams, we‚Äôll need to assume extra contexts to the left and right of the sentence end.\n For example, to compute trigram probabilities at the very beginning of the sentence,\n we use two pseudo-words for the first trigram (i.e., P(I|<s><s>).\n Some large n-gram datasets have been created, like the million most frequent\n n-grams drawn from the Corpus of Contemporary American English (COCA), a\n curated 1 billion word corpus of American English (Davies, 2020), Google‚Äôs Web\n 5-gram corpus from 1 trillion words of English web text (Franz and Brants, 2006),\n or the Google Books Ngrams corpora (800 billion tokens from Chinese, English,\n French, German, Hebrew, Italian, Russian, and Spanish) (Lin et al., 2012)).\n It‚Äôs even possible to use extremely long-range n-gram context. The infini-gram\n (‚àû-gram) project (Liu et al., 2024) allows n-grams of any length. Their idea is to\n avoid the expensive (in space and time) pre-computation of huge n-gram count tables. Instead, n-gram probabilities with arbitrary n are computed quickly at inference\n time by using an efficient representation called suffix arrays. This allows computing\n of n-grams of every length for enormous corpora of 5 trillion tokens.\n Efficiency considerations are important when building large n-gram language\n models. It is standard to quantize the probabilities using only 4-8 bits (instead of\n 8-byte floats), store the word strings on disk and represent them in memory only as\n a 64-bit hash, and represent n-grams in special data structures like ‚Äòreverse tries‚Äô.\n It is also common to prune n-gram language models, for example by only keeping\n n-grams with counts greater than some threshold or using entropy to prune lessimportant n-grams (Stolcke, 1998). Efficient language model toolkits like KenLM\n (Heafield 2011, Heafield et al. 2013) use sorted arrays and use merge sorts to efficiently build the probability tables in a minimal number of passes through a large\n corpus.\n\n The best way to evaluate the performance of a language model is to embed it in\n an application and measure how much the application improves. Such end-to-end\n extrinsic evaluation is called extrinsic evaluation. Extrinsic evaluation is the only way to\n evaluation\n know if a particular improvement in the language model (or any component) is really\n going to help the task at hand. Thus for evaluating n-gram language models that are\n a component of some task like speech recognition or machine translation, we can\n compare the performance of two candidate language models by running the speech\n recognizer or machine translator twice, once with each language model, and seeing\n which gives the more accurate transcription.\n Unfortunately, running big NLP systems end-to-end is often very expensive. Instead, it‚Äôs helpful to have a metric that can be used to quickly evaluate potential\n intrinsic improvements in a language model. An intrinsic evaluation metric is one that meaevaluation\n sures the quality of a model independent of any application. In the next section we‚Äôll\n introduce perplexity, which is the standard intrinsic metric for measuring language\n model performance, both for simple n-gram language models and for the more sophisticated neural large language models of Chapter 8.\n In order to evaluate any machine learning model, we need to have at least three\n training set distinct data sets: the training set, the development set, and the test set.\n development\n set\n test set\n8 C HAPTER 3 ‚Ä¢ N- GRAM L ANGUAGE M ODELS\n\n The training set is the data we use to learn the parameters of our model; for\n simple n-gram language models it‚Äôs the corpus from which we get the counts that\n we normalize into the probabilities of the n-gram language model.\n The test set is a different, held-out set of data, not overlapping with the training\n set, that we use to evaluate the model. We need a separate test set to give us an\n unbiased estimate of how well the model we trained can generalize when we apply\n it to some new unknown dataset. A machine learning model that perfectly captured\n the training data, but performed terribly on any other data, wouldn‚Äôt be much use\n when it comes time to apply it to any new data or problem! We thus measure the\n quality of an n-gram model by its performance on this unseen test set or test corpus.\n How should we choose a training and test set? The test set should reflect the\n language we want to use the model for. If we‚Äôre going to use our language model\n for speech recognition of chemistry lectures, the test set should be text of chemistry\n lectures. If we‚Äôre going to use it as part of a system for translating hotel booking requests from Chinese to English, the test set should be text of hotel booking requests.\n If we want our language model to be general purpose, then the test set should be\n drawn from a wide variety of texts. In such cases we might collect a lot of texts\n from different sources, and then divide it up into a training set and a test set. It‚Äôs\n important to do the dividing carefully; if we‚Äôre building a general purpose model,\n we don‚Äôt want the test set to consist of only text from one document, or one author,\n since that wouldn‚Äôt be a good measure of general performance.\n Thus if we are given a corpus of text and want to compare the performance of\n two different n-gram models, we divide the data into training and test sets, and train\n the parameters of both models on the training set. We can then compare how well\n the two trained models fit the test set.\n But what does it mean to ‚Äúfit the test set‚Äù? The standard answer is simple:\n whichever language model assigns a higher probability to the test set‚Äîwhich\n means it more accurately predicts the test set‚Äîis a better model. Given two probabilistic models, the better model is the one that better predicts the details of the test\n data, and hence will assign a higher probability to the test data.\n Since our evaluation metric is based on test set probability, it‚Äôs important not\n to let the test sentences into the training set. Suppose we are trying to compute\n the probability of a particular ‚Äútest‚Äù sentence. If our test sentence is part of the\n training corpus, we will mistakenly assign it an artificially high probability when\n it occurs in the test set. We call this situation training on the test set or also data\n data contamination. Training on the test set introduces a bias that makes the probabilities\n contamination\n all look too high, and causes huge inaccuracies in perplexity, the probability-based\n metric we introduce below.\n Even if we don‚Äôt train on the test set, if we test our language model on the\n test set many times after making different changes, we might implicitly tune to its\n characteristics, by noticing which changes seem to make the model better. For this\n reason, we only want to run our model on the test set once, or a very few number of\n times, once we are sure our model is ready.\n development For this reason we normally instead have a third dataset called a development\n test\n test set or, devset. We do all our testing on this dataset until the very end, and then\n we test on the test set once to see how good our model is.\n How do we divide our data into training, development, and test sets? We want\n our test set to be as large as possible, since a small test set may be accidentally unrepresentative, but we also want as much training data as possible. At the minimum,\n we would want to pick the smallest test set that gives us enough statistical power\n 3.3 ‚Ä¢ E VALUATING L ANGUAGE M ODELS : P ERPLEXITY 9\n\n to measure a statistically significant difference between two potential models. It‚Äôs\n important that the devset be drawn from the same kind of text as the test set, since\n its goal is to measure how we would do on the test set.\n\n We said above that we evaluate language models based on which one assigns a\n higher probability to the test set. A better model is better at predicting upcoming\n words, and so it will be less surprised by (i.e., assign a higher probability to) each\n word when it occurs in the test set. Indeed, a perfect language model would correctly\n guess each next word in a corpus, assigning it a probability of 1, and all the other\n words a probability of zero. So given a test corpus, a better language model will\n assign a higher probability to it than a worse language model.\n But in fact, we often do not use raw probability as our metric for evaluating\n language models. The reason is that the probability of a test set (or any sequence)\n depends on the number of words or tokens in it; the probability of a test set gets\n smaller the longer the text. It‚Äôs useful to have a metric that is per-word, normalized\n by length, so we could compare across texts of different lengths. There is a such a\n metric! It‚Äôs a function of probability called perplexity, and it is used for evaluating\n large language models as well as n-gram models.\n perplexity The perplexity (sometimes abbreviated as PP or PPL) of a language model on a\n test set is the inverse probability of the test set (one over the probability of the test\n set), normalized by the number of words (or tokens). For this reason it‚Äôs sometimes\n called the per-word or per-token perplexity. We normalize by the number of words\n N by taking the Nth root. For a test set W = w1 w2 . . . wN ,:\n perplexity(W ) = P(w1 w2 . . . wN )‚àí N (3.14)\n s\n = N\n P(w1 w2 . . . wN )\n Or we can use the chain rule to expand the probability of W :\n v\n uN\n uY\n N 1\n perplexity(W ) = t (3.15)\n P(wi |w1 . . . wi‚àí1 )\n i=1\n\n Note that because of the inverse in Eq. 3.15, the higher the probability of the word\n sequence, the lower the perplexity. Thus the the lower the perplexity of a model on\n the data, the better the model. Minimizing perplexity is equivalent to maximizing\n the test set probability according to the language model. Why does perplexity use\n the inverse probability? It turns out the inverse arises from the original definition\n of perplexity from cross-entropy rate in information theory; for those interested, the\n explanation is in the advanced Section 3.7. Meanwhile, we just have to remember\n that perplexity has an inverse relationship with probability.\n The details of computing the perplexity of a test set W depends on which language model we use. Here‚Äôs the perplexity of W with a unigram language model\n (just the geometric mean of the inverse of the unigram probabilities):\n v\n uN\n uY 1\n N\n perplexity(W ) = t (3.16)\n P(wi )\n i=1\n10 C HAPTER 3 ‚Ä¢ N- GRAM L ANGUAGE M ODELS\n\n The perplexity of W computed with a bigram language model is still a geometric\n mean, but now of the inverse of the bigram probabilities:\n v\n uN\n uY\n N 1\n perplexity(W ) = t (3.17)\n P(wi |wi‚àí1 )\n i=1\n\n What we generally use for word sequence in Eq. 3.15 or Eq. 3.17 is the entire\n sequence of words in some test set. Since this sequence will cross many sentence\n boundaries, if our vocabulary includes a between-sentence token <EOS> or separate\n begin- and end-sentence markers <s> and </s> then we can include them in the\n probability computation. If we do, then we also include one token per sentence in\n the total count of word tokens N.2\n We mentioned above that perplexity is a function of both the text and the language model: given a text W , different language models will have different perplexities. Because of this, perplexity can be used to compare different language models.\n For example, here we trained unigram, bigram, and trigram models on 38 million\n words from the Wall Street Journal newspaper. We then computed the perplexity of\n each of these models on a WSJ test set using Eq. 3.16 for unigrams, Eq. 3.17 for\n bigrams, and the corresponding equation for trigrams. The table below shows the\n perplexity of the 1.5 million word test set according to each of the language models.\n Unigram Bigram Trigram\n Perplexity 962 170 109\n As we see above, the more information the n-gram gives us about the word\n sequence, the higher the probability the n-gram will assign to the string. A trigram\n model is less surprised than a unigram model because it has a better idea of what\n words might come next, and so it assigns them a higher probability. And the higher\n the probability, the lower the perplexity (since as Eq. 3.15 showed, perplexity is\n related inversely to the probability of the test sequence according to the model). So\n a lower perplexity tells us that a language model is a better predictor of the test set.\n Note that in computing perplexities, the language model must be constructed\n without any knowledge of the test set, or else the perplexity will be artificially low.\n And the perplexity of two language models is only comparable if they use identical\n vocabularies.\n An (intrinsic) improvement in perplexity does not guarantee an (extrinsic) improvement in the performance of a language processing task like speech recognition\n or machine translation. Nonetheless, because perplexity usually correlates with task\n improvements, it is commonly used as a convenient evaluation metric. Still, when\n possible a model‚Äôs improvement in perplexity should be confirmed by an end-to-end\n evaluation on a real task.\n\n 3.3.1 Perplexity as Weighted Average Branching Factor\n It turns out that perplexity can also be thought of as the weighted average branching factor of a language. The branching factor of a language is the number of\n possible next words that can follow any word. For example consider a mini artificial\n 2 For example if we use both begin and end tokens, we would include the end-of-sentence marker </s>\n but not the beginning-of-sentence marker <s> in our count of N; This is because the end-sentence token is\n followed directly by the begin-sentence token with probability almost 1, so we don‚Äôt want the probability\n of that fake transition to influence our perplexity.\n 3.4 ‚Ä¢ S AMPLING SENTENCES FROM A LANGUAGE MODEL 11\n\n language that is deterministic (no probabilities), any word can follow any word, and\n whose vocabulary consists of only three colors:\n\n L = {red, blue, green} (3.18)\n\n The branching factor of this language is 3.\n Now let‚Äôs make a probabilistic version of the same LM, let‚Äôs call it A, where each\n word follows each other with equal probability 13 (it was trained on a training set with\n equal counts for the 3 colors), and a test set T = ‚Äúred red red red blue‚Äù.\n Let‚Äôs first convince ourselves that if we compute the perplexity of this artificial\n color language on this test set (or any such test set) we indeed get 3. By Eq. 3.15,\n the perplexity of A on T is:\n perplexityA (T ) = PA (red red red red blue)‚àí 5\n \u0012 \u00135 !‚àí 15\n \u0012 \u0013‚àí1\n = =3 (3.19)\n But now suppose red was very likely in the training set of a different LM B, and so\n B has the following probabilities:\n\n P(red) = 0.8 P(green) = 0.1 P(blue) = 0.1 (3.20)\n\n We should expect the perplexity of the same test set red red red red blue for\n language model B to be lower since most of the time the next color will be red, which\n is very predictable, i.e. has a high probability. So the probability of the test set will\n be higher, and since perplexity is inversely related to probability, the perplexity will\n be lower. Thus, although the branching factor is still 3, the perplexity or weighted\n branching factor is smaller:\n\n perplexityB (T ) = PB (red red red red blue)‚àí1/5\n = 0.04096‚àí 5\n = 0.527‚àí1 = 1.89 (3.21)\n\n One important way to visualize what kind of knowledge a language model embodies\n sampling is to sample from it. Sampling from a distribution means to choose random points\n according to their likelihood. Thus sampling from a language model‚Äîwhich represents a distribution over sentences‚Äîmeans to generate some sentences, choosing\n each sentence according to its likelihood as defined by the model. Thus we are more\n likely to generate sentences that the model thinks have a high probability and less\n likely to generate sentences that the model thinks have a low probability.\n This technique of visualizing a language model by sampling was first suggested\n very early on by Shannon (1948) and Miller and Selfridge (1950). It‚Äôs simplest to\n visualize how this works for the unigram case. Imagine all the words of the English\n language covering the number line between 0 and 1, each word covering an interval\n12 C HAPTER 3 ‚Ä¢ N- GRAM L ANGUAGE M ODELS\n\n polyphonic\n p=0.0000018\n however\n the of a to in (p=0.0003)\n\n ‚Ä¶ ‚Ä¶\n .06 .09 .11 .13 .15 .66 .99\n 0 1\n\n ordered them from most frequent to least frequent, but the choice of order is arbitrary). The\n number line shows the cumulative probabilities. If we choose a random number between 0\n and 1, it will fall in an interval corresponding to some word. The expectation for the random\n number to fall in the larger intervals of one of the frequent words (the, of, a) is much higher\n than in the smaller interval of one of the rare words (polyphonic).\n\n proportional to its frequency. Fig. 3.3 shows a visualization, using a unigram LM\n computed from the text of this book. We choose a random value between 0 and 1,\n find that point on the probability line, and print the word whose interval includes this\n chosen value. We continue choosing random numbers and generating words until\n we randomly generate the sentence-final token </s>.\n We can use the same technique to generate bigrams by first generating a random bigram that starts with <s> (according to its bigram probability). Let‚Äôs say the\n second word of that bigram is w. We next choose a random bigram starting with w\n (again, drawn according to its bigram probability), and so on.\n\n The n-gram model, like many statistical models, is dependent on the training corpus.\n One implication of this is that the probabilities often encode specific facts about a\n given training corpus. Another implication is that n-grams do a better and better job\n of modeling the training corpus as we increase the value of n.\n We can use the sampling method from the prior section to visualize both of\n these facts! To give an intuition for the increasing power of higher-order n-grams,\n Fig. 3.4 shows random sentences generated from unigram, bigram, trigram, and 4gram models trained on Shakespeare‚Äôs works.\n The longer the context, the more coherent the sentences. The unigram sentences show no coherent relation between words nor any sentence-final punctuation. The bigram sentences have some local word-to-word coherence (especially\n considering punctuation as words). The trigram sentences are beginning to look a\n lot like Shakespeare. Indeed, the 4-gram sentences look a little too much like Shakespeare. The words It cannot be but so are directly from King John. This is because,\n not to put the knock on Shakespeare, his oeuvre is not very large as corpora go\n (N = 884, 647,V = 29, 066), and our n-gram probability matrices are ridiculously\n sparse. There are V 2 = 844, 000, 000 possible bigrams alone, and the number of\n possible 4-grams is V 4 = 7 √ó 1017 . Thus, once the generator has chosen the first\n 3-gram (It cannot be), there are only seven possible next words for the 4th element\n (but, I, that, thus, this, and the period).\n To get an idea of the dependence on the training set, let‚Äôs look at LMs trained on a\n completely different corpus: the Wall Street Journal (WSJ) newspaper. Shakespeare\n 3.5 ‚Ä¢ G ENERALIZING VS . OVERFITTING THE TRAINING SET 13\n\n ‚ÄìTo him swallowed confess hear both. Which. Of save on trail for are ay device and\n gram\n rote life have\n ‚ÄìHill he late speaks; or! a more to leg less first you enter\n ‚ÄìWhy dost stand forth thy canopy, forsooth; he is this palpable hit the King Henry. Live\n gram\n king. Follow.\n ‚ÄìWhat means, sir. I confess she? then all sorts, he is trim, captain.\n ‚ÄìFly, and will rid me these news of price. Therefore the sadness of parting, as they say,\n gram\n ‚Äôtis done.\n ‚ÄìThis shall forbid it should be branded, if renown made it empty.\n ‚ÄìKing Henry. What! I will go seek the traitor Gloucester. Exeunt some of the watch. A\n gram\n great banquet serv‚Äôd in;\n ‚ÄìIt cannot be but so.\nworks. All characters were mapped to lower-case and punctuation marks were treated as words. Output is\nhand-corrected for capitalization to improve readability.\n\n and the WSJ are both English, so we might have expected some overlap between our\n n-grams for the two genres. Fig. 3.5 shows sentences generated by unigram, bigram,\n and trigram models trained on 40 million words from WSJ.\n\n Months the my and issue of year foreign new exchange‚Äôs september\n gram\n were recession exchange new endorsed a acquire to six executives\n Last December through the way to preserve the Hudson corporation N.\n gram\n B. E. C. Taylor would seem to complete the major central planners one\n point five percent of U. S. E. has already old M. X. corporation of living\n on information such as more frequently fishing to keep her\n They also point to ninety nine point six billion dollars from two hundred\n gram\n four oh six three percent of the rates of interest stores as Mexico and\n Brazil on market conditions\n 40 million words of the Wall Street Journal, lower-casing all characters and treating punctuation as words. Output was then hand-corrected for capitalization to improve readability.\n\n Compare these examples to the pseudo-Shakespeare in Fig. 3.4. While they both\n model ‚ÄúEnglish-like sentences‚Äù, there is no overlap in the generated sentences, and\n little overlap even in small phrases. Statistical models are pretty useless as predictors\n if the training sets and the test sets are as different as Shakespeare and the WSJ.\n How should we deal with this problem when we build n-gram models? One step\n is to be sure to use a training corpus that has a similar genre to whatever task we are\n trying to accomplish. To build a language model for translating legal documents,\n we need a training corpus of legal documents. To build a language model for a\n question-answering system, we need a training corpus of questions.\n It is equally important to get training data in the appropriate dialect or variety,\n especially when processing social media posts or spoken transcripts. For example some tweets will use features of African American English (AAE)‚Äî the name\n for the many variations of language used in African American communities (King,\n 2020). Such features can include words like finna‚Äîan auxiliary verb that marks\n immediate future tense ‚Äîthat don‚Äôt occur in other varieties, or spellings like den for\n then, in tweets like this one (Blodgett and O‚ÄôConnor, 2017):\n14 C HAPTER 3 ‚Ä¢ N- GRAM L ANGUAGE M ODELS\n\n (3.22) Bored af den my phone finna die!!!\n while tweets from English-based languages like Nigerian Pidgin have markedly different vocabulary and n-gram patterns from American English (Jurgens et al., 2017):\n (3.23) @username R u a wizard or wat gan sef: in d mornin - u tweet, afternoon - u\n tweet, nyt gan u dey tweet. beta get ur IT placement wiv twitter\n Is it possible for the testset nonetheless to have a word we have never seen before? What happens if the word Jurafsky never occurs in our training set, but pops up\n in the test set? The answer is that although words might be unseen, we normally run\n our NLP algorithms not on words but on subword tokens. With subword tokenization (like the BPE algorithm of Chapter 2) any word can be modeled as a sequence\n of known smaller subwords, if necessary by a sequence of tokens corresponding to\n individual letters. So although for convenience we‚Äôve been referring to words in\n this chapter, the language model vocabulary is normally the set of tokens rather than\n words, and in this way the test set can never contain unseen tokens.\n\n There is a problem with using maximum likelihood estimates for probabilities: any\n finite training corpus will be missing some perfectly acceptable English word sequences. That is, cases where a particular n-gram never occurs in the training data\n but appears in the test set. Perhaps our training corpus has the words ruby and\n slippers in it but just happens not to have the phrase ruby slippers.\n zeros These unseen sequences or zeros‚Äîsequences that don‚Äôt occur in the training set\n but do occur in the test set‚Äîare a problem for two reasons. First, their presence\n means we are underestimating the probability of word sequences that might occur,\n which hurts the performance of any application we want to run on this data. Second,\n if the probability of any word in the test set is 0, the probability of the whole test\n set is 0. Perplexity is defined based on the inverse probability of the test set. Thus\n if some words in context have zero probability, we can‚Äôt compute perplexity at all,\n since we can‚Äôt divide by zero!\n The standard way to deal with putative ‚Äúzero probability n-grams‚Äù that should resmoothing ally have some non-zero probability is called smoothing or discounting. Smoothing\n discounting algorithms shave off a bit of probability mass from some more frequent events and\n give it to unseen events. Here we‚Äôll introduce some simple smoothing algorithms:\n Laplace (add-one) smoothing, stupid backoff, and n-gram interpolation.\n\n 3.6.1 Laplace Smoothing\n The simplest way to do smoothing is to add one to all the n-gram counts, before\n we normalize them into probabilities. All the counts that used to be zero will now\n have a count of 1, the counts of 1 will be 2, and so on. This algorithm is called\n Laplace\n smoothing Laplace smoothing. Laplace smoothing does not perform well enough to be used\n in modern n-gram models, but it usefully introduces many of the concepts that we\n see in other smoothing algorithms, gives a useful baseline, and is also a practical\n smoothing algorithm for other tasks like text classification (Appendix K).\n Let‚Äôs start with the application of Laplace smoothing to unigram probabilities.\n Recall that the unsmoothed maximum likelihood estimate of the unigram probability\n 3.6 ‚Ä¢ S MOOTHING , I NTERPOLATION , AND BACKOFF 15\n\n of the word wi is its count ci normalized by the total number of word tokens N:\n ci\n P(wi ) =\n N\n Laplace smoothing merely adds one to each count (hence its alternate name addadd-one one smoothing). Since there are V words in the vocabulary and each one was incremented, we also need to adjust the denominator to take into account the extra V\n observations. (What happens to our P values if we don‚Äôt increase the denominator?)\n ci + 1\n PLaplace (wi ) = (3.24)\n N +V\n Now that we have the intuition for the unigram case, let‚Äôs smooth our Berkeley\n Restaurant Project bigrams. Figure 3.6 shows the add-one smoothed counts for the\n bigrams in Fig. 3.1.\n\n i want to eat chinese food lunch spend\n i 6 828 1 10 1 1 1 3\n want 3 1 609 2 7 7 6 2\n to 3 1 5 687 3 1 7 212\n eat 1 1 3 1 17 3 43 1\n chinese 2 1 1 1 1 83 2 1\n food 16 1 16 1 2 5 1 1\n lunch 3 1 1 1 1 2 1 1\n spend 2 1 2 1 1 1 1 1\n the Berkeley Restaurant Project corpus of 9332 sentences. Previously-zero counts are in gray.\n\n computed by Eq. 3.26 below. Recall that normal bigram probabilities are computed\n by normalizing each row of counts by the unigram count:\n C(wn‚àí1 wn )\n PMLE (wn |wn‚àí1 ) = (3.25)\n C(wn‚àí1 )\n For add-one smoothed bigram counts, we need to augment the unigram count in the\n denominator by the number of total word types in the vocabulary V . We can see\n why this is in the following equation, which makes it explicit that the unigram count\n in the denominator is really the sum over all the bigrams that start with wn‚àí1 . Since\n we add one to each of these, and there are V of them, we add a total of V to the\n denominator:\n C(wn‚àí1 wn ) + 1 C(wn‚àí1 wn ) + 1\n PLaplace (wn |wn‚àí1 ) = P = (3.26)\n w (C(wn‚àí1 w) + 1) C(wn‚àí1 ) +V\n Thus, each of the unigram counts given on page 5 will need to be augmented by V =\n 1446. The result, using Eq. 3.26, is the smoothed bigram probabilities in Fig. 3.7.\n One useful visualization technique is to reconstruct an adjusted count matrix\n so we can see how much a smoothing algorithm has changed the original counts.\n This adjusted count C‚àó is the count that, if divided by C(wn‚àí1 ), would result in\n the smoothed probability. This adjusted count is easier to compare directly with\n the MLE counts. That is, the Laplace probability can equally be expressed as the\n adjusted count divided by the (non-smoothed) denominator from Eq. 3.25:\n C(wn‚àí1 wn ) + 1 C‚àó (wn‚àí1 wn )\n PLaplace (wn |wn‚àí1 ) = =\n C(wn‚àí1 ) +V C(wn‚àí1 )\n16 C HAPTER 3 ‚Ä¢ N- GRAM L ANGUAGE M ODELS\n\n i want to eat chinese food lunch spend\ni 0.0015 0.21 0.00025 0.0025 0.00025 0.00025 0.00025 0.00075\nwant 0.0013 0.00042 0.26 0.00084 0.0029 0.0029 0.0025 0.00084\nto 0.00078 0.00026 0.0013 0.18 0.00078 0.00026 0.0018 0.055\neat 0.00046 0.00046 0.0014 0.00046 0.0078 0.0014 0.02 0.00046\nchinese 0.0012 0.00062 0.00062 0.00062 0.00062 0.052 0.0012 0.00062\nfood 0.0063 0.00039 0.0063 0.00039 0.00079 0.002 0.00039 0.00039\nlunch 0.0017 0.00056 0.00056 0.00056 0.00056 0.0011 0.00056 0.00056\nspend 0.0012 0.00058 0.0012 0.00058 0.00058 0.00058 0.00058 0.00058\ncorpus of 9332 sentences computed by Eq. 3.26. Previously-zero probabilities are in gray.\n\n Rearranging terms, we can solve for C‚àó (wn‚àí1 wn ) :\n\n [C(wn‚àí1 wn ) + 1] √óC(wn‚àí1 )\n C‚àó (wn‚àí1 wn ) = (3.27)\n C(wn‚àí1 ) +V\n i want to eat chinese food lunch spend\n i 3.8 527 0.64 6.4 0.64 0.64 0.64 1.9\n want 1.2 0.39 238 0.78 2.7 2.7 2.3 0.78\n to 1.9 0.63 3.1 430 1.9 0.63 4.4 133\n eat 0.34 0.34 1 0.34 5.8 1 15 0.34\n chinese 0.2 0.098 0.098 0.098 0.098 8.2 0.2 0.098\n food 6.9 0.43 6.9 0.43 0.86 2.2 0.43 0.43\n lunch 0.57 0.19 0.19 0.19 0.19 0.38 0.19 0.19\n spend 0.32 0.16 0.32 0.16 0.16 0.16 0.16 0.16\n of 9332 sentences, computed by Eq. 3.27. Previously-zero counts are in gray.\n\n Note that add-one smoothing has made a very big change to the counts. Comparing Fig. 3.8 to the original counts in Fig. 3.1, we can see that C(want to) changed\n from 608 to 238. We can see this in probability space as well: P(to|want) decreases\n from 0.66 in the unsmoothed case to 0.26 in the smoothed case. Looking at the discount d, defined as the ratio between new and old counts, shows us how strikingly\n the counts for each prefix word have been reduced; the discount for the bigram want\n to is 0.39, while the discount for Chinese food is 0.10, a factor of 10. The sharp\n change occurs because too much probability mass is moved to all the zeros.\n\n 3.6.2 Add-k smoothing\n One alternative to add-one smoothing is to move a bit less of the probability mass\n from the seen to the unseen events. Instead of adding 1 to each count, we add a\n add-k fractional count k (0.5? 0.01?). This algorithm is therefore called add-k smoothing.\n\n ‚àó C(wn‚àí1 wn ) + k\n PAdd-k (wn |wn‚àí1 ) = (3.28)\n C(wn‚àí1 ) + kV\n Add-k smoothing requires that we have a method for choosing k; this can be\n done, for example, by optimizing on a devset. Although add-k is useful for some\n tasks (including text classification), it turns out that it still doesn‚Äôt work well for\n 3.6 ‚Ä¢ S MOOTHING , I NTERPOLATION , AND BACKOFF 17\n\n language modeling, generating counts with poor variances and often inappropriate\n discounts (Gale and Church, 1994).\n\n 3.6.3 Language Model Interpolation\n There is an alternative source of knowledge we can draw on to solve the problem\n of zero frequency n-grams. If we are trying to compute P(wn |wn‚àí2 wn‚àí1 ) but we\n have no examples of a particular trigram wn‚àí2 wn‚àí1 wn , we can instead estimate its\n probability by using the bigram probability P(wn |wn‚àí1 ). Similarly, if we don‚Äôt have\n counts to compute P(wn |wn‚àí1 ), we can look to the unigram P(wn ). In other words,\n sometimes using less context can help us generalize more for contexts that the model\n hasn‚Äôt learned much about.\ninterpolation The most common way to use this n-gram hierarchy is called interpolation:\n computing a new probability by interpolating (weighting and combining) the trigram, bigram, and unigram probabilities. In simple linear interpolation, we combine different order n-grams by linearly interpolating them. Thus, we estimate the\n trigram probability P(wn |wn‚àí2 wn‚àí1 ) by mixing together the unigram, bigram, and\n trigram probabilities, each weighted by a Œª :\n\n PÃÇ(wn |wn‚àí2 wn‚àí1 ) = Œª1 P(wn )\n +Œª2 P(wn |wn‚àí1 )\n +Œª3 P(wn |wn‚àí2 wn‚àí1 ) (3.29)\n\n The Œª s must sum to 1, making Eq. 3.29 equivalent to a weighted average. In a\n slightly more sophisticated version of linear interpolation, each Œª weight is computed by conditioning on the context. This way, if we have particularly accurate\n counts for a particular bigram, we assume that the counts of the trigrams based on\n this bigram will be more trustworthy, so we can make the Œª s for those trigrams\n higher and thus give that trigram more weight in the interpolation. Equation 3.30\n shows the equation for interpolation with context-conditioned weights, where each\n lambda takes an argument that is the two prior word context:\n\n PÃÇ(wn |wn‚àí2 wn‚àí1 ) = Œª1 (wn‚àí2:n‚àí1 )P(wn )\n +Œª2 (wn‚àí2:n‚àí1 )P(wn |wn‚àí1 )\n + Œª3 (wn‚àí2:n‚àí1 )P(wn |wn‚àí2 wn‚àí1 ) (3.30)\n\n How are these Œª values set? Both the simple interpolation and conditional interpoheld-out lation Œª s are learned from a held-out corpus. A held-out corpus is an additional\n training corpus, so-called because we hold it out from the training data, that we use\n to set these Œª values.3 We do so by choosing the Œª values that maximize the likelihood of the held-out corpus. That is, we fix the n-gram probabilities and then search\n for the Œª values that‚Äîwhen plugged into Eq. 3.29‚Äîgive us the highest probability\n of the held-out set. There are various ways to find this optimal set of Œª s. One way\n is to use the EM algorithm, an iterative learning algorithm that converges on locally\n optimal Œª s (Jelinek and Mercer, 1980).\n\n 3.6.4 Stupid Backoff\n backoff An alternative to interpolation is backoff. In a backoff model, if the n-gram we need\n 3 Held-out corpora are generally used to set hyperparameters, which are special parameters, unlike\n regular counts that are learned from the training data; we‚Äôll discuss hyperparameters in Chapter 6.\n18 C HAPTER 3 ‚Ä¢ N- GRAM L ANGUAGE M ODELS\n\n has zero counts, we approximate it by backing off to the (n-1)-gram. We continue\n backing off until we reach a history that has some counts. For a backoff model to\n discount give a correct probability distribution, we have to discount the higher-order n-grams\n to save some probability mass for the lower order n-grams. In practice, instead of\n discounting, it‚Äôs common to use a much simpler non-discounted backoff algorithm\n stupid backoff called stupid backoff (Brants et al., 2007).\n Stupid backoff gives up the idea of trying to make the language model a true\n probability distribution. There is no discounting of the higher-order probabilities. If\n a higher-order n-gram has a zero count, we simply backoff to a lower order n-gram,\n weighed by a fixed (context-independent) weight. This algorithm does not produce\n a probability distribution, so we‚Äôll follow Brants et al. (2007) in referring to it as S:\n Ô£±\n count(wi‚àíN+1 : i )\n count(wi‚àíN+1 : i‚àí1 ) if count(wi‚àíN+1 : i ) > 0\n Ô£≤\n S(wi |wi‚àíN+1 : i‚àí1 ) = (3.31)\n Œª S(wi |wi‚àíN+2 : i‚àí1 ) otherwise\n Ô£≥\n\n The backoff terminates in the unigram, which has score S(w) = count(w)\n N . Brants et al.\n (2007) find that a value of 0.4 worked well for Œª .\n\n We introduced perplexity in Section 3.3 as a way to evaluate n-gram models on\n a test set. A better n-gram model is one that assigns a higher probability to the\n test data, and perplexity is a normalized version of the probability of the test set.\n The perplexity measure actually arises from the information-theoretic concept of\n cross-entropy, which explains otherwise mysterious properties of perplexity (why\n Entropy the inverse probability, for example?) and its relationship to entropy. Entropy is a\n measure of information. Given a random variable X ranging over whatever we are\n predicting (words, letters, parts of speech), the set of which we‚Äôll call œá, and with a\n particular probability function, call it p(x), the entropy of the random variable X is:\n X\n H(X) = ‚àí p(x) log2 p(x) (3.32)\n x‚ààœá\n\n The log can, in principle, be computed in any base. If we use log base 2, the\n resulting value of entropy will be measured in bits.\n One intuitive way to think about entropy is as a lower bound on the number of\n bits it would take to encode a certain decision or piece of information in the optimal\n coding scheme. Consider an example from the standard information theory textbook\n Cover and Thomas (1991). Imagine that we want to place a bet on a horse race but\n it is too far to go all the way to Yonkers Racetrack, so we‚Äôd like to send a short\n message to the bookie to tell him which of the eight horses to bet on. One way to\n encode this message is just to use the binary representation of the horse‚Äôs number\n as the code; thus, horse 1 would be 001, horse 2 010, horse 3 011, and so on, with\n horse 8 coded as 000. If we spend the whole day betting and each horse is coded\n with 3 bits, on average we would be sending 3 bits per race.\n Can we do better? Suppose that the spread is the actual distribution of the bets\n placed and that we represent it as the prior probability of each horse as follows:\n 3.7 ‚Ä¢ A DVANCED : P ERPLEXITY ‚Äô S R ELATION TO E NTROPY 19\n\n 1 1\n Horse 1 2 Horse 5 64\n 1 1\n Horse 2 4 Horse 6 64\n 1 1\n Horse 3 8 Horse 7 64\n 1 1\n Horse 4 16 Horse 8 64\n\n The entropy of the random variable X that ranges over horses gives us a lower\n bound on the number of bits and is\n i=8\n X\n H(X) = ‚àí p(i) log2 p(i)\n i=1\n = ‚àí 12 log2 12 ‚àí 14 log2 41 ‚àí 18 log2 18 ‚àí 161 log2 161 ‚àí4( 641 log2 641 )\n = 2 bits (3.33)\n\n A code that averages 2 bits per race can be built with short encodings for more\n probable horses, and longer encodings for less probable horses. For example, we\n could encode the most likely horse with the code 0, and the remaining horses as 10,\n then 110, 1110, 111100, 111101, 111110, and 111111.\n What if the horses are equally likely? We saw above that if we used an equallength binary code for the horse numbers, each horse took 3 bits to code, so the\n average was 3. Is the entropy the same? In this case each horse would have a\n probability of 18 . The entropy of the choice of horses is then\n\n i=8\n X 1 1 1\n H(X) = ‚àí log2 = ‚àí log2 = 3 bits (3.34)\n 8 8 8\n i=1\n\n Until now we have been computing the entropy of a single variable. But most of\n what we will use entropy for involves sequences. For a grammar, for example, we\n will be computing the entropy of some sequence of words W = {w1 , w2 , . . . , wn }.\n One way to do this is to have a variable that ranges over sequences of words. For\n example we can compute the entropy of a random variable that ranges over all sequences of words of length n in some language L as follows:\n X\n H(w1 , w2 , . . . , wn ) = ‚àí p(w1 : n ) log p(w1 : n ) (3.35)\n w1 : n ‚ààL\n\nentropy rate We could define the entropy rate (we could also think of this as the per-word\n entropy) as the entropy of this sequence divided by the number of words:\n\n 1 1 X\n H(w1 : n ) = ‚àí p(w1 : n ) log p(w1 : n ) (3.36)\n n n\n w1 : n ‚ààL\n\n But to measure the true entropy of a language, we need to consider sequences of\n infinite length. If we think of a language as a stochastic process L that produces a\n sequence of words, and allow W to represent the sequence of words w1 , . . . , wn , then\n L‚Äôs entropy rate H(L) is defined as\n\n H(L) = lim H(w1 : n )\n n‚Üí‚àû n\n 1X\n = ‚àí lim p(w1 : n ) log p(w1 : n ) (3.37)\n n‚Üí‚àû n\n W ‚ààL\n20 C HAPTER 3 ‚Ä¢ N- GRAM L ANGUAGE M ODELS\n\n The Shannon-McMillan-Breiman theorem (Algoet and Cover 1988, Cover and Thomas\n 1991) states that if the language is regular in certain ways (to be exact, if it is both\n stationary and ergodic),\n H(L) = lim ‚àí log p(w1 : n ) (3.38)\n n‚Üí‚àû n\n\n That is, we can take a single sequence that is long enough instead of summing over\n all possible sequences. The intuition of the Shannon-McMillan-Breiman theorem\n is that a long-enough sequence of words will contain in it many other shorter sequences and that each of these shorter sequences will reoccur in the longer sequence\n according to their probabilities.\n Stationary A stochastic process is said to be stationary if the probabilities it assigns to a\n sequence are invariant with respect to shifts in the time index. In other words, the\n probability distribution for words at time t is the same as the probability distribution\n at time t + 1. Markov models, and hence n-grams, are stationary. For example, in\n a bigram, Pi is dependent only on Pi‚àí1 . So if we shift our time index by x, Pi+x is\n still dependent on Pi+x‚àí1 . But natural language is not stationary, since as we show\n in Appendix D, the probability of upcoming words can be dependent on events that\n were arbitrarily distant and time dependent. Thus, our statistical models only give\n an approximation to the correct distributions and entropies of natural language.\n To summarize, by making some incorrect but convenient simplifying assumptions, we can compute the entropy of some stochastic process by taking a very long\n sample of the output and computing its average log probability.\n cross-entropy Now we are ready to introduce cross-entropy. The cross-entropy is useful when\n we don‚Äôt know the actual probability distribution p that generated some data. It\n allows us to use some m, which is a model of p (i.e., an approximation to p). The\n cross-entropy of m on p is defined by\n 1X\n H(p, m) = lim ‚àí p(w1 , . . . , wn ) log m(w1 , . . . , wn ) (3.39)\n n‚Üí‚àû n\n W ‚ààL\n\n That is, we draw sequences according to the probability distribution p, but sum the\n log of their probabilities according to m.\n Again, following the Shannon-McMillan-Breiman theorem, for a stationary ergodic process:\n H(p, m) = lim ‚àí log m(w1 w2 . . . wn ) (3.40)\n n‚Üí‚àû n\n\n This means that, as for entropy, we can estimate the cross-entropy of a model m\n on some distribution p by taking a single sequence that is long enough instead of\n summing over all possible sequences.\n What makes the cross-entropy useful is that the cross-entropy H(p, m) is an upper bound on the entropy H(p). For any model m:\n\n H(p) ‚â§ H(p, m) (3.41)\n\n This means that we can use some simplified model m to help estimate the true entropy of a sequence of symbols drawn according to probability p. The more accurate\n m is, the closer the cross-entropy H(p, m) will be to the true entropy H(p). Thus,\n the difference between H(p, m) and H(p) is a measure of how accurate a model is.\n Between two models m1 and m2 , the more accurate model will be the one with the\n 3.8 ‚Ä¢ S UMMARY 21\n\n lower cross-entropy. (The cross-entropy can never be lower than the true entropy, so\n a model cannot err by underestimating the true entropy.)\n We are finally ready to see the relation between perplexity and cross-entropy\n as we saw it in Eq. 3.40. Cross-entropy is defined in the limit as the length of the\n observed word sequence goes to infinity. We approximate this cross-entropy by\n relying on a (sufficiently long) sequence of fixed length. This approximation to the\n cross-entropy of a model M = P(wi |wi‚àíN+1 : i‚àí1 ) on a sequence of words W is\n\n H(W ) = ‚àí log P(w1 w2 . . . wN ) (3.42)\n N\n perplexity The perplexity of a model P on a sequence of words W is now formally defined as\n 2 raised to the power of this cross-entropy:\n\n Perplexity(W ) = 2H(W )\n = P(w1 w2 . . . wN )‚àí N\n s\n = N\n P(w1 w2 . . . wN )\n\n This chapter introduced language modeling via the n-gram model, a classic model\n that allows us to introduce many of the basic concepts in language modeling.\n ‚Ä¢ Language models offer a way to assign a probability to a sentence or other\n sequence of words or tokens, and to predict a word or token from preceding\n words or tokens.\n ‚Ä¢ N-grams are perhaps the simplest kind of language model. They are Markov\n models that estimate words from a fixed window of previous words. N-gram\n models can be trained by counting in a training corpus and normalizing the\n counts (the maximum likelihood estimate).\n ‚Ä¢ N-gram language models can be evaluated on a test set using perplexity.\n ‚Ä¢ The perplexity of a test set according to a language model is a function of\n the probability of the test set: the inverse test set probability according to the\n model, normalized by the length.\n ‚Ä¢ Sampling from a language model means to generate some sentences, choosing each sentence according to its likelihood as defined by the model.\n ‚Ä¢ Smoothing algorithms provide a way to estimate probabilities for events that\n were unseen in training. Commonly used smoothing algorithms for n-grams\n include add-1 smoothing, or rely on lower-order n-gram counts through interpolation.\n\nHistorical Notes\n The underlying mathematics of the n-gram was first proposed by Markov (1913),\n who used what are now called Markov chains (bigrams and trigrams) to predict\n whether an upcoming letter in Pushkin‚Äôs Eugene Onegin would be a vowel or a consonant. Markov classified 20,000 letters as V or C and computed the bigram and\n22 C HAPTER 3 ‚Ä¢ N- GRAM L ANGUAGE M ODELS\n\n trigram probability that a given letter would be a vowel given the previous one or\n two letters. Shannon (1948) applied n-grams to compute approximations to English\n word sequences. Based on Shannon‚Äôs work, Markov models were commonly used in\n engineering, linguistic, and psychological work on modeling word sequences by the\n 1950s. In a series of extremely influential papers starting with Chomsky (1956) and\n including Chomsky (1957) and Miller and Chomsky (1963), Noam Chomsky argued\n that ‚Äúfinite-state Markov processes‚Äù, while a possibly useful engineering heuristic,\n were incapable of being a complete cognitive model of human grammatical knowledge. These arguments led many linguists and computational linguists to ignore\n work in statistical modeling for decades.\n The resurgence of n-gram language models came from Fred Jelinek and colleagues at the IBM Thomas J. Watson Research Center, who were influenced by\n Shannon, and James Baker at CMU, who was influenced by the prior, classified\n work of Leonard Baum and colleagues on these topics at labs like the US Institute\n for Defense Analyses (IDA) after they were declassified. Independently these two\n labs successfully used n-grams in their speech recognition systems at the same time\n (Baker 1975b, Jelinek et al. 1975, Baker 1975a, Bahl et al. 1983, Jelinek 1990). The\n terms ‚Äúlanguage model‚Äù and ‚Äúperplexity‚Äù were first used for this technology by the\n IBM group. Jelinek and his colleagues used the term language model in a pretty\n modern way, to mean the entire set of linguistic influences on word sequence probabilities, including grammar, semantics, discourse, and even speaker characteristics,\n rather than just the particular n-gram model itself.\n Add-one smoothing derives from Laplace‚Äôs 1812 law of succession and was first\n applied as an engineering solution to the zero frequency problem by Jeffreys (1948)\n based on an earlier Add-K suggestion by Johnson (1932). Problems with the addone algorithm are summarized in Gale and Church (1994).\n A wide variety of different language modeling and smoothing techniques were\n proposed in the 80s and 90s, including Good-Turing discounting‚Äîfirst applied to the\n n-gram smoothing at IBM by Katz (NaÃÅdas 1984, Church and Gale 1991)‚Äî Wittenclass-based\n n-gram Bell discounting (Witten and Bell, 1991), and varieties of class-based n-gram models that used information about word classes. Starting in the late 1990s, Chen and\n Goodman performed a number of carefully controlled experiments comparing different algorithms and parameters (Chen and Goodman 1999, Goodman 2006, inter\n alia). They showed the advantages of Modified Interpolated Kneser-Ney, which\n became the standard baseline for n-gram language modeling around the turn of the\n century, especially because they showed that caches and class-based models provided only minor additional improvement. SRILM (Stolcke, 2002) and KenLM\n (Heafield 2011, Heafield et al. 2013) are publicly available toolkits for building ngram language models.\n Large language models are based on neural networks rather than n-grams, enabling them to solve the two major problems with n-grams: (1) the number of parameters increases exponentially as the n-gram order increases, and (2) n-grams have no\n way to generalize from training examples to test set examples unless they use identical words. Neural language models instead project words into a continuous space\n in which words with similar contexts have similar representations. We‚Äôll introduce\n transformer-based large language models in Chapter 8, along the way introducing\n feedforward language models (Bengio et al. 2006, Schwenk 2007) in Chapter 6 and\n recurrent language models (Mikolov, 2012) in Chapter 13.\n E XERCISES 23\n\nExercises\n Now write out all the non-zero trigram probabilities for the I am Sam corpus\n on page 4.\n probabilities, one using Fig. 3.2 and the ‚Äòuseful probabilities‚Äô just below it on\n page 6, and another using the add-1 smoothed table in Fig. 3.7. Assume the\n additional add-1 smoothed probabilities P(i|<s>) = 0.19 and P(</s>|food) =\n 0.40.\n unsmoothed or smoothed? Explain why.\n <s> I am Sam </s>\n <s> Sam I am </s>\n <s> I am Sam </s>\n <s> I do not like green eggs and Sam </s>\n Using a bigram language model with add-one smoothing, what is P(Sam |\n am)? Include <s> and </s> in your counts just like any other token.\n grammar on the following training corpus without using the end-symbol </s>:\n <s> a b\n <s> b b\n <s> b a\n <s> a a\n Demonstrate that your bigram model does not assign a single probability distribution across all sentence lengths by showing that the sum of the probability\n of the four possible 2 word sentences over the alphabet {a,b} is 1.0, and the\n sum of the probability of all possible 3 word sentences over the alphabet {a,b}\n is also 1.0.\n given corpus. The corpus contains V word types. Express a formula for estimating P(w3|w1,w2), where w3 is a word which follows the bigram (w1,w2),\n in terms of various n-gram counts and V. Use the notation c(w1,w2,w3) to\n denote the number of times that trigram (w1,w2,w3) occurs in the corpus, and\n so on for bigrams and unigrams.\n <s> I am Sam </s>\n <s> Sam I am </s>\n <s> I am Sam </s>\n <s> I do not like green eggs and Sam </s>\n If we use linear interpolation smoothing between a maximum-likelihood bigram model and a maximum-likelihood unigram model with Œª1 = 12 and Œª2 =\n 2 , what is P(Sam|am)? Include <s> and </s> in your counts just like any\n other token.\n24 C HAPTER 3 ‚Ä¢ N- GRAM L ANGUAGE M ODELS\n\n might use email text or newsgroups). Now compare the statistics of the two\n corpora. What are the differences in the most common unigrams between the\n two? How about interesting differences in bigrams?\n each of the other digits 1-9. Now we see the following test set: 0 0 0 0 0 3 0 0\n 0 0. What is the unigram perplexity?\n Exercises 25\n\nAlgoet, P. H. and T. M. Cover. 1988. A sandwich proof of Jelinek, F. and R. L. Mercer. 1980. Interpolated estimation\n the Shannon-McMillan-Breiman theorem. The Annals of of Markov source parameters from sparse data. In E. S.\n Probability, 16(2):899‚Äì909. Gelsema and L. N. Kanal, eds, Proceedings, Workshop\nBahl, L. R., F. Jelinek, and R. L. Mercer. 1983. A maxi- on Pattern Recognition in Practice, 381‚Äì397. North Holmum likelihood approach to continuous speech recogni- land.\n tion. IEEE Transactions on Pattern Analysis and Machine Jelinek, F., R. L. Mercer, and L. R. Bahl. 1975. Design of a\n Intelligence, 5(2):179‚Äì190. linguistic statistical decoder for the recognition of contin-\nBaker, J. K. 1975a. The DRAGON system ‚Äì An overview. uous speech. IEEE Transactions on Information Theory,\n IEEE Transactions on ASSP, ASSP-23(1):24‚Äì29. IT-21(3):250‚Äì256.\nBaker, J. K. 1975b. Stochastic modeling for automatic Johnson, W. E. 1932. Probability: deductive and inductive\n speech understanding. In D. R. Reddy, ed., Speech Recog- problems (appendix to). Mind, 41(164):421‚Äì423.\n nition. Academic Press. Jurafsky, D., C. Wooters, G. Tajchman, J. Segal, A. Stolcke,\nBengio, Y., H. Schwenk, J.-S. SeneÃÅcal, F. Morin, and J.-L. E. Fosler, and N. Morgan. 1994. The Berkeley restaurant\n Gauvain. 2006. Neural probabilistic language models. In project. ICSLP.\n Innovations in Machine Learning, 137‚Äì186. Springer. Jurgens, D., Y. Tsvetkov, and D. Jurafsky. 2017. Incorpo-\nBlodgett, S. L. and B. O‚ÄôConnor. 2017. Racial disparity in rating dialectal variability for socially equitable language\n natural language processing: A case study of social media identification. ACL.\n African-American English. FAT/ML Workshop, KDD. Kane, S. K., M. R. Morris, A. Paradiso, and J. Campbell.\nBrants, T., A. C. Popat, P. Xu, F. J. Och, and J. Dean. 2017. ‚Äúat times avuncular and cantankerous, with the\n 2007. Large language models in machine translation. reflexes of a mongoose‚Äù: Understanding self-expression\n EMNLP/CoNLL. through augmentative and alternative communication de-\nChen, S. F. and J. Goodman. 1999. An empirical study of vices. CSCW.\n smoothing techniques for language modeling. Computer King, S. 2020. From African American Vernacular English\n Speech and Language, 13:359‚Äì394. to African American Language: Rethinking the study of\nChomsky, N. 1956. Three models for the description of race and language in African Americans‚Äô speech. Annual\n language. IRE Transactions on Information Theory, Review of Linguistics, 6:285‚Äì300.\n 2(3):113‚Äì124. Lin, Y., J.-B. Michel, E. Aiden Lieberman, J. Orwant,\nChomsky, N. 1957. Syntactic Structures. Mouton. W. Brockman, and S. Petrov. 2012. Syntactic annotations\n for the Google books NGram corpus. ACL.\nChurch, K. W. and W. A. Gale. 1991. A comparison of the\n enhanced Good-Turing and deleted estimation methods Liu, J., S. Min, L. Zettlemoyer, Y. Choi, and H. Hajishirzi.\n for estimating probabilities of English bigrams. Com- 2024. Infini-gram: Scaling unbounded n-gram language\n puter Speech and Language, 5:19‚Äì54. models to a trillion tokens. ArXiv preprint.\nCover, T. M. and J. A. Thomas. 1991. Elements of Informa- Markov, A. A. 1913. Essai d‚Äôune recherche statistique sur\n tion Theory. Wiley. le texte du roman ‚ÄúEugene Onegin‚Äù illustrant la liaison\n des epreuve en chain (‚ÄòExample of a statistical investiga-\nDavies, M. 2020. The Corpus of Contemporary Amertion of the text of ‚ÄúEugene Onegin‚Äù illustrating the deican English (COCA): One billion words, 1990-2019.\n pendence between samples in chain‚Äô). Izvistia Imperahttps://www.english-corpora.org/coca/.\n torskoi Akademii Nauk (Bulletin de l‚ÄôAcadeÃÅmie ImpeÃÅriale\nFranz, A. and T. Brants. 2006. All our n-gram are des Sciences de St.-PeÃÅtersbourg), 7:153‚Äì162.\n belong to you. https://research.google/blog/\n Mikolov, T. 2012. Statistical language models based on neuall-our-n-gram-are-belong-to-you/.\n ral networks. Ph.D. thesis, Brno University of Technol-\nGale, W. A. and K. W. Church. 1994. What is wrong with ogy.\n adding one? In N. Oostdijk and P. de Haan, eds, Corpus-\nBased Research into Language, 189‚Äì198. Rodopi. Miller, G. A. and N. Chomsky. 1963. Finitary models of language users. In R. D. Luce, R. R. Bush, and E. Galanter,\nGoodman, J. 2006. A bit of progress in language model- eds, Handbook of Mathematical Psychology, volume II,\n ing: Extended version. Technical Report MSR-TR-2001- 419‚Äì491. John Wiley.\n 72, Machine Learning and Applied Statistics Group, Microsoft Research, Redmond, WA. Miller, G. A. and J. A. Selfridge. 1950. Verbal context and\n the recall of meaningful material. American Journal of\nHeafield, K. 2011. KenLM: Faster and smaller language Psychology, 63:176‚Äì185.\n model queries. Workshop on Statistical Machine Translation. NaÃÅdas, A. 1984. Estimation of probabilities in the language\n model of the IBM speech recognition system. IEEE\nHeafield, K., I. Pouzyrevsky, J. H. Clark, and P. Koehn. 2013.\n Transactions on ASSP, 32(4):859‚Äì861.\n Scalable modified Kneser-Ney language model estimation. ACL. Schwenk, H. 2007. Continuous space language models.\n Computer Speech & Language, 21(3):492‚Äì518.\nJeffreys, H. 1948. Theory of Probability, 2nd edition. Clarendon Press. Section 3.23. Shannon, C. E. 1948. A mathematical theory of communication. Bell System Technical Journal, 27(3):379‚Äì423.\nJelinek, F. 1990. Self-organized language modeling for\n Continued in the following volume.\n speech recognition. In A. Waibel and K.-F. Lee, eds,\n Readings in Speech Recognition, 450‚Äì506. Morgan Kauf- Stolcke, A. 1998. Entropy-based pruning of backoff lanmann. Originally distributed as IBM technical report in guage models. Proc. DARPA Broadcast News Transcrip-\n1985. tion and Understanding Workshop.\n26 Chapter 3 ‚Ä¢ N-gram Language Models\n\nStolcke, A. 2002. SRILM ‚Äì an extensible language modeling\n toolkit. ICSLP.\nTrnka, K., D. Yarrington, J. McCaw, K. F. McCoy, and\n C. Pennington. 2007. The effects of word prediction on\n communication rate for AAC. NAACL-HLT.\nWitten, I. H. and T. C. Bell. 1991. The zero-frequency problem: Estimating the probabilities of novel events in adaptive text compression. IEEE Transactions on Information\n Theory, 37(4):1085‚Äì1094.\n",
    "file_path": "/Users/colinsidberry/nlp-textbook-rag/data/extracted_text/n-gram.txt",
    "file_size_kb": 72.18
  },
  {
    "id": "ee3fc8dd2dc33a2a",
    "source": "nlp_textbook",
    "chapter": "Logistic Regression and Text 4 Classification",
    "filename": "logistic-regression.txt",
    "content": "rights reserved. Draft of August 24, 2025.\n\nCHAPTER\n\n Logistic Regression and Text\n4 Classification\n En sus remotas paÃÅginas estaÃÅ escrito que los animales se dividen en:\n a. pertenecientes al Emperador h. incluidos en esta clasificacioÃÅn\n b. embalsamados i. que se agitan como locos\n c. amaestrados j. innumerables\n d. lechones k. dibujados con un pincel finƒ±ÃÅsimo de pelo de camello\n e. sirenas l. etceÃÅtera\n f. fabulosos m. que acaban de romper el jarroÃÅn\n g. perros sueltos n. que de lejos parecen moscas\n Borges (1964)\n\n Classification lies at the heart of language processing and intelligence. Recognizing a letter, a word, or a face, sorting mail, assigning grades to homeworks; these\n are all examples of assigning a category to an input. The challenges of classification\n were famously highlighted by the fabulist Jorge Luis Borges (1964), who imagined\n an ancient mythical encyclopedia that classified animals into:\n (a) those that belong to the Emperor, (b) embalmed ones, (c) those that\n are trained, (d) suckling pigs, (e) mermaids, (f) fabulous ones, (g) stray\n dogs, (h) those that are included in this classification, (i) those that\n tremble as if they were mad, (j) innumerable ones, (k) those drawn with\n a very fine camel‚Äôs hair brush, (l) others, (m) those that have just broken\n a flower vase, (n) those that resemble flies from a distance.\n Luckily, the classes we use for language processing are easier to define than\n those of Borges. In this chapter we introduce the logistic regression algorithm for\n text\n categorization classification, and apply it to text categorization, the task of assigning a label or\n category to a text or document. We‚Äôll focus on one text categorization task, sentisentiment\n analysis ment analysis, the categorization of sentiment, the positive or negative orientation\n that a writer expresses toward some object. A review of a movie, book, or product\n expresses the author‚Äôs sentiment toward the product, while an editorial or political\n text expresses sentiment toward an action or candidate. Extracting sentiment is thus\n relevant for fields from marketing to politics.\n For the binary task of labeling a text as indicating positive or negative stance,\n words (like awesome and love, or awful and ridiculously are very informative, as we\n can see from these sample extracts from movie/restaurant reviews:\n + ...awesome caramel sauce and sweet toasty almonds. I love this place!\n ‚àí ...awful pizza and ridiculously overpriced...\nspam detection There are many text classification tasks. In spam detection we assign an email\n language id to one of the two classes spam or not-spam. Language id is the task of determinauthorship ing what language a text is written in, while authorship attribution is the task of\n attribution\n determining a text‚Äôs author, relevant to both humanistic and forensic analysis.\n2 C HAPTER 4 ‚Ä¢ L OGISTIC R EGRESSION\n\n But what makes classification so important is that language modeling can also\n be viewed as classification: each word can be thought of as a class, and so predicting\n the next word is classifying the context-so-far into a class for each next word. As\n we‚Äôll see, this intuition underlies large language models.\n The algorithm for classification we introduce in this chapter, logistic regression,\n is equally important, in a number of ways. First, logistic regression has a close\n relationship with neural networks. As we will see in Chapter 6, a neural network\n can be viewed as a series of logistic regression classifiers stacked on top of each\n other. Second, logistic regression introduces ideas that are fundamental to neural\n sigmoid networks and language models, like the sigmoid and softmax functions, the logit,\n softmax and the key gradient descent algorithm for learning. Finally, logistic regression is\n logit also one of the most important analytic tools in the social and natural sciences.\n\n The goal of classification is to take a single input (we call each input an observaobservation tion), extract some useful features or properties of the input, and thereby classify\n the observation into one of a set of discrete classes. We‚Äôll call the input x, and say\n that the output comes from a fixed set of output classes Y = {y1 , y2 , ..., yM }. Our goal\n hat is return a predicted class yÃÇ ‚àà Y . The hat or circumflex notation yÃÇ is used to refer to\n an estimated or predicted value. Sometimes you‚Äôll see the output classes referred to\n as the set C instead of Y .\n For sentiment analysis, the input x might be a review, or some other text. And\n the output set Y might be the set:\n {positive, negative}\n or the set\n {0, 1}\n For language id, the input might be a text that we need to know what language it was\n written in, and the output set Y is the set of languages, i.e.,\n Y = {Abkhaz, Ainu, Albanian, Amharic, ...Zulu, ZunÃÉi}\n There are many ways to do classification. One method is to use rules handwritten\n by humans. For example, we might have a rule like:\n If the word ‚Äò‚Äòlove‚Äô‚Äô appears in x, and it‚Äôs not preceded by the\n word ‚Äò‚Äòdon‚Äôt\", classify as positive\n Handwritten rules can be components of modern NLP systems, such as the handwritten lists of positive and negative words that can be used in sentiment analysis,\n as we‚Äôll see below. But rules can be fragile, as situations or data change over time,\n and for many tasks there are complex interactions between different features (like\n the example of negation with ‚Äúdon‚Äôt‚Äù in the rule above), so it can be quite hard for\n humans to come up with rules that are successful over many situations.\n Another method that we will introduce later is to ask a large language model (of\n the type we will introduce in Chapter 7) by prompting the model to give a label to\n some text. Prompting can be powerful, but again has weaknesses: language models\n often hallucinate, and may not be able to explain why they chose the class they did.\n supervised\n For these reasons the most common way to do classification is to use supermachine vised machine learning. Supervised machine learning is a paradigm in which, in\n learning\n 4.1 ‚Ä¢ M ACHINE LEARNING AND CLASSIFICATION 3\n\naddition to the input and the set of output classes, we have a labeled training set\nand a learning algorithm. We talked about training sets in Chapter 3 as a locus for\ncomputing n-gram statistics. But in supervised machine learning the training set is\nlabeled, meaning that it contains a set of input observations, each observation associated with the correct output (a ‚Äòsupervision signal‚Äô). We can generally refer to a\ntraining set of m input/output pairs, where each input x is a text, in the case of text\nclassification, and each is hand-labeled with an associated class (the correct label):.\n\n training set: {(x(1) , y(1) ), (x(2) , y(2) ), . . . , (x(m) , y(m) )} (4.1)\n\nWe‚Äôll use superscripts in parentheses to refer to individual observations or instances\nin the training set. So for sentiment classification, a training set might be a set of\nsentences or other texts, each with their correct sentiment label.\n Our goal is to learn from this training set a classifier that is capable of mapping\nfrom a new input x to its correct class y ‚àà Y . It does this by learn to find features in\nthese training sentences (perhaps words like ‚Äúawesome‚Äù or ‚Äúawful‚Äù). Probabilistic\nclassifiers are the subset of machine learning classifiers that in addition to giving an\nanswer (which class is this observation in?), additionally will tell us the probability\nof the observation being in the class. This full distribution over the classes can be\nuseful information for downstream decisions; avoiding making discrete decisions\nearly on can be useful when combining systems.\n There are many algorithms for achieving this supervised machine learning task,\n(naive Bayes, support vector machines, neural networks, fine-tuned language models), but logistic regression has the advantages we discussed above and so we‚Äôll\nintroduce it! Any machine learning classifier thus has four components:\n 1. A feature representation of the input. For each input observation x(i) , this\n will be a vector of features [x1 , x2 , ..., xn ]. We will generally refer to feature\n ( j)\n i for input x( j) as xi , sometimes simplified as xi , but we will also see the\n notation fi , fi (x), or, for multiclass classification, fi (c, x).\n 2. A classification function that computes yÃÇ, the estimated class, via P(y|x). We\n will introduce the sigmoid and softmax tools for classification.\n 3. An objective function that we want to optimize for learning, usually involving minimizing a loss function corresponding to error on training examples.\n We will introduce the cross-entropy loss function.\n 4. An algorithm for optimizing the objective function. We introduce the stochastic gradient descent algorithm.\n At the highest level, logistic regression, and really any supervised machine learning classifier, has two phases\n training: We train the system (in the case of logistic regression that means training the weights w and b, introduced below) using stochastic gradient descent\n and the cross-entropy loss.\n test: Given a test example x we compute the probability P(yi |x) of each class yi ,\n and return the higher probability label y = 1 or y = 0.\n Logistic regression can be used to classify an observation into one of two classes\n(like ‚Äòpositive sentiment‚Äô and ‚Äònegative sentiment‚Äô), or into one of many classes.\nBecause the mathematics for the two-class case is simpler, we‚Äôll first describe this\nspecial case of logistic regression in the next few sections, beginning with the sigmoid function, and then turn to multinomial logistic regression for more than two\nclasses and the use of the softmax function in Section 4.4.\n4 C HAPTER 4 ‚Ä¢ L OGISTIC R EGRESSION\n\n The goal of binary logistic regression is to train a classifier that can make a binary\n decision about the class of a new input observation. Here we introduce the sigmoid\n classifier that will help us make this decision.\n Consider a single input observation x, which we will represent by a vector of\n features [x1 , x2 , ..., xn ]. (We‚Äôll show sample features in the next subsection.) The\n classifier output y can be 1 (meaning the observation is a member of the class) or\n 0 (the observation is not a member of the class). We want to know the probability\n P(y = 1|x) that this observation is a member of the class. So perhaps the decision\n is ‚Äúpositive sentiment‚Äù versus ‚Äúnegative sentiment‚Äù, the features represent counts of\n words in a document, P(y = 1|x) is the probability that the document has positive\n sentiment, and P(y = 0|x) is the probability that the document has negative sentiment.\n Logistic regression solves this task by learning, from a training set, a vector of\n weights and a bias term. Each weight wi is a real number, and is associated with one\n of the input features xi . The weight wi represents how important that input feature\n is to the classification decision, and can be positive (providing evidence that the instance being classified belongs in the positive class) or negative (providing evidence\n that the instance being classified belongs in the negative class). Thus we might\n expect in a sentiment task the word awesome to have a high positive weight, and\n bias term abysmal to have a very negative weight. The bias term, also called the intercept, is\n intercept another real number that‚Äôs added to the weighted inputs.\n To make a decision on a test instance‚Äîafter we‚Äôve learned the weights in training‚Äî\n the classifier first multiplies each xi by its weight wi , sums up the weighted features,\n and adds the bias term b. The resulting single number z expresses the weighted sum\n of the evidence for the class.\n n\n !\n X\n z = wi xi + b (4.2)\n i=1\n\n dot product In the rest of the book we‚Äôll represent such sums using the dot product notation\n from linear algebra. The dot product of two vectors a and b, written as a ¬∑ b, is the\n sum of the products of the corresponding elements of each vector. (Notice that we\n represent vectors using the boldface notation b). Thus the following is an equivalent\n formation to Eq. 4.2:\n z = w¬∑x+b (4.3)\n\n But note that nothing in Eq. 4.3 forces z to be a legal probability, that is, to lie\n between 0 and 1. In fact, since weights are real-valued, the output might even be\n negative; z ranges from ‚àí‚àû to ‚àû.\n sigmoid To create a probability, we‚Äôll pass z through the sigmoid function, œÉ (z). The\n sigmoid function (named because it looks like an s) is also called the logistic funclogistic tion, and gives logistic regression its name. The sigmoid has the following equation,\n function\n shown graphically in Fig. 4.1:\n 1 1\n œÉ (z) = ‚àíz\n = (4.4)\n 1+e 1 + exp (‚àíz)\n (For the rest of the book, we‚Äôll use the notation exp(x) to mean ex .) The sigmoid\n has a number of advantages; it takes a real-valued number and maps it into the range\n 4.3 ‚Ä¢ C LASSIFICATION WITH L OGISTIC R EGRESSION 5\n\n (0, 1). It is nearly linear around 0 but outlier values get squashed toward 0 or 1.\n\n (0, 1), which is just what we want for a probability. Because it is nearly linear around\n 0 but flattens toward the ends, it tends to squash outlier values toward 0 or 1. And\n it‚Äôs differentiable, which as we‚Äôll see in Section 4.15 will be handy for learning.\n We‚Äôre almost there. If we apply the sigmoid to the sum of the weighted features,\n we get a number between 0 and 1. To make it a probability, we just need to make\n sure that the two cases, P(y = 1) and P(y = 0), sum to 1. We can do this as follows:\n P(y = 1) = œÉ (w ¬∑ x + b)\n 1 + exp (‚àí(w ¬∑ x + b))\n\n P(y = 0) = 1 ‚àí œÉ (w ¬∑ x + b)\n = 1‚àí\n 1 + exp (‚àí(w ¬∑ x + b))\n exp (‚àí(w ¬∑ x + b))\n = (4.5)\n 1 + exp (‚àí(w ¬∑ x + b))\n The sigmoid function has the property\n 1 ‚àí œÉ (x) = œÉ (‚àíx) (4.6)\n\n so we could also have expressed P(y = 0) as œÉ (‚àí(w ¬∑ x + b)).\n Finally, one terminological point. The input to the sigmoid function, the score\n logit z = w ¬∑ x + b from Eq. 4.3, is often called the logit. This is because the logit function\n p\n is the inverse of the sigmoid. The logit function is the log of the odds ratio 1‚àíp :\n p\n logit(p) = œÉ ‚àí1 (p) = ln (4.7)\n 1‚àí p\n Using the term logit for z is a way of reminding us that by using the sigmoid to turn\n z (which ranges from ‚àí‚àû to ‚àû) into a probability, we are implicitly interpreting z as\n not just any real-valued number, but as specifically a log odds.\n\n The sigmoid function from the prior section thus gives us a way to take an instance\n x and compute the probability P(y = 1|x).\n How do we make a decision about which class to apply to a test instance x? For\n a given x, we say yes if the probability P(y = 1|x) is more than .5, and no otherwise.\n decision\n boundary We call .5 the decision boundary:\n6 C HAPTER 4 ‚Ä¢ L OGISTIC R EGRESSION\n \u001a\n 1 if P(y = 1|x) > 0.5\n decision(x) =\n 0 otherwise\n\n Let‚Äôs have some examples of applying logistic regression as a classifier for language\n tasks.\n\n 4.3.1 Sentiment Classification\n Suppose we are doing binary sentiment classification on movie review text, and\n we would like to know whether to assign the sentiment class + or ‚àí to a review\n document doc. We‚Äôll represent each input observation by the 6 features x1 . . . x6 of\n the input shown in the following table; Fig. 4.2 shows features in a sample mini test\n document.\n\n Var Definition Value in Fig. 4.2\n x1 count(positive lexicon words ‚àà doc) 3\n x2 count(negative\n \u001a lexicon words ‚àà doc) 2\n 1 if ‚Äúno‚Äù ‚àà doc\n x3 1\n 0 otherwise\n x4 count(1st\n \u001a and 2nd pronouns ‚àà doc) 3\n 1 if ‚Äú!‚Äù ‚àà doc\n x5 0\n 0 otherwise\n x6 ln(word+punctuation count of doc) ln(66) = 4.19\n\n x2=2\n x3=1\n It's hokey . There are virtually no surprises , and the writing is second-rate .\n So why was it so enjoyable ? For one thing , the cast is\n great . Another nice touch is the music . I was overcome with the urge to get off\n the couch and start dancing . It sucked me in , and it'll do the same to you .\n x4=3\n x1=3 x5=0 x6=4.19\n\n Let‚Äôs assume for the moment that we‚Äôve already learned a real-valued weight\n for each of these features, and that the 6 weights corresponding to the 6 features\n are [2.5, ‚àí5.0, ‚àí1.2, 0.5, 2.0, 0.7], while b = 0.1. (We‚Äôll discuss in the next section\n how the weights are learned.) The weight w1 , for example indicates how important\n a feature the number of positive lexicon words (great, nice, enjoyable, etc.) is to\n a positive sentiment decision, while w2 tells us the importance of negative lexicon\n words. Note that w1 = 2.5 is positive, while w2 = ‚àí5.0, meaning that negative words\n are negatively associated with a positive sentiment decision, and are about twice as\n important as positive words.\n Given these 6 features and the input review x, P(+|x) and P(‚àí|x) can be com-\n4.3 ‚Ä¢ C LASSIFICATION WITH L OGISTIC R EGRESSION 7\n\n puted using Eq. 4.5:\n P(+|x) = P(y = 1|x) = œÉ (w ¬∑ x + b)\n = œÉ ([2.5, ‚àí5.0, ‚àí1.2, 0.5, 2.0, 0.7] ¬∑ [3, 2, 1, 3, 0, 4.19] + 0.1)\n = œÉ (.833)\n = 0.70 (4.8)\n P(‚àí|x) = P(y = 0|x) = 1 ‚àí œÉ (w ¬∑ x + b)\n = 0.30\n\n 4.3.2 Other classification tasks and features\n Logistic regression is applied to all sorts of NLP tasks, and any property of the input\n period\ndisambiguation can be a feature. Consider the task of period disambiguation: deciding if a period\n is the end of a sentence or part of a word, by classifying each period into one of two\n classes, EOS (end-of-sentence) and not-EOS. We might use features like x1 below\n expressing that the current word is lower case, perhaps with a positive weight. Or a\n feature expressing that the current word is in our abbreviations dictionary (‚ÄúProf.‚Äù),\n perhaps with a negative weight. A feature can also express a combination of properties. For example a period following an upper case word is likely to be an EOS, but\n if the word itself is St. and the previous word is capitalized then the period is likely\n part of a shortening of the word street following a street name.\n \u001a\n 1 if ‚ÄúCase(wi ) = Lower‚Äù\n x1 =\n 0 otherwise\n \u001a\n 1 if ‚Äúwi ‚àà AcronymDict‚Äù\n x2 =\n 0 otherwise\n \u001a\n 1 if ‚Äúwi = St. & Case(wi‚àí1 ) = Upper‚Äù\n x3 =\n 0 otherwise\n Designing versus learning features: In classic models, features are designed by\n hand by examining the training set with an eye to linguistic intuitions and literature,\n supplemented by insights from error analysis on the training set of an early version\n feature of a system. We can also consider feature interactions, complex features that are\n interactions\n combinations of more primitive features. We saw such a feature for period disambiguation above, where a period on the word St. was less likely to be the end of the\n sentence if the previous word was capitalized. Features can be created automatically\n feature\n templates via feature templates, abstract specifications of features. For example a bigram\n template for period disambiguation might create a feature for every pair of words\n that occurs before a period in the training set. Thus the feature space is sparse, since\n we only have to create a feature if that n-gram exists in that position in the training\n set. The feature is generally created as a hash from the string descriptions. A user\n description of a feature as, ‚Äúbigram(American breakfast)‚Äù is hashed into a unique\n integer i that becomes the feature number fi .\n It should be clear from the prior paragraph that designing features by hand requires extensive human effort. For this reason, recent NLP systems avoid handdesigned features and instead focus on representation learning: ways to learn features automatically in an unsupervised way from the input. We‚Äôll introduce methods\n for representation learning in Chapter 5 and Chapter 6.\n Scaling input features: When different input features have extremely different\n ranges of values, it‚Äôs common to rescale them so they have comparable ranges. We\n8 C HAPTER 4 ‚Ä¢ L OGISTIC R EGRESSION\n\n standardize standardize input values by centering them to result in a zero mean and a standard\n z-score deviation of one (this transformation is sometimes called the z-score). That is, if ¬µi\n is the mean of the values of feature xi across the m observations in the input dataset,\n and œÉi is the standard deviation of the values of features xi across the input dataset,\n we can replace each feature xi by a new feature xi0 computed as follows:\n v\n m\n u 1 m \u0010 ( j)\n u X \u00112\n 1 X ( j)\n ¬µi = xi œÉi = t xi ‚àí ¬µi\n m m\n j=1 j=1\n xi ‚àí ¬µi\n xi0 = (4.9)\n œÉi\n\n normalize Alternatively, we can normalize the input features values to lie between 0 and 1:\n\n xi ‚àí min(xi )\n xi0 = (4.10)\n max(xi ) ‚àí min(xi )\n\n Having input data with comparable range is useful when comparing values across\n features. Data scaling is especially important in large neural networks, since it helps\n speed up gradient descent.\n\n 4.3.3 Processing many examples at once\n We‚Äôve shown the equations for logistic regression for a single example. But in practice we‚Äôll of course want to process an entire test set with many examples. Let‚Äôs\n suppose we have a test set consisting of m test examples each of which we‚Äôd like to\n classify. We‚Äôll continue to use the notation from page 3, in which a superscript value\n in parentheses refers to the example index in some set of data (either for training or\n for test). So in this case each test example x(i) has a feature vector x(i) , 1 ‚â§ i ‚â§ m.\n (As usual, we‚Äôll represent vectors and matrices in bold.)\n One way to compute each output value yÃÇ(i) is just to have a for-loop, and compute\n each test example one at a time:\n\n foreach x(i) in input [x(1) , x(2) , ..., x(m) ]\n y(i) = œÉ (w ¬∑ x(i) + b) (4.11)\n\n For the first 3 test examples, then, we would be separately computing the predicted yÃÇ(i) as follows:\n\n P(y(1) = 1|x(1) ) = œÉ (w ¬∑ x(1) + b)\n P(y(2) = 1|x(2) ) = œÉ (w ¬∑ x(2) + b)\n P(y(3) = 1|x(3) ) = œÉ (w ¬∑ x(3) + b)\n\n But it turns out that we can slightly modify our original equation Eq. 4.5 to do\n this much more efficiently. We‚Äôll use matrix arithmetic to assign a class to all the\n examples with one matrix operation!\n First, we‚Äôll pack all the input feature vectors for each input x into a single input\n matrix X, where each row i is a row vector consisting of the feature vector for input example x(i) (i.e., the vector x(i) ). Assuming each example has f features and\n 4.4 ‚Ä¢ M ULTINOMIAL LOGISTIC REGRESSION 9\n\n weights, X will therefore be a matrix of shape [m √ó f ], as follows:\n Ô£Æ (1) (1) (1)\n Ô£π\n x1 x2 . . . x f\n Ô£Ø (2) (2) (2) Ô£∫\n Ô£Øx\n 1 x2 . . . x f Ô£∫\n X = Ô£Ø Ô£Ø Ô£∫ (4.12)\n Ô£∞ x1(3) x2(3) . . . x(3)\n Ô£∫\n f\n Ô£ª\n ...\n\n Now if we introduce b as a vector of length m which consists of the scalar bias\n term b repeated m times, b = [b, b, ..., b], and yÃÇ = [yÃÇ(1) , yÃÇ(2) ..., yÃÇ(m) ] as the vector of\n outputs (one scalar yÃÇ(i) for each input x(i) and its feature vector x(i) ), and represent\n the weight vector w as a column vector, we can compute all the outputs with a single\n matrix multiplication and one addition:\n\n yÃÇ = œÉ (Xw + b) (4.13)\n\n You should convince yourself that Eq. 4.13 computes the same thing as our for-loop\n in Eq. 4.11. For example yÃÇ(1) , the first entry of the output vector y, will correctly be:\n (1) (1) (1)\n yÃÇ(1) = [x1 , x2 , ..., x f ] ¬∑ [w1 , w2 , ..., w f ] + b (4.14)\n\n Note that we had to reorder X and w from the order they appeared in in Eq. 4.5 to\n make the multiplications come out properly. Here is Eq. 4.13 again with the shapes\n shown:\n\n yÃÇ = œÉ (X w + b)\n (m √ó 1) (m √ó f ) ( f √ó 1) (m √ó 1) (4.15)\n\n Modern compilers and compute hardware can compute this matrix operation very\n efficiently, making the computation much faster, which becomes important when\n training or testing on very large datasets.\n Note by the way that we could have kept X and w in the original order (as\n yÃÇ = œÉ (wX + b)) if we had chosen to define X differently as a matrix of column\n vectors, one vector for each input example, instead of row vectors, and then it would\n have shape [ f √ó m]. But we conventionally represent inputs as rows.\n\n Sometimes we need more than two classes. Perhaps we might want to do 3-way\n sentiment classification (positive, negative, or neutral). Or we could be assigning\n some of the labels we will introduce in Chapter 17, like the part of speech of a word\n (choosing from 10, 30, or even 50 different parts of speech), or the named entity\n type of a phrase (choosing from tags like person, location, organization). Or, for\n large language models, we‚Äôll be predicting the next word out of the |V | possible\n multinomial\n words in the vocabulary, so it‚Äôs |V |-way classification.\n logistic In such cases we use multinomial logistic regression, also called softmax reregression\n gression (in older NLP literature you will sometimes see the name maxent classifier). In multinomial logistic regression we want to label each observation with a\n class k from a set of K classes, under the stipulation that only one of these classes is\n the correct one (sometimes called hard classification; an observation can not be in\n10 C HAPTER 4 ‚Ä¢ L OGISTIC R EGRESSION\n\n multiple classes). Let‚Äôs use the following representation: the output y for each input\n x will be a vector of length K. If class c is the correct class, we‚Äôll set yc = 1, and\n set all the other elements of y to be 0, i.e., yc = 1 and y j = 0 ‚àÄ j 6= c. A vector like\n this y, with one value=1 and the rest 0, is called a one-hot vector. The job of the\n classifier is to produce an estimate vector yÃÇ. For each class k, the value yÃÇk will be\n the classifier‚Äôs estimate of the probability P(yk = 1|x).\n\n 4.4.1 Softmax\n The multinomial logistic classifier uses a generalization of the sigmoid, called the\n softmax softmax function, to compute p(yk = 1|x). The softmax function takes a vector\n z = [z1 , z2 , ..., zK ] of K arbitrary values and maps them to a probability distribution,\n with each value in the range [0,1], and all the values summing to 1. Like the sigmoid,\n it is an exponential function.\n For a vector z of dimensionality K, the softmax is defined as:\n exp (zi )\n softmax(zi ) = PK 1‚â§i‚â§K (4.16)\n j=1 exp (z j )\n\n The softmax of an input vector z = [z1 , z2 , ..., zK ] is thus a vector itself:\n \" #\n exp (z1 ) exp (z2 ) exp (zK )\n softmax(z) = PK , PK , ..., PK (4.17)\n i=1 exp (zi ) i=1 exp (zi ) i=1 exp (zi )\n\n The denominator Ki=1 exp (zi ) is used to normalize all the values into probabilities.\n P\n Thus for example given a vector:\n z = [0.6, 1.1, ‚àí1.5, 1.2, 3.2, ‚àí1.1]\n the resulting (rounded) softmax(z) is\n [0.05, 0.09, 0.01, 0.1, 0.74, 0.01]\n Like the sigmoid, the softmax has the property of squashing values toward 0 or 1.\n Thus if one of the inputs is larger than the others, it will tend to push its probability\n toward 1, and suppress the probabilities of the smaller inputs.\n Finally, note that, just as for the sigmoid, we refer to z, the vector of scores that\n is the input to the softmax, as logits (see Eq. 4.7).\n\n 4.4.2 Applying softmax in logistic regression\n When we apply softmax for logistic regression, the input will (just as for the sigmoid) be the dot product between a weight vector w and an input vector x (plus a\n bias). But now we‚Äôll need separate weight vectors wk and bias bk for each of the K\n classes. The probability of each of our output classes yÃÇk can thus be computed as:\n\n exp (wk ¬∑ x + bk )\n P(yk = 1|x) = K\n (4.18)\n X\n exp (w j ¬∑ x + b j )\n j=1\n\n The form of Eq. 4.18 makes it seem that we would compute each output separately. Instead, it‚Äôs more common to set up the equation for more efficient computation by modern vector processing hardware. We‚Äôll do this by representing the\n 4.4 ‚Ä¢ M ULTINOMIAL LOGISTIC REGRESSION 11\n\n set of K weight vectors as a weight matrix W and a bias vector b. Each row k of\n W corresponds to the vector of weights wk . W thus has shape [K √ó f ], for K the\n number of output classes and f the number of input features. The bias vector b has\n one value for each of the K output classes. If we represent the weights in this way,\n we can compute yÃÇ, the vector of output probabilities for each of the K classes, by a\n single elegant equation:\n yÃÇ = softmax(Wx + b) (4.19)\n\n If you work out the matrix arithmetic, you can see that the estimated score of\n the first output class yÃÇ1 (before we take the softmax) will correctly turn out to be\n w1 ¬∑ x + b1 .\n One helpful interpretation of the weight matrix W is to see each row wk as a\nprototype prototype of class k. The weight vector wk that is learned represents the class as\n a kind of template. Since two vectors that are more similar to each other have a\n higher dot product with each other, the dot product acts as a similarity function.\n Logistic regression is thus learning an exemplar representation for each class, such\n that incoming vectors are assigned the class k they are most similar to from the K\n classes (Doumbouya et al., 2025).\n Fig. 4.3 shows the difference between binary and multinomial logistic regression\n by illustrating the weight vector versus weight matrix in the computation of the\n output class probabilities.\n\n 4.4.3 Features in Multinomial Logistic Regression\n Features in multinomial logistic regression act like features in binary logistic regression, with the difference mentioned above that we‚Äôll need separate weight vectors\n and biases for each of the K classes. Recall our binary exclamation point feature x5\n from page 6:\n \u001a\n 1 if ‚Äú!‚Äù ‚àà doc\n x5 =\n 0 otherwise\n In binary classification a positive weight w5 on a feature influences the classifier\n toward y = 1 (positive sentiment) and a negative weight influences it toward y = 0\n (negative sentiment) with the absolute value indicating how important the feature\n is. For multinomial logistic regression, by contrast, with separate weights for each\n class, a feature can be evidence for or against each individual class.\n In 3-way multiclass sentiment classification, for example, we must assign each\n document one of the 3 classes +, ‚àí, or 0 (neutral). Now a feature related to exclamation marks might have a negative weight for 0 documents, and a positive weight\n for + or ‚àí documents:\n\n Feature Definition\n \u001a w5,+ w5,‚àí w5,0\n 1 if ‚Äú!‚Äù ‚àà doc\n f5 (x) 3.5 3.1 ‚àí5.3\n 0 otherwise\n\n Because these feature weights are dependent both on the input text and the output\n class, we sometimes make this dependence explicit and represent the features themselves as f (x, y): a function of both the input and the class. Using such a notation\n f5 (x) above could be represented as three features f5 (x, +), f5 (x, ‚àí), and f5 (x, 0),\n each of which has a single weight. We‚Äôll use this kind of notation in our description\n of the CRF in Chapter 17.\n12 C HAPTER 4 ‚Ä¢ L OGISTIC R EGRESSION\n\n Binary Logistic Regression\n p(+) = 1- p(-)\n\n Output y y^\n sigmoid [scalar]\n\n Weight vector w\n [1‚®âf]\n\n Input feature x x1 x2 x3 ‚Ä¶ xf\n vector [f ‚®â1]\n wordcount positive lexicon count of\n =3 words = 1 ‚Äúno‚Äù = 0\n\n Input words dessert was great\n\n Multinomial Logistic Regression\n p(+) p(-) p(neut)\n\n Output y y^1 ^y y^3 These f red weights\n softmax 2\n [K‚®â1] are a row of W\n corresponding\n Weight W to weight vector w3,\n matrix [K‚®âf] (= weights for class 3,\n = a prototype of class 3)\n Input feature x x1 x2 x3 ‚Ä¶ xf\n vector [f‚®â1]\n wordcount positive lexicon count of\n =3 words = 1 ‚Äúno‚Äù = 0\n\n Input words dessert was great\n\n single weight vector w, and has a scalar output yÃÇ. In multinomial logistic regression we have\n K separate weight vectors corresponding to the K classes, all packed into a single weight\n matrix W, and a vector output yÃÇ. We omit the biases from both figures for clarity.\n\n How are the parameters of the model, the weights w and bias b, learned? Logistic\n regression is an instance of supervised classification in which we know the correct\n label y (either 0 or 1) for each observation x. What the system produces via Eq. 4.5\n is yÃÇ, the system‚Äôs estimate of the true y. We want to learn parameters (meaning w\n and b) that make yÃÇ for each training observation as close as possible to the true y.\n This requires two components that we foreshadowed in the introduction to the\n chapter. The first is a metric for how close the current label (yÃÇ) is to the true gold\n label y. Rather than measure similarity, we usually talk about the opposite of this:\n the distance between the system output and the gold output, and we call this distance\n loss the loss function or the cost function. In the next section we‚Äôll introduce the loss\n function that is commonly used for logistic regression and also for neural networks,\n 4.6 ‚Ä¢ T HE CROSS - ENTROPY LOSS FUNCTION 13\n\n the cross-entropy loss.\n The second thing we need is an optimization algorithm for iteratively updating\n the weights so as to minimize this loss function. The standard algorithm for this is\n gradient descent; we‚Äôll introduce the stochastic gradient descent algorithm in the\n following section.\n We‚Äôll describe these algorithms for the simpler case of binary logistic regression in the next two sections, and then turn to multinomial logistic regression in\n Section 4.8.\n\n We need a loss function that expresses, for an observation x, how close the classifier\n output (yÃÇ = œÉ (w ¬∑ x + b)) is to the correct output (y, which is 0 or 1). We‚Äôll call this:\n\n L(yÃÇ, y) = How much yÃÇ differs from the true y (4.20)\n\n We do this via a loss function that prefers the correct class labels of the training examples to be more likely. This is called conditional maximum likelihood\n estimation: we choose the parameters w, b that maximize the log probability of\n the true y labels in the training data given the observations x. The resulting loss\n cross-entropy function is the negative log likelihood loss, generally called the cross-entropy loss.\n loss\n Let‚Äôs derive this loss function, applied to a single observation x. We‚Äôd like to\n learn weights that maximize the probability of the correct label p(y|x). Since there\n are only two discrete outcomes (1 or 0), this is a Bernoulli distribution, and we can\n express the probability p(y|x) that our classifier produces for one observation as the\n following (keeping in mind that if y = 1, Eq. 4.21 simplifies to yÃÇ; if y = 0, Eq. 4.21\n simplifies to 1 ‚àí yÃÇ):\n\n p(y|x) = yÃÇ y (1 ‚àí yÃÇ)1‚àíy (4.21)\n\n Now we take the log of both sides. This will turn out to be handy mathematically,\n and doesn‚Äôt hurt us; whatever values maximize a probability will also maximize the\n log of the probability:\n\n log p(y|x) = log yÃÇ y (1 ‚àí yÃÇ)1‚àíy\n \u0002 \u0003\n\n = y log yÃÇ + (1 ‚àí y) log(1 ‚àí yÃÇ) (4.22)\n\n Eq. 4.22 describes a log likelihood that should be maximized. In order to turn this\n into a loss function (something that we need to minimize), we‚Äôll just flip the sign on\n Eq. 4.22. The result is the cross-entropy loss LCE :\n\n LCE (yÃÇ, y) = ‚àí log p(y|x) = ‚àí [y log yÃÇ + (1 ‚àí y) log(1 ‚àí yÃÇ)] (4.23)\n\n Finally, we can plug in the definition of yÃÇ = œÉ (w ¬∑ x + b):\n\n LCE (yÃÇ, y) = ‚àí [y log œÉ (w ¬∑ x + b) + (1 ‚àí y) log (1 ‚àí œÉ (w ¬∑ x + b))] (4.24)\n\n Let‚Äôs see if this loss function does the right thing for our example from Fig. 4.2. We\n want the loss to be smaller if the model‚Äôs estimate is close to correct, and bigger if\n the model is confused. So first let‚Äôs suppose the correct gold label for the sentiment\n example in Fig. 4.2 is positive, i.e., y = 1. In this case our model is doing well, since\n14 C HAPTER 4 ‚Ä¢ L OGISTIC R EGRESSION\n\n from Eq. 4.8 it indeed gave the example a higher probability of being positive (.70)\n than negative (.30). If we plug œÉ (w ¬∑ x + b) = .70 and y = 1 into Eq. 4.24, the right\n side of the equation drops out, leading to the following loss (we‚Äôll use log to mean\n natural log when the base is not specified):\n LCE (yÃÇ, y) = ‚àí[y log œÉ (w ¬∑ x + b) + (1 ‚àí y) log (1 ‚àí œÉ (w ¬∑ x + b))]\n = ‚àí [log œÉ (w ¬∑ x + b)]\n = ‚àí log(.70)\n = .36\n By contrast, let‚Äôs pretend instead that the example in Fig. 4.2 was actually negative,\n i.e., y = 0 (perhaps the reviewer went on to say ‚ÄúBut bottom line, the movie is\n terrible! I beg you not to see it!‚Äù). In this case our model is confused and we‚Äôd want\n the loss to be higher. Now if we plug y = 0 and 1 ‚àí œÉ (w ¬∑ x + b) = .30 from Eq. 4.8\n into Eq. 4.24, the left side of the equation drops out:\n LCE (yÃÇ, y) = ‚àí[y log œÉ (w ¬∑ x + b)+(1 ‚àí y) log (1 ‚àí œÉ (w ¬∑ x + b))]\n = ‚àí [log (1 ‚àí œÉ (w ¬∑ x + b))]\n = ‚àí log (.30)\n = 1.2\n Sure enough, the loss for the first classifier (.36) is less than the loss for the second\n classifier (1.2).\n Why does minimizing this negative log probability do what we want? A perfect\n classifier would assign probability 1 to the correct outcome (y = 1 or y = 0) and\n probability 0 to the incorrect outcome. That means if y equals 1, the higher yÃÇ is (the\n closer it is to 1), the better the classifier; the lower yÃÇ is (the closer it is to 0), the\n worse the classifier. If y equals 0, instead, the higher 1 ‚àí yÃÇ is (closer to 1), the better\n the classifier. The negative log of yÃÇ (if the true y equals 1) or 1 ‚àí yÃÇ (if the true y\n equals 0) is a convenient loss metric since it goes from 0 (negative log of 1, no loss)\n to infinity (negative log of 0, infinite loss). This loss function also ensures that as\n the probability of the correct answer is maximized, the probability of the incorrect\n answer is minimized; since the two sum to one, any increase in the probability of the\n correct answer is coming at the expense of the incorrect answer. It‚Äôs called the crossentropy loss, because Eq. 4.22 is also the formula for the cross-entropy between the\n true probability distribution y and our estimated distribution yÃÇ.\n Now we know what we want to minimize; in the next section, we‚Äôll see how to\n find the minimum.\n\n Our goal with gradient descent is to find the optimal weights: minimize the loss\n function we‚Äôve defined for the model. In Eq. 4.25 below, we‚Äôll explicitly represent\n the fact that the cross-entropy loss function LCE is parameterized by the weights. In\n machine learning in general we refer to the parameters being learned as Œ∏ ; in the\n case of logistic regression Œ∏ = {w, b}. So the goal is to find the set of weights which\n minimizes the loss function, averaged over all examples:\n m\n 1X\n Œ∏ÃÇ = argmin LCE ( f (x(i) ; Œ∏ ), y(i) ) (4.25)\n Œ∏ m\n i=1\n 4.7 ‚Ä¢ G RADIENT D ESCENT 15\n\n How shall we find the minimum of this (or any) loss function? Gradient descent is\n a method that finds a minimum of a function by figuring out in which direction (in\n the space of the parameters Œ∏ ) the function‚Äôs slope is rising the most steeply, and\n moving in the opposite direction. The intuition is that if you are hiking in a canyon\n and trying to descend most quickly down to the river at the bottom, you might look\n around yourself in all directions, find the direction where the ground is sloping the\n steepest, and walk downhill in that direction.\n convex For logistic regression, this loss function is conveniently convex. A convex function has at most one minimum; there are no local minima to get stuck in, so gradient\n descent starting from any point is guaranteed to find the minimum. (By contrast,\n the loss for multi-layer neural networks is non-convex, and gradient descent may\n get stuck in local minima for neural network training and never find the global optimum.)\n Although the algorithm (and the concept of gradient) are designed for direction\n vectors, let‚Äôs first consider a visualization of the case where the parameter of our\n system is just a single scalar w, shown in Fig. 4.4.\n Given a random initialization of w at some value w1 , and assuming the loss\n function L happened to have the shape in Fig. 4.4, we need the algorithm to tell us\n whether at the next iteration we should move left (making w2 smaller than w1 ) or\n right (making w2 bigger than w1 ) to reach the minimum.\n\n Loss\n\n one step\n of gradient\n slope of loss at w1 descent\n is negative\n\n w1 wmin w\n 0 (goal)\n w in the reverse direction from the slope of the function. Since the slope is negative, we need\n to move w in a positive direction, to the right. Here superscripts are used for learning steps,\n so w1 means the initial value of w (which is 0), w2 the value at the second step, and so on.\n\n gradient The gradient descent algorithm answers this question by finding the gradient\n of the loss function at the current point and moving in the opposite direction. The\n gradient of a function of many variables is a vector pointing in the direction of the\n greatest increase in a function. The gradient is a multi-variable generalization of the\n slope, so for a function of one variable like the one in Fig. 4.4, we can informally\n think of the gradient as the slope. The dotted line in Fig. 4.4 shows the slope of this\n hypothetical loss function at point w = w1 . You can see that the slope of this dotted\n line is negative. Thus to find the minimum, gradient descent tells us to go in the\n opposite direction: moving w in a positive direction.\n The magnitude of the amount to move in gradient descent is the value of the\n d\nlearning rate slope dw L( f (x; w), y) weighted by a learning rate Œ∑. A higher (faster) learning\n16 C HAPTER 4 ‚Ä¢ L OGISTIC R EGRESSION\n\n rate means that we should move w more on each step. The change we make in our\n parameter is the learning rate times the gradient (or the slope, in our single-variable\n example):\n d\n wt+1 = wt ‚àí Œ∑ L( f (x; w), y) (4.26)\n dw\n Now let‚Äôs extend the intuition from a function of one scalar variable w to many\n variables, because we don‚Äôt just want to move left or right, we want to know where\n in the N-dimensional space (of the N parameters that make up Œ∏ ) we should move.\n The gradient is just such a vector; it expresses the directional components of the\n sharpest slope along each of those N dimensions. If we‚Äôre just imagining two weight\n dimensions (say for one weight w and one bias b), the gradient might be a vector with\n two orthogonal components, each of which tells us how much the ground slopes in\n the w dimension and in the b dimension. Fig. 4.5 shows a visualization of the value\n of a 2-dimensional gradient vector taken at the red point.\n In an actual logistic regression, the parameter vector w is much longer than 1 or\n 2, since the input feature vector x can be quite long, and we need a weight wi for\n each xi . For each dimension/variable wi in w (plus the bias b), the gradient will have\n a component that tells us the slope with respect to that variable. In each dimension\n wi , we express the slope as a partial derivative ‚àÇ‚àÇwi of the loss function. Essentially\n we‚Äôre asking: ‚ÄúHow much would a small change in that variable wi influence the\n total loss function L?‚Äù\n Formally, then, the gradient of a multi-variable function f is a vector in which\n each component expresses the partial derivative of f with respect to one of the variables. We‚Äôll use the inverted Greek delta symbol ‚àá to refer to the gradient, and\n represent yÃÇ as f (x; Œ∏ ) to make the dependence on Œ∏ more obvious:\n Ô£Æ ‚àÇ Ô£π\n ‚àÇ w1 L( f (x; Œ∏ ), y)\n Ô£Ø ‚àÇ L( f (x; Œ∏ ), y)Ô£∫\n Ô£Ø ‚àÇ w2 Ô£∫\n ‚àáL( f (x; Œ∏ ), y) = Ô£Ø\n Ô£Ø .. Ô£∫\n Ô£∫ (4.27)\n Ô£Ø . Ô£∫\n Ô£Ø ‚àÇ Ô£∫\n Ô£∞ ‚àÇ w L( f (x; Œ∏ ), y)Ô£ª\n n\n ‚àÇ\n ‚àÇ b L( f (x; Œ∏ ), y)\n The final equation for updating Œ∏ based on the gradient is thus\n Œ∏ t+1 = Œ∏ t ‚àí Œ∑‚àáL( f (x; Œ∏ ), y) (4.28)\n\n Cost(w,b)\n\n b\n w\n b, showing a red arrow in the x-y plane pointing in the direction we will go to look for the\n minimum: the opposite direction of the gradient (recall that the gradient points in the direction\n of increase not decrease).\n 4.7 ‚Ä¢ G RADIENT D ESCENT 17\n\n 4.7.1 The Gradient for Logistic Regression\n In order to update Œ∏ , we need a definition for the gradient ‚àáL( f (x; Œ∏ ), y). Recall that\n for logistic regression, the cross-entropy loss function is:\n LCE (yÃÇ, y) = ‚àí [y log œÉ (w ¬∑ x + b) + (1 ‚àí y) log (1 ‚àí œÉ (w ¬∑ x + b))] (4.29)\n\n It turns out that the derivative of this function for one observation vector x is Eq. 4.30\n (the interested reader can see Section 4.15 for the derivation of this equation):\n ‚àÇ LCE (yÃÇ, y)\n = [œÉ (w ¬∑ x + b) ‚àí y]x j\n ‚àÇwj\n = (yÃÇ ‚àí y)x j (4.30)\n\n You‚Äôll also sometimes see this equation in the equivalent form:\n ‚àÇ LCE (yÃÇ, y)\n = ‚àí(y ‚àí yÃÇ)x j (4.31)\n ‚àÇwj\n Note in these equations that the gradient with respect to a single weight w j represents a very intuitive value: the difference between the true y and our estimated\n yÃÇ = œÉ (w ¬∑ x + b) for that observation, multiplied by the corresponding input value\n x j.\n\n 4.7.2 The Stochastic Gradient Descent Algorithm\n Stochastic gradient descent is an online algorithm that minimizes the loss function\n by computing its gradient after each training example, and nudging Œ∏ in the right\n direction (the opposite direction of the gradient). (An ‚Äúonline algorithm‚Äù is one that\n processes its input example by example, rather than waiting until it sees the entire\n input.) Stochastic gradient descent is called stochastic because it chooses a single\n random example at a time; in Section 4.7.4 we‚Äôll discuss other versions of gradient\n descent that batch many examples at once. Fig. 4.6 shows the algorithm.\nhyperparameter The learning rate Œ∑ is a hyperparameter that must be adjusted. If it‚Äôs too high,\n the learner will take steps that are too large, overshooting the minimum of the loss\n function. If it‚Äôs too low, the learner will take steps that are too small, and take too\n long to get to the minimum. It is common to start with a higher learning rate and then\n slowly decrease it, so that it is a function of the iteration k of training; the notation\n Œ∑k can be used to mean the value of the learning rate at iteration k.\n We‚Äôll discuss hyperparameters in more detail in Chapter 6, but in short, they are\n a special kind of parameter for any machine learning model. Unlike regular parameters of a model (weights like w and b), which are learned by the algorithm from\n the training set, hyperparameters are special parameters chosen by the algorithm\n designer that affect how the algorithm works.\n\n 4.7.3 Working through an example\n Let‚Äôs walk through a single step of the gradient descent algorithm. We‚Äôll use a\n simplified version of the example in Fig. 4.2 as it sees a single observation x, whose\n correct value is y = 1 (this is a positive review), and with a feature vector x = [x1 , x2 ]\n consisting of these two features:\n x1 = 3 (count of positive lexicon words)\n x2 = 2 (count of negative lexicon words)\n18 C HAPTER 4 ‚Ä¢ L OGISTIC R EGRESSION\n\n function S TOCHASTIC G RADIENT D ESCENT(L(), f (), x, y) returns Œ∏\n # where: L is the loss function\n # f is a function parameterized by Œ∏\n # x is the set of training inputs x(1) , x(2) , ..., x(m)\n # y is the set of training outputs (labels) y(1) , y(2) , ..., y(m)\n\n Œ∏ ‚Üê0 # (or small random values)\n repeat til done # see caption\n For each training tuple (x(i) , y(i) ) (in random order)\n 1. Optional (for reporting): # How are we doing on this tuple?\n Compute yÃÇ (i) = f (x(i) ; Œ∏ ) # What is our estimated output yÃÇ?\n Compute the loss L(yÃÇ (i) , y(i) ) # How far off is yÃÇ(i) from the true output y(i) ?\n 2. g ‚Üê ‚àáŒ∏ L( f (x(i) ; Œ∏ ), y(i) ) # How should we move Œ∏ to maximize loss?\n 3. Œ∏ ‚Üê Œ∏ ‚àí Œ∑ g # Go the other way instead\n return Œ∏\n\n mainly to report how well we are doing on the current tuple; we don‚Äôt need to compute the\n loss in order to compute the gradient. The algorithm can terminate when it converges (when\n the gradient norm < \u000f), or when progress halts (for example when the loss starts going up on\n a held-out set). Weights are initialized to 0 for logistic regression, but to small random values\n for neural networks, as we‚Äôll see in Chapter 6.\n\n Let‚Äôs assume the initial weights and bias in Œ∏ 0 are all set to 0, and the initial learning\n rate Œ∑ is 0.1:\n\n w1 = w2 = b = 0\n Œ∑ = 0.1\n\n The single update step requires that we compute the gradient, multiplied by the\n learning rate\n\n Œ∏ t+1 = Œ∏ t ‚àí Œ∑‚àáŒ∏ L( f (x(i) ; Œ∏ ), y(i) )\n\n In our mini example there are three parameters, so the gradient vector has 3 dimensions, for w1 , w2 , and b. We can compute the first gradient as follows:\n Ô£Æ ‚àÇ LCE (yÃÇ,y) Ô£π Ô£Æ Ô£π Ô£Æ Ô£π Ô£Æ Ô£π Ô£Æ Ô£π\n ‚àÇ w1 (œÉ (w ¬∑ x + b) ‚àí y)x1 (œÉ (0) ‚àí 1)x1 ‚àí0.5x1 ‚àí1.5\n (yÃÇ,y) Ô£∫\n‚àáw,b L = Ô£∞ ‚àÇ LCE Ô£ª = Ô£∞ (œÉ (w ¬∑ x + b) ‚àí y)x2 Ô£ª = Ô£∞ (œÉ (0) ‚àí 1)x2 Ô£ª = Ô£∞ ‚àí0.5x2 Ô£ª = Ô£∞ ‚àí1.0 Ô£ª\n Ô£Ø\n ‚àÇ w2\n ‚àÇ LCE (yÃÇ,y) œÉ (w ¬∑ x + b) ‚àí y œÉ (0) ‚àí 1 ‚àí0.5 ‚àí0.5\n ‚àÇb\n\n Now that we have a gradient, we compute the new parameter vector Œ∏ 1 by moving\n Œ∏ 0 in the opposite direction from the gradient:\n Ô£Æ Ô£π Ô£Æ Ô£π Ô£Æ Ô£π\n w1 ‚àí1.5 .15\n Œ∏ 1 = Ô£∞ w2 Ô£ª ‚àí Œ∑ Ô£∞ ‚àí1.0 Ô£ª = Ô£∞ .1 Ô£ª\n b ‚àí0.5 .05\n\n So after one step of gradient descent, the weights have shifted to be: w1 = .15,\n w2 = .1, and b = .05.\n Note that this observation x happened to be a positive example. We would expect\n that after seeing more negative examples with high counts of negative words, that\n the weight w2 would shift to have a negative value.\n 4.7 ‚Ä¢ G RADIENT D ESCENT 19\n\n 4.7.4 Mini-batch training\n Stochastic gradient descent is called stochastic because it chooses a single random\n example at a time, moving the weights so as to improve performance on that single\n example. That can result in very choppy movements, so it‚Äôs common to compute the\n gradient over batches of training instances rather than a single instance.\nbatch training For example in batch training we compute the gradient over the entire dataset.\n By seeing so many examples, batch training offers a superb estimate of which direction to move the weights, at the cost of spending a lot of time processing every\n single example in the training set to compute this perfect direction.\n mini-batch A compromise is mini-batch training: we train on a group of m examples (perhaps 512, or 1024) that is less than the whole dataset. (If m is the size of the dataset,\n then we are doing batch gradient descent; if m = 1, we are back to doing stochastic gradient descent.) Mini-batch training also has the advantage of computational\n efficiency. The mini-batches can easily be vectorized, choosing the size of the minibatch based on the computational resources. This allows us to process all the examples in one mini-batch in parallel and then accumulate the loss, something that‚Äôs not\n possible with individual or batch training.\n We just need to define mini-batch versions of the cross-entropy loss function\n we defined in Section 4.6 and the gradient in Section 4.7.1. Let‚Äôs extend the crossentropy loss for one example from Eq. 4.23 to mini-batches of size m. We‚Äôll continue\n to use the notation that x(i) and y(i) mean the ith training features and training label,\n respectively. We make the assumption that the training examples are independent:\n m\n Y\n log p(training labels) = log p(y(i) |x(i) )\n i=1\n m\n X\n = log p(y(i) |x(i) )\n i=1\n m\n X\n = ‚àí LCE (yÃÇ(i) , y(i) ) (4.32)\n i=1\n\n Now the cost function for the mini-batch of m examples is the average loss for each\n example:\n m\n 1X\n Cost(yÃÇ, y) = LCE (yÃÇ(i) , y(i) )\n m\n i=1\n m\n 1 X \u0010 \u0011\n = ‚àí y(i) log œÉ (w ¬∑ x(i) + b) + (1 ‚àí y(i) ) log 1 ‚àí œÉ (w ¬∑ x(i) + b) (4.33)\n m\n i=1\n\n The mini-batch gradient is the average of the individual gradients from Eq. 4.30:\n m\n ‚àÇCost(yÃÇ, y) 1 Xh i\n (i)\n = œÉ (w ¬∑ x(i) + b) ‚àí y(i) x j (4.34)\n ‚àÇwj m\n i=1\n\n Instead of using the sum notation, we can more efficiently compute the gradient\n in its matrix form, following the vectorization we saw on page 9, where we have a\n matrix X of size [m √ó f ] representing the m inputs in the batch, and a vector y of size\n [m √ó 1] representing the correct outputs:\n20 C HAPTER 4 ‚Ä¢ L OGISTIC R EGRESSION\n\n ‚àÇCost(yÃÇ, y) 1\n = (yÃÇ ‚àí y)| X\n ‚àÇw m\n = (œÉ (Xw + b) ‚àí y)| X (4.35)\n m\n\n The loss function for multinomial logistic regression generalizes the loss function\n for binary logistic regression from 2 to K classes. Recall that that the cross-entropy\n loss for binary logistic regression (repeated from Eq. 4.23) is:\n\n LCE (yÃÇ, y) = ‚àí log p(y|x) = ‚àí [y log yÃÇ + (1 ‚àí y) log(1 ‚àí yÃÇ)] (4.36)\n\n The loss function for multinomial logistic regression generalizes the two terms in\n Eq. 4.36 (one that is non-zero when y = 1 and one that is non-zero when y = 0) to\n K terms. As we mentioned above, for multinomial regression we‚Äôll represent both y\n and yÃÇ as vectors. The true label y is a vector with K elements, each corresponding\n to a class, with yc = 1 if the correct class is c, with all other elements of y being 0.\n And our classifier will produce an estimate vector with K elements yÃÇ, each element\n yÃÇk of which represents the estimated probability p(yk = 1|x).\n The loss function for a single example x, generalizing from binary logistic regression, is the sum of the logs of the K output classes, each weighted by the indicator function yk (Eq. 4.37). This turns out to be just the negative log probability of\n the correct class c (Eq. 4.38):\n K\n X\n LCE (yÃÇ, y) = ‚àí yk log yÃÇk (4.37)\n k=1\n\n = ‚àí log yÃÇc , (where c is the correct class) (4.38)\n = ‚àí log pÃÇ(yc = 1|x) (where c is the correct class)\n exp (wc ¬∑ x + bc )\n = ‚àí log PK (c is the correct class) (4.39)\n j=1 exp (wj ¬∑ x + b j )\n\n How did we get from Eq. 4.37 to Eq. 4.38? Because only one class (let‚Äôs call it c) is\n the correct one, the vector y takes the value 1 only for this value of k, i.e., has yc = 1\n and y j = 0 ‚àÄ j 6= c. That means the terms in the sum in Eq. 4.37 will all be 0 except\n for the term corresponding to the true class c. Hence the cross-entropy loss is simply\n the log of the output probability corresponding to the correct class, and we therefore\n negative log also call Eq. 4.38 the negative log likelihood loss.\n likelihood loss\n Of course for gradient descent we don‚Äôt need the loss, we need its gradient. The\n gradient for a single example turns out to be very similar to the gradient for binary\n logistic regression, (yÃÇ ‚àí y)x, that we saw in Eq. 4.30. Let‚Äôs consider one piece of the\n gradient, the derivative for a single weight. For each class k, the weight of the ith\n element of input x is wk,i . What is the partial derivative of the loss with respect to\n wk,i ? This derivative turns out to be just the difference between the true value for the\n class k (which is either 1 or 0) and the probability the classifier outputs for class k,\n weighted by the value of the input xi corresponding to the ith element of the weight\n 4.9 ‚Ä¢ E VALUATION : P RECISION , R ECALL , F- MEASURE 21\n\n vector for class k:\n ‚àÇ LCE\n = ‚àí(yk ‚àí yÃÇk )xi\n ‚àÇ wk,i\n = ‚àí(yk ‚àí p(yk = 1|x))xi\n !\n exp (wk ¬∑ x + bk )\n = ‚àí yk ‚àí PK xi (4.40)\n j=1 exp (wj ¬∑ x + b j )\n\n We‚Äôll return to this case of the gradient for softmax regression when we introduce\n neural networks in Chapter 6, and at that time we‚Äôll also discuss the derivation of\n this gradient in equations Eq. ??‚ÄìEq. ??.\n\n To introduce the methods for evaluating text classification, let‚Äôs first consider some\n simple binary detection tasks. For example, in spam detection, our goal is to label\n every text as being in the spam category (‚Äúpositive‚Äù) or not in the spam category\n (‚Äúnegative‚Äù). For each item (email document) we therefore need to know whether\n our system called it spam or not. We also need to know whether the email is actually\n spam or not, i.e. the human-defined labels for each document that we are trying to\n gold labels match. We will refer to these human labels as the gold labels.\n Or imagine you‚Äôre the CEO of the Delicious Pie Company and you need to know\n what people are saying about your pies on social media, so you build a system that\n detects tweets concerning Delicious Pie. Here the positive class is tweets about\n Delicious Pie and the negative class is all other tweets.\n In both cases, we need a metric for knowing how well our spam detector (or\n pie-tweet-detector) is doing. To evaluate any system for detecting things, we start\n confusion by building a confusion matrix like the one shown in Fig. 4.7. A confusion matrix\n matrix\n is a table for visualizing how an algorithm performs with respect to the human gold\n labels, using two dimensions (system output and gold labels), and each cell labeling\n a set of possible outcomes. In the spam detection case, for example, true positives\n are documents that are indeed spam (indicated by human-created gold labels) that\n our system correctly said were spam. False negatives are documents that are indeed\n spam but our system incorrectly labeled as non-spam.\n To the bottom right of the table is the equation for accuracy, which asks what\n percentage of all the observations (for the spam or pie examples that means all emails\n or tweets) our system labeled correctly. Although accuracy might seem a natural\n metric, we generally don‚Äôt use it for text classification tasks. That‚Äôs because accuracy\n doesn‚Äôt work well when the classes are unbalanced (as indeed they are with spam,\n which is a large majority of email, or with tweets, which are mainly not about pie).\n To make this more explicit, imagine that we looked at a million tweets, and\n let‚Äôs say that only 100 of them are discussing their love (or hatred) for our pie,\n while the other 999,900 are tweets about something completely unrelated. Imagine a\n simple classifier that stupidly classified every tweet as ‚Äúnot about pie‚Äù. This classifier\n would have 999,900 true negatives and only 100 false negatives for an accuracy of\n 999,900/1,000,000 or 99.99%! What an amazing accuracy level! Surely we should\n be happy with this classifier? But of course this fabulous ‚Äòno pie‚Äô classifier would\n be completely useless, since it wouldn‚Äôt find a single one of the customer comments\n we are looking for. In other words, accuracy is not a good metric when the goal is\n22 C HAPTER 4 ‚Ä¢ L OGISTIC R EGRESSION\n\n gold standard labels\n gold positive gold negative\n system system tp\n positive true positive false positive precision = tp+fp\n output\n labels system\n negative false negative true negative\n tp tp+tn\n recall = accuracy =\n tp+fn tp+fp+tn+fn\n\n to discover something that is rare, or at least not completely balanced in frequency,\n which is a very common situation in the world.\n That‚Äôs why instead of accuracy we generally turn to two other metrics shown in\n precision Fig. 4.7: precision and recall. Precision measures the percentage of the items that\n the system detected (i.e., the system labeled as positive) that are in fact positive (i.e.,\n are positive according to the human gold labels). Precision is defined as\n\n true positives\n Precision =\n true positives + false positives\n\n recall Recall measures the percentage of items actually present in the input that were\n correctly identified by the system. Recall is defined as\n\n true positives\n Recall =\n true positives + false negatives\n\n Precision and recall will help solve the problem with the useless ‚Äúnothing is\n pie‚Äù classifier. This classifier, despite having a fabulous accuracy of 99.99%, has\n a terrible recall of 0 (since there are no true positives, and 100 false negatives, the\n recall is 0/100). You should convince yourself that the precision at finding relevant\n tweets is equally problematic. Thus precision and recall, unlike accuracy, emphasize\n true positives: finding the things that we are supposed to be looking for.\n There are many ways to define a single metric that incorporates aspects of both\n F-measure precision and recall. The simplest of these combinations is the F-measure (van\n Rijsbergen, 1975) , defined as:\n\n (Œ≤ 2 + 1)PR\n FŒ≤ =\n Œ≤ 2P + R\n\n The Œ≤ parameter differentially weights the importance of recall and precision,\n based perhaps on the needs of an application. Values of Œ≤ > 1 favor recall, while\n values of Œ≤ < 1 favor precision. When Œ≤ = 1, precision and recall are equally bal-\nF1 anced; this is the most frequently used metric, and is called FŒ≤ =1 or just F1 :\n 2PR\n F1 = (4.41)\n P+R\n F-measure comes from a weighted harmonic mean of precision and recall. The\n harmonic mean of a set of numbers is the reciprocal of the arithmetic mean of recip-\n4.9 ‚Ä¢ E VALUATION : P RECISION , R ECALL , F- MEASURE 23\n\n rocals:\n n\n HarmonicMean(a1 , a2 , a3 , a4 , ..., an ) = 1 1 1 1\n (4.42)\n a1 + a2 + a3 + ... + an\n\n and hence F-measure is\n (Œ≤ 2 + 1)PR\n \u0012 \u0013\n 1 1‚àíŒ±\n F= 1 or with Œ≤ = F= (4.43)\n Œ± P + (1 ‚àí Œ±) R1 Œ± Œ≤ 2P + R\n\n Harmonic mean is used because the harmonic mean of two values is closer to the\n minimum of the two values than the arithmetic mean is. Thus it weighs the lower of\n the two numbers more heavily, which is more conservative in this situation.\n\n 4.9.1 Evaluating with more than two classes\n Up to now we have been describing text classification tasks with only two classes.\n But lots of classification tasks in language processing have more than two classes.\n For sentiment analysis we generally have 3 classes (positive, negative, neutral) and\n even more classes are common for tasks like part-of-speech tagging, word sense\n disambiguation, semantic role labeling, emotion detection, and so on. Luckily the\n naive Bayes algorithm is already a multi-class classification algorithm.\n\n gold labels\n urgent normal spam\n urgent 8 10 1 precisionu=\n 8+10+1\n system 60\n output normal 5 60 50 precisionn=\n 5+60+50\n spam 3 30 200 precisions=\n 3+30+200\n recallu = recalln = recalls =\n 8 60 200\n 8+5+3 10+60+30 1+50+200\n\n classes (c1 , c2 ), how many documents from c1 were (in)correctly assigned to c2 .\n\n But we‚Äôll need to slightly modify our definitions of precision and recall. Consider the sample confusion matrix for a hypothetical 3-way one-of email categorization decision (urgent, normal, spam) shown in Fig. 4.8. The matrix shows, for\n example, that the system mistakenly labeled one spam document as urgent, and we\n have shown how to compute a distinct precision and recall value for each class. In\n order to derive a single metric that tells us how well the system is doing, we can commacroaveraging bine these values in two ways. In macroaveraging, we compute the performance\nmicroaveraging for each class, and then average over classes. In microaveraging, we collect the decisions for all classes into a single confusion matrix, and then compute precision and\n recall from that table. Fig. 4.9 shows the confusion matrix for each class separately,\n and shows the computation of microaveraged and macroaveraged precision.\n As the figure shows, a microaverage is dominated by the more frequent class (in\n this case spam), since the counts are pooled. The macroaverage better reflects the\n statistics of the smaller classes, and so is more appropriate when performance on all\n the classes is equally important.\n24 C HAPTER 4 ‚Ä¢ L OGISTIC R EGRESSION\n\n Class 1: Urgent Class 2: Normal Class 3: Spam Pooled\n true true true true true true true true\n urgent not normal not spam not yes no\n system system system system\n urgent 8 11 normal 60 55 spam 200 33 yes 268 99\n system system system system\n not 8 340 not 40 212 not 51 83 no 99 635\n 8 60 200 microaverage = 268\n precision = = .42 precision = = .52 precision = = .86 = .73\n 8+11 60+55 200+33 precision 268+99\n\n macroaverage = .42+.52+.86\n = .60\n precision 3\n\n The training and testing procedure for text classification follows what we saw with\n language modeling (Section ??): we use the training set to train the model, then use\n development the development test set (also called a devset) to perhaps tune some parameters,\n test set\n devset and in general decide what the best model is. Once we come up with what we think\n is the best model, we run it on the (hitherto unseen) test set to report its performance.\n While the use of a devset avoids overfitting the test set, having a fixed training set, devset, and test set creates another problem: in order to save lots of data\n for training, the test set (or devset) might not be large enough to be representative.\n Wouldn‚Äôt it be better if we could somehow use all our data for training and still use\ncross-validation all our data for test? We can do this by cross-validation.\n In cross-validation, we choose a number k, and partition our data into k disjoint\n folds subsets called folds. Now we choose one of those k folds as a test set, train our\n classifier on the remaining k ‚àí 1 folds, and then compute the error rate on the test\n set. Then we repeat with another fold as the test set, again training on the other k ‚àí 1\n folds. We do this sampling process k times and average the test set error rate from\n these k runs to get an average error rate. If we choose k = 10, we would train 10\n different models (each on 90% of our data), test the model 10 times, and average\n 10-fold these 10 values. This is called 10-fold cross-validation.\ncross-validation\n The only problem with cross-validation is that because all the data is used for\n testing, we need the whole corpus to be blind; we can‚Äôt examine any of the data\n to suggest possible features and in general see what‚Äôs going on, because we‚Äôd be\n peeking at the test set, and such cheating would cause us to overestimate the performance of our system. However, looking at the corpus to understand what‚Äôs going\n on is important in designing NLP systems! What to do? For this reason, it is common to create a fixed training set and test set, then do 10-fold cross-validation inside\n the training set, but compute error rate the normal way in the test set, as shown in\n Fig. 4.10.\n 4.11 ‚Ä¢ S TATISTICAL S IGNIFICANCE T ESTING 25\n\n Training Iterations Testing\n 1 Dev Training\n 2 Dev Training\n 3 Dev Training\n 4 Dev Training\n Test\n 5 Training Dev Training\n Set\n 6 Training Dev\n 7 Training Dev\n 8 Training Dev\n 9 Training Dev\n 10 Training Dev\n\n In building systems we often need to compare the performance of two systems. How\n can we know if the new system we just built is better than our old one? Or better\n than some other system described in the literature? This is the domain of statistical\n hypothesis testing, and in this section we introduce tests for statistical significance\n for NLP classifiers, drawing especially on the work of Dror et al. (2020) and Berg-\nKirkpatrick et al. (2012).\n Suppose we‚Äôre comparing the performance of classifiers A and B on a metric M\n such as F1 , or accuracy. Perhaps we want to know if our new sentiment classifier\n A gets a higher F1 score than our previous sentiment classifier B on a particular test\n set x. Let‚Äôs call M(A, x) the score that system A gets on test set x, and Œ¥ (x) the\n performance difference between A and B on x:\n\n Œ¥ (x) = M(A, x) ‚àí M(B, x) (4.44)\n\n We would like to know if Œ¥ (x) > 0, meaning that our logistic regression classifier\n effect size has a higher F1 than our naive Bayes classifier on x. Œ¥ (x) is called the effect size; a\n bigger Œ¥ means that A seems to be way better than B; a small Œ¥ means A seems to\n be only a little better.\n Why don‚Äôt we just check if Œ¥ (x) is positive? Suppose we do, and we find that\n the F1 score of A is higher than B‚Äôs by .04. Can we be certain that A is better? We\n cannot! That‚Äôs because A might just be accidentally better than B on this particular x.\n We need something more: we want to know if A‚Äôs superiority over B is likely to hold\n again if we checked another test set x0 , or under some other set of circumstances.\n In the paradigm of statistical hypothesis testing, we test this by formalizing two\n hypotheses.\n\n H0 : Œ¥ (x) ‚â§ 0\n H1 : Œ¥ (x) > 0 (4.45)\n\nnull hypothesis The hypothesis H0 , called the null hypothesis, supposes that Œ¥ (x) is actually negative or zero, meaning that A is not better than B. We would like to know if we can\n confidently rule out this hypothesis, and instead support H1 , that A is better.\n We do this by creating a random variable X ranging over all test sets. Now we\n ask how likely is it, if the null hypothesis H0 was correct, that among these test sets\n26 C HAPTER 4 ‚Ä¢ L OGISTIC R EGRESSION\n\n we would encounter the value of Œ¥ (x) that we found, if we repeated the experiment\n p-value a great many times. We formalize this likelihood as the p-value: the probability,\n assuming the null hypothesis H0 is true, of seeing the Œ¥ (x) that we saw or one even\n greater\n P(Œ¥ (X) ‚â• Œ¥ (x)|H0 is true) (4.46)\n\n So in our example, this p-value is the probability that we would see Œ¥ (x) assuming\n A is not better than B. If Œ¥ (x) is huge (let‚Äôs say A has a very respectable F1 of .9\n and B has a terrible F1 of only .2 on x), we might be surprised, since that would be\n extremely unlikely to occur if H0 were in fact true, and so the p-value would be low\n (unlikely to have such a large Œ¥ if A is in fact not better than B). But if Œ¥ (x) is very\n small, it might be less surprising to us even if H0 were true and A is not really better\n than B, and so the p-value would be higher.\n A very small p-value means that the difference we observed is very unlikely\n under the null hypothesis, and we can reject the null hypothesis. What counts as very\n small? It is common to use values like .05 or .01 as the thresholds. A value of .01\n means that if the p-value (the probability of observing the Œ¥ we saw assuming H0 is\n true) is less than .01, we reject the null hypothesis and assume that A is indeed better\n statistically\n significant than B. We say that a result (e.g., ‚ÄúA is better than B‚Äù) is statistically significant if\n the Œ¥ we saw has a probability that is below the threshold and we therefore reject\n this null hypothesis.\n How do we compute this probability we need for the p-value? In NLP we generally don‚Äôt use simple parametric tests like t-tests or ANOVAs that you might be\n familiar with. Parametric tests make assumptions about the distributions of the test\n statistic (such as normality) that don‚Äôt generally hold in our cases. So in NLP we\n usually use non-parametric tests based on sampling: we artificially create many versions of the experimental setup. For example, if we had lots of different test sets x0\n we could just measure all the Œ¥ (x0 ) for all the x0 . That gives us a distribution. Now\n we set a threshold (like .01) and if we see in this distribution that 99% or more of\n those deltas are smaller than the delta we observed, i.e., that p-value(x)‚Äîthe probability of seeing a Œ¥ (x) as big as the one we saw‚Äîis less than .01, then we can reject\n the null hypothesis and agree that Œ¥ (x) was a sufficiently surprising difference and\n A is really a better algorithm than B.\n There are two common non-parametric tests used in NLP: approximate ranapproximate domization (Noreen, 1989) and the bootstrap test. We will describe bootstrap\n randomization\n below, showing the paired version of the test, which again is most common in NLP.\n paired Paired tests are those in which we compare two sets of observations that are aligned:\n each observation in one set can be paired with an observation in another. This happens naturally when we are comparing the performance of two systems on the same\n test set; we can pair the performance of system A on an individual observation xi\n with the performance of system B on the same xi .\n\n 4.11.1 The Paired Bootstrap Test\n bootstrap test The bootstrap test (Efron and Tibshirani, 1993) can apply to any metric; from precision, recall, or F1 to the BLEU metric used in machine translation. The word\n bootstrapping bootstrapping refers to repeatedly drawing large numbers of samples with replacement (called bootstrap samples) from an original set. The intuition of the bootstrap\n test is that we can create many virtual test sets from an observed test set by repeatedly sampling from it. The method only makes the assumption that the sample is\n representative of the population.\n 4.11 ‚Ä¢ S TATISTICAL S IGNIFICANCE T ESTING 27\n\n Consider a tiny text classification example with a test set x of 10 documents. The\nfirst row of Fig. 4.11 shows the results of two classifiers (A and B) on this test set.\nEach document is labeled by one of the four possibilities (A and B both right, both\nwrong, A right and B wrong, A wrong and B right). A slash through a letter (\u0013 B)\nmeans that that classifier got the answer wrong. On the first document both A and\nB get the correct class (AB), while on the second document A got it right but B got\nit wrong (A\u0013 B). If we assume for simplicity that our metric is accuracy, A has an\naccuracy of .70 and B of .50, so Œ¥ (x) is .20.\n Now we create a large number b (perhaps 105 ) of virtual test sets x(i) , each of size\nn = 10. Fig. 4.11 shows a couple of examples. To create each virtual test set x(i) , we\nrepeatedly (n = 10 times) select a cell from row x with replacement. For example, to\ncreate the first cell of the first virtual test set x(1) , if we happened to randomly select\nthe second cell of the x row, we would copy the value A\u0013 B into our new cell, and\nmove on to create the second cell of x(1) , each time sampling (randomly choosing)\nfrom the original x with replacement.\n\n 1 2 3 4 5 6 7 8 9 10 A% B% Œ¥ ()\nx AB AB \u0013 AB AB AB\n \u0013 AB AB\n \u0013 AB AB\n \u0013 AB\n \u0013 .70 .50 .20\nx(1) AB\u0013 AB AB\u0013 AB AB AB\n \u0013 AB AB AB\n \u0013 AB .60 .60 .00\nx(2) AB\u0013 AB AB\u0013 AB AB AB AB AB\n \u0013 AB AB .60 .70 -.10\n...\nx(b)\nfrom an initial true test set x. Each pseudo test set is created by sampling n = 10 times with\nreplacement; thus an individual sample is a single cell, a document with its gold label and\nthe correct or incorrect performance of classifiers A and B. Of course real test sets don‚Äôt have\nonly 10 examples, and b needs to be large as well.\n\n Now that we have the b test sets, providing a sampling distribution, we can do\nstatistics on how often A has an accidental advantage. There are various ways to\ncompute this advantage; here we follow the version laid out in Berg-Kirkpatrick\net al. (2012). Assuming H0 (A isn‚Äôt better than B), we would expect that Œ¥ (X),\nestimated over many test sets, would be zero or negative; a much higher value would\nbe surprising, since H0 specifically assumes A isn‚Äôt better than B. To measure exactly\nhow surprising our observed Œ¥ (x) is, we would in other circumstances compute the\np-value by counting over many test sets how often Œ¥ (x(i) ) exceeds the expected zero\nvalue by Œ¥ (x) or more:\n\n b\n 1 X \u0010 (i) \u0011\n p-value(x) = 1 Œ¥ (x ) ‚àí Œ¥ (x) ‚â• 0\n b\n i=1\n\n(We use the notation 1(x) to mean ‚Äú1 if x is true, and 0 otherwise‚Äù.) However,\nalthough it‚Äôs generally true that the expected value of Œ¥ (X) over many test sets,\n(again assuming A isn‚Äôt better than B) is 0, this isn‚Äôt true for the bootstrapped test\nsets we created. That‚Äôs because we didn‚Äôt draw these samples from a distribution\nwith 0 mean; we happened to create them from the original test set x, which happens\nto be biased (by .20) in favor of A. So to measure how surprising is our observed\nŒ¥ (x), we actually compute the p-value by counting over many test sets how often\n28 C HAPTER 4 ‚Ä¢ L OGISTIC R EGRESSION\n\n Œ¥ (x(i) ) exceeds the expected value of Œ¥ (x) by Œ¥ (x) or more:\n\n b\n 1 X \u0010 (i) \u0011\n p-value(x) = 1 Œ¥ (x ) ‚àí Œ¥ (x) ‚â• Œ¥ (x)\n b\n i=1\n b\n 1X \u0010 \u0011\n = 1 Œ¥ (x(i) ) ‚â• 2Œ¥ (x) (4.47)\n b\n i=1\n\n So if for example we have 10,000 test sets x(i) and a threshold of .01, and in only 47\n of the test sets do we find that A is accidentally better Œ¥ (x(i) ) ‚â• 2Œ¥ (x), the resulting\n p-value of .0047 is smaller than .01, indicating that the delta we found, Œ¥ (x) is indeed\n sufficiently surprising and unlikely to have happened by accident, and we can reject\n the null hypothesis and conclude A is better than B.\n\n function B OOTSTRAP(test set x, num of samples b) returns p-value(x)\n\n Calculate Œ¥ (x) # how much better does algorithm A do than B on x\n s=0\n for i = 1 to b do\n for j = 1 to n do # Draw a bootstrap sample x(i) of size n\n Select a member of x at random and add it to x(i)\n Calculate Œ¥ (x(i) ) # how much better does algorithm A do than B on x(i)\n s ‚Üê s + 1 if Œ¥ (x(i) ) ‚â• 2Œ¥ (x)\n p-value(x) ‚âà bs # on what % of the b samples did algorithm A beat expectations?\n return p-value(x) # if very few did, our observed Œ¥ is probably not accidental\n\n (2012).\n\n The full algorithm for the bootstrap is shown in Fig. 4.12. It is given a test set\n x, a number of samples b, and counts the percentage of the b bootstrap test sets in\n which Œ¥ (x(i) ) > 2Œ¥ (x). This percentage then acts as a one-sided empirical p-value.\n\n It is important to avoid harms that may result from classifiers, harms that exist both\n for naive Bayes classifiers and for the other classification algorithms we introduce\n in later chapters.\nrepresentational One class of harms is representational harms (Crawford 2017, Blodgett et al.\n harms\n 2020), harms caused by a system that demeans a social group, for example by perpetuating negative stereotypes about them. For example Kiritchenko and Mohammad (2018) examined the performance of 200 sentiment analysis systems on pairs of\n sentences that were identical except for containing either a common African American first name (like Shaniqua) or a common European American first name (like\n Stephanie), chosen from the Caliskan et al. (2017) study discussed in Chapter 5.\n They found that most systems assigned lower sentiment and more negative emotion\n to sentences with African American names, reflecting and perpetuating stereotypes\n that associate African Americans with negative emotions (Popp et al., 2003).\n 4.13 ‚Ä¢ I NTERPRETING MODELS 29\n\n In other tasks classifiers may lead to both representational harms and other\n harms, such as silencing. For example the important text classification task of toxtoxicity icity detection is the task of detecting hate speech, abuse, harassment, or other\n detection\n kinds of toxic language. While the goal of such classifiers is to help reduce societal harm, toxicity classifiers can themselves cause harms. For example, researchers\n have shown that some widely used toxicity classifiers incorrectly flag as being toxic\n sentences that are non-toxic but simply mention identities like women (Park et al.,\n 2018), blind people (Hutchinson et al., 2020) or gay people (Dixon et al., 2018;\n Dias Oliva et al., 2021), or simply use linguistic features characteristic of varieties\n like African-American Vernacular English (Sap et al. 2019, Davidson et al. 2019).\n Such false positive errors could lead to the silencing of discourse by or about these\n groups.\n These model problems can be caused by biases or other problems in the training\n data; in general, machine learning systems replicate and even amplify the biases\n in their training data. But these problems can also be caused by the labels (for\n example due to biases in the human labelers), by the resources used (like lexicons,\n or model components like pretrained embeddings), or even by model architecture\n (like what the model is trained to optimize). While the mitigation of these biases\n (for example by carefully considering the training data sources) is an important area\n of research, we currently don‚Äôt have general solutions. For this reason it‚Äôs important,\n when introducing any NLP model, to study these kinds of factors and make them\n model card clear. One way to do this is by releasing a model card (Mitchell et al., 2019) for\n each version of a model. A model card documents a machine learning model with\n information like:\n ‚Ä¢ training algorithms and parameters\n ‚Ä¢ training data sources, motivation, and preprocessing\n ‚Ä¢ evaluation data sources, motivation, and preprocessing\n ‚Ä¢ intended use and users\n ‚Ä¢ model performance across different demographic or other groups and environmental situations\n\n Often we want to know more than just the correct classification of an observation.\n We want to know why the classifier made the decision it did. That is, we want our\n interpretable decision to be interpretable. Interpretability can be hard to define strictly, but the\n core idea is that as humans we should know why our algorithms reach the conclusions they do. Because the features to logistic regression are often human-designed,\n one way to understand a classifier‚Äôs decision is to understand the role each feature\n plays in the decision. Logistic regression can be combined with statistical tests (the\n likelihood ratio test, or the Wald test); investigating whether a particular feature is\n significant by one of these tests, or inspecting its magnitude (how large is the weight\n w associated with the feature?) can help us interpret why the classifier made the\n decision it makes. This is enormously important for building transparent models.\n Furthermore, in addition to its use as a classifier, logistic regression in NLP and\n many other fields is widely used as an analytic tool for testing hypotheses about the\n effect of various explanatory variables (features). In text classification, perhaps we\n want to know if logically negative words (no, not, never) are more likely to be asso-\n30 C HAPTER 4 ‚Ä¢ L OGISTIC R EGRESSION\n\n ciated with negative sentiment, or if negative reviews of movies are more likely to\n discuss the cinematography. However, in doing so it‚Äôs necessary to control for potential confounds: other factors that might influence sentiment (the movie genre, the\n year it was made, perhaps the length of the review in words). Or we might be studying the relationship between NLP-extracted linguistic features and non-linguistic\n outcomes (hospital readmissions, political outcomes, or product sales), but need to\n control for confounds (the age of the patient, the county of voting, the brand of the\n product). In such cases, logistic regression allows us to test whether some feature is\n associated with some outcome above and beyond the effect of other features.\n\n Numquam ponenda est pluralitas sine necessitate\n ‚ÄòPlurality should never be proposed unless needed‚Äô\n William of Occam\n\n There is a problem with learning weights that make the model perfectly match the\n training data. If a feature is perfectly predictive of the outcome because it happens\n to only occur in one class, it will be assigned a very high weight. The weights for\n features will attempt to perfectly fit details of the training set, in fact too perfectly,\n modeling noisy factors that just accidentally correlate with the class. This problem is\n overfitting called overfitting. A good model should be able to generalize well from the training\n generalize data to the unseen test set, but a model that overfits will have poor generalization.\n regularization To avoid overfitting, a new regularization term R(Œ∏ ) is added to the loss function in Eq. 4.25, resulting in the following loss for a batch of m examples (slightly\n rewritten from Eq. 4.25 to be maximizing log probability rather than minimizing\n loss, and removing the m1 term which doesn‚Äôt affect the argmax):\n\n m\n X\n Œ∏ÃÇ = argmax log P(y(i) |x(i) ) ‚àí Œ±R(Œ∏ ) (4.48)\n Œ∏ i=1\n\n The new regularization term R(Œ∏ ) is used to penalize large weights. Thus a setting of\n the weights that matches the training data perfectly‚Äî but uses many weights with\n high values to do so‚Äîwill be penalized more than a setting that matches the data\n a little less well, but does so using smaller weights. The higher the regularization\n strength parameter Œ±, the lower the model‚Äôs weights will be, reducing its reliance on\n the training data.\n There are two common ways to compute this regularization term R(Œ∏ ). L2 reg-\nL2\n regularization ularization is a quadratic function of the weight values, named because it uses the\n (square of the) L2 norm of the weight values. The L2 norm, ||Œ∏ ||2 , is the same as\n the Euclidean distance of the vector Œ∏ from the origin. If Œ∏ consists of n weights,\n then:\n n\n X\n R(Œ∏ ) = ||Œ∏ ||22 = Œ∏ j2 (4.49)\n j=1\n 4.14 ‚Ä¢ A DVANCED : R EGULARIZATION 31\n\n The L2 regularized loss function becomes:\n \" m # n\n X X\n (i) (i)\n Œ∏ÃÇ = argmax log P(y |x ; Œ∏ ) ‚àí Œ± Œ∏ j2 (4.50)\n Œ∏ i=1 j=1\n\n L1\nregularization L1 regularization is a linear function of the weight values, named after the L1 norm\n ||W ||1 , the sum of the absolute values of the weights, or Manhattan distance (the\n Manhattan distance is the distance you‚Äôd have to walk between two points in a city\n with a street grid like New York):\n n\n X\n R(Œ∏ ) = ||Œ∏ ||1 = |Œ∏i | (4.51)\n i=1\n\n The L1 regularized loss function becomes:\n \" m # n\n X X\n (i) (i)\n Œ∏ÃÇ = argmax log P(y |x ; Œ∏ ) ‚àí Œ± |Œ∏ j | (4.52)\n Œ∏ i=1 j=1\n\n These kinds of regularization come from statistics, where L1 regularization is called\n lasso lasso regression (Tibshirani, 1996) and L2 regularization is called ridge regression,\n ridge and both are commonly used in language processing. L2 regularization is easier to\n optimize because of its simple derivative (the derivative of Œ∏ 2 is just 2Œ∏ ), while\n L1 regularization is more complex (the derivative of |Œ∏ | is non-continuous at zero).\n But while L2 prefers weight vectors with many small weights, L1 prefers sparse\n solutions with some larger weights but many more weights set to zero. Thus L1\n regularization leads to much sparser weight vectors, that is, far fewer features.\n Both L1 and L2 regularization have Bayesian interpretations as constraints on\n the prior of how weights should look. L1 regularization can be viewed as a Laplace\n prior on the weights. L2 regularization corresponds to assuming that weights are\n distributed according to a Gaussian distribution with mean ¬µ = 0. In a Gaussian\n or normal distribution, the further away a value is from the mean, the lower its\n probability (scaled by the variance œÉ ). By using a Gaussian prior on the weights, we\n are saying that weights prefer to have the value 0. A Gaussian for a weight Œ∏ j is\n !\n 1 (Œ∏ j ‚àí ¬µ j )2\n exp ‚àí (4.53)\n 2œÉ 2j\n q\n 2œÄœÉ 2 j\n\n If we multiply each weight by a Gaussian prior on the weight, we are thus maximizing the following constraint:\n m n\n !\n Y\n (i) (i)\n Y 1 (Œ∏ j ‚àí ¬µ j )2\n Œ∏ÃÇ = argmax P(y |x ) √ó exp ‚àí (4.54)\n 2œÉ 2j\n q\n Œ∏ i=1 j=1 2œÄœÉ 2 j\n\n which in log space, with ¬µ = 0, and assuming 2œÉ 2 = 1, corresponds to\n m\n X n\n X\n (i) (i)\n Œ∏ÃÇ = argmax log P(y |x ) ‚àí Œ± Œ∏ j2 (4.55)\n Œ∏ i=1 j=1\n\n which is in the same form as Eq. 4.50.\n32 C HAPTER 4 ‚Ä¢ L OGISTIC R EGRESSION\n\n In this section we give the derivation of the gradient of the cross-entropy loss function LCE for logistic regression. Let‚Äôs start with some quick calculus refreshers.\n First, the derivative of ln(x):\n\n d 1\n ln(x) = (4.56)\n dx x\n Second, the (very elegant) derivative of the sigmoid:\n\n dœÉ (z)\n = œÉ (z)(1 ‚àí œÉ (z)) (4.57)\n dz\n chain rule Finally, the chain rule of derivatives. Suppose we are computing the derivative\n of a composite function f (x) = u(v(x)). The derivative of f (x) is the derivative of\n u(x) with respect to v(x) times the derivative of v(x) with respect to x:\n\n df du dv\n = ¬∑ (4.58)\n dx dv dx\n First, we want to know the derivative of the loss function with respect to a single\n weight w j (we‚Äôll need to compute it for each weight, and for the bias):\n\n ‚àÇ LCE ‚àÇ\n = ‚àí [y log œÉ (w ¬∑ x + b) + (1 ‚àí y) log (1 ‚àí œÉ (w ¬∑ x + b))]\n ‚àÇwj ‚àÇwj\n \u0014 \u0015\n ‚àÇ ‚àÇ\n = ‚àí y log œÉ (w ¬∑ x + b) + (1 ‚àí y) log [1 ‚àí œÉ (w ¬∑ x + b)]\n ‚àÇwj ‚àÇwj\n (4.59)\n\n Next, using the chain rule, and relying on the derivative of log:\n\n ‚àÇ LCE y ‚àÇ 1‚àíy ‚àÇ\n = ‚àí œÉ (w ¬∑ x + b) ‚àí [1 ‚àí œÉ (w ¬∑ x + b)]\n ‚àÇwj œÉ (w ¬∑ x + b) ‚àÇ w j 1 ‚àí œÉ (w ¬∑ x + b) ‚àÇ w j\n (4.60)\n\n Rearranging terms:\n \u0014 \u0015\n ‚àÇ LCE y 1‚àíy ‚àÇ\n = ‚àí ‚àí œÉ (w ¬∑ x + b)\n ‚àÇwj œÉ (w ¬∑ x + b) 1 ‚àí œÉ (w ¬∑ x + b) ‚àÇ w j\n\n And now plugging in the derivative of the sigmoid, and using the chain rule one\n more time, we end up with Eq. 4.61:\n \u0014 \u0015\n ‚àÇ LCE y ‚àí œÉ (w ¬∑ x + b) ‚àÇ (w ¬∑ x + b)\n = ‚àí œÉ (w ¬∑ x + b)[1 ‚àí œÉ (w ¬∑ x + b)]\n ‚àÇwj œÉ (w ¬∑ x + b)[1 ‚àí œÉ (w ¬∑ x + b)] ‚àÇwj\n \u0014 \u0015\n y ‚àí œÉ (w ¬∑ x + b)\n = ‚àí œÉ (w ¬∑ x + b)[1 ‚àí œÉ (w ¬∑ x + b)]x j\n œÉ (w ¬∑ x + b)[1 ‚àí œÉ (w ¬∑ x + b)]\n = ‚àí[y ‚àí œÉ (w ¬∑ x + b)]x j\n = [œÉ (w ¬∑ x + b) ‚àí y]x j (4.61)\n 4.16 ‚Ä¢ S UMMARY 33\n\n This chapter introduced the logistic regression model of classification.\n ‚Ä¢ Logistic regression is a supervised machine learning classifier that extracts\n real-valued features from the input, multiplies each by a weight, sums them,\n and passes the sum through a sigmoid function to generate a probability. A\n threshold is used to make a decision.\n ‚Ä¢ Logistic regression can be used with two classes (e.g., positive and negative\n sentiment) or with multiple classes (multinomial logistic regression, for example for n-ary text classification, part-of-speech labeling, etc.).\n ‚Ä¢ Multinomial logistic regression uses the softmax function to compute probabilities.\n ‚Ä¢ The weights (vector w and bias b) are learned from a labeled training set via a\n loss function, such as the cross-entropy loss, that must be minimized.\n ‚Ä¢ Minimizing this loss function is a convex optimization problem, and iterative\n algorithms like gradient descent are used to find the optimal weights.\n ‚Ä¢ Regularization is used to avoid overfitting.\n ‚Ä¢ Logistic regression is also one of the most useful analytic tools, because of its\n ability to transparently study the importance of individual features.\n\nHistorical Notes\n Logistic regression was developed in the field of statistics, where it was used for\n the analysis of binary data by the 1960s, and was particularly common in medicine\n (Cox, 1969). Starting in the late 1970s it became widely used in linguistics as one\n of the formal foundations of the study of linguistic variation (Sankoff and Labov,\n 1979).\n Nonetheless, logistic regression didn‚Äôt become common in natural language processing until the 1990s, when it seems to have appeared simultaneously from two\n directions. The first source was the neighboring fields of information retrieval and\n speech processing, both of which had made use of regression, and both of which\n lent many other statistical techniques to NLP. Indeed a very early use of logistic\n regression for document routing was one of the first NLP applications to use (LSI)\n embeddings as word representations (SchuÃàtze et al., 1995).\n At the same time in the early 1990s logistic regression was developed and apmaximum\n entropy plied to NLP at IBM Research under the name maximum entropy modeling or\n maxent (Berger et al., 1996), seemingly independent of the statistical literature. Under that name it was applied to language modeling (Rosenfeld, 1996), part-of-speech\n tagging (Ratnaparkhi, 1996), parsing (Ratnaparkhi, 1997), coreference resolution\n (Kehler, 1997), and text classification (Nigam et al., 1999).\n There are a variety of sources covering the many kinds of text classification\n tasks. For sentiment analysis see Pang and Lee (2008), and Liu and Zhang (2012).\n Stamatatos (2009) surveys authorship attribute algorithms. On language identification see Jauhiainen et al. (2019); Jaech et al. (2016) is an important early neural\n system. The task of newswire indexing was often used as a test case for text classification algorithms, based on the Reuters-21578 collection of newswire articles.\n See Manning et al. (2008) and Aggarwal and Zhai (2012) on text classification;\n classification in general is covered in machine learning textbooks (Hastie et al. 2001,\n34 C HAPTER 4 ‚Ä¢ L OGISTIC R EGRESSION\n\n Witten and Frank 2005, Bishop 2006, Murphy 2012).\n Non-parametric methods for computing statistical significance were used first in\n NLP in the MUC competition (Chinchor et al., 1993), and even earlier in speech\n recognition (Gillick and Cox 1989, Bisani and Ney 2004). Our description of the\n bootstrap draws on the description in Berg-Kirkpatrick et al. (2012). Recent work\n has focused on issues including multiple test sets and multiple metrics (S√∏gaard et al.\n 2014, Dror et al. 2017).\n Feature selection is a method of removing features that are unlikely to generalize\n well. Features are generally ranked by how informative they are about the classificainformation\n gain tion decision. A very common metric, information gain, tells us how many bits of\n information the presence of the word gives us for guessing the class. Other feature\n selection metrics include œá 2 , pointwise mutual information, and GINI index; see\n Yang and Pedersen (1997) for a comparison and Guyon and Elisseeff (2003) for an\n introduction to feature selection.\n\nExercises\n Exercises 35\n\nAggarwal, C. C. and C. Zhai. 2012. A survey of text classi- Hastie, T., R. J. Tibshirani, and J. H. Friedman. 2001. The\n fication algorithms. In C. C. Aggarwal and C. Zhai, eds, Elements of Statistical Learning. Springer.\n Mining text data, 163‚Äì222. Springer. Hutchinson, B., V. Prabhakaran, E. Denton, K. Webster,\nBerg-Kirkpatrick, T., D. Burkett, and D. Klein. 2012. An Y. Zhong, and S. Denuyl. 2020. Social biases in NLP\n empirical investigation of statistical significance in NLP. models as barriers for persons with disabilities. ACL.\n EMNLP. Jaech, A., G. Mulcaire, S. Hathi, M. Ostendorf, and N. A.\nBerger, A., S. A. Della Pietra, and V. J. Della Pietra. 1996. A Smith. 2016. Hierarchical character-word models for lanmaximum entropy approach to natural language process- guage identification. ACL Workshop on NLP for Social\n ing. Computational Linguistics, 22(1):39‚Äì71. Media.\nBisani, M. and H. Ney. 2004. Bootstrap estimates for confi- Jauhiainen, T., M. Lui, M. Zampieri, T. Baldwin, and\n dence intervals in ASR performance evaluation. ICASSP. K. LindeÃÅn. 2019. Automatic language identification in\nBishop, C. M. 2006. Pattern recognition and machine learn- texts: A survey. JAIR, 65(1):675‚Äì682.\n ing. Springer. Kehler, A. 1997. Probabilistic coreference in information\nBlodgett, S. L., S. Barocas, H. DaumeÃÅ III, and H. Wallach. extraction. EMNLP.\n 2020. Language (technology) is power: A critical survey Kiritchenko, S. and S. M. Mohammad. 2018. Examining\n of ‚Äúbias‚Äù in NLP. ACL. gender and race bias in two hundred sentiment analysis\nBorges, J. L. 1964. The analytical language of john wilkins. systems. *SEM.\n In Other inquisitions 1937‚Äì1952. University of Texas Liu, B. and L. Zhang. 2012. A survey of opinion mining and\n Press. Trans. Ruth L. C. Simms. sentiment analysis. In C. C. Aggarwal and C. Zhai, eds,\nCaliskan, A., J. J. Bryson, and A. Narayanan. 2017. Seman- Mining text data, 415‚Äì464. Springer.\n tics derived automatically from language corpora contain Manning, C. D., P. Raghavan, and H. SchuÃàtze. 2008. Introhuman-like biases. Science, 356(6334):183‚Äì186. duction to Information Retrieval. Cambridge.\nChinchor, N., L. Hirschman, and D. L. Lewis. 1993. Eval- Mitchell, M., S. Wu, A. Zaldivar, P. Barnes, L. Vasserman,\n uating Message Understanding systems: An analysis of B. Hutchinson, E. Spitzer, I. D. Raji, and T. Gebru. 2019.\n the third Message Understanding Conference. Computa- Model cards for model reporting. ACM FAccT.\n tional Linguistics, 19(3):409‚Äì449. Murphy, K. P. 2012. Machine learning: A probabilistic per-\nCox, D. 1969. Analysis of Binary Data. Chapman and Hall, spective. MIT Press.\n London. Nigam, K., J. D. Lafferty, and A. McCallum. 1999. Using\nCrawford, K. 2017. The trouble with bias. Keynote at maximum entropy for text classification. IJCAI-99 work-\nNeurIPS. shop on machine learning for information filtering.\nDavidson, T., D. Bhattacharya, and I. Weber. 2019. Racial Noreen, E. W. 1989. Computer Intensive Methods for Testing\n bias in hate speech and abusive language detection Hypothesis. Wiley.\n datasets. Third Workshop on Abusive Language Online. Pang, B. and L. Lee. 2008. Opinion mining and sentiment\nDias Oliva, T., D. Antonialli, and A. Gomes. 2021. Fighting analysis. Foundations and trends in information retrieval,\n hate speech, silencing drag queens? artificial intelligence 2(1-2):1‚Äì135.\n in content moderation and risks to lgbtq voices online. Park, J. H., J. Shin, and P. Fung. 2018. Reducing gender bias\n Sexuality & Culture, 25:700‚Äì732. in abusive language detection. EMNLP.\nDixon, L., J. Li, J. Sorensen, N. Thain, and L. Vasserman. Popp, D., R. A. Donovan, M. Crawford, K. L. Marsh, and\n 2018. Measuring and mitigating unintended bias in text M. Peele. 2003. Gender, race, and speech style stereoclassification. 2018 AAAI/ACM Conference on AI, Ethics, types. Sex Roles, 48(7-8):317‚Äì325.\n and Society.\n Ratnaparkhi, A. 1996. A maximum entropy part-of-speech\nDoumbouya, M. K. B., D. Jurafsky, and C. D. Manning. tagger. EMNLP.\n 2025. Tversky neural networks: Psychologically plausible deep learning with differentiable tversky similarity. Ratnaparkhi, A. 1997. A linear observed time statistical\n ArXiv preprint. parser based on maximum entropy models. EMNLP.\nDror, R., G. Baumer, M. Bogomolov, and R. Reichart. 2017. Rosenfeld, R. 1996. A maximum entropy approach to adap-\nReplicability analysis for natural language processing: tive statistical language modeling. Computer Speech and\n Testing significance with multiple datasets. TACL, 5:471‚Äì Language, 10:187‚Äì228.\n ‚Äì486. Sankoff, D. and W. Labov. 1979. On the uses of variable\nDror, R., L. Peled-Cohen, S. Shlomov, and R. Reichart. rules. Language in society, 8(2-3):189‚Äì222.\n 2020. Statistical Significance Testing for Natural Lan- Sap, M., D. Card, S. Gabriel, Y. Choi, and N. A. Smith. 2019.\n guage Processing, volume 45 of Synthesis Lectures on The risk of racial bias in hate speech detection. ACL.\n Human Language Technologies. Morgan & Claypool. SchuÃàtze, H., D. A. Hull, and J. Pedersen. 1995. A compar-\nEfron, B. and R. J. Tibshirani. 1993. An introduction to the ison of classifiers and document representations for the\n bootstrap. CRC press. routing problem. SIGIR-95.\nGillick, L. and S. J. Cox. 1989. Some statistical issues in the S√∏gaard, A., A. Johannsen, B. Plank, D. Hovy, and H. M.\n comparison of speech recognition algorithms. ICASSP. Alonso. 2014. What‚Äôs in a p-value in NLP? CoNLL.\nGuyon, I. and A. Elisseeff. 2003. An introduction to variable Stamatatos, E. 2009. A survey of modern authorship attribuand feature selection. JMLR, 3:1157‚Äì1182. tion methods. JASIST, 60(3):538‚Äì556.\n36 Chapter 4 ‚Ä¢ Logistic Regression\n\nTibshirani, R. J. 1996. Regression shrinkage and selection\n via the lasso. Journal of the Royal Statistical Society. Series B (Methodological), 58(1):267‚Äì288.\nvan Rijsbergen, C. J. 1975. Information Retrieval. Butterworths.\nWitten, I. H. and E. Frank. 2005. Data Mining: Practical Machine Learning Tools and Techniques, 2nd edition.\n Morgan Kaufmann.\nYang, Y. and J. Pedersen. 1997. A comparative study on\n feature selection in text categorization. ICML.\n",
    "file_path": "/Users/colinsidberry/nlp-textbook-rag/data/extracted_text/logistic-regression.txt",
    "file_size_kb": 96.52
  },
  {
    "id": "1abcdbecfd04b51d",
    "source": "nlp_textbook",
    "chapter": "Embeddings 5 ËçÉËÄÖÊâÄ‰ª•Âú®È±ºÔºåÂæóÈ±ºËÄåÂøòËçÉ Nets are for fish; Once you get the fish, you can forget the net.",
    "filename": "embeddings.txt",
    "content": "rights reserved. Draft of August 24, 2025.\n\nCHAPTER\n\n Embeddings\n5 ËçÉËÄÖÊâÄ‰ª•Âú®È±ºÔºåÂæóÈ±ºËÄåÂøòËçÉ Nets are for fish;\n Once you get the fish, you can forget the net.\n Ë®ÄËÄÖÊâÄ‰ª•Âú®ÊÑèÔºåÂæóÊÑèËÄåÂøòË®Ä Words are for meaning;\n Once you get the meaning, you can forget the words\n Â∫ÑÂ≠ê(Zhuangzi), Chapter 26\n\n The asphalt that Los Angeles is famous for occurs mainly on its freeways. But\n in the middle of the city is another patch of asphalt, the La Brea tar pits, and this\n asphalt preserves millions of fossil bones from the last of the Ice Ages of the Pleistocene Epoch. One of these fossils is the Smilodon, or saber-toothed tiger, instantly\n recognizable by its long canines. Five million years ago or so, a completely different\n saber-tooth tiger called Thylacosmilus lived\n in Argentina and other parts of South America. Thylacosmilus was a marsupial whereas\n Smilodon was a placental mammal, but Thylacosmilus had the same long upper canines\n and, like Smilodon, had a protective bone\n flange on the lower jaw. The similarity of\n these two mammals is one of many examples\n of parallel or convergent evolution, in which particular contexts or environments\n lead to the evolution of very similar structures in different species (Gould, 1980).\n The role of context is also important in the similarity of a less biological kind\n of organism: the word. Words that occur in similar contexts tend to have similar\n meanings. This link between similarity in how words are distributed and similarity\n distributional\n hypothesis in what they mean is called the distributional hypothesis. The hypothesis was\n first formulated in the 1950s by linguists like Joos (1950), Harris (1954), and Firth\n (1957), who noticed that words which are synonyms (like oculist and eye-doctor)\n tended to occur in the same environment (e.g., near words like eye or examined)\n with the amount of meaning difference between two words ‚Äúcorresponding roughly\n to the amount of difference in their environments‚Äù (Harris, 1954, p. 157).\n embeddings In this chapter we introduce embeddings, vector representations of the meaning\n of words that are learned directly from word distributions in texts. Embeddings lie\n at the heart of large language models and other modern applications. The static embeddings we introduce here underlie the more powerful dynamic or contextualized\n embeddings like BERT that we will see in Chapter 10 and Chapter 8.\n The linguistic field that studies embeddings and their meanings is called vector\n vector semantics. Embeddings are also the first example in this book of representation\n semantics\n representation\n learning learning, automatically learning useful representations of the input text. Finding\n such self-supervised ways to learn representations of language, instead of creating representations by hand via feature engineering, is an important principle of\n modern NLP (Bengio et al., 2013).\n2 C HAPTER 5 ‚Ä¢ E MBEDDINGS\n\n Let‚Äôs begin by introducing some basic principles of word meaning. How should\n we represent the meaning of a word? In the n-gram models of Chapter 3, and in\n classical NLP applications, our only representation of a word is as a string of letters,\n or an index in a vocabulary list. This representation is not that different from a\n tradition in philosophy, perhaps you‚Äôve seen it in introductory logic classes, in which\n the meaning of words is represented by just spelling the word with small capital\n letters; representing the meaning of ‚Äúdog‚Äù as DOG, and ‚Äúcat‚Äù as CAT, or by using an\n apostrophe (DOG ‚Äô).\n Representing the meaning of a word by capitalizing it is a pretty unsatisfactory\n model. You might have seen a version of a joke due originally to semanticist Barbara\n Partee (Carlson, 1977):\n Q: What‚Äôs the meaning of life?\n A: LIFE ‚Äô\n Surely we can do better than this! After all, we‚Äôll want a model of word meaning\n to do all sorts of things for us. It should tell us that some words have similar meanings (cat is similar to dog), others are antonyms (cold is the opposite of hot), some\n have positive connotations (happy) while others have negative connotations (sad). It\n should represent the fact that the meanings of buy, sell, and pay offer differing perspectives on the same underlying purchasing event. (If I buy something from you,\n you‚Äôve probably sold it to me, and I likely paid you.) More generally, a model of\n word meaning should allow us to draw inferences to address meaning-related tasks\n like question-answering or dialogue.\n In this section we summarize some of these desiderata, drawing on results in the\n lexical linguistic study of word meaning, which is called lexical semantics; we‚Äôll return to\n semantics\n and expand on this list in Appendix G and Chapter 21.\n Lemmas and Senses Let‚Äôs start by looking at how one word (we‚Äôll choose mouse)\n might be defined in a dictionary (simplified from the online dictionary WordNet):\n mouse (N)\n 1. any of numerous small rodents...\n 2. a hand-operated device that controls a cursor...\n lemma Here the form mouse is the lemma, also called the citation form. The form\n citation form mouse would also be the lemma for the word mice; dictionaries don‚Äôt have separate\n definitions for inflected forms like mice. Similarly sing is the lemma for sing, sang,\n sung. In many languages the infinitive form is used as the lemma for the verb, so\n Spanish dormir ‚Äúto sleep‚Äù is the lemma for duermes ‚Äúyou sleep‚Äù. The specific forms\n wordform sung or carpets or sing or duermes are called wordforms.\n As the example above shows, each lemma can have multiple meanings; the\n lemma mouse can refer to the rodent or the cursor control device. We call each\n of these aspects of the meaning of mouse a word sense. The fact that lemmas can\n be polysemous (have multiple senses) can make interpretation difficult (is someone who searches for ‚Äúmouse info‚Äù looking for a pet or a widget?). Chapter 10\n and Appendix G will discuss the problem of polysemy, and introduce word sense\n disambiguation, the task of determining which sense of a word is being used in a\n particular context.\n Synonymy One important component of word meaning is the relationship between word senses. For example when one word has a sense whose meaning is\n 5.1 ‚Ä¢ L EXICAL S EMANTICS 3\n\n identical to a sense of another word, or nearly identical, we say the two senses of\n synonym those two words are synonyms. Synonyms include such pairs as\n couch/sofa vomit/throw up filbert/hazelnut car/automobile\n A more formal definition of synonymy (between words rather than senses) is that\n two words are synonymous if they are substitutable for one another in any sentence\n without changing the truth conditions of the sentence, the situations in which the\n sentence would be true.\n While substitutions between some pairs of words like car / automobile or water / H2 O are truth preserving, the words are still not identical in meaning. Indeed,\n probably no two words are absolutely identical in meaning. One of the fundamental\n principle of tenets of semantics, called the principle of contrast (Girard 1718, BreÃÅal 1897, Clark\n contrast\n 1987), states that a difference in linguistic form is always associated with some difference in meaning. For example, the word H2 O is used in scientific contexts and\n would be inappropriate in a hiking guide‚Äîwater would be more appropriate‚Äî and\n this genre difference is part of the meaning of the word. In practice, the word synonym is therefore used to describe a relationship of approximate or rough synonymy.\n Word Similarity While words don‚Äôt have many synonyms, most words do have\n lots of similar words. Cat is not a synonym of dog, but cats and dogs are certainly\n similar words. In moving from synonymy to similarity, it will be useful to shift from\n talking about relations between word senses (like synonymy) to relations between\n words (like similarity). Dealing with words avoids having to commit to a particular\n representation of word senses, which will turn out to simplify our task.\n similarity The notion of word similarity is very useful in larger semantic tasks. Knowing\n how similar two words are can help in computing how similar the meaning of two\n phrases or sentences are, a very important component of tasks like question answering, paraphrasing, and summarization. One way of getting values for word similarity\n is to ask humans to judge how similar one word is to another. A number of datasets\n have resulted from such experiments. For example the SimLex-999 dataset (Hill\n et al., 2015) gives values on a scale from 0 to 10, like the examples below, which\n range from near-synonyms (vanish, disappear) to pairs that scarcely seem to have\n anything in common (hole, agreement):\n vanish disappear 9.8\n belief impression 5.95\n muscle bone 3.65\n modest flexible 0.98\n hole agreement 0.3\n\n Word Relatedness The meaning of two words can be related in ways other than\n relatedness similarity. One such class of connections is called word relatedness (Budanitsky\n association and Hirst, 2006), also traditionally called word association in psychology.\n Consider the meanings of the words coffee and cup. Coffee is not similar to cup;\n they share practically no features (coffee is a plant or a beverage, while a cup is a\n manufactured object with a particular shape). But coffee and cup are clearly related;\n they are associated by co-participating in an everyday event (the event of drinking\n coffee out of a cup). Similarly scalpel and surgeon are not similar but are related\n eventively (a surgeon tends to make use of a scalpel).\n One common kind of relatedness between words is if they belong to the same\nsemantic field semantic field. A semantic field is a set of words which cover a particular semantic\n domain and bear structured relations with each other. For example, words might be\n4 C HAPTER 5 ‚Ä¢ E MBEDDINGS\n\n related by being in the semantic field of hospitals (surgeon, scalpel, nurse, anesthetic, hospital), restaurants (waiter, menu, plate, food, chef), or houses (door, roof,\n topic models kitchen, family, bed). Semantic fields are also related to topic models, like Latent\n Dirichlet Allocation, LDA, which apply unsupervised learning on large sets of texts\n to induce sets of associated words from text. Semantic fields and topic models are\n very useful tools for discovering topical structure in documents.\n In Appendix G we‚Äôll introduce more relations between senses like hypernymy\n or IS-A, antonymy (opposites) and meronymy (part-whole relations).\n connotations Connotation Finally, words have affective meanings or connotations. The word\n connotation has different meanings in different fields, but here we use it to mean the\n aspects of a word‚Äôs meaning that are related to a writer or reader‚Äôs emotions, sentiment, opinions, or evaluations. For example some words have positive connotations\n (wonderful) while others have negative connotations (dreary). Even words whose\n meanings are similar in other ways can vary in connotation; consider the difference\n in connotations between fake, knockoff, forgery, on the one hand, and copy, replica,\n reproduction on the other, or innocent (positive connotation) and naive (negative\n connotation). Some words describe positive evaluation (great, love) and others negative evaluation (terrible, hate). Positive or negative evaluation language is called\n sentiment sentiment, as we saw in Appendix K, and word sentiment plays a role in important tasks like sentiment analysis, stance detection, and applications of NLP to the\n language of politics and consumer reviews.\n Early work on affective meaning (Osgood et al., 1957) found that words varied\n along three important dimensions of affective meaning:\n valence: the pleasantness of the stimulus\n arousal: the intensity of emotion provoked by the stimulus\n dominance: the degree of control exerted by the stimulus\n Thus words like happy or satisfied are high on valence, while unhappy or annoyed are low on valence. Excited is high on arousal, while calm is low on arousal.\n Controlling is high on dominance, while awed or influenced are low on dominance.\n Each word is thus represented by three numbers, corresponding to its value on each\n of the three dimensions:\n Valence Arousal Dominance\n courageous 8.0 5.5 7.4\n music 7.7 5.6 6.5\n heartbreak 2.5 5.7 3.6\n cub 6.7 4.0 4.2\n Osgood et al. (1957) noticed that in using these 3 numbers to represent the\n meaning of a word, the model was representing each word as a point in a threedimensional space, a vector whose three dimensions corresponded to the word‚Äôs\n rating on the three scales. This revolutionary idea that word meaning could be represented as a point in space (e.g., that part of the meaning of heartbreak can be\n represented as the point [2.5, 5.7, 3.6]) was the first expression of the vector semantics models that we introduce next.\n\n vector Vector semantics is the standard way to represent word meaning in NLP, helping\n semantics\n 5.2 ‚Ä¢ V ECTOR S EMANTICS : T HE I NTUITION 5\n\n us model many of the aspects of word meaning we saw in the previous section. The\n roots of the model lie in the 1950s when two big ideas converged: Osgood‚Äôs 1957\n idea mentioned above to use a point in three-dimensional space to represent the\n connotation of a word, and the proposal by linguists like Joos (1950), Harris (1954),\n and Firth (1957) to define the meaning of a word by its distribution in language\n use, meaning its neighboring words or grammatical environments. Their idea was\n that two words that occur in very similar distributions (whose neighboring words are\n similar) have similar meanings.\n For example, suppose you didn‚Äôt know the meaning of the word ongchoi (a recent borrowing from Cantonese) but you see it in the following contexts:\n (5.1) Ongchoi is delicious sauteed with garlic.\n (5.2) Ongchoi is superb over rice.\n (5.3) ...ongchoi leaves with salty sauces...\n And suppose that you had seen many of these context words in other contexts:\n (5.4) ...spinach sauteed with garlic over rice...\n (5.5) ...chard stems and leaves are delicious...\n (5.6) ...collard greens and other salty leafy greens\n The fact that ongchoi occurs with words like rice and garlic and delicious and\n salty, as do words like spinach, chard, and collard greens might suggest that ongchoi\n is a leafy green similar to these other leafy greens.1 We can implement the same\n intuition computationally by just counting words in the context of ongchoi.\n\n https://projector.tensorflow.org/.\n\n The idea of vector semantics is to represent a word as a point in a multidimensional semantic space that is derived (in different ways we‚Äôll see) from the distriembeddings butions of word neighbors. Vectors for representing words are called embeddings.\n The word ‚Äúembedding‚Äù derives historically from its mathematical sense as a mapping from one space or structure to another, although the meaning has shifted; see\n the end of the chapter.\n Fig. 5.1 shows a visualization of embeddings learned by the word2vec algorithm,\n showing the location of selected words (neighbors of ‚Äúsweet‚Äù) projected down from\n 1 It‚Äôs in fact Ipomoea aquatica, a relative of morning glory sometimes called water spinach in English.\n6 C HAPTER 5 ‚Ä¢ E MBEDDINGS\n\n 200-dimensional space into a 2-dimensional space. Note that the nearest neighbors\n of sweet are semantically related words like honey, candy, juice, chocolate. This idea\n that similar words are near each other in high-dimensional space is an important\n that offers enormous power to language models and other NLP applications. For\n example the sentiment classifiers of Chapter 4 depend on the same words appearing\n in the training and test sets. But by representing words as embeddings, a classifier\n can assign sentiment as long as it sees some words with similar meanings. And as\n we‚Äôll see, vector semantic models like the ones showed in Fig. 5.1 can be learned\n automatically from text without supervision.\n In this chapter we‚Äôll begin with a simple pedagogical model of embeddings in\n which the meaning of a word is defined by a vector with the counts of nearby words.\n We introduce this model as a helpful way to understand the concept of vectors and\n what it means for a vector to be a representation of word meaning, but more sophisticated variants like the tf-idf model we will introduce in Chapter 11 are important\n methods you should understand. We will see that this method results in very long\n vectors that are sparse, i.e. mostly zeros (since most words simply never occur in the\n context of others). We‚Äôll then introduce the word2vec model family for constructing\n short, dense vectors that have even more useful semantic properties.\n We‚Äôll also introduce the cosine, the standard way to use embeddings to compute semantic similarity, between two words, two sentences, or two documents, an\n important tool in practical applications.\n\n ‚ÄúThe most important attributes of a vector in 3-space are {Location, Location, Location}‚Äù\n Randall Munroe, the hover from https://xkcd.com/2358/\n Let‚Äôs now introduce the first way to compute word vector embeddings. This simplest vector model of meaning is based on the co-occurrence matrix, a way of representing how often words co-occur. We‚Äôll define a particular kind of co-occurrence\n word-context matrix, the word-context matrix, in which each row in the matrix represents a word\n matrix\n in the vocabulary and each column represents how often each other word in the vocabulary appears nearby. This matrix is thus of dimensionality |V | √ó |V | and each\n cell records the number of times the row (target) word and the column (context)\n word co-occur nearby in some training corpus.\n What do we mean by ‚Äònearby‚Äô? We could implement various methods, but let‚Äôs\n start with a very simple one: a context window around the word, let‚Äôs say of 4 words\n to the left and 4 words to the right. If we do that, each cell will represents the\n number of times (in some training corpus) the column word occurs in such a ¬±4\n word window around the row word.\n Let‚Äôs see how this works for 4 words: cherry, strawberry, digital, and information. For each word we took a single instance from a corpus, and we show the ¬±4\n word window from that instance:\n is traditionally followed by cherry pie, a traditional dessert\n often mixed, such as strawberry rhubarb pie. Apple pie\n computer peripherals and personal digital assistants. These devices usually\n a computer. This includes information available on the internet\n If we then take every occurrence of each word in a large corpus and count the\n context words around it, we get a word-context co-occurrence matrix. The full word-\n5.3 ‚Ä¢ S IMPLE COUNT- BASED EMBEDDINGS 7\n\n context co-occurrence matrix is very large, because for each word in the vocabulary\n (since |V |) we have to count how often it occurs with every other word in the vocabulary, hence dimensionality |V | √ó |V |. Let‚Äôs therefore instead sketch the process\n on a smaller scale. Imagine that we are going to look at only the 4 words, and only\n consider the following 3 context words: a, computer, and pie. Furthermore let‚Äôs\n assume we only count occurrences in the mini-corpus above.\n So before looking at Fig. 5.2, compute by hand the counts for these 3 context\n words for the four words cherry, strawberry, digital, and information.\n\n a computer pie\n cherry 1 0 1\n strawberry 0 0 2\n digital 0 1 0\n information 1 1 0\n showing just 3 of the potential context word dimensions. The vector for cherry is outlined in\n red. Note that a real vector would have vastly more dimensions and thus be even sparser.\n\n Hopefully your count matches what is shown in Fig. 5.2, so that each cell represents the number of times a particular word (defined by the row) occurs in a particular context (defined by the word column).\n Each row, then, is a vector representing a word. To review some basic linear\n vector algebra, a vector is, at heart, just a list or array of numbers. So cherry is represented\n as the list [1,0,1] (the first row vector in Fig. 5.2) and information is represented as\n the list [1,1,0] (the fourth row vector).\nvector space A vector space is a collection of vectors, and is characterized by its dimension.\n dimension Vectors in a 3-dimensional vector space have an element for each dimension of the\n space. We will loosely refer to a vector in a 3-dimensional space as a 3-dimensional\n vector, with one element along each dimension. In the example in Fig. 5.2, we‚Äôve\n chosen to make the document vectors of dimension 3, just so they fit on the page; in\n real term-document matrices, the document vectors would have dimensionality |V |,\n the vocabulary size.\n The ordering of the numbers in a vector space indicates the different dimensions\n on which documents vary. The third dimension for all these vectors corresponds\n to the number of times pie occurs in the context. The second dimension for all of\n them corresponds to the number of times the word computer occurs. Notice that\n the vectors for information and digital have the same value (1) for this ‚Äúcomputer‚Äù\n dimension.\n In reality, we don‚Äôt compute word vectors on a single context window. Instead,\n we compute them over an entire corpus. Let‚Äôs see what some real counts look like.\n Let‚Äôs look at some vectors computed in this way. Fig. 5.3 shows a subset of the\n word-word co-occurrence matrix for these four words, where, again because it‚Äôs\n impossible to visualize all |V | possible context words on the page of this textbook,\n we show a subset of 6 of the dimensions, with counts computed from the Wikipedia\n corpus (Davies, 2015).\n Note in Fig. 5.3 that the two words cherry and strawberry are more similar to\n each other (both pie and sugar tend to occur in their window) than they are to other\n words like digital; conversely, digital and information are more similar to each other\n than, say, to strawberry.\n We can think of the vector for a document as a point in |V |-dimensional space;\n thus the documents in Fig. 5.3 are points in 3-dimensional space. Fig. 5.4 shows a\n spatial visualization.\n8 C HAPTER 5 ‚Ä¢ E MBEDDINGS\n\n aardvark ... computer data result pie sugar ...\n cherry 0 ... 2 8 9 442 25 ...\n strawberry 0 ... 0 0 1 60 19 ...\n digital 0 ... 1670 1683 85 5 4 ...\n information 0 ... 3325 3982 378 5 13 ...\n the dimensions (hand-picked for pedagogical purposes). The vector for digital is outlined in\n red. Note that a real vector would have vastly more dimensions and thus be much sparser, i.e.\n would have zero values in most dimensions.\n\n computer information\n 3000 [3982,3325]\n digital\n 2000 [1683,1670]\n\n 1000 2000 3000 4000\n data\n two of the dimensions, corresponding to the words data and computer.\n\n Note that |V |, the dimensionality of the vector, is generally the size of the vocabulary, often between 10,000 and 50,000 words (using the most frequent words\n in the training corpus; keeping words after about the most frequent 50,000 or so is\n generally not helpful). Since most of these numbers are zero these are sparse vector\n representations; there are efficient algorithms for storing and computing with sparse\n matrices.\n It‚Äôs also possible to applying various kinds of weighting functions to the counts\n in these cells. The most popular such weighting is tf-idf, which we‚Äôll introduce in\n Chapter 11, but there have historically been a wide variety of other weightings.\n Now that we have some intuitions, let‚Äôs move on to examine the details of computing word similarity.\n\n To measure similarity between two target words v and w, we need a metric that\n takes two vectors (of the same dimensionality, either both with words as dimensions,\n hence of length |V |, or both with documents as dimensions, of length |D|) and gives\n a measure of their similarity. By far the most common similarity metric is the cosine\n of the angle between the vectors.\n The cosine‚Äîlike most measures for vector similarity used in NLP‚Äîis based on\n dot product the dot product operator from linear algebra, also called the inner product:\n inner product\n N\n X\n dot product(v, w) = v ¬∑ w = vi wi = v1 w1 + v2 w2 + ... + vN wN (5.7)\n i=1\n\n The dot product acts as a similarity metric because it will tend to be high just when\n the two vectors have large values in the same dimensions. Alternatively, vectors that\n 5.4 ‚Ä¢ C OSINE FOR MEASURING SIMILARITY 9\n\n have zeros in different dimensions‚Äîorthogonal vectors‚Äîwill have a dot product of\n 0, representing their strong dissimilarity.\n This raw dot product, however, has a problem as a similarity metric: it favors\nvector length long vectors. The vector length is defined as\n v\n u N\n uX\n |v| = t v2i (5.8)\n i=1\n\n The dot product is higher if a vector is longer, with higher values in each dimension.\n More frequent words have longer vectors, since they tend to co-occur with more\n words and have higher co-occurrence values with each of them. The raw dot product\n thus will be higher for frequent words. But this is a problem; we‚Äôd like a similarity\n metric that tells us how similar two words are regardless of their frequency.\n We modify the dot product to normalize for the vector length by dividing the\n dot product by the lengths of each of the two vectors. This normalized dot product\n turns out to be the same as the cosine of the angle between the two vectors, following\n from the definition of the dot product between two vectors a and b:\n\n a ¬∑ b = |a||b| cos Œ∏\n a¬∑b\n = cos Œ∏ (5.9)\n cosine The cosine similarity metric between two vectors v and w thus can be computed as:\n N\n X\n vi wi\n v¬∑w i=1\n cosine(v, w) = =v v (5.10)\n |v||w| u\n uXN u N\n uX\n t v2 t iw2 i\n i=1 i=1\n\n For some applications we pre-normalize each vector, by dividing it by its length,\n unit vector creating a unit vector of length 1. Thus we could compute a unit vector from a by\n dividing it by |a|. For unit vectors, the dot product is the same as the cosine.\n The cosine value ranges from 1 for vectors pointing in the same direction, through\n 0 for orthogonal vectors, to -1 for vectors pointing in opposite directions. But since\n raw frequency values are non-negative, the cosine for these vectors ranges from 0‚Äì1.\n Let‚Äôs see how the cosine computes which of the words cherry or digital is closer\n in meaning to information, just using raw counts from the following shortened table:\n pie data computer\n cherry 442 8 2\n digital 5 1683 1670\n information 5 3982 3325\n\n 442 ‚àó 5 + 8 ‚àó 3982 + 2 ‚àó 3325\n cos(cherry, information) = ‚àö ‚àö = .018\n 4422 + 82 + 22 52 + 39822 + 33252\n 5 ‚àó 5 + 1683 ‚àó 3982 + 1670 ‚àó 3325\n cos(digital, information) = ‚àö ‚àö = .996\n 5 + 16832 + 16702 52 + 39822 + 33252\n\n The model decides that information is way closer to digital than it is to cherry, a\n result that seems sensible. Fig. 5.5 shows a visualization.\n10 C HAPTER 5 ‚Ä¢ E MBEDDINGS\n\n Dimension 1: ‚Äòpie‚Äô\n cherry\n digital information\n\n 500 1000 1500 2000 2500 3000\n\n Dimension 2: ‚Äòcomputer‚Äô\n three words (cherry, digital, and information) in the two dimensional space defined by counts\n of the words computer and pie nearby. The figure doesn‚Äôt show the cosine, but it highlights the\n angles; note that the angle between digital and information is smaller than the angle between\n cherry and information. When two vectors are more similar, the cosine is larger but the angle\n is smaller; the cosine has its maximum (1) when the angle between two vectors is smallest\n (0‚ó¶ ); the cosine of all other angles is less than 1.\n\n can be used to compute word similarity, for tasks like finding word paraphrases,\n tracking changes in word meaning, or automatically discovering meanings of words\n in different corpora. For example, we can find the 10 most similar words to any\n target word w by computing the cosines between w and each of the |V | ‚àí 1 other\n words, sorting, and looking at the top 10.\n\n In the previous sections we saw how to represent a word as a sparse, long vector with\n dimensions corresponding to words in the vocabulary. We now introduce a more\n powerful word representation: embeddings, short dense vectors. Unlike the vectors\n we‚Äôve seen so far, embeddings are short, with number of dimensions d ranging from\n 50-1000, rather than the much larger vocabulary size |V |.These d dimensions don‚Äôt\n have a clear interpretation. And the vectors are dense: instead of vector entries\n being sparse, mostly-zero counts or functions of counts, the values will be realvalued numbers that can be negative.\n It turns out that dense vectors work better in every NLP task than sparse vectors.\n While we don‚Äôt completely understand all the reasons for this, we have some intuitions. Representing words as 300-dimensional dense vectors requires our classifiers\n to learn far fewer weights than if we represented words as 50,000-dimensional vectors, and the smaller parameter space possibly helps with generalization and avoiding overfitting. Dense vectors may also do a better job of capturing synonymy.\n For example, in a sparse vector representation, dimensions for synonyms like car\n and automobile dimension are distinct and unrelated; sparse vectors may thus fail\n to capture the similarity between a word with car as a neighbor and a word with\n automobile as a neighbor.\n skip-gram In this section we introduce one method for computing embeddings: skip-gram\n SGNS with negative sampling, sometimes called SGNS. The skip-gram algorithm is one\n word2vec of two algorithms in a software package called word2vec, and so sometimes the\n algorithm is loosely referred to as word2vec (Mikolov et al. 2013a, Mikolov et al.\n 2013b). The word2vec methods are fast, efficient to train, and easily available online with code and pretrained embeddings. Word2vec embeddings are static em-\n5.5 ‚Ä¢ W ORD 2 VEC 11\n\n static\n embeddings beddings, meaning that the method learns one fixed embedding for each word in the\n vocabulary. In Chapter 10 we‚Äôll introduce methods for learning dynamic contextual\n embeddings like the popular family of BERT representations, in which the vector\n for each word is different in different contexts.\n The intuition of word2vec is that instead of counting how often each word w occurs near, say, apricot, we‚Äôll instead train a classifier on a binary prediction task: ‚ÄúIs\n word w likely to show up near apricot?‚Äù We don‚Äôt actually care about this prediction\n task; instead we‚Äôll take the learned classifier weights as the word embeddings.\n The revolutionary intuition here is that we can just use running text as implicitly\n supervised training data for such a classifier; a word c that occurs near the target\n word apricot acts as gold ‚Äòcorrect answer‚Äô to the question ‚ÄúIs word c likely to show\nself-supervision up near apricot?‚Äù This method, often called self-supervision, avoids the need for\n any sort of hand-labeled supervision signal. This idea was first proposed in the task\n of neural language modeling, when Bengio et al. (2003) and Collobert et al. (2011)\n showed that a neural language model (a neural network that learned to predict the\n next word from prior words) could just use the next word in running text as its\n supervision signal, and could be used to learn an embedding representation for each\n word as part of doing this prediction task.\n We‚Äôll see how to do neural networks in the next chapter, but word2vec is a\n much simpler model than the neural network language model, in two ways. First,\n word2vec simplifies the task (making it binary classification instead of word prediction). Second, word2vec simplifies the architecture (training a logistic regression\n classifier instead of a multi-layer neural network with hidden layers that demand\n more sophisticated training algorithms). The intuition of skip-gram is:\n 1. Treat the target word and a neighboring context word as positive examples.\n 2. Randomly sample other words in the lexicon to get negative samples.\n 3. Use logistic regression to train a classifier to distinguish those two cases.\n 4. Use the learned weights as the embeddings.\n\n 5.5.1 The classifier\n Let‚Äôs start by thinking about the classification task, and then turn to how to train.\n Imagine a sentence like the following, with a target word apricot, and assume we‚Äôre\n using a window of ¬±2 context words:\n ... lemon, a [tablespoon of apricot jam, a] pinch ...\n c1 c2 w c3 c4\n Our goal is to train a classifier such that, given a tuple (w, c) of a target word\n w paired with a candidate context word c (for example (apricot, jam), or perhaps\n (apricot, aardvark)) it will return the probability that c is a real context word (true\n for jam, false for aardvark):\n\n P(+|w, c) (5.11)\n The probability that word c is not a real context word for w is just 1 minus\n Eq. 5.11:\n\n P(‚àí|w, c) = 1 ‚àí P(+|w, c) (5.12)\n How does the classifier compute the probability P? The intuition of the skipgram model is to base this probability on embedding similarity: a word is likely to\n12 C HAPTER 5 ‚Ä¢ E MBEDDINGS\n\n occur near the target if its embedding vector is similar to the target embedding. To\n compute similarity between these dense embeddings, we rely on the intuition that\n two vectors are similar if they have a high dot product (after all, cosine is just a\n normalized dot product). In other words:\n\n Similarity(w, c) ‚âà c ¬∑ w (5.13)\n\n The dot product c ¬∑ w is not a probability, it‚Äôs just a number ranging from ‚àí‚àû to ‚àû\n (since the elements in word2vec embeddings can be negative, the dot product can be\n negative). To turn the dot product into a probability, we‚Äôll use the logistic or sigmoid\n function œÉ (x), the fundamental core of logistic regression:\n œÉ (x) = (5.14)\n 1 + exp (‚àíx)\n We model the probability that word c is a real context word for target word w as:\n P(+|w, c) = œÉ (c ¬∑ w) = (5.15)\n 1 + exp (‚àíc ¬∑ w)\n The sigmoid function returns a number between 0 and 1, but to make it a probability\n we‚Äôll also need the total probability of the two possible events (c is a context word,\n and c isn‚Äôt a context word) to sum to 1. We thus estimate the probability that word c\n is not a real context word for w as:\n\n P(‚àí|w, c) = 1 ‚àí P(+|w, c)\n = œÉ (‚àíc ¬∑ w) = (5.16)\n 1 + exp (c ¬∑ w)\n Equation 5.15 gives us the probability for one word, but there are many context\n words in the window. Skip-gram makes the simplifying assumption that all context\n words are independent, allowing us to just multiply their probabilities:\n L\n Y\n P(+|w, c1:L ) = œÉ (ci ¬∑ w) (5.17)\n i=1\n XL\n log P(+|w, c1:L ) = log œÉ (ci ¬∑ w) (5.18)\n i=1\n\n In summary, skip-gram trains a probabilistic classifier that, given a test target word\n w and its context window of L words c1:L , assigns a probability based on how similar\n this context window is to the target word. The probability is based on applying the\n logistic (sigmoid) function to the dot product of the embeddings of the target word\n with each context word. To compute this probability, we just need embeddings for\n each target word and context word in the vocabulary.\n Fig. 5.6 shows the intuition of the parameters we‚Äôll need. Skip-gram actually\n stores two embeddings for each word, one for the word as a target, and one for the\n word considered as context. Thus the parameters we need to learn are two matrices\n W and C, each containing an embedding for every one of the |V | words in the\n vocabulary V .2 Let‚Äôs now turn to learning these embeddings (which is the real goal\n of training this classifier in the first place).\n 2 In principle the target matrix and the context matrix could use different vocabularies, but we‚Äôll simplify\n by assuming one shared vocabulary V .\n 5.5 ‚Ä¢ W ORD 2 VEC 13\n\n 1..d\n aardvark 1\n\n apricot\n\n ùúΩ=\n ‚Ä¶ ‚Ä¶ W target words\n\n zebra |V|\n aardvark |V|+1\n apricot\n\n C context & noise\n ‚Ä¶ ‚Ä¶\n words\n zebra 2|V|\n\nthe context embedding (sometimes called the output embedding). The parameter Œ∏ that the algorithm learns is thus a matrix of 2|V | vectors, each of dimension d, formed by concatenating\ntwo matrices, the target embeddings W and the context+noise embeddings C.\n\n5.5.2 Learning skip-gram embeddings\nThe learning algorithm for skip-gram embeddings takes as input a corpus of text,\nand a chosen vocabulary size N. It begins by assigning a random embedding vector\nfor each of the N vocabulary words, and then proceeds to iteratively shift the embedding of each word w to be more like the embeddings of words that occur nearby\nin texts, and less like the embeddings of words that don‚Äôt occur nearby. Let‚Äôs start\nby considering a single piece of training data:\n... lemon, a [tablespoon of apricot jam, a] pinch ...\n c1 c2 w c3 c4\n This example has a target word w (apricot), and 4 context words in the L = ¬±2\nwindow, resulting in 4 positive training instances (on the left below):\n positive examples + negative examples -\n w cpos w cneg w cneg\n apricot tablespoon apricot aardvark apricot seven\n apricot of apricot my apricot forever\n apricot jam apricot where apricot dear\n apricot a apricot coaxial apricot if\n For training a binary classifier we also need negative examples. In fact skipgram with negative sampling (SGNS) uses more negative examples than positive\nexamples (with the ratio between them set by a parameter k). So for each of these\n(w, cpos ) training instances we‚Äôll create k negative samples, each consisting of the\ntarget w plus a ‚Äònoise word‚Äô cneg . A noise word is a random word from the lexicon,\nconstrained not to be the target word w. The table right above shows the setting\nwhere k = 2, so we‚Äôll have 2 negative examples in the negative training set ‚àí for\neach positive example w, cpos .\n The noise words are chosen according to their weighted unigram probability\npŒ± (w), where Œ± is a weight. If we were sampling according to unweighted probability P(w), it would mean that with unigram probability P(‚Äúthe‚Äù) we would choose\nthe word the as a noise word, with unigram probability P(‚Äúaardvark‚Äù) we would\nchoose aardvark, and so on. But in practice it is common to set Œ± = 0.75, i.e. use\n14 C HAPTER 5 ‚Ä¢ E MBEDDINGS\n\n the weighting P3 (w):\n\n count(w)Œ±\n PŒ± (w) = P 0 Œ±\n (5.19)\n w0 count(w )\n\n Setting Œ± = .75 gives better performance because it gives rare noise words slightly\n higher probability: for rare words, PŒ± (w) > P(w). To illustrate this intuition, it\n might help to work out the probabilities for an example with Œ± = .75 and two events,\n P(a) = 0.99 and P(b) = 0.01:\n\n .99.75\n PŒ± (a) = = 0.97\n .99.75 + .01.75\n .01.75\n PŒ± (b) = = 0.03 (5.20)\n .99 + .01.75\n .75\n\n Thus using Œ± = .75 increases the probability of the rare event b from 0.01 to 0.03.\n Given the set of positive and negative training instances, and an initial set of\n embeddings, the goal of the learning algorithm is to adjust those embeddings to\n\n ‚Ä¢ Maximize the similarity of the target word, context word pairs (w, cpos ) drawn\n from the positive examples\n ‚Ä¢ Minimize the similarity of the (w, cneg ) pairs from the negative examples.\n\n If we consider one word/context pair (w, cpos ) with its k noise words cneg1 ...cnegk ,\n we can express these two goals as the following loss function L to be minimized\n (hence the ‚àí); here the first term expresses that we want the classifier to assign the\n real context word cpos a high probability of being a neighbor, and the second term\n expresses that we want to assign each of the noise words cnegi a high probability of\n being a non-neighbor, all multiplied because we assume independence:\n \" k\n #\n Y\n L = ‚àí log P(+|w, cpos ) P(‚àí|w, cnegi )\n i=1\n \" k\n #\n X\n = ‚àí log P(+|w, cpos ) + log P(‚àí|w, cnegi )\n i=1\n \" k\n #\n X \u0001\n = ‚àí log P(+|w, cpos ) + log 1 ‚àí P(+|w, cnegi )\n i=1\n \" k\n #\n X\n = ‚àí log œÉ (cpos ¬∑ w) + log œÉ (‚àícnegi ¬∑ w) (5.21)\n i=1\n\n That is, we want to maximize the dot product of the word with the actual context\n words, and minimize the dot products of the word with the k negative sampled nonneighbor words.\n We minimize this loss function using stochastic gradient descent. Fig. 5.7 shows\n the intuition of one step of learning.\n To get the gradient, we need to take the derivative of Eq. 5.21 with respect to\n the different embeddings. It turns out the derivatives are the following (we leave the\n 5.5 ‚Ä¢ W ORD 2 VEC 15\n\n aardvark\n move apricot and jam closer,\n apricot w increasing cpos z w\n W\n ‚Äú‚Ä¶apricot jam‚Ä¶‚Äù\n zebra\n ! aardvark move apricot and matrix apart\n cpos decreasing cneg1 z w\n jam\n\n C matrix cneg1\n k=2\n Tolstoy cneg2 move apricot and Tolstoy apart\n decreasing cneg2 z w\n zebra\n\n with) context embeddings for nearby words (here jam) and further from (lower dot product\n with) context embeddings for noise words that don‚Äôt occur nearby (here Tolstoy and matrix).\n\n proof as an exercise at the end of the chapter):\n\n ‚àÇL\n = [œÉ (cpos ¬∑ w) ‚àí 1]w (5.22)\n ‚àÇ cpos\n ‚àÇL\n = [œÉ (cneg ¬∑ w)]w (5.23)\n ‚àÇ cneg\n k\n ‚àÇL X\n = [œÉ (cpos ¬∑ w) ‚àí 1]cpos + [œÉ (cnegi ¬∑ w)]cnegi (5.24)\n ‚àÇw\n i=1\n\n The update equations going from time step t to t + 1 in stochastic gradient descent\n are thus:\n\n ct+1 t t t\n pos = cpos ‚àí Œ∑[œÉ (cpos ¬∑ w ) ‚àí 1]w\n t\n (5.25)\n ct+1\n neg = ctneg ‚àí Œ∑[œÉ (ctneg ¬∑ wt )]wt (5.26)\n \" k\n #\n X\n wt+1 = wt ‚àí Œ∑ [œÉ (ctpos ¬∑ wt ) ‚àí 1]ctpos + [œÉ (ctnegi ¬∑ wt )]ctnegi (5.27)\n i=1\n\n Just as in logistic regression, then, the learning algorithm starts with randomly initialized W and C matrices, and then walks through the training corpus using gradient\n descent to move W and C so as to minimize the loss in Eq. 5.21 by making the updates in (Eq. 5.25)-(Eq. 5.27).\n Recall that the skip-gram model learns two separate embeddings for each word i:\n target\nembedding the target embedding wi and the context embedding ci , stored in two matrices, the\n context\nembedding target matrix W and the context matrix C. It‚Äôs common to just add them together,\n representing word i with the vector wi + ci . Alternatively we can throw away the C\n matrix and just represent each word i by the vector wi .\n As with the simple count-based methods like tf-idf, the context window size L\n affects the performance of skip-gram embeddings, and experiments often tune the\n parameter L on a devset.\n 16 C HAPTER 5 ‚Ä¢ E MBEDDINGS\n\n 5.5.3 Other kinds of static embeddings\n fasttext There are many kinds of static embeddings. An extension of word2vec, fasttext\n (Bojanowski et al., 2017), addresses a problem with word2vec as we have presented\n it so far: it has no good way to deal with unknown words‚Äîwords that appear in\n a test corpus but were unseen in the training corpus. A related problem is word\n sparsity, such as in languages with rich morphology, where some of the many forms\n for each noun and verb may only occur rarely. Fasttext deals with these problems\n by using subword models, representing each word as itself plus a bag of constituent\n n-grams, with special boundary symbols < and > added to each word. For example,\n with n = 3 the word where would be represented by the sequence <where> plus the\n character n-grams:\n <wh, whe, her, ere, re>\n Then a skipgram embedding is learned for each constituent n-gram, and the word\n where is represented by the sum of all of the embeddings of its constituent n-grams.\nhde, Gonnerman, Plaut Unknown words\n Modeling can then\n Word Meaning Usingbe presented\n Lexical only by the sum of the constituent n-grams.\n Co-Occurrence\n A fasttext open-source library, including pretrained embeddings for 157 languages,\n is available at https://fasttext.cc.\n Another very widely used static embedding model is GloVe (Pennington et al.,\n RUSSIA\n\n 2014), short for Global Vectors, because the model is based on capturing global\n FRANCE\n CHINA\n\n WRIST\n corpus statistics. GloVe is based on ratios of probabilities from the word-word co-\nEUROPE\n ASIA\n AFRICA\n ANKLE\n ARM\n occurrence matrix. AMERICA\n BRAZIL\n SHOULDER\n FINGER\n EAR\n EYE\n FACE\n HAND It turns out that dense embeddings like word2vec actually have an elegant math-\nMOSCOW\n TOE LEG\n FOOT ematical relationship with count-based embeddings, in which word2vec can be seen\n HAWAII\n TOOTH\n NOSE\n HEAD as implicitly optimizing a function of a count matrix with a particular (PPMI) weight-\nTOKYO\n\n ing (Levy and Goldberg, 2014c).\n MONTREAL\n CHICAGO\n ATLANTA\n MOUSE\n\n 5.6\n DOG\n CAT\n Visualizing Embeddings\n TURTLE\n LION NASHVILLE\n PUPPY\n KITTEN COW\n\n OYSTER\n ‚ÄúI see well in many dimensions as long as the dimensions are around two.‚Äù\n BULL The late economist Martin Shubik\n Visualizing embeddings is an important goal in helping understand, apply, and\n improve these models of word meaning. But how can we visualize a (for example)\n 100-dimensional vector?\n WRIST The simplest way to visualize the meaning of a word\n ANKLE\n SHOULDER\n ARM\n w embedded in a space is to list the most similar words to\n LEG\n HAND w by sorting the vectors for all words in the vocabulary by\n FOOT\n HEAD\n NOSE\n their cosine with the vector for w. For example the 7 closest\n FINGER\n TOE words to frog using a particular embeddings computed with\n FACE\n EAR\n EYE\n the GloVe algorithm are: frogs, toad, litoria, leptodactyli-\nTOOTH\n DOG dae, rana, lizard, and eleutherodactylus (Pennington et al.,\n CAT\n PUPPY\n KITTEN\n 2014).\n COW\n MOUSE Yet another visualization method is to use a clustering\n TURTLE\n\n LION\n OYSTER algorithm to show a hierarchical representation of which\n BULL\n CHICAGO words are similar to others in the embedding space. The\n ATLANTA\n MONTREAL\n NASHVILLE\n uncaptioned figure on the left uses hierarchical clustering\n TOKYO\n CHINA of some embedding vectors for nouns as a visualization\n RUSSIA\n AFRICA\n ASIA\n method (Rohde et al., 2006).\n EUROPE\n AMERICA\n BRAZIL\n MOSCOW\n FRANCE\n HAWAII\n\n 5.7 ‚Ä¢ S EMANTIC PROPERTIES OF EMBEDDINGS 17\n\n Probably the most common visualization method, however, is to project the 100 dimensions of a word down into 2\n dimensions. Fig. 5.1 showed one such visualization, as does\n Fig. 5.9, using a projection method called t-SNE (van der\n Maaten and Hinton, 2008).\n\n In this section we briefly summarize some of the semantic properties of embeddings\n that have been studied.\n Different types of similarity or association: One parameter of vector semantic\n models that is relevant to both sparse PPMI vectors and dense word2vec vectors is\n the size of the context window used to collect counts. This is generally between 1\n and 10 words on each side of the target word (for a total context of 2-20 words).\n The choice depends on the goals of the representation. Shorter context windows\n tend to lead to representations that are a bit more syntactic, since the information is\n coming from immediately nearby words. When the vectors are computed from short\n context windows, the most similar words to a target word w tend to be semantically\n similar words with the same parts of speech. When vectors are computed from long\n context windows, the highest cosine words to a target word w tend to be words that\n are topically related but not similar.\n For example Levy and Goldberg (2014a) showed that using skip-gram with a\n window of ¬±2, the most similar words to the word Hogwarts (from the Harry Potter\n series) were names of other fictional schools: Sunnydale (from Buffy the Vampire\n Slayer) or Evernight (from a vampire series). With a window of ¬±5, the most similar\n words to Hogwarts were other words topically related to the Harry Potter series:\n Dumbledore, Malfoy, and half-blood.\n It‚Äôs also often useful to distinguish two kinds of similarity or association between\n first-order words (SchuÃàtze and Pedersen, 1993). Two words have first-order co-occurrence\n co-occurrence\n (sometimes called syntagmatic association) if they are typically nearby each other.\n Thus wrote is a first-order associate of book or poem. Two words have second-order\n second-order co-occurrence (sometimes called paradigmatic association) if they have similar\n co-occurrence\n neighbors. Thus wrote is a second-order associate of words like said or remarked.\n Analogy/Relational Similarity: Another semantic property of embeddings is their\n ability to capture relational meanings. In an important early vector space model of\nparallelogram cognition, Rumelhart and Abrahamson (1973) proposed the parallelogram model\n model\n for solving simple analogy problems of the form a is to b as a* is to what?. In such\n problems, a system is given a problem like apple:tree::grape:?, i.e., apple is to tree\n as grape is to , and must fill in the word vine. In the parallelogram model, il-\n # ¬ª # ¬ª\n lustrated in Fig. 5.8, the vector from the word apple to the word tree (= tree ‚àí apple)\n # ¬ª the nearest word to that point is returned.\n is added to the vector for grape (grape);\n In early work with sparse embeddings, scholars showed that sparse vector models of meaning could solve such analogy problems (Turney and Littman, 2005),\n but the parallelogram method received more modern attention because of its success with word2vec or GloVe vectors (Mikolov et al. 2013c, Levy and Goldberg\n # ¬ª\n 2014b, Pennington et al. 2014). For example, the result of the expression king ‚àí\n # ¬ª + woman\n # ¬ª is a vector close to queen. # ¬ª # ¬ª # ¬ª\n # ¬ª Similarly, Paris ‚àí France + Italy results\n man\n # ¬ª\n in a vector that is close to Rome. The embedding model thus seems to be extract-\n18 C HAPTER 5 ‚Ä¢ E MBEDDINGS\n\n tree\n apple\n\n vine\n grape\n # ¬ª # ¬ª # ¬ª # ¬ª\n 1973): the location of vine can be found by subtracting apple from tree and adding grape.\n\n ing representations of relations like MALE - FEMALE, or CAPITAL - CITY- OF, or even\n COMPARATIVE / SUPERLATIVE , as shown in Fig. 5.9 from GloVe.\n\n (a) (b)\n # ¬ª # ¬ª # ¬ª is close to queen.\n # ¬ª (b) offsets seem to capture comparative and superlative morphology\n(a) king ‚àí man + woman\n(Pennington et al., 2014).\n\n For a a : b :: a‚àó : b‚àó problem, meaning the algorithm is given vectors a, b, and\n a‚àó and must find b‚àó , the parallelogram method is thus:\n\n bÃÇ‚àó = argmin distance(x, b ‚àí a + a‚àó ) (5.28)\n x\n\n with some distance function, such as Euclidean distance.\n There are some caveats. For example, the closest value returned by the parallelogram algorithm in word2vec or GloVe embedding spaces is usually not in fact\n b* but one of the 3 input words or their morphological variants (i.e., cherry:red ::\n potato:x returns potato or potatoes instead of brown), so these must be explicitly\n excluded. Furthermore while embedding spaces perform well if the task involves\n frequent words, small distances, and certain relations (like relating countries with\n their capitals or verbs/nouns with their inflected forms), the parallelogram method\n with embeddings doesn‚Äôt work as well for other relations (Linzen 2016, Gladkova\n et al. 2016, Schluter 2018, Ethayarajh et al. 2019a), and indeed Peterson et al. (2020)\n argue that the parallelogram method is in general too simple to model the human\n cognitive process of forming analogies of this kind.\n 5.8 ‚Ä¢ B IAS AND E MBEDDINGS 19\n\n 5.7.1 Embeddings and Historical Semantics\n Embeddings can also be a useful tool for studying how meaning changes over time,\n by computing multiple embedding spaces, each from texts written in a particular\n time period. For example Fig. 5.10 shows a visualization of changes in meaning in\n English words over the last two centuries, computed by building separate embedding\n spaces for each\n CHAPTER decade fromSOCIAL\n 5. DYNAMIC historicalREPRESENTATIONS\n corpora like Google n-grams (Lin etMEANING79\n OF WORD al., 2012)\n and the Corpus of Historical American English (Davies, 2012).\n\n Figure\n word2vec5.1: Two-dimensional\n vectors. The modern sensevisualization of semantic\n of each word, and the change in English\n grey context words,using SGNS\n are comvectors\n puted from(seetheSection 5.8 for\n most recent the visualization\n (modern) algorithm).\n time-point embedding A,Earlier\n space. The word\n pointsgay shifted\n are comfrom\n puted meaning ‚Äúcheerful‚Äù\n from earlier historicalorembedding\n ‚Äúfrolicsome‚Äù to referring\n spaces. to homosexuality.\n The visualizations A, In the\n show the changes early\n in the\n 20th century broadcast referred to ‚Äúcasting out seeds‚Äù; with the rise of television\n word gay from meanings related to ‚Äúcheerful‚Äù or ‚Äúfrolicsome‚Äù to referring to homosexuality, and\n radio\n the development of the modern ‚Äútransmission‚Äù sense of broadcast from its original sense of of\n its meaning shifted to ‚Äútransmitting signals‚Äù. C, Awful underwent a process\n pejoration,\n sowing seeds,asandit shifted from meaning\n the pejoration ‚Äúfull\n of the word of awe‚Äù\n awful to meaning\n as it shifted ‚Äúterrible‚Äúfull\n from meaning or appalling‚Äù\n of awe‚Äù\n [212].\n to meaning ‚Äúterrible or appalling‚Äù (Hamilton et al., 2016).\n\n that adverbials (e.g., actually) have a general tendency to undergo subjectification\n\n andtheyEmbeddings\n shift from objective statements about the world (e.g., ‚ÄúSorry, the car is\n actually broken‚Äù) to subjective statements (e.g., ‚ÄúI can‚Äôt believe he actually did that‚Äù,\n indicating surprise/disbelief).\n In addition to their ability to learn word meaning from text, embeddings, alas,\n also reproduce the implicit biases and stereotypes that were latent in the text. As\n 5.2.2\n the prior Computational\n section just showed, linguistic\n embeddings can studies\n roughly model relational similarity: ‚Äòqueen‚Äô as the closest word to ‚Äòking‚Äô - ‚Äòman‚Äô + ‚Äòwoman‚Äô implies the analogy\n There are also a number of recent works analyzing semantic change using computational\n man:woman::king:queen. But these same embedding analogies also exhibit gender\n methods.\n stereotypes.[200] Foruse latent semantic\n example Bolukbasianalysis to analyze\n et al. (2016) how the\n find that word meanings\n closest broaden\n occupation\n to ‚Äòcomputer\n and narrow over programmer‚Äô\n time. [113] - ‚Äòman‚Äô + ‚Äòwoman‚Äô\n use raw in word2vec\n co-occurrence vectorsembeddings\n to perform trained\n a numberon of\n news text is ‚Äòhomemaker‚Äô, and that the embeddings similarly suggest the\n historical case-studies on semantic change, and [252] perform a similar set of small- analogy\n ‚Äòfather‚Äô is to ‚Äòdoctor‚Äô as ‚Äòmother‚Äô is to ‚Äònurse‚Äô. This could result in what Crawford\n allocational scale case-studies using temporal topic models. [87] construct point-wise mutual\n harm (2017) and Blodgett et al. (2020) call an allocational harm, when a system alloinformation-based\n cates resources (jobs embeddings and found\n or credit) unfairly that semantic\n to different groups.changes uncovered\n For example by their\n algorithms\n that use had\n method embeddings\n reasonable as agreement\n part of a search for hiring\n with human potential [129]\n judgments. programmers\n and [119]oruse\n doctors\n ‚Äúneural‚Äù\n might thus incorrectly downweight documents with women‚Äôs names.\n word-embedding methods to detect linguistic change points. Finally, [257] analyze\n It turns out that embeddings don‚Äôt just reflect the statistics of their input, but also\n bias historical co-occurrences to test whether synonyms tend to change in similar ways.\n amplification amplify bias; gendered terms become more gendered in embedding space than they\n were in the input text statistics (Zhao et al. 2017, Ethayarajh et al. 2019b, Jia et al.\n 2020), and biases are more exaggerated than in actual labor employment statistics\n (Garg et al., 2018).\n Embeddings also encode the implicit associations that are a property of human\n reasoning. The Implicit Association Test (Greenwald et al., 1998) measures peo-\n20 C HAPTER 5 ‚Ä¢ E MBEDDINGS\n\n ple‚Äôs associations between concepts (like ‚Äòflowers‚Äô or ‚Äòinsects‚Äô) and attributes (like\n ‚Äòpleasantness‚Äô and ‚Äòunpleasantness‚Äô) by measuring differences in the latency with\n which they label words in the various categories.3 Using such methods, people\n in the United States have been shown to associate African-American names with\n unpleasant words (more than European-American names), male names more with\n mathematics and female names with the arts, and old people‚Äôs names with unpleasant words (Greenwald et al. 1998, Nosek et al. 2002a, Nosek et al. 2002b). Caliskan\n et al. (2017) replicated all these findings of implicit associations using GloVe vectors\n and cosine similarity instead of human latencies. For example African-American\n names like ‚ÄòLeroy‚Äô and ‚ÄòShaniqua‚Äô had a higher GloVe cosine with unpleasant words\n while European-American names (‚ÄòBrad‚Äô, ‚ÄòGreg‚Äô, ‚ÄòCourtney‚Äô) had a higher cosine\n with pleasant words. These problems with embeddings are an example of a reprerepresentational sentational harm (Crawford 2017, Blodgett et al. 2020), which is a harm caused by\n harm\n a system demeaning or even ignoring some social groups. Any embedding-aware algorithm that made use of word sentiment could thus exacerbate bias against African\n Americans.\n Recent research focuses on ways to try to remove these kinds of biases, for\n example by developing a transformation of the embedding space that removes gender stereotypes but preserves definitional gender (Bolukbasi et al. 2016, Zhao et al.\n 2017) or changing the training procedure (Zhao et al., 2018). However, although\n debiasing these sorts of debiasing may reduce bias in embeddings, they do not eliminate it\n (Gonen and Goldberg, 2019), and this remains an open problem.\n Historical embeddings are also being used to measure biases in the past. Garg\n et al. (2018) used embeddings from historical texts to measure the association between embeddings for occupations and embeddings for names of various ethnicities or genders (for example the relative cosine similarity of women‚Äôs names versus\n men‚Äôs to occupation words like ‚Äòlibrarian‚Äô or ‚Äòcarpenter‚Äô) across the 20th century.\n They found that the cosines correlate with the empirical historical percentages of\n women or ethnic groups in those occupations. Historical embeddings also replicated old surveys of ethnic stereotypes; the tendency of experimental participants in\n 1933 to associate adjectives like ‚Äòindustrious‚Äô or ‚Äòsuperstitious‚Äô with, e.g., Chinese\n ethnicity, correlates with the cosine between Chinese last names and those adjectives\n using embeddings trained on 1930s text. They also were able to document historical\n gender biases, such as the fact that embeddings for adjectives related to competence\n (‚Äòsmart‚Äô, ‚Äòwise‚Äô, ‚Äòthoughtful‚Äô, ‚Äòresourceful‚Äô) had a higher cosine with male than female words, and showed that this bias has been slowly decreasing since 1960. We\n return in later chapters to this question about the role of bias in natural language\n processing.\n\n The most important evaluation metric for vector models is extrinsic evaluation on\n tasks, i.e., using vectors in an NLP task and seeing whether this improves performance over some other model.\n 3 Roughly speaking, if humans associate ‚Äòflowers‚Äô with ‚Äòpleasantness‚Äô and ‚Äòinsects‚Äô with ‚Äòunpleasantness‚Äô, when they are instructed to push a green button for ‚Äòflowers‚Äô (daisy, iris, lilac) and ‚Äòpleasant words‚Äô\n (love, laughter, pleasure) and a red button for ‚Äòinsects‚Äô (flea, spider, mosquito) and ‚Äòunpleasant words‚Äô\n (abuse, hatred, ugly) they are faster than in an incongruous condition where they push a red button for\n ‚Äòflowers‚Äô and ‚Äòunpleasant words‚Äô and a green button for ‚Äòinsects‚Äô and ‚Äòpleasant words‚Äô.\n 5.10 ‚Ä¢ S UMMARY 21\n\n Nonetheless it is useful to have intrinsic evaluations. The most common metric\n is to test their performance on similarity, computing the correlation between an\n algorithm‚Äôs word similarity scores and word similarity ratings assigned by humans.\n WordSim-353 (Finkelstein et al., 2002) is a commonly used set of ratings from 0\n to 10 for 353 noun pairs; for example (plane, car) had an average score of 5.77.\n SimLex-999 (Hill et al., 2015) is a more complex dataset that quantifies similarity\n (cup, mug) rather than relatedness (cup, coffee), and includes concrete and abstract\n adjective, noun and verb pairs. The TOEFL dataset is a set of 80 questions, each\n consisting of a target word with 4 additional word choices; the task is to choose\n which is the correct synonym, as in the example: Levied is closest in meaning to:\n imposed, believed, requested, correlated (Landauer and Dumais, 1997). All of these\n datasets present words without context.\n Slightly more realistic are intrinsic similarity tasks that include context. The\n Stanford Contextual Word Similarity (SCWS) dataset (Huang et al., 2012) and the\n Word-in-Context (WiC) dataset (Pilehvar and Camacho-Collados, 2019) offer richer\n evaluation scenarios. SCWS gives human judgments on 2,003 pairs of words in\n their sentential context, while WiC gives target words in two sentential contexts that\n are either in the same or different senses; see Appendix G. The semantic textual\n similarity task (Agirre et al. 2012, Agirre et al. 2015) evaluates the performance of\n sentence-level similarity algorithms, consisting of a set of pairs of sentences, each\n pair with human-labeled similarity scores.\n Another task used for evaluation is the analogy task, discussed on page 17, where\n the system has to solve problems of the form a is to b as a* is to b*, given a, b, and a*\n and having to find b* (Turney and Littman, 2005). A number of sets of tuples have\n been created for this task (Mikolov et al. 2013a, Mikolov et al. 2013c, Gladkova\n et al. 2016), covering morphology (city:cities::child:children), lexicographic relations (leg:table::spout:teapot) and encyclopedia relations (Beijing:China::Dublin:Ireland),\n some drawing from the SemEval-2012 Task 2 dataset of 79 different relations (Jurgens et al., 2012).\n All embedding algorithms suffer from inherent variability. For example because\n of randomness in the initialization and the random negative sampling, algorithms\n like word2vec may produce different results even from the same dataset, and individual documents in a collection may strongly impact the resulting embeddings\n (Tian et al. 2016, Hellrich and Hahn 2016, Antoniak and Mimno 2018). When embeddings are used to study word associations in particular corpora, therefore, it is\n best practice to train multiple embeddings with bootstrap sampling over documents\n and average the results (Antoniak and Mimno, 2018).\n\n ‚Ä¢ In vector semantics, a word is modeled as a vector‚Äîa point in high-dimensional\n space, also called an embedding. In this chapter we focus on static embeddings, where each word is mapped to a fixed embedding.\n ‚Ä¢ Vector semantic models fall into two classes: sparse and dense. In sparse\n models each dimension corresponds to a word in the vocabulary V and cells\n are functions of co-occurrence counts. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each\n context term in the vocabulary.\n22 C HAPTER 5 ‚Ä¢ E MBEDDINGS\n\n ‚Ä¢ Dense vector models typically have dimensionality 50‚Äì1000. Word2vec algorithms like skip-gram are a popular way to compute dense embeddings.\n Skip-gram trains a logistic regression classifier to compute the probability that\n two words are ‚Äòlikely to occur nearby in text‚Äô. This probability is computed\n from the dot product between the embeddings for the two words.\n ‚Ä¢ Skip-gram uses stochastic gradient descent to train the classifier, by learning\n embeddings that have a high dot product with embeddings of words that occur\n nearby and a low dot product with noise words.\n ‚Ä¢ Other important embedding algorithms include GloVe, a method based on\n ratios of word co-occurrence probabilities.\n ‚Ä¢ Whether using sparse or dense vectors, word and document similarities are\n computed by some function of the dot product between vectors. The cosine\n of two vectors‚Äîa normalized dot product‚Äîis the most popular such metric.\n\nHistorical Notes\n The idea of vector semantics arose out of research in the 1950s in three distinct\n fields: linguistics, psychology, and computer science, each of which contributed a\n fundamental aspect of the model.\n The idea that meaning is related to the distribution of words in context was\n widespread in linguistic theory of the 1950s, among distributionalists like Zellig\n Harris, Martin Joos, and J. R. Firth, and semioticians like Thomas Sebeok. As Joos\n (1950) put it,\n the linguist‚Äôs ‚Äúmeaning‚Äù of a morpheme. . . is by definition the set of conditional\n probabilities of its occurrence in context with all other morphemes.\n The idea that the meaning of a word might be modeled as a point in a multidimensional semantic space came from psychologists like Charles E. Osgood, who\n had been studying how people responded to the meaning of words by assigning values along scales like happy/sad or hard/soft. Osgood et al. (1957) proposed that the\n meaning of a word in general could be modeled as a point in a multidimensional\n Euclidean space, and that the similarity of meaning between two words could be\n modeled as the distance between these points in the space.\n A final intellectual source in the 1950s and early 1960s was the field then called\n mechanical\n indexing mechanical indexing, now known as information retrieval. In what became known\n as the vector space model for information retrieval (Salton 1971, Sparck Jones\n 1986), researchers demonstrated new ways to define the meaning of words in terms\n of vectors (Switzer, 1965), and refined methods for word similarity based on measures of statistical association between words like mutual information (Giuliano,\n 1965) and idf (Sparck Jones, 1972), and showed that the meaning of documents\n could be represented in the same vector spaces used for words. Around the same\n time, (Cordier, 1965) showed that factor analysis of word association probabilities\n could be used to form dense vector representations of words.\n Some of the philosophical underpinning of the distributional way of thinking\n came from the late writings of the philosopher Wittgenstein, who was skeptical of\n the possibility of building a completely formal theory of meaning definitions for\n each word. Wittgenstein suggested instead that ‚Äúthe meaning of a word is its use in\n the language‚Äù (Wittgenstein, 1953, PI 43). That is, instead of using some logical language to define each word, or drawing on denotations or truth values, Wittgenstein‚Äôs\n H ISTORICAL N OTES 23\n\n idea is that we should define a word by how it is used by people in speaking and understanding in their day-to-day interactions, thus prefiguring the movement toward\n embodied and experiential models in linguistics and NLP (Glenberg and Robertson\n 2000, Lake and Murphy 2021, Bisk et al. 2020, Bender and Koller 2020).\n More distantly related is the idea of defining words by a vector of discrete features, which has roots at least as far back as Descartes and Leibniz (Wierzbicka 1992,\n Wierzbicka 1996). By the middle of the 20th century, beginning with the work of\n Hjelmslev (Hjelmslev, 1969) (originally 1943) and fleshed out in early models of\n generative grammar (Katz and Fodor, 1963), the idea arose of representing meansemantic ing with semantic features, symbols that represent some sort of primitive meaning.\n feature\n For example words like hen, rooster, or chick, have something in common (they all\n describe chickens) and something different (their age and sex), representable as:\n hen +female, +chicken, +adult\n rooster -female, +chicken, +adult\n chick +chicken, -adult\n The dimensions used by vector models of meaning to define words, however, are\n only abstractly related to this idea of a small fixed number of hand-built dimensions.\n Nonetheless, there has been some attempt to show that certain dimensions of embedding models do contribute some specific compositional aspect of meaning like\n these early semantic features.\n The use of dense vectors to model word meaning, and indeed the term embedding, grew out of the latent semantic indexing (LSI) model (Deerwester et al.,\n 1988) recast as LSA (latent semantic analysis) (Deerwester et al., 1990). In LSA\n SVD singular value decomposition‚ÄîSVD‚Äî is applied to a term-document matrix (each\n cell weighted by log frequency and normalized by entropy), and then the first 300\n dimensions are used as the LSA embedding. Singular Value Decomposition (SVD)\n is a method for finding the most important dimensions of a data set, those dimensions along which the data varies the most. LSA was then quickly widely applied:\n as a cognitive model (Landauer and Dumais, 1997), and for tasks like spell checking\n (Jones and Martin, 1997), language modeling (Bellegarda 1997, Coccaro and Jurafsky 1998, Bellegarda 2000), morphology induction (Schone and Jurafsky 2000,\n Schone and Jurafsky 2001b), multiword expressions (MWEs) (Schone and Jurafsky,\n 2001a), and essay grading (Rehder et al., 1998). Related models were simultaneously developed and applied to word sense disambiguation by SchuÃàtze (1992). LSA\n also led to the earliest use of embeddings to represent words in a probabilistic classifier, in the logistic regression document router of SchuÃàtze et al. (1995). The idea of\n SVD on the term-term matrix (rather than the term-document matrix) as a model of\n meaning for NLP was proposed soon after LSA by SchuÃàtze (1992). SchuÃàtze applied\n the low-rank (97-dimensional) embeddings produced by SVD to the task of word\n sense disambiguation, analyzed the resulting semantic space, and also suggested\n possible techniques like dropping high-order dimensions. See SchuÃàtze (1997).\n A number of alternative matrix models followed on from the early SVD work,\n including Probabilistic Latent Semantic Indexing (PLSI) (Hofmann, 1999), Latent\n Dirichlet Allocation (LDA) (Blei et al., 2003), and Non-negative Matrix Factorization (NMF) (Lee and Seung, 1999).\n The LSA community seems to have first used the word ‚Äúembedding‚Äù in Landauer\n et al. (1997), in a variant of its mathematical meaning as a mapping from one space\n or mathematical structure to another. In LSA, the word embedding seems to have\n described the mapping from the space of sparse count vectors to the latent space of\n SVD dense vectors. Although the word thus originally meant the mapping from one\n24 C HAPTER 5 ‚Ä¢ E MBEDDINGS\n\n space to another, it has metonymically shifted to mean the resulting dense vector in\n the latent space, and it is in this sense that we currently use the word.\n By the next decade, Bengio et al. (2003) and Bengio et al. (2006) showed that\n neural language models could also be used to develop embeddings as part of the task\n of word prediction. Collobert and Weston (2007), Collobert and Weston (2008), and\n Collobert et al. (2011) then demonstrated that embeddings could be used to represent\n word meanings for a number of NLP tasks. Turian et al. (2010) compared the value\n of different kinds of embeddings for different NLP tasks. Mikolov et al. (2011)\n showed that recurrent neural nets could be used as language models. The idea of\n simplifying the hidden layer of these neural net language models to create the skipgram (and also CBOW) algorithms was proposed by Mikolov et al. (2013a). The\n negative sampling training algorithm was proposed in Mikolov et al. (2013b). There\n are numerous surveys of static embeddings and their parameterizations (Bullinaria\n and Levy 2007, Bullinaria and Levy 2012, Lapesa and Evert 2014, Kiela and Clark\n 2014, Levy et al. 2015).\n See Manning et al. (2008) and Chapter 11 for a deeper understanding of the role\n of vectors in information retrieval, including how to compare queries with documents, more details on tf-idf, and issues of scaling to very large datasets. See Kim\n (2019) for a clear and comprehensive tutorial on word2vec. Cruse (2004) is a useful\n introductory linguistic text on lexical semantics.\n\nExercises\n Exercises 25\n\nAgirre, E., C. Banea, C. Cardie, D. Cer, M. Diab, Carlson, G. N. 1977. Reference to kinds in English. Ph.D.\n A. Gonzalez-Agirre, W. Guo, I. Lopez-Gazpio, M. Mar- thesis, University of Massachusetts, Amherst. Forward.\n itxalar, R. Mihalcea, G. Rigau, L. Uria, and J. Wiebe. Clark, E. 1987. The principle of contrast: A constraint on\n 2015. SemEval-2015 task 2: Semantic textual similarity, language acquisition. In B. MacWhinney, ed., Mecha-\nEnglish, Spanish and pilot on interpretability. SemEval- nisms of language acquisition, 1‚Äì33. LEA.\n 15.\n Coccaro, N. and D. Jurafsky. 1998. Towards better integra-\nAgirre, E., M. Diab, D. Cer, and A. Gonzalez-Agirre. 2012. tion of semantic predictors in statistical language model-\nSemEval-2012 task 6: A pilot on semantic textual simi- ing. ICSLP.\n larity. SemEval-12.\n Collobert, R. and J. Weston. 2007. Fast semantic extraction\nAntoniak, M. and D. Mimno. 2018. Evaluating the stability\n using a novel neural network architecture. ACL.\n of embedding-based word similarities. TACL, 6:107‚Äì119.\n Collobert, R. and J. Weston. 2008. A unified architecture for\nBellegarda, J. R. 1997. A latent semantic analysis framework\n natural language processing: Deep neural networks with\n for large-span language modeling. EUROSPEECH.\n multitask learning. ICML.\nBellegarda, J. R. 2000. Exploiting latent semantic information in statistical language modeling. Proceedings of the Collobert, R., J. Weston, L. Bottou, M. Karlen,\n IEEE, 89(8):1279‚Äì1296. K. Kavukcuoglu, and P. Kuksa. 2011. Natural language\n processing (almost) from scratch. JMLR, 12:2493‚Äì2537.\nBender, E. M. and A. Koller. 2020. Climbing towards NLU:\n On meaning, form, and understanding in the age of data. Cordier, B. 1965. Factor-analysis of correspondences. COL-\nACL. ING 1965.\nBengio, Y., A. Courville, and P. Vincent. 2013. Represen- Crawford, K. 2017. The trouble with bias. Keynote at\n tation learning: A review and new perspectives. IEEE NeurIPS.\n Transactions on Pattern Analysis and Machine Intelli- Cruse, D. A. 2004. Meaning in Language: an Introduction\n gence, 35(8):1798‚Äì1828. to Semantics and Pragmatics. Oxford University Press.\nBengio, Y., R. Ducharme, P. Vincent, and C. Jauvin. 2003. Second edition.\n A neural probabilistic language model. JMLR, 3:1137‚Äì Davies, M. 2012. Expanding horizons in historical lin-\n1155. guistics with the 400-million word Corpus of Historical\nBengio, Y., H. Schwenk, J.-S. SeneÃÅcal, F. Morin, and J.-L. American English. Corpora, 7(2):121‚Äì157.\n Gauvain. 2006. Neural probabilistic language models. In Davies, M. 2015. The Wikipedia Corpus: 4.6 million arti-\nInnovations in Machine Learning, 137‚Äì186. Springer. cles, 1.9 billion words. Adapted from Wikipedia. https:\nBisk, Y., A. Holtzman, J. Thomason, J. Andreas, Y. Bengio, //www.english-corpora.org/wiki/.\n J. Chai, M. Lapata, A. Lazaridou, J. May, A. Nisnevich, Deerwester, S. C., S. T. Dumais, G. W. Furnas, R. A. Harsh-\nN. Pinto, and J. Turian. 2020. Experience grounds lan- man, T. K. Landauer, K. E. Lochbaum, and L. Streeter.\n guage. EMNLP. 1988. Computer information retrieval using latent seman-\nBlei, D. M., A. Y. Ng, and M. I. Jordan. 2003. Latent Dirich- tic structure: US Patent 4,839,853.\n let allocation. JMLR, 3(5):993‚Äì1022. Deerwester, S. C., S. T. Dumais, T. K. Landauer, G. W. Fur-\nBlodgett, S. L., S. Barocas, H. DaumeÃÅ III, and H. Wallach. nas, and R. A. Harshman. 1990. Indexing by latent se-\n2020. Language (technology) is power: A critical survey mantics analysis. JASIS, 41(6):391‚Äì407.\n of ‚Äúbias‚Äù in NLP. ACL.\n Ethayarajh, K., D. Duvenaud, and G. Hirst. 2019a. Towards\nBojanowski, P., E. Grave, A. Joulin, and T. Mikolov. 2017. understanding linear word analogies. ACL.\n Enriching word vectors with subword information. TACL,\n Ethayarajh, K., D. Duvenaud, and G. Hirst. 2019b. Under-\n5:135‚Äì146.\n standing undesirable word embedding associations. ACL.\nBolukbasi, T., K.-W. Chang, J. Zou, V. Saligrama, and A. T.\n Kalai. 2016. Man is to computer programmer as woman Finkelstein, L., E. Gabrilovich, Y. Matias, E. Rivlin,\n is to homemaker? Debiasing word embeddings. NeurIPS. Z. Solan, G. Wolfman, and E. Ruppin. 2002. Placing\n search in context: The concept revisited. ACM Trans-\nBreÃÅal, M. 1897. Essai de SeÃÅmantique: Science des significa- actions on Information Systems, 20(1):116‚Äî-131.\n tions. Hachette.\n Firth, J. R. 1957. A synopsis of linguistic theory 1930‚Äì\nBudanitsky, A. and G. Hirst. 2006. Evaluating WordNet-\n1955. In Studies in Linguistic Analysis. Philological Sobased measures of lexical semantic relatedness. Compuciety. Reprinted in Palmer, F. (ed.) 1968. Selected Papers\n tational Linguistics, 32(1):13‚Äì47.\n of J. R. Firth. Longman, Harlow.\nBullinaria, J. A. and J. P. Levy. 2007. Extracting seman-\nGarg, N., L. Schiebinger, D. Jurafsky, and J. Zou. 2018.\n tic representations from word co-occurrence statistics:\n Word embeddings quantify 100 years of gender and eth-\nA computational study. Behavior research methods,\n nic stereotypes. Proceedings of the National Academy of\n 39(3):510‚Äì526.\n Sciences, 115(16):E3635‚ÄìE3644.\nBullinaria, J. A. and J. P. Levy. 2012. Extracting semantic\n representations from word co-occurrence statistics: stop- Girard, G. 1718. La justesse de la langue francÃßoise: ou les\n lists, stemming, and SVD. Behavior research methods, diffeÃÅrentes significations des mots qui passent pour syn-\n44(3):890‚Äì907. onimes. Laurent d‚ÄôHoury, Paris.\nCaliskan, A., J. J. Bryson, and A. Narayanan. 2017. Semantics derived automatically from language corpora contain\n human-like biases. Science, 356(6334):183‚Äì186.\n26 Chapter 5 ‚Ä¢ Embeddings\n\nGiuliano, V. E. 1965. The interpretation of word Kim, E. 2019. Optimize computational efficiency\n associations. Statistical Association Methods For of skip-gram with negative sampling. https://\n Mechanized Documentation. Symposium Proceed- aegis4048.github.io/optimize_computational_\n ings. Washington, D.C., USA, March 17, 1964. efficiency_of_skip-gram_with_negative_\n https://nvlpubs.nist.gov/nistpubs/Legacy/ sampling.\n MP/nbsmiscellaneouspub269.pdf. Lake, B. M. and G. L. Murphy. 2021. Word meaning in\nGladkova, A., A. Drozd, and S. Matsuoka. 2016. Analogy- minds and machines. Psychological Review. In press.\n based detection of morphological and semantic relations Landauer, T. K. and S. T. Dumais. 1997. A solution to Plato‚Äôs\n with word embeddings: what works and what doesn‚Äôt. problem: The Latent Semantic Analysis theory of acqui-\nNAACL Student Research Workshop. sition, induction, and representation of knowledge. Psy-\nGlenberg, A. M. and D. A. Robertson. 2000. Symbol ground- chological Review, 104:211‚Äì240.\n ing and meaning: A comparison of high-dimensional and Landauer, T. K., D. Laham, B. Rehder, and M. E. Schreiner.\n embodied theories of meaning. Journal of memory and 1997. How well can passage meaning be derived withlanguage, 43(3):379‚Äì401. out using word order? A comparison of Latent Semantic\nGonen, H. and Y. Goldberg. 2019. Lipstick on a pig: Debi- Analysis and humans. COGSCI.\n asing methods cover up systematic gender biases in word Lapesa, G. and S. Evert. 2014. A large scale evaluation of\n embeddings but do not remove them. NAACL HLT. distributional semantic models: Parameters, interactions\nGould, S. J. 1980. The Panda‚Äôs Thumb. Penguin Group. and model selection. TACL, 2:531‚Äì545.\nGreenwald, A. G., D. E. McGhee, and J. L. K. Schwartz. Lee, D. D. and H. S. Seung. 1999. Learning the parts of\n 1998. Measuring individual differences in implicit cogni- objects by non-negative matrix factorization. Nature,\n tion: the implicit association test. Journal of personality 401(6755):788‚Äì791.\n and social psychology, 74(6):1464‚Äì1480. Levy, O. and Y. Goldberg. 2014a. Dependency-based word\nHamilton, W. L., J. Leskovec, and D. Jurafsky. 2016. Di- embeddings. ACL.\n achronic word embeddings reveal statistical laws of se- Levy, O. and Y. Goldberg. 2014b. Linguistic regularities in\n mantic change. ACL. sparse and explicit word representations. CoNLL.\nHarris, Z. S. 1954. Distributional structure. Word, 10:146‚Äì Levy, O. and Y. Goldberg. 2014c. Neural word embedding\n 162. as implicit matrix factorization. NeurIPS.\nHellrich, J. and U. Hahn. 2016. Bad company‚Äî Levy, O., Y. Goldberg, and I. Dagan. 2015. Improving dis-\nNeighborhoods in neural embedding spaces considered tributional similarity with lessons learned from word emharmful. COLING. beddings. TACL, 3:211‚Äì225.\nHill, F., R. Reichart, and A. Korhonen. 2015. Simlex-999: Lin, Y., J.-B. Michel, E. Lieberman Aiden, J. Orwant,\n Evaluating semantic models with (genuine) similarity es- W. Brockman, and S. Petrov. 2012. Syntactic annotations\n timation. Computational Linguistics, 41(4):665‚Äì695. for the Google Books NGram corpus. ACL.\nHjelmslev, L. 1969. Prologomena to a Theory of Language. Linzen, T. 2016. Issues in evaluating semantic spaces us-\nUniversity of Wisconsin Press. Translated by Francis J. ing word analogies. 1st Workshop on Evaluating Vector-\nWhitfield; original Danish edition 1943. Space Representations for NLP.\nHofmann, T. 1999. Probabilistic latent semantic indexing. Manning, C. D., P. Raghavan, and H. SchuÃàtze. 2008. Intro-\nSIGIR-99. duction to Information Retrieval. Cambridge.\nHuang, E. H., R. Socher, C. D. Manning, and A. Y. Ng. 2012. Mikolov, T., K. Chen, G. S. Corrado, and J. Dean. 2013a. Ef-\nImproving word representations via global context and ficient estimation of word representations in vector space.\n multiple word prototypes. ACL. ICLR 2013.\nJia, S., T. Meng, J. Zhao, and K.-W. Chang. 2020. Mitigat- Mikolov, T., S. Kombrink, L. Burget, J. H. CÃåernockyÃÄ, and\n ing gender bias amplification in distribution by posterior S. Khudanpur. 2011. Extensions of recurrent neural netregularization. ACL. work language model. ICASSP.\nJones, M. P. and J. H. Martin. 1997. Contextual spelling cor- Mikolov, T., I. Sutskever, K. Chen, G. S. Corrado, and\n rection using latent semantic analysis. ANLP. J. Dean. 2013b. Distributed representations of words and\n phrases and their compositionality. NeurIPS.\nJoos, M. 1950. Description of language design. JASA,\n 22:701‚Äì708. Mikolov, T., W.-t. Yih, and G. Zweig. 2013c. Linguistic regularities in continuous space word representations.\nJurgens, D., S. M. Mohammad, P. Turney, and K. Holyoak. NAACL HLT.\n 2012. SemEval-2012 task 2: Measuring degrees of rela-\nNosek, B. A., M. R. Banaji, and A. G. Greenwald. 2002a.\n tional similarity. *SEM 2012.\n Harvesting implicit group attitudes and beliefs from a\nKatz, J. J. and J. A. Fodor. 1963. The structure of a semantic demonstration web site. Group Dynamics: Theory, Retheory. Language, 39:170‚Äì210. search, and Practice, 6(1):101.\nKiela, D. and S. Clark. 2014. A systematic study of semantic Nosek, B. A., M. R. Banaji, and A. G. Greenwald. 2002b.\n vector space model parameters. EACL 2nd Workshop on Math=male, me=female, therefore math6= me. Journal of\n Continuous Vector Space Models and their Composition- personality and social psychology, 83(1):44.\n ality (CVSC).\n Osgood, C. E., G. J. Suci, and P. H. Tannenbaum. 1957. The\n Measurement of Meaning. University of Illinois Press.\n Exercises 27\n\nPennington, J., R. Socher, and C. D. Manning. 2014. GloVe: Turney, P. D. and M. L. Littman. 2005. Corpus-based learn-\nGlobal vectors for word representation. EMNLP. ing of analogies and semantic relations. Machine Learn-\nPeterson, J. C., D. Chen, and T. L. Griffiths. 2020. Parallelo- ing, 60(1-3):251‚Äì278.\n grams revisited: Exploring the limitations of vector space van der Maaten, L. and G. E. Hinton. 2008. Visualizing highmodels for simple analogies. Cognition, 205. dimensional data using t-SNE. JMLR, 9:2579‚Äì2605.\nPilehvar, M. T. and J. Camacho-Collados. 2019. WiC: the Wierzbicka, A. 1992. Semantics, Culture, and Cognition:\n word-in-context dataset for evaluating context-sensitive University Human Concepts in Culture-Specific Configumeaning representations. NAACL HLT. rations. Oxford University Press.\nRehder, B., M. E. Schreiner, M. B. W. Wolfe, D. Laham, Wierzbicka, A. 1996. Semantics: Primes and Universals.\n T. K. Landauer, and W. Kintsch. 1998. Using Latent Oxford University Press.\n Semantic Analysis to assess knowledge: Some technical Wittgenstein, L. 1953. Philosophical Investigations. (Transconsiderations. Discourse Processes, 25(2-3):337‚Äì354. lated by Anscombe, G.E.M.). Blackwell.\nRohde, D. L. T., L. M. Gonnerman, and D. C. Plaut. 2006. Zhao, J., T. Wang, M. Yatskar, V. Ordonez, and K.-\n An improved model of semantic similarity based on lexi- W. Chang. 2017. Men also like shopping: Reducing\n cal co-occurrence. CACM, 8:627‚Äì633. gender bias amplification using corpus-level constraints.\nRumelhart, D. E. and A. A. Abrahamson. 1973. A model for EMNLP.\n analogical reasoning. Cognitive Psychology, 5(1):1‚Äì28. Zhao, J., Y. Zhou, Z. Li, W. Wang, and K.-W. Chang. 2018.\nSalton, G. 1971. The SMART Retrieval System: Experiments Learning gender-neutral word embeddings. EMNLP.\n in Automatic Document Processing. Prentice Hall.\nSchluter, N. 2018. The word analogy testing caveat. NAACL\n HLT.\nSchone, P. and D. Jurafsky. 2000. Knowlege-free induction\n of morphology using latent semantic analysis. CoNLL.\nSchone, P. and D. Jurafsky. 2001a. Is knowledge-free induction of multiword unit dictionary headwords a solved\n problem? EMNLP.\nSchone, P. and D. Jurafsky. 2001b. Knowledge-free induction of inflectional morphologies. NAACL.\nSchuÃàtze, H. 1992. Dimensions of meaning. Proceedings of\n Supercomputing ‚Äô92. IEEE Press.\nSchuÃàtze, H. 1997. Ambiguity Resolution in Language Learning ‚Äì Computational and Cognitive Models. CSLI, Stanford, CA.\nSchuÃàtze, H., D. A. Hull, and J. Pedersen. 1995. A comparison of classifiers and document representations for the\n routing problem. SIGIR-95.\nSchuÃàtze, H. and J. Pedersen. 1993. A vector model for syntagmatic and paradigmatic relatedness. 9th Annual Conference of the UW Centre for the New OED and Text Research.\nSparck Jones, K. 1972. A statistical interpretation of term\n specificity and its application in retrieval. Journal of Documentation, 28(1):11‚Äì21.\nSparck Jones, K. 1986. Synonymy and Semantic Classification. Edinburgh University Press, Edinburgh. Republication of 1964 PhD Thesis.\nSwitzer, P. 1965. Vector images in document retrieval.\n Statistical Association Methods For Mechanized Documentation. Symposium Proceedings. Washington, D.C.,\n USA, March 17, 1964. https://nvlpubs.nist.gov/\n nistpubs/Legacy/MP/nbsmiscellaneouspub269.\n pdf.\nTian, Y., V. Kulkarni, B. Perozzi, and S. Skiena. 2016. On\n the convergent properties of word embedding methods.\n ArXiv preprint arXiv:1605.03956.\nTurian, J., L. Ratinov, and Y. Bengio. 2010. Word representations: a simple and general method for semi-supervised\n learning. ACL.\n",
    "file_path": "/Users/colinsidberry/nlp-textbook-rag/data/extracted_text/embeddings.txt",
    "file_size_kb": 84.53
  },
  {
    "id": "97165c700e59a51c",
    "source": "nlp_textbook",
    "chapter": "Neural Networks",
    "filename": "neural-networks.txt",
    "content": "rights reserved. Draft of August 24, 2025.\n\nCHAPTER\n\n Neural Networks\n ‚Äú[M]achines of this character can behave in a very complicated manner when\n the number of units is large.‚Äù\n Alan Turing (1948) ‚ÄúIntelligent Machines‚Äù, page 6\n\n Neural networks are a fundamental computational tool for language processing, and a very old one. They are called neural because their origins lie in the\n McCulloch-Pitts neuron (McCulloch and Pitts, 1943), a simplified model of the\n biological neuron as a kind of computing element that could be described in terms\n of propositional logic. But the modern use in language processing no longer draws\n on these early biological inspirations.\n Instead, a modern neural network is a network of small computing units, each\n of which takes a vector of input values and produces a single output value. In this\n chapter we introduce the neural net applied to classification. The architecture we\n feedforward introduce is called a feedforward network because the computation proceeds iteratively from one layer of units to the next. The use of modern neural nets is often\n deep learning called deep learning, because modern networks are often deep (have many layers).\n Neural networks share much of the same mathematics as logistic regression. But\n neural networks are a more powerful classifier than logistic regression, and indeed a\n minimal neural network (technically one with a single ‚Äòhidden layer‚Äô) can be shown\n to learn any function.\n Neural net classifiers are different from logistic regression in another way. With\n logistic regression, we applied the regression classifier to many different tasks by\n developing many rich kinds of feature templates based on domain knowledge. When\n working with neural networks, it is more common to avoid most uses of rich handderived features, instead building neural networks that take raw tokens as inputs\n and learn to induce features as part of the process of learning to classify. We saw\n examples of this kind of representation learning for embeddings in Chapter 5, and\n we‚Äôll see lots of examples once we start studying deep transformers networks. Nets\n that are very deep are particularly good at representation learning. For that reason\n deep neural nets are the right tool for tasks that offer sufficient data to learn features\n automatically.\n In this chapter we‚Äôll introduce feedforward networks as classifiers, first with\n hand-built features, and then using the embeddings that we studied in Chapter 5.\n In subsequent chapters we‚Äôll introduce many other kinds of neural models, most\n importantly the transformer and attention, (Chapter 8), but also recurrent neural\n networks (Chapter 13) and convolutional neural networks (Chapter 15). And in\n the next chapter we‚Äôll introduce the paradigm of neural large language models.\n2 C HAPTER 6 ‚Ä¢ N EURAL N ETWORKS\n\n The building block of a neural network is a single computational unit. A unit takes\n a set of real valued numbers as input, performs some computation on them, and\n produces an output.\n At its heart, a neural unit is taking a weighted sum of its inputs, with one addibias term tional term in the sum called a bias term. Given a set of inputs x1 ...xn , a unit has\n a set of corresponding weights w1 ...wn and a bias b, so the weighted sum z can be\n represented as: X\n z = b+ wi xi (6.1)\n i\n Often it‚Äôs more convenient to express this weighted sum using vector notation; recall\n vector from linear algebra that a vector is, at heart, just a list or array of numbers. Thus\n we‚Äôll talk about z in terms of a weight vector w, a scalar bias b, and an input vector\n x, and we‚Äôll replace the sum with the convenient dot product:\n z = w¬∑x+b (6.2)\n As defined in Eq. 6.2, z is just a real valued number.\n Finally, instead of using z, a linear function of x, as the output, neural units\n apply a non-linear function f to z. We will refer to the output of this function as\n activation the activation value for the unit, a. Since we are just modeling a single unit, the\n activation for the node is in fact the final output of the network, which we‚Äôll generally\n call y. So the value y is defined as:\n y = a = f (z)\n We‚Äôll discuss three popular non-linear functions f below (the sigmoid, the tanh, and\n the rectified linear unit or ReLU) but it‚Äôs pedagogically convenient to start with the\n sigmoid sigmoid function since we saw it in Chapter 4:\n y = œÉ (z) = (6.3)\n 1 + e‚àíz\n The sigmoid (shown in Fig. 6.1) has a number of advantages; it maps the output\n into the range (0, 1), which is useful in squashing outliers toward 0 or 1. And it‚Äôs\n differentiable, which as we saw in Section ?? will be handy for learning.\n\n nearly linear around 0 but outlier values get squashed toward 0 or 1.\n\n Substituting Eq. 6.2 into Eq. 6.3 gives us the output of a neural unit:\n y = œÉ (w ¬∑ x + b) = (6.4)\n 1 + exp(‚àí(w ¬∑ x + b))\n 6.1 ‚Ä¢ U NITS 3\n\n Fig. 6.2 shows a final schematic of a basic neural unit. In this example the unit\n takes 3 input values x1 , x2 , and x3 , and computes a weighted sum, multiplying each\n value by a weight (w1 , w2 , and w3 , respectively), adds them to a bias term b, and then\n passes the resulting sum through a sigmoid function to result in a number between 0\n and 1.\n\n x1 w1\n\n w2 z a\n x2 ‚àë œÉ y\n w3\n\n x3 b\n\n +1\n\n weight for an input clamped at +1) and producing an output y. We include some convenient\n intermediate variables: the output of the summation, z, and the output of the sigmoid, a. In\n this case the output of the unit y is the same as a, but in deeper networks we‚Äôll reserve y to\n mean the final output of the entire network, leaving a as the activation of an individual node.\n\n Let‚Äôs walk through an example just to get an intuition. Let‚Äôs suppose we have a\n unit with the following weight vector and bias:\n\n w = [0.2, 0.3, 0.9]\n b = 0.5\n\n What would this unit do with the following input vector:\n\n x = [0.5, 0.6, 0.1]\n\n The resulting output y would be:\n 1 1 1\n y = œÉ (w ¬∑ x + b) = = = = .70\n 1 + e‚àí(w¬∑x+b) 1 + e‚àí(.5‚àó.2+.6‚àó.3+.1‚àó.9+.5) 1 + e‚àí0.87\n In practice, the sigmoid is not commonly used as an activation function. A function\ntanh that is very similar but almost always better is the tanh function shown in Fig. 6.3a;\n tanh is a variant of the sigmoid that ranges from -1 to +1:\n\n ez ‚àí e‚àíz\n y = tanh(z) = (6.5)\n ez + e‚àíz\n The simplest activation function, and perhaps the most commonly used, is the rec-\nReLU tified linear unit, also called the ReLU, shown in Fig. 6.3b. It‚Äôs just the same as z\n when z is positive, and 0 otherwise:\n\n y = ReLU(z) = max(z, 0) (6.6)\n\n These activation functions have different properties that make them useful for different language applications or network architectures. For example, the tanh function\n has the nice properties of being smoothly differentiable and mapping outlier values\n toward the mean. The rectifier function, on the other hand, has nice properties that\n4 C HAPTER 6 ‚Ä¢ N EURAL N ETWORKS\n\n (a) (b)\n\n result from it being very close to linear. In the sigmoid or tanh functions, very high\n saturated values of z result in values of y that are saturated, i.e., extremely close to 1, and have\n derivatives very close to 0. Zero derivatives cause problems for learning, because as\n we‚Äôll see in Section 6.6, we‚Äôll train networks by propagating an error signal backwards, multiplying gradients (partial derivatives) from each layer of the network;\n gradients that are almost 0 cause the error signal to get smaller and smaller until it is\n vanishing\n gradient too small to be used for training, a problem called the vanishing gradient problem.\n Rectifiers don‚Äôt have this problem, since the derivative of ReLU for high values of z\n is 1 rather than very close to 0.\n\n Early in the history of neural networks it was realized that the power of neural networks, as with the real neurons that inspired them, comes from combining these\n units into larger networks.\n One of the most clever demonstrations of the need for multi-layer networks was\n the proof by Minsky and Papert (1969) that a single neural unit cannot compute\n some very simple functions of its input. Consider the task of computing elementary\n logical functions of two inputs, like AND, OR, and XOR. As a reminder, here are\n the truth tables for those functions:\n\n AND OR XOR\n x1 x2 y x1 x2 y x1 x2 y\n 0 0 0 0 0 0 0 0 0\n 0 1 0 0 1 1 0 1 1\n 1 0 0 1 0 1 1 0 1\n 1 1 1 1 1 1 1 1 0\n\n perceptron This example was first shown for the perceptron, which is a very simple neural\n unit that has a binary output and has a very simple step function as its non-linear\n activation function. The output y of a perceptron is 0 or 1, and is computed as\n follows (using the same weight w, input x, and bias b as in Eq. 6.2):\n \u001a\n 0, if w ¬∑ x + b ‚â§ 0\n y= (6.7)\n 1, if w ¬∑ x + b > 0\n 6.2 ‚Ä¢ T HE XOR PROBLEM 5\n\n It‚Äôs very easy to build a perceptron that can compute the logical AND and OR\n functions of its binary inputs; Fig. 6.4 shows the necessary weights.\n\n x1 x1\n 1 1\n x2 1 x2 1\n -1 0\n +1 +1\n (a) (b)\n inputs are shown as x1 and x2 and the bias as a special node with value +1 which is multiplied\n with the bias weight b. (a) logical AND, with weights w1 = 1 and w2 = 1 and bias weight\n b = ‚àí1. (b) logical OR, with weights w1 = 1 and w2 = 1 and bias weight b = 0. These\n weights/biases are just one from an infinite number of possible sets of weights and biases that\n would implement the functions.\n\n It turns out, however, that it‚Äôs not possible to build a perceptron to compute\n logical XOR! (It‚Äôs worth spending a moment to give it a try!)\n The intuition behind this important result relies on understanding that a perceptron is a linear classifier. For a two-dimensional input x1 and x2 , the perceptron\n equation, w1 x1 + w2 x2 + b = 0 is the equation of a line. (We can see this by putting\n it in the standard linear format: x2 = (‚àíw1 /w2 )x1 + (‚àíb/w2 ).) This line acts as a\n decision\nboundary decision boundary in two-dimensional space in which the output 0 is assigned to all\n inputs lying on one side of the line, and the output 1 to all input points lying on the\n other side of the line. If we had more than 2 inputs, the decision boundary becomes\n a hyperplane instead of a line, but the idea is the same, separating the space into two\n categories.\n Fig. 6.5 shows the possible logical inputs (00, 01, 10, and 11) and the line drawn\n by one possible set of parameters for an AND and an OR classifier. Notice that there\n is simply no way to draw a line that separates the positive cases of XOR (01 and 10)\n linearly\nseparable from the negative cases (00 and 11). We say that XOR is not a linearly separable\n function. Of course we could draw a boundary with a curve, or some other function,\n but not a single line.\n\n 6.2.1 The solution: neural networks\n While the XOR function cannot be calculated by a single perceptron, it can be calculated by a layered network of perceptron units. Rather than see this with networks\n of simple perceptrons, however, let‚Äôs see how to compute XOR using two layers of\n ReLU-based units following Goodfellow et al. (2016). Fig. 6.6 shows a figure with\n the input being processed by two layers of neural units. The middle layer (called\n h) has two units, and the output layer (called y) has one unit. A set of weights and\n biases are shown that allows the network to correctly compute the XOR function.\n Let‚Äôs walk through what happens with the input x = [0, 0]. If we multiply each\n input value by the appropriate weight, sum, and then add the bias b, we get the vector\n [0, -1], and we then apply the rectified linear transformation to give the output of the\n h layer as [0, 0]. Now we once again multiply by the weights, sum, and add the\n bias (0 in this case) resulting in the value 0. The reader should work through the\n computation of the remaining 3 possible input pairs to see that the resulting y values\n are 1 for the inputs [0, 1] and [1, 0] and 0 for [0, 0] and [1, 1].\n6 C HAPTER 6 ‚Ä¢ N EURAL N ETWORKS\n\n x2 x2 x2\n\n 1 1 1\n\n ?\n 0 x1 0 x1 0 x1\n 0 1 0 1 0 1\n\n a) x1 AND x2 b) x1 OR x2 c) x1 XOR x2\n\ny-axis. Filled circles represent perceptron outputs of 1, and white circles perceptron outputs of 0. There is no\nway to draw a line that correctly separates the two categories for XOR. Figure styled after Russell and Norvig\n(2002).\n\n x1 1 h1\n y1\n -2\n x2 1 h2 0\n -1\n +1 +1\n two layers; we‚Äôve called them h1 , h2 (h for ‚Äúhidden layer‚Äù) and y1 . As before, the numbers\n on the arrows represent the weights w for each unit, and we represent the bias b as a weight\n on a unit clamped to +1, with the bias weights/units in gray.\n\n It‚Äôs also instructive to look at the intermediate results, the outputs of the two\n hidden nodes h1 and h2 . We showed in the previous paragraph that the h vector for\n the inputs x = [0, 0] was [0, 0]. Fig. 6.7b shows the values of the h layer for all\n 4 inputs. Notice that hidden representations of the two input points x = [0, 1] and\n x = [1, 0] (the two cases with XOR output = 1) are merged to the single point h =\n [1, 0]. The merger makes it easy to linearly separate the positive and negative cases\n of XOR. In other words, we can view the hidden layer of the network as forming a\n representation of the input.\n In this example we just stipulated the weights in Fig. 6.6. But for real examples\n the weights for neural networks are learned automatically using the error backpropagation algorithm to be introduced in Section 6.6. That means the hidden layers will\n learn to form useful representations. This intuition, that neural networks can automatically learn useful representations of the input, is one of their key advantages,\n and one that we will return to again and again in later chapters.\n 6.3 ‚Ä¢ F EEDFORWARD N EURAL N ETWORKS 7\n\n x2 h2\n\n 1 1\n\n 0 x1 0\n h1\n 0 1 0 1 2\n\n a) The original x space b) The new (linearly separable) h space\n representation of the hidden layer, h, compared to the original input representation x in (a).\n Notice that the input point [0, 1] has been collapsed with the input point [1, 0], making it\n possible to linearly separate the positive and negative cases of XOR. After Goodfellow et al.\n (2016).\n\n Let‚Äôs now walk through a slightly more formal presentation of the simplest kind of\n feedforward neural network, the feedforward network. A feedforward network is a multilayer\n network\n network in which the units are connected with no cycles; the outputs from units in\n each layer are passed to units in the next higher layer, and no outputs are passed\n back to lower layers. (In Chapter 13 we‚Äôll introduce networks with cycles, called\n recurrent neural networks.)\n For historical reasons multilayer networks, especially feedforward networks, are\n multi-layer\n perceptrons sometimes called multi-layer perceptrons (or MLPs); this is a technical misnomer,\n MLP since the units in modern multilayer networks aren‚Äôt perceptrons (perceptrons have a\n simple step-function as their activation function, but modern networks are made up\n of units with many kinds of non-linearities like ReLUs and sigmoids), but at some\n point the name stuck.\n Simple feedforward networks have three kinds of nodes: input units, hidden\n units, and output units.\n Fig. 6.8 shows a picture. The input layer x is a vector of simple scalar values just\n as we saw in Fig. 6.2.\n hidden layer The core of the neural network is the hidden layer h formed of hidden units hi ,\n each of which is a neural unit as described in Section 6.1, taking a weighted sum of\n its inputs and then applying a non-linearity. In the standard architecture, each layer\nfully-connected is fully-connected, meaning that each unit in each layer takes as input the outputs\n from all the units in the previous layer, and there is a link between every pair of units\n from two adjacent layers. Thus each hidden unit sums over all the input units.\n Recall that a single hidden unit has as parameters a weight vector and a bias. We\n represent the parameters for the entire hidden layer by combining the weight vector\n and bias for each unit i into a single weight matrix W and a single bias vector b for\n the whole layer (see Fig. 6.8). Each element W ji of the weight matrix W represents\n the weight of the connection from the ith input unit xi to the jth hidden unit h j .\n The advantage of using a single matrix W for the weights of the entire layer is\n that now the hidden layer computation for a feedforward network can be done very\n8 C HAPTER 6 ‚Ä¢ N EURAL N ETWORKS\n\n x1 W U\n y1\n h1\n\n x2 h2 y2\n h3\n\n ‚Ä¶\n\n ‚Ä¶\n ‚Ä¶\n xn\n 0 hn\n b yn\n +1\n input layer hidden layer output layer\n\n and one input layer (the input layer is usually not counted when enumerating layers).\n\n efficiently with simple matrix operations. In fact, the computation only has three\n steps: multiplying the weight matrix by the input vector x, adding the bias vector b,\n and applying the activation function g (such as the sigmoid, tanh, or ReLU activation\n function defined above).\n The output of the hidden layer, the vector h, is thus the following (for this example we‚Äôll use the sigmoid function œÉ as our activation function):\n h = œÉ (Wx + b) (6.8)\n\n Notice that we‚Äôre applying the œÉ function here to a vector, while in Eq. 6.3 it was\n applied to a scalar. We‚Äôre thus allowing œÉ (¬∑), and indeed any activation function\n g(¬∑), to apply to a vector element-wise, so g[z1 , z2 , z3 ] = [g(z1 ), g(z2 ), g(z3 )].\n Let‚Äôs introduce some constants to represent the dimensionalities of these vectors\n and matrices. We‚Äôll refer to the input layer as layer 0 of the network, and have\n n0 represent the number of inputs, so x is a vector of real numbers of dimension\n n0 , or more formally x ‚àà Rn0 , a column vector of dimensionality [n0 √ó 1]. Let‚Äôs\n call the hidden layer layer 1 and the output layer layer 2. The hidden layer has\n dimensionality n1 , so h ‚àà Rn1 and also b ‚àà Rn1 (since each hidden unit can take a\n different bias value). And the weight matrix W has dimensionality W ‚àà Rn1 √ón0 , i.e.\n [n1 √ó n0 ].\n Take a moment to convince yourself\n Pn0 that the matrix \u0001 multiplication in Eq. 6.8 will\n compute the value of each h j as œÉ i=1 W ji xi + b j .\n As we saw in Section 6.2, the resulting value h (for hidden but also for hypothesis) forms a representation of the input. The role of the output layer is to take\n this new representation h and compute a final output. This output could be a realvalued number, but in many cases the goal of the network is to make some sort of\n classification decision, and so we will focus on the case of classification.\n If we are doing a binary task like sentiment classification, we might have a single output node, and its scalar value y is the probability of positive versus negative\n sentiment. If we are doing multinomial classification, such as assigning a part-ofspeech tag, we might have one output node for each potential part-of-speech, whose\n output value is the probability of that part-of-speech, and the values of all the output\n nodes must sum to one. The output layer is thus a vector y that gives a probability\n distribution across the output nodes.\n 6.3 ‚Ä¢ F EEDFORWARD N EURAL N ETWORKS 9\n\n Let‚Äôs see how this happens. Like the hidden layer, the output layer has a weight\n matrix (let‚Äôs call it U), but some models don‚Äôt include a bias vector b in the output\n layer, so we‚Äôll simplify by eliminating the bias vector in this example. The weight\n matrix is multiplied by its input vector (h) to produce the intermediate output z:\n z = Uh\n There are n2 output nodes, so z ‚àà Rn2 , weight matrix U has dimensionality U ‚àà\n Rn2 √ón1 , and element Ui j is the weight from unit j in the hidden layer to unit i in the\n output layer.\n However, z can‚Äôt be the output of the classifier, since it‚Äôs a vector of real-valued\n numbers, while what we need for classification is a vector of probabilities. There is\nnormalizing a convenient function for normalizing a vector of real values, by which we mean\n converting it to a vector that encodes a probability distribution (all the numbers lie\n softmax between 0 and 1 and sum to 1): the softmax function that we saw on page ?? of\n Chapter 4. More generally for any vector z of dimensionality d, the softmax is\n defined as:\n exp(zi )\n softmax(zi ) = Pd 1‚â§i‚â§d (6.9)\n j=1 exp(z j )\n Thus for example given a vector\n z = [0.6, 1.1, ‚àí1.5, 1.2, 3.2, ‚àí1.1], (6.10)\n the softmax function will normalize it to a probability distribution (shown rounded):\n softmax(z) = [0.055, 0.090, 0.0067, 0.10, 0.74, 0.010] (6.11)\n You may recall that we used softmax to create a probability distribution from a\n vector of real-valued numbers (computed from summing weights times features) in\n the multinomial version of logistic regression in Chapter 4.\n That means we can think of a neural network classifier with one hidden layer\n as building a vector h which is a hidden layer representation of the input, and then\n running standard multinomial logistic regression on the features that the network\n develops in h. By contrast, in Chapter 4 the features were mainly designed by hand\n via feature templates. So a neural network is like multinomial logistic regression,\n but (a) with many layers, since a deep neural network is like layer after layer of logistic regression classifiers; (b) with those intermediate layers having many possible\n activation functions (tanh, ReLU, sigmoid) instead of just sigmoid (although we‚Äôll\n continue to use œÉ for convenience to mean any activation function); (c) rather than\n forming the features by feature templates, the prior layers of the network induce the\n feature representations themselves.\n Here are the final equations for a feedforward network with a single hidden layer,\n which takes an input vector x, outputs a probability distribution y, and is parameterized by weight matrices W and U and a bias vector b:\n h = œÉ (Wx + b)\n z = Uh\n y = softmax(z) (6.12)\n And just to remember the shapes of all our variables, x ‚àà Rn0 , h ‚àà Rn1 , b ‚àà Rn1 ,\n W ‚àà Rn1 √ón0 , U ‚àà Rn2 √ón1 , and the output vector y ‚àà Rn2 . We‚Äôll call this network a 2layer network (we traditionally don‚Äôt count the input layer when numbering layers,\n but do count the output layer). So by this terminology logistic regression is a 1-layer\n network.\n10 C HAPTER 6 ‚Ä¢ N EURAL N ETWORKS\n\n 6.3.1 More details on feedforward networks\n Let‚Äôs now set up some notation to make it easier to talk about deeper networks of\n depth more than 2. We‚Äôll use superscripts in square brackets to mean layer numbers, starting at 0 for the input layer. So W[1] will mean the weight matrix for the\n (first) hidden layer, and b[1] will mean the bias vector for the (first) hidden layer. n j\n will mean the number of units at layer j. We‚Äôll use g(¬∑) to stand for the activation\n function, which will tend to be ReLU or tanh for intermediate layers and softmax\n for output layers. We‚Äôll use a[i] to mean the output from layer i, and z[i] to mean the\n combination of previous layer output, weights and biases W[i] a[i‚àí1] + b[i] . The 0th\n layer is for inputs, so we‚Äôll refer to the inputs x more generally as a[0] .\n Thus we can re-represent our 2-layer net from Eq. 6.12 as follows:\n\n z[1] = W[1] a[0] + b[1]\n a[1] = g[1] (z[1] )\n z[2] = W[2] a[1] + b[2]\n a[2] = g[2] (z[2] )\n yÃÇ = a[2] (6.13)\n\n Note that with this notation, the equations for the computation done at each layer are\n the same. The algorithm for computing the forward step in an n-layer feedforward\n network, given the input vector a[0] is thus simply:\n\n for i in 1,...,n\n z[i] = W[i] a[i‚àí1] + b[i]\n a[i] = g[i] (z[i] )\n yÃÇ = a[n]\n\n It‚Äôs often useful to have a name for the final set of activations right before the final\n softmax. So however many layers we have, we‚Äôll generally call the unnormalized\n values in the final vector z[n] , the vector of scores right before the final softmax, the\n logits logits (see Eq. ??).\n The need for non-linear activation functions One of the reasons we use nonlinear activation functions for each layer in a neural network is that if we did not, the\n resulting network is exactly equivalent to a single-layer network. Let‚Äôs see why this\n is true. Imagine the first two layers of such a network of purely linear layers:\n\n z[1] = W[1] x + b[1]\n z[2] = W[2] z[1] + b[2]\n\n We can rewrite the function that the network is computing as:\n\n z[2] = W[2] z[1] + b[2]\n = W[2] (W[1] x + b[1] ) + b[2]\n = W[2] W[1] x + W[2] b[1] + b[2]\n = W 0 x + b0 (6.14)\n\n This generalizes to any number of layers. So without non-linear activation functions,\n a multilayer network is just a notational variant of a single layer network with a\n different set of weights, and we lose all the representational power of multilayer\n networks.\n 6.4 ‚Ä¢ F EEDFORWARD NETWORKS FOR NLP: C LASSIFICATION 11\n\n Replacing the bias unit In describing networks, we will sometimes use a slightly\n simplified notation that represents exactly the same function without referring to an\n explicit bias node b. Instead, we add a dummy node a0 to each layer whose value\n [0]\n will always be 1. Thus layer 0, the input layer, will have a dummy node a0 = 1,\n [1]\n layer 1 will have a0 = 1, and so on. This dummy node still has an associated weight,\n and that weight represents the bias value b. For example instead of an equation like\n h = œÉ (Wx + b) (6.15)\n\n we‚Äôll use:\n h = œÉ (Wx) (6.16)\n\n But now instead of our vector x having n0 values: x = x1 , . . . , xn0 , it will have n0 +\n 1 values, with a new 0th dummy value x0 = 1: x = x0 , . . . , xn0 . And instead of\n computing each h j as follows:\n n0\n !\n X\n hj = œÉ Wji xi + b j , (6.17)\n i=1\n\n we‚Äôll instead use:\n n0\n !\n X\n hj = œÉ Wji xi , (6.18)\n i=0\n\n where the value Wj0 replaces what had been b j . Fig. 6.9 shows a visualization.\n\n W U W U\n x1 h1 y1 x0=1\n h1 y1\n h2\n x2 y2 x1 h2 y2\n h3\n ‚Ä¶\n\n x2 h3\n ‚Ä¶\n ‚Ä¶\n\n ‚Ä¶\n ‚Ä¶\n\n xn\n hn\n ‚Ä¶\n\n yn hn\n b 2 xn 1 yn\n +1 0 2\n (a) (b)\n\n We‚Äôll continue showing the bias as b when we go over the learning algorithm\n in Section 6.6, but going forward in the book, for most figures and some equations\n we‚Äôll use this simplified notation without explicit bias terms.\n\n Let‚Äôs see how to apply feedforward networks to NLP classification tasks. In practice,\n simple feedforward networks aren‚Äôt the way we do text classification; for real applications we would use more sophisticated architectures like the BERT transformers\n12 C HAPTER 6 ‚Ä¢ N EURAL N ETWORKS\n\n of Chapter 10. Nonetheless seeing a feedforward network text classifier will let us\n introduce key ideas that will play a role throughout the rest of the book, including the ideas of the embedding matrix, representation pooling, and representation\n learning.\n But before introducing any of these ideas, let‚Äôs start with a classifier by making\n only minimal change from the sentiment classifiers we saw in Chapter 4. Like them,\n we‚Äôll take hand-built features, pass them through a classifier, and produce a class\n probability. The only difference is that we‚Äôll use a neural network instead of logistic\n regression as the classifier.\n\n 6.4.1 Neural net classifiers with hand-built features\n Let‚Äôs begin with a simple 2-layer sentiment classifier by taking our logistic regression classifier from Chapter 4, which corresponds to a 1-layer network, and just\n adding a hidden layer. The input element xi can be scalar features like those in\n Fig. ??, e.g., x1 = count(words ‚àà doc), x2 = count(positive lexicon words ‚àà doc),\n x3 = 1 if ‚Äúno‚Äù ‚àà doc, and so on, for a total of d features. And the output layer\n yÃÇ could have two nodes (one each for positive and negative), or 3 nodes (positive,\n negative, neutral), in which case yÃÇ1 would be the estimated probability of positive\n sentiment, yÃÇ2 the probability of negative and yÃÇ3 the probability of neutral. The resulting equations would be just what we saw above for a 2-layer network (as always,\n we‚Äôll continue to use the œÉ to stand for any non-linearity, whether sigmoid, ReLU\n or other).\n\n x = [x1 , x2 , ...xd ] (each xi is a hand-designed feature)\n h = œÉ (Wx + b)\n z = Uh\n yÃÇ = softmax(z) (6.19)\n\n Fig. 6.10 shows a sketch of this architecture. As we mentioned earlier, adding this\n hidden layer to our logistic regression classifier allows the network to represent the\n non-linear interactions between features. This alone might give us a better sentiment\n classifier.\n\n h1\n dessert wordcount x1\n =3\n h2\n y^1 p(+)\n positive lexicon x\n was words = 1 2 y^ 2 p(-)\n h3\n\n y^3\n ‚Ä¶\n\n great count of ‚Äúno‚Äù x3 p(neut)\n =0\n hdh\n Input words x W h U y\n [d‚®â1] [dh‚®âd] [3‚®âdh] [3‚®â1]\n [dh‚®â1]\n Input layer Hidden layer Output layer\n d=3 features softmax\n\n of the input text.\n 6.5 ‚Ä¢ E MBEDDINGS AS THE INPUT TO NEURAL NET CLASSIFIERS 13\n\n 6.4.2 Vectorizing for parallelizing inference\n While Eq. 6.19 shows how to classify a single example x, in practice we want to\n efficiently classify an entire test set of m examples. We do this by vectorizing the\n process, just as we saw with logistic regression; instead of using for-loops to go\n through each example, we‚Äôll use matrix multiplication to do the entire computation\n of an entire test set at once. First, we pack all the input feature vectors for each input\n x into a single input matrix X, with each row i a row vector consisting of the features\n for input example x(i) (i.e., the vector x(i) ). If the dimensionality of our input feature\n vector is d, X will be a matrix of shape [m √ó d].\n Because we are now modeling each input as a row vector rather than a column\n vector, we also need to slightly modify Eq. 6.19. X is of shape [m √ó d] and W is of\n shape [dh √ó d], so we‚Äôll reorder how we multiply X and W and transpose W so they\n correctly multiply to yield a matrix H of shape [m √ó dh ]. 1\n The bias vector b from Eq. 6.19 of shape [1 √ó dh ] will now have to be replicated\n into a matrix of shape [m √ó dh ]. We‚Äôll need to similarly reorder the next step and\n transpose U. Finally, our output matrix YÃÇ will be of shape [m √ó 3] (or more generally [m √ó do ], where do is the number of output classes), with each row i of our\n output matrix YÃÇ consisting of the output vector yÃÇ(i) . Here are the final equations for\n computing the output class distribution for an entire test set:\n\n H = œÉ (XW| + b)\n Z = HU|\n YÃÇ = softmax(Z) (6.20)\n\n In this book, we‚Äôll sometimes see orderings like WX + b and sometimes XW + b.\n That‚Äôs why it‚Äôs always important to be very aware of the shapes of your weight\n matrices participating in any given equation.\n\n While hand-built features are a traditional way to design classifiers, most applications of neural networks for NLP don‚Äôt use hand-built human-engineered features as\n inputs. Instead, we draw on deep learning‚Äôs ability to learn features from the data by\n representing tokens as embeddings. For this section we‚Äôll represent each token by\n its static word2vec or GloVe embeddings that we saw how to compute in Chapter 5.\n By static embedding, we mean that each token is represented by a fixed vector that\n we train once, and then just put into a big dictionary. When we want to refer to that\n token, we grab its embedding out of the dictionary.\n However when we apply neural models to the task of language modeling (as\n we‚Äôll see in Chapter 8) the situation is more complex, and we‚Äôll use a more powerful kind of embedding called a contextual embedding. Contextual embeddings are\n different for each time a word occurs in a different context. Furthermore, we‚Äôll have\n the network learn these embeddings as part of the task of word prediction.\n So let‚Äôs explore the text classification domain above, but using static embeddings\n as features instead of the hand-designed features. Let‚Äôs focus on the inference stage,\n 1 Note that we could have kept the original order of our products if we had instead made our input\n matrix X represent each input as a column vector instead of a row vector, making it of shape [d √ó m]. But\n representing inputs as row vectors is convenient and common in neural network models.\n14 C HAPTER 6 ‚Ä¢ N EURAL N ETWORKS\n\n in which we have already learned embeddings for all the input tokens. An embedding is a vector of dimension d that represents the input token. The dictionary of\n embedding static embeddings in which we store these embeddings is the embedding matrix\n matrix\n E. Each row of the embedding matrix represents each token of the vocabulary V\n as a (row) vector of dimensionality d. Since E has a row for each of the |V | tokens in the vocabulary, E has shape [|V | √ó d]. This embedding matrix E plays a role\n whenever we are using embeddings as input to neural NLP systems, including in the\n transformer-based large language models we will introduce over the next chapters.\n Given an input token string like dessert was great we first convert the tokens\n into vocabulary indices (these were created when we first tokenized the input using\n BPE or SentencePiece). So the representation of dessert was great might be\n w = [3, 9824, 226]. Next we use indexing to select the corresponding rows from E\n (row 3, row 4000, row 10532).\n Another way to think about selecting token embeddings from the embedding\n matrix is to represent input tokens as one-hot vectors of shape [1 √ó |V |], i.e., with\n one-hot vector one dimension for each word in the vocabulary. Recall that in a one-hot vector all\n the elements are 0 except one, the element whose dimension is the word‚Äôs index\n in the vocabulary, which has value 1. So if the word ‚Äúdessert‚Äù has index 3 in the\n vocabulary, x3 = 1, and xi = 0 ‚àÄi 6= 3, as shown here:\n [0 0 1 0 0 0 0 ... 0 0 0 0]\n 1 2 3 4 5 6 7 ... ... |V|\n Multiplying by a one-hot vector that has only one non-zero element xi = 1 simply\n selects out the relevant row vector for word i, resulting in the embedding for word i,\n as depicted in Fig. 6.11.\n\n d\n\n 3 |V| 3 d\n 1 0010000‚Ä¶0000 ‚úï E = 1\n\n matrix E with a one-hot vector with a 1 in index 3.\n\n We can extend this idea to represent the entire input token sequence as a matrix\n of one-hot vectors, one for each of the N input positions as shown in Fig. 6.12.\n\n d\n |V| d\n 0010000‚Ä¶0000\n 0000000‚Ä¶0010\n 1000000‚Ä¶0000 ‚úï E =\n ‚Ä¶ N\n N 0000100‚Ä¶0000\n | V|\n\n We now need to classify this input of N [1 √ó d] embeddings, representing a window of N tokens, into a single class (like positive or negative).\n There are two common ways to to pass embeddings to a classifier: concatenation and pooling. First, we can take this input of shape [N √ó d] and reshape it\n 6.5 ‚Ä¢ E MBEDDINGS AS THE INPUT TO NEURAL NET CLASSIFIERS 15\n\n by concatenating all the input vectors into one very long vector of shape [1 √ó dN].\n Then we pass this input to our classifier and let it make its decision. This gives\n us lots of information, at the cost of using a pretty large network. Second, we can\n pool pool the N embeddings into a single embedding and then pass that single pooled\n embedding to the classifier. Pooling gives us less information than would have been\n present in all the original embeddings, but has the advantage of being small and efficient and is especially useful in tasks for which we don‚Äôt care as much about the\n original word order. Let‚Äôs give an example of each: pooling for the sentiment task,\n and concatenation for the language modeling task.\n Pooling input embeddings for sentiment So let‚Äôs begin with seeing how pooling\n can work for the sentiment classification task. The intuition of pooling is that for\n sentiment, the exact position of the input (is some word like great the first word?\n the second word?) is less important than the identity of the word itself.\n A pooling function is a way to turn a set of embeddings into a single embedding.\n For example, for a text with N input words/tokens w1 , ..., wN , we want to turn\n the N row embeddings e(w1 ), ..., e(wN ) (each of dimensionality d) into a single\n embedding also of dimensionality d.\nmean-pooling There are various ways to pool. The simplest is mean-pooling: taking the mean\n by summing the embeddings and then dividing by N:\n N\n 1X\n xmean = e(wi ) (6.21)\n N\n i=1\n\n Here are the equations for this classifier assuming mean pooling:\n\n x = mean(e(w1 ), e(w2 ), . . . , e(wn ))\n h = œÉ (xW + b)\n z = hU\n yÃÇ = softmax(z) (6.22)\n\n The architecture is sketched in Fig. 6.13, where we also give the shapes for all the\n relevant matrices.\n max-pooling There are many other options for pooling, like max-pooling, in which case for\n each dimension we take the element-wise max over all the inputs. The element-wise\n max of a set of N vectors is a new vector whose kth element is the max of the kth\n elements of all the N vectors.\n Concatenating input embeddings for language modeling For sentiment analysis we saw how to generate an output vector with probabilities over three classes:\n positive, negative, or neutral, given as input a window of N input tokens, by first\n pooling those token embeddings into a single embedding vector.\n Now let‚Äôs consider language modeling: predicting upcoming words from prior\n words. In this task we are given the same window of N input tokens, but our task\n now is to predict the next token that should follow the window. We‚Äôll sketch a\n simple feedforward neural language model, drawing on an algorithm first introduced\n by Bengio et al. (2003). The feedforward language model introduces many of the\n important concepts of large language modeling that we will return to in Chapter 7\n and Chapter 8.\n Neural language models have many advantages over the n-gram language models of Chapter 3. Neural language models can handle much longer histories, can\n16 C HAPTER 6 ‚Ä¢ N EURAL N ETWORKS\n\n p(+) p(-) p(neut) Output probabilities\n\n y^ 1 y^2 y^3 y [1‚®â3] Output layer softmax\n\n [dh‚®â3] weights\n U\n h1 h2 h3 ‚Ä¶ h\n dh\n h [1‚®âdh] Hidden layer\n\n W [d‚®âdh] weights\n x [1‚®âd] Input layer\n pooled embedding\n + pooling\n embedding for ‚Äúdessert‚Äù\n embedding for ‚Äúwas‚Äù N‚®âd embeddings\n embedding for ‚Äúgreat‚Äù\n\n E E E |V|‚®âd E matrix\n shared across words\n 1 3 |V|\n 1 524 |V|\n 00 1 00 1 902 |V| N‚®â|V| one-hot vectors\n 00 0 1 0 0\n 00 0 1 0 0\n ‚Äúdessert‚Äù = V3 ‚Äúwas‚Äù = V524 ‚Äúgreat‚Äù = V902\n\n dessert was great Input words\ntimestep the network computes a d-dimensional embedding for each context word (by multiplying a one-hot\nvector by the embedding matrix E), and pools the resulting N embeddings to get a single embedding that\nrepresents the context window as the layer e.\n\n generalize better over contexts of similar words, and are far more accurate at wordprediction. On the other hand, neural net language models are slower, more complex, need vast amounts of energy to train, and are less interpretable than n-gram\n models, so for some smaller tasks an n-gram language model is still the right tool.\n A feedforward neural language model is a feedforward network that takes as\n input at time t a representation of some number of previous words (wt‚àí1 , wt‚àí2 , etc.)\n and outputs a probability distribution over possible next words. Thus‚Äîlike the ngram LM‚Äîthe feedforward neural LM approximates the probability of a word given\n the entire prior context P(wt |w1:t‚àí1 ) by approximating based on the N ‚àí 1 previous\n words:\n\n P(wt |w1 , . . . , wt‚àí1 ) ‚âà P(wt |wt‚àíN+1 , . . . , wt‚àí1 ) (6.23)\n\n In the following examples we‚Äôll use a 4-gram example, so we‚Äôll show a neural net to\n estimate the probability P(wt = i|wt‚àí3 , wt‚àí2 , wt‚àí1 ).\n Neural language models represent words in this prior context by their embeddings, rather than just by their word identity as used in n-gram language models.\n Using embeddings allows neural language models to generalize better to unseen\n data. For example, suppose we‚Äôve seen this sentence in training:\n I have to make sure that the cat gets fed.\n 6.5 ‚Ä¢ E MBEDDINGS AS THE INPUT TO NEURAL NET CLASSIFIERS 17\n\nbut have never seen the words ‚Äúgets fed‚Äù after the word ‚Äúdog‚Äù. Our test set has the\nprefix ‚ÄúI forgot to make sure that the dog gets‚Äù. What‚Äôs the next word? An n-gram\nlanguage model will predict ‚Äúfed‚Äù after ‚Äúthat the cat gets‚Äù, but not after ‚Äúthat the dog\ngets‚Äù. But a neural LM, knowing that ‚Äúcat‚Äù and ‚Äúdog‚Äù have similar embeddings, will\nbe able to generalize from the ‚Äúcat‚Äù context to assign a high enough probability to\n‚Äúfed‚Äù even after seeing ‚Äúdog‚Äù.\n\n p(wt=aardvark|wt-3,wt-2,wt-1) p(wt=do|‚Ä¶) p(wt=fish|‚Ä¶) p(wt=zebra|‚Ä¶)\n\n output layer y y^1 ‚Ä¶ ^y\n 34 ‚Ä¶ ^y ‚Ä¶ ^y\n ^\n 35102 ‚Ä¶ y|V| 1‚®â|V|\n softmax\n U dh‚®â|V|\n\n hidden layer h h1 h2 h3 ‚Ä¶ hdh 1‚®âdh\n\n W Nd‚®âdh\n\n embedding layer e 1‚®âNd\n E is shared\n across words\n E E E |V|‚®âd\n 1 35 |V| 1 992 |V| 1 451 |V|\n Input layer N‚®â|V|\n 00 1 00 00 0 1 0 0 00 0 1 0 0\n one-hot\n vectors ‚Äúfor‚Äù = V35 ‚Äúall‚Äù = V992 ‚Äúthe‚Äù = V451\n\n ...\n\n ‚Ä¶ and thanks for all the ? ‚Ä¶\n\n wt-3 wt-2 wt-1 wt\n\nt the network computes a d-dimensional embedding for each of the N = 3 context tokens (by\nmultiplying a one-hot vector by the embedding matrix E), and concatenates the three to get\nthe embedding e. This embedding e is multiplied by weight matrix W and then an activation\nfunction is applied element-wise to produce the hidden layer h, which is then multiplied by\nanother weight matrix U. A softmax layer predicts at each output node i the probability that\nthe next word wt will be vocabulary word Vi . We show the context window size N as 3 just to\nfit on the page, but in practice language modeling requires a much longer context.\n\n This prediction task requires an output vector that expresses |V | probabilities:\none probability value for each possible next token. We might have a vocabulary\nbetween 60,000 and 300,000 tokens, so the output vector for the task of language\nmodeling is much longer than 3. Another difference for language modeling is that\ninstead of pooling the embeddings of the N input tokens to create a single embedding, we concatenate the inputs into one very long input vector. To predict the next\ntoken, it helps to know each of the preceding tokens and what order they were in.\n Fig. 6.14 shows the language modeling task, sketched with a very short context\nwindow of N = 3 just to fit on the page. These 3 embedding vectors are concatenated\nto produce e, the embedding layer. This is multiplied by a weight matrix W to produce a hidden layer, and another weight matrix U to produce an output layer whose\nsoftmax gives a probability distribution over words. For example y42 , the value of\noutput node 42, is the probability of the next word wt being V42 , the vocabulary word\nwith index 42 (which is the word ‚Äòfish‚Äô in our example).\n The equations for a simple feedforward neural language model with a window\n18 C HAPTER 6 ‚Ä¢ N EURAL N ETWORKS\n\n size of 3, given one-hot input vectors for each input context word, are:\n\n e = [Ext‚àí3 ; Ext‚àí2 ; Ext‚àí1 ]\n h = œÉ (We + b)\n z = Uh\n yÃÇ = softmax(z) (6.24)\n\n Note that we we use semicolons to mean concatenation of vectors, so we form the\n embedding layer e by concatenating the 3 embeddings for the three context vectors.\n We‚Äôll return to this idea of using neural networks to do language modeling in\n Chapter 7 and Chapter 8 when we introduce transformer language models.\n\n A feedforward neural net is an instance of supervised machine learning in which we\n know the correct output y for each observation x. What the system produces, via\n Eq. 6.13, is yÃÇ, the system‚Äôs estimate of the true y. The goal of the training procedure\n is to learn parameters W[i] and b[i] for each layer i that make yÃÇ for each training\n observation as close as possible to the true y.\n In general, we do all this by drawing on the methods we introduced in Chapter 4\n for logistic regression, so the reader should be comfortable with that chapter before\n proceeding. We‚Äôll explore the algorithm on simple generic networks rather than\n networks designed for sentiment or language modeling.\n First, we‚Äôll need a loss function that models the distance between the system\n output and the gold output, and it‚Äôs common to use the loss function used for logistic\n regression, the cross-entropy loss.\n Second, to find the parameters that minimize this loss function, we‚Äôll use the\n gradient descent optimization algorithm introduced in Chapter 4.\n Third, gradient descent requires knowing the gradient of the loss function, the\n vector that contains the partial derivative of the loss function with respect to each\n of the parameters. In logistic regression, for each observation we could directly\n compute the derivative of the loss function with respect to an individual w or b. But\n for neural networks, with millions of parameters in many layers, it‚Äôs much harder to\n see how to compute the partial derivative of some weight in layer 1 when the loss\n is attached to some much later layer. How do we partial out the loss over all those\n intermediate layers? The answer is the algorithm called error backpropagation or\n backward differentiation.\n\n 6.6.1 Loss function\n cross-entropy The cross-entropy loss that is used in neural networks is the same one we saw for\n loss\n logistic regression. If the neural network is being used as a binary classifier, with\n the sigmoid at the final layer, the loss function is the same logistic regression loss\n we saw in Eq. ??:\n\n LCE (yÃÇ, y) = ‚àí log p(y|x) = ‚àí [y log yÃÇ + (1 ‚àí y) log(1 ‚àí yÃÇ)] (6.25)\n\n If we are using the network to classify into 3 or more classes, the loss function is\n exactly the same as the loss for multinomial regression that we saw in Chapter 4 on\n 6.6 ‚Ä¢ T RAINING N EURAL N ETS 19\n\n page ??. Let‚Äôs briefly summarize the explanation here for convenience. First, when\n we have more than 2 classes we‚Äôll need to represent both y and yÃÇ as vectors. Let‚Äôs\n assume we‚Äôre doing hard classification, where only one class is the correct one.\n The true label y is then a vector with K elements, each corresponding to a class,\n with yc = 1 if the correct class is c, with all other elements of y being 0. Recall that\n a vector like this, with one value equal to 1 and the rest 0, is called a one-hot vector.\n And our classifier will produce an estimate vector with K elements yÃÇ, each element\n yÃÇk of which represents the estimated probability p(yk = 1|x).\n The loss function for a single example x is the negative sum of the logs of the K\n output classes, each weighted by their probability yk :\n K\n X\n LCE (yÃÇ, y) = ‚àí yk log yÃÇk (6.26)\n k=1\n\n We can simplify this equation further; let‚Äôs first rewrite the equation using the function 1{} which evaluates to 1 if the condition in the brackets is true and to 0 otherwise. This makes it more obvious that the terms in the sum in Eq. 6.26 will be 0\n except for the term corresponding to the true class for which yk = 1:\n K\n 1{yk = 1} log yÃÇk\n X\n LCE (yÃÇ, y) = ‚àí\n k=1\n\n In other words, the cross-entropy loss is simply the negative log of the output probability corresponding to the correct class, and we therefore also call this the negative\n negative log log likelihood loss:\nlikelihood loss\n\n LCE (yÃÇ, y) = ‚àí log yÃÇc (where c is the correct class) (6.27)\n\n Plugging in the softmax formula from Eq. 6.9, and with K the number of classes:\n\n exp(zc )\n LCE (yÃÇ, y) = ‚àí log PK (where c is the correct class) (6.28)\n j=1 exp(z j )\n\n Let‚Äôs think about the negative log probability as a loss function. A perfect classifier would assign the correct class i probability 1 and all the incorrect classes probability 0. That means the higher p(yÃÇi ) (the closer it is to 1), the better the classifier;\n p(yÃÇi ) is (the closer it is to 0), the worse the classifier. The negative log of this probability is a beautiful loss metric since it goes from 0 (negative log of 1, no loss)\n to infinity (negative log of 0, infinite loss). This loss function also insures that as\n probability of the correct answer is maximized, the probability of all the incorrect\n answers is minimized; since they all sum to one, any increase in the probability of\n the correct answer is coming at the expense of the incorrect answers.\n The number K of classes of the output vector yÃÇ can be small or large. Perhaps\n our task is 3-way sentiment, and then the classes might be positive, negative, and\n neutral. Or if our task is deciding the part of speech of a word (i.e., whether it is a\n noun or verb or adjective, etc.), then K is set of possible parts of speech in our tagset\n (of which there are 17 in the tagset we will define in Chapter 17). And if our task\n is language modeling, and our classifier is trying to predict which word is next, then\n our set of classes is the set of words, which might be 50,000 or 100,000.\n20 C HAPTER 6 ‚Ä¢ N EURAL N ETWORKS\n\n 6.6.2 Computing the Gradient\n How do we compute the gradient of this loss function? Computing the gradient\n requires the partial derivative of the loss function with respect to each parameter.\n For a network with one weight layer and sigmoid output (which is what logistic\n regression is), we could simply use the derivative of the loss that we used for logistic\n regression in Eq. 6.29 (and derived in Section ??):\n ‚àÇ LCE (yÃÇ, y)\n = (yÃÇ ‚àí y) x j\n ‚àÇwj\n = (œÉ (w ¬∑ x + b) ‚àí y) x j (6.29)\n\n Or for a network with one weight layer and softmax output (=multinomial logistic\n regression), we could use the derivative of the softmax loss from Eq. ??, shown for\n a particular weight wk and input xi\n ‚àÇ LCE (yÃÇ, y)\n = ‚àí(yk ‚àí yÃÇk )xi\n ‚àÇ wk,i\n = ‚àí(yk ‚àí p(yk = 1|x))xi\n !\n exp (wk ¬∑ x + bk )\n = ‚àí yk ‚àí PK xi (6.30)\n j=1 exp (w j ¬∑ x + b j )\n\n But these derivatives only give correct updates for one weight layer: the last one!\n For deep networks, computing the gradients for each weight is much more complex,\n since we are computing the derivative with respect to weight parameters that appear\n all the way back in the very early layers of the network, even though the loss is\n computed only at the very end of the network.\n The solution to computing this gradient is an algorithm called error backproperror backpropagation agation or backprop (Rumelhart et al., 1986). While backprop was invented specially for neural networks, it turns out to be the same as a more general procedure\n called backward differentiation, which depends on the notion of computation\n graphs. Let‚Äôs see how that works in the next subsection.\n\n 6.6.3 Computation Graphs\n A computation graph is a representation of the process of computing a mathematical\n expression, in which the computation is broken down into separate operations, each\n of which is modeled as a node in a graph.\n Consider computing the function L(a, b, c) = c(a + 2b). If we make each of the\n component addition and multiplication operations explicit, and add names (d and e)\n for the intermediate outputs, the resulting series of computations is:\n d = 2‚àób\n e = a+d\n L = c‚àóe\n We can now represent this as a graph, with nodes for each operation, and directed edges showing the outputs from each operation as the inputs to the next, as\n in Fig. 6.15. The simplest use of computation graphs is to compute the value of\n the function with some given inputs. In the figure, we‚Äôve assumed the inputs a = 3,\n b = 1, c = ‚àí2, and we‚Äôve shown the result of the forward pass to compute the result L(3, 1, ‚àí2) = ‚àí10. In the forward pass of a computation graph, we apply each\n 6.6 ‚Ä¢ T RAINING N EURAL N ETS 21\n\n operation left to right, passing the outputs of each computation as the input to the\n next node.\n\n forward pass\n\n a a=3\n e=a+d e=5\n d=2\n b=1\n b d = 2b L=ce L=-10\n c=-2\n c\n nodes a = 3, b = 1, c = ‚àí2, showing the forward pass computation of L.\n\n 6.6.4 Backward differentiation on computation graphs\n The importance of the computation graph comes from the backward pass, which\n is used to compute the derivatives that we‚Äôll need for the weight update. In this\n example our goal is to compute the derivative of the output function L with respect\n to each of the input variables, i.e., ‚àÇ‚àÇ La , ‚àÇ‚àÇ Lb , and ‚àÇ‚àÇ Lc . The derivative ‚àÇ‚àÇ La tells us how\n much a small change in a affects L.\nchain rule Backwards differentiation makes use of the chain rule in calculus, so let‚Äôs remind ourselves of that. Suppose we are computing the derivative of a composite\n function f (x) = u(v(x)). The derivative of f (x) is the derivative of u(x) with respect\n to v(x) times the derivative of v(x) with respect to x:\n df du dv\n = ¬∑ (6.31)\n dx dv dx\n The chain rule extends to more than two functions. If computing the derivative of a\n composite function f (x) = u(v(w(x))), the derivative of f (x) is:\n df du dv dw\n = ¬∑ ¬∑ (6.32)\n dx dv dw dx\n The intuition of backward differentiation is to pass gradients back from the final\n node to all the nodes in the graph. Fig. 6.16 shows part of the backward computation\n at one node e. Each node takes an upstream gradient that is passed in from its parent\n node to the right, and for each of its inputs computes a local gradient (the gradient\n of its output with respect to its input), and uses the chain rule to multiply these two\n to compute a downstream gradient to be passed on to the next earlier node.\n Let‚Äôs now compute the 3 derivatives we need. Since in the computation graph\n L = ce, we can directly compute the derivative ‚àÇ‚àÇ Lc :\n\n ‚àÇL\n =e (6.33)\n ‚àÇc\n For the other two, we‚Äôll need to use the chain rule:\n ‚àÇL ‚àÇL ‚àÇe\n ‚àÇa ‚àÇe ‚àÇa\n ‚àÇL ‚àÇL ‚àÇe ‚àÇd\n = (6.34)\n ‚àÇb ‚àÇe ‚àÇd ‚àÇb\n22 C HAPTER 6 ‚Ä¢ N EURAL N ETWORKS\n\n d e\n d e L\n ‚àÇL = ‚àÇL ‚àÇe ‚àÇe ‚àÇL\n ‚àÇd ‚àÇe ‚àÇd ‚àÇd ‚àÇe\n downstream local upstream\n gradient gradient gradient\n\n gradient (the gradient of its output with respect to its input), and uses the chain rule to compute\n a downstream gradient to be passed on to a prior node. A node may have multiple local\n gradients if it has multiple inputs.\n\n Eq. 6.34 and Eq. 6.33 thus require five intermediate derivatives: ‚àÇ‚àÇ Le , ‚àÇ‚àÇ Lc , ‚àÇ‚àÇ ae , ‚àÇ‚àÇ de , and\n ‚àÇd\n ‚àÇ b , which are as follows (making use of the fact that the derivative of a sum is the\n sum of the derivatives):\n\n ‚àÇL ‚àÇL\n L = ce : = c, =e\n ‚àÇe ‚àÇc\n ‚àÇe ‚àÇe\n e = a+d : = 1, =1\n ‚àÇa ‚àÇd\n ‚àÇd\n d = 2b : =2\n ‚àÇb\n In the backward pass, we compute each of these partials along each edge of the\n graph from right to left, using the chain rule just as we did above. Thus we begin by\n computing the downstream gradients from node L, which are ‚àÇ‚àÇ Le and ‚àÇ‚àÇ Lc . For node e,\n we then multiply this upstream gradient ‚àÇ‚àÇ Le by the local gradient (the gradient of the\n output with respect to the input), ‚àÇ‚àÇ de to get the output we send back to node d: ‚àÇ‚àÇ Ld .\n And so on, until we have annotated the graph all the way to all the input variables.\n The forward pass conveniently already will have computed the values of the forward\n intermediate variables we need (like d and e) to compute these derivatives. Fig. 6.17\n shows the backward pass.\n\n a=3\n a\n ‚àÇL = ‚àÇL ‚àÇe =-2\n ‚àÇa ‚àÇe ‚àÇa e=5\n e=d+a\n d=2\n b=1 ‚àÇe ‚àÇe\n =1 =1 ‚àÇL\n b d = 2b ‚àÇL = ‚àÇL ‚àÇe =-2 ‚àÇa ‚àÇd =-2 L=-10\n ‚àÇe L=ce\n ‚àÇL = ‚àÇL ‚àÇd =-4 ‚àÇd ‚àÇd ‚àÇe ‚àÇd\n =2 ‚àÇL\n ‚àÇb ‚àÇd ‚àÇb ‚àÇb =-2\n ‚àÇe\n c=-2 ‚àÇL\n =5\n ‚àÇc\n ‚àÇL =5 backward pass\n c ‚àÇc\n 6.6 ‚Ä¢ T RAINING N EURAL N ETS 23\n\n Backward differentiation for a neural network\n Of course computation graphs for real neural networks are much more complex.\n Fig. 6.18 shows a sample computation graph for a 2-layer neural network with n0 =\n 2, n1 = 2, and n2 = 1, assuming binary classification and hence using a sigmoid\n output unit for simplicity. The function that the computation graph is computing is:\n\n z[1] = W[1] x + b[1]\n a[1] = ReLU(z[1] )\n z[2] = W[2] a[1] + b[2]\n a[2] = œÉ (z[2] )\n yÃÇ = a[2] (6.35)\n\n For the backward pass we‚Äôll also need to compute the loss L. The loss function\n for binary sigmoid output from Eq. 6.25 is\n\n LCE (yÃÇ, y) = ‚àí [y log yÃÇ + (1 ‚àí y) log(1 ‚àí yÃÇ)] (6.36)\n\n Our output yÃÇ = a[2] , so we can rephrase this as\n h i\n LCE (a[2] , y) = ‚àí y log a[2] + (1 ‚àí y) log(1 ‚àí a[2] ) (6.37)\n\n [1]\n w11\n w[1]\n 12 z[1] = a1[1] =\n * 1\n + ReLU\n x1\n b[1]\n 1 z[2] =\n w[2] a[2] = œÉ L (a[2],y)\n x2 11 +\n w[1] z[1] a[1] w[2]\n 21 * 2 = 2 = 12\n + ReLU\n w[1] b[2]\n 22 1\n b[1]\n\nand 2 hidden units. We‚Äôve adjusted the notation a bit to avoid long equations in the nodes by just mentioning\n [1]\nthe function that is being computed, and the resulting variable name. Thus the * to the right of node w11 means\n [1]\nthat w11 is to be multiplied by x1 , and the node z[1] = + means that the value of z[1] is computed by summing\n [1]\nthe three nodes that feed into it (the two products, and the bias term bi ).\n\n The weights that need updating (those for which we need to know the partial\n derivative of the loss function) are shown in teal. In order to do the backward pass,\n we‚Äôll need to know the derivatives of all the functions in the graph. We already saw\n in Section ?? the derivative of the sigmoid œÉ :\n dœÉ (z)\n = œÉ (z)(1 ‚àí œÉ (z)) (6.38)\n dz\n24 C HAPTER 6 ‚Ä¢ N EURAL N ETWORKS\n\n We‚Äôll also need the derivatives of each of the other activation functions. The\n derivative of tanh is:\n d tanh(z)\n = 1 ‚àí tanh2 (z) (6.39)\n dz\n The derivative of the ReLU is2\n \u001a\n d ReLU(z) 0 f or z < 0\n = (6.40)\n dz 1 f or z ‚â• 0\n\n We‚Äôll give the start of the computation, computing the derivative of the loss function\n L with respect to z, or ‚àÇ‚àÇLz (and leaving the rest of the computation as an exercise for\n the reader). By the chain rule:\n\n ‚àÇL ‚àÇ L ‚àÇ a[2]\n = [2] (6.41)\n ‚àÇz ‚àÇa ‚àÇz\n\n So let‚Äôs first compute ‚àÇ‚àÇaL[2] , taking the derivative of Eq. 6.37, repeated here:\n h i\n LCE (a[2] , y) = ‚àí y log a[2] + (1 ‚àí y) log(1 ‚àí a[2] )\n ! !\n ‚àÇL ‚àÇ log(a[2] ) ‚àÇ log(1 ‚àí a[2] )\n = ‚àí y + (1 ‚àí y)\n ‚àÇ a[2] ‚àÇ a[2] ‚àÇ a[2]\n \u0012\u0012 \u0013 \u0013\n 1 1\n = ‚àí y [2] + (1 ‚àí y) (‚àí1)\n a 1 ‚àí a[2]\n \u0012 \u0013\n y y‚àí1\n = ‚àí [2] + (6.42)\n a 1 ‚àí a[2]\n Next, by the derivative of the sigmoid:\n\n ‚àÇ a[2]\n = a[2] (1 ‚àí a[2] )\n ‚àÇz\n Finally, we can use the chain rule:\n\n ‚àÇL ‚àÇ L ‚àÇ a[2]\n ‚àÇz ‚àÇ a\u0012[2] ‚àÇ z \u0013\n y y‚àí1\n = ‚àí [2] + a[2] (1 ‚àí a[2] )\n a 1 ‚àí a[2]\n = a[2] ‚àí y (6.43)\n\n Continuing the backward computation of the gradients (next by passing the gra-\n [2]\n dients over b1 and the two product nodes, and so on, back to all the teal nodes), is\n left as an exercise for the reader.\n\n 6.6.5 More details on learning\n Optimization in neural networks is a non-convex optimization problem, more complex than for logistic regression, and for that and other reasons there are many best\n practices for successful learning.\n 2 The derivative is actually undefined at the point z = 0, but by convention we treat it as 1.\n 6.7 ‚Ä¢ S UMMARY 25\n\n For logistic regression we can initialize gradient descent with all the weights and\n biases having the value 0. In neural networks, by contrast, we need to initialize the\n weights with small random numbers. It‚Äôs also helpful to normalize the input values\n to have 0 mean and unit variance.\n Various forms of regularization are used to prevent overfitting. One of the most\n dropout important is dropout: randomly dropping some units and their connections from\n the network during training (Hinton et al. 2012, Srivastava et al. 2014). At each\n iteration of training (whenever we update parameters, i.e. each mini-batch if we are\n using mini-batch gradient descent), we repeatedly choose a probability p and for\n each unit we replace its output with zero with probability p (and renormalize the\n rest of the outputs from that layer).\nhyperparameter Tuning of hyperparameters is also important. The parameters of a neural network are the weights W and biases b; those are learned by gradient descent. The\n hyperparameters are things that are chosen by the algorithm designer; optimal values are tuned on a devset rather than by gradient descent learning on the training\n set. Hyperparameters include the learning rate Œ∑, the mini-batch size, the model\n architecture (the number of layers, the number of hidden nodes per layer, the choice\n of activation functions), how to regularize, and so on. Gradient descent itself also\n has many architectural variants such as Adam (Kingma and Ba, 2015).\n Finally, most modern neural networks are built using computation graph formalisms that make it easy and natural to do gradient computation and parallelization\n on vector-based GPUs (Graphic Processing Units). PyTorch (Paszke et al., 2017)\n and TensorFlow (Abadi et al., 2015) are two of the most popular. The interested\n reader should consult a neural network textbook for further details; some suggestions are at the end of the chapter.\n\n ‚Ä¢ Neural networks are built out of neural units, originally inspired by biological\n neurons but now simply an abstract computational device.\n ‚Ä¢ Each neural unit multiplies input values by a weight vector, adds a bias, and\n then applies a non-linear activation function like sigmoid, tanh, or rectified\n linear unit.\n ‚Ä¢ In a fully-connected, feedforward network, each unit in layer i is connected\n to each unit in layer i + 1, and there are no cycles.\n ‚Ä¢ The power of neural networks comes from the ability of early layers to learn\n representations that can be utilized by later layers in the network.\n ‚Ä¢ Neural networks are trained by optimization algorithms like gradient descent.\n ‚Ä¢ Error backpropagation, backward differentiation on a computation graph,\n is used to compute the gradients of the loss function for a network.\n ‚Ä¢ Neural language models use a neural network as a probabilistic classifier, to\n compute the probability of the next word given the previous n words.\n ‚Ä¢ Neural language models can use pretrained embeddings, or can learn embeddings from scratch in the process of language modeling.\n26 C HAPTER 6 ‚Ä¢ N EURAL N ETWORKS\n\nHistorical Notes\n The origins of neural networks lie in the 1940s McCulloch-Pitts neuron (McCulloch and Pitts, 1943), a simplified model of the biological neuron as a kind of computing element that could be described in terms of propositional logic. By the late\n 1950s and early 1960s, a number of labs (including Frank Rosenblatt at Cornell and\n Bernard Widrow at Stanford) developed research into neural networks; this phase\n saw the development of the perceptron (Rosenblatt, 1958), and the transformation\n of the threshold into a bias, a notation we still use (Widrow and Hoff, 1960).\n The field of neural networks declined after it was shown that a single perceptron\n unit was unable to model functions as simple as XOR (Minsky and Papert, 1969).\n While some small amount of work continued during the next two decades, a major\n revival for the field didn‚Äôt come until the 1980s, when practical tools for building\n deeper networks like error backpropagation became widespread (Rumelhart et al.,\n 1986). During the 1980s a wide variety of neural network and related architectures were developed, particularly for applications in psychology and cognitive science (Rumelhart and McClelland 1986b, McClelland and Elman 1986, Rumelhart\n connectionist and McClelland 1986a, Elman 1990), for which the term connectionist or parallel distributed processing was often used (Feldman and Ballard 1982, Smolensky\n 1988). Many of the principles and techniques developed in this period are foundational to modern work, including the ideas of distributed representations (Hinton,\n 1986), recurrent networks (Elman, 1990), and the use of tensors for compositionality\n (Smolensky, 1990).\n By the 1990s larger neural networks began to be applied to many practical language processing tasks as well, like handwriting recognition (LeCun et al. 1989) and\n speech recognition (Morgan and Bourlard 1990). By the early 2000s, improvements\n in computer hardware and advances in optimization and training techniques made it\n possible to train even larger and deeper networks, leading to the modern term deep\n learning (Hinton et al. 2006, Bengio et al. 2007). We cover more related history in\n Chapter 13 and Chapter 15.\n There are a number of excellent books on neural networks, including Goodfellow\n et al. (2016) and Nielsen (2015).\n Historical Notes 27\n\nAbadi, M., A. Agarwal, P. Barham, E. Brevdo, Z. Chen, Rumelhart, D. E. and J. L. McClelland. 1986a. On learning\n C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, the past tense of English verbs. In D. E. Rumelhart and\n S. Ghemawat, I. Goodfellow, A. Harp, G. Irving, M. Is- J. L. McClelland, eds, Parallel Distributed Processing,\n ard, Y. Jia, R. Jozefowicz, L. Kaiser, M. Kudlur, J. Leven- volume 2, 216‚Äì271. MIT Press.\n berg, D. ManeÃÅ, R. Monga, S. Moore, D. Murray, C. Olah, Rumelhart, D. E. and J. L. McClelland, eds. 1986b. Parallel\n M. Schuster, J. Shlens, B. Steiner, I. Sutskever, K. Tal- Distributed Processing. MIT Press.\n war, P. Tucker, V. Vanhoucke, V. Vasudevan, F. VieÃÅgas,\n O. Vinyals, P. Warden, M. Wattenberg, M. Wicke, Y. Yu, Russell, S. and P. Norvig. 2002. Artificial Intelligence: A\n and X. Zheng. 2015. TensorFlow: Large-scale machine Modern Approach, 2nd edition. Prentice Hall.\n learning on heterogeneous systems. Software available Smolensky, P. 1988. On the proper treatment of connectionfrom tensorflow.org. ism. Behavioral and brain sciences, 11(1):1‚Äì23.\nBengio, Y., R. Ducharme, P. Vincent, and C. Jauvin. 2003. Smolensky, P. 1990. Tensor product variable binding and\n A neural probabilistic language model. JMLR, 3:1137‚Äì the representation of symbolic structures in connectionist\n 1155. systems. Artificial intelligence, 46(1-2):159‚Äì216.\nBengio, Y., P. Lamblin, D. Popovici, and H. Larochelle. Srivastava, N., G. E. Hinton, A. Krizhevsky, I. Sutskever,\n 2007. Greedy layer-wise training of deep networks. and R. R. Salakhutdinov. 2014. Dropout: a simple\n NeurIPS. way to prevent neural networks from overfitting. JMLR,\nElman, J. L. 1990. Finding structure in time. Cognitive sci- 15(1):1929‚Äì1958.\n ence, 14(2):179‚Äì211. Widrow, B. and M. E. Hoff. 1960. Adaptive switching cir-\nFeldman, J. A. and D. H. Ballard. 1982. Connectionist mod- cuits. IRE WESCON Convention Record, volume 4.\n els and their properties. Cognitive Science, 6:205‚Äì254.\nGoodfellow, I., Y. Bengio, and A. Courville. 2016. Deep\n Learning. MIT Press.\nHinton, G. E. 1986. Learning distributed representations of\n concepts. COGSCI.\nHinton, G. E., S. Osindero, and Y.-W. Teh. 2006. A fast\n learning algorithm for deep belief nets. Neural computation, 18(7):1527‚Äì1554.\nHinton, G. E., N. Srivastava, A. Krizhevsky, I. Sutskever, and\n R. R. Salakhutdinov. 2012. Improving neural networks\n by preventing co-adaptation of feature detectors. ArXiv\n preprint arXiv:1207.0580.\nKingma, D. and J. Ba. 2015. Adam: A method for stochastic\n optimization. ICLR 2015.\nLeCun, Y., B. Boser, J. S. Denker, D. Henderson, R. E.\n Howard, W. Hubbard, and L. D. Jackel. 1989. Backpropagation applied to handwritten zip code recognition. Neural computation, 1(4):541‚Äì551.\nMcClelland, J. L. and J. L. Elman. 1986. The TRACE model\n of speech perception. Cognitive Psychology, 18:1‚Äì86.\nMcCulloch, W. S. and W. Pitts. 1943. A logical calculus of\n ideas immanent in nervous activity. Bulletin of Mathematical Biophysics, 5:115‚Äì133.\nMinsky, M. and S. Papert. 1969. Perceptrons. MIT Press.\nMorgan, N. and H. Bourlard. 1990. Continuous speech\n recognition using multilayer perceptrons with hidden\n markov models. ICASSP.\nNielsen, M. A. 2015. Neural networks and Deep learning.\n Determination Press USA.\nPaszke, A., S. Gross, S. Chintala, G. Chanan, E. Yang, Z. De-\nVito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer.\n 2017. Automatic differentiation in pytorch. NIPS-W.\nRosenblatt, F. 1958. The perceptron: A probabilistic model\n for information storage and organization in the brain. Psychological review, 65(6):386‚Äì408.\nRumelhart, D. E., G. E. Hinton, and R. J. Williams. 1986.\n Learning internal representations by error propagation. In\n D. E. Rumelhart and J. L. McClelland, eds, Parallel Distributed Processing, volume 2, 318‚Äì362. MIT Press.\n",
    "file_path": "/Users/colinsidberry/nlp-textbook-rag/data/extracted_text/neural-networks.txt",
    "file_size_kb": 66.44
  },
  {
    "id": "b3cf0ac479709afb",
    "source": "nlp_textbook",
    "chapter": "Large Language Models",
    "filename": "7.txt",
    "content": "rights reserved. Draft of August 24, 2025.\n\nCHAPTER\n\n Large Language Models\n ‚ÄúHow much do we know at any time? Much more, or so I believe, than we\n know we know.‚Äù\n Agatha Christie, The Moving Finger\n\n The literature of the fantastic abounds in inanimate objects magically endowed with\n the gift of speech. From Ovid‚Äôs statue of Pygmalion to Mary Shelley‚Äôs story about\n Frankenstein, we continually reinvent stories about\n creating something and then having a chat with it.\n Legend has it that after finishing his sculpture Moses,\n Michelangelo thought it so lifelike that he tapped it\n on the knee and commanded it to speak. Perhaps\n this shouldn‚Äôt be surprising. Language is the mark\n of humanity and sentience. conversation is the most\n fundamental arena of language, the first kind of language we learn as children, and the kind we engage in\n constantly, whether we are teaching or learning, ordering lunch, or talking with our families or friends.\n This chapter introduces the Large Language\n Model, or LLM, a computational agent that can interact conversationally with people. The fact that LLMs are designed for interaction\n with people has strong implications for their design and use.\n Many of these implications already became clear in a computational system from\n 60 years ago, ELIZA (Weizenbaum, 1966). ELIZA, designed to simulate a Rogerian\n psychologist, illustrates a number of important issues with chatbots. For example\n people became deeply emotionally involved and conducted very personal conversations, even to the extent of asking Weizenbaum to leave the room while they were\n typing. These issues of emotional engagement and privacy mean we need to think\n carefully about how we deploy language models and consider their effect on the\n people who are interacting with them.\n In this chapter we begin by introducing the computational principles of LLMs;\n we‚Äôll discuss their implementation in the transformer architecture in the following\n chapter. The central new idea that makes LLMs possible is the idea of pretraining,\n so let‚Äôs begin by thinking about the idea of learning from text, the basic way that\n LLMs are trained.\n We know that fluent speakers of a language bring an enormous amount of knowledge to bear during comprehension and production. This knowledge is embodied in\n many forms, perhaps most obviously in the vocabulary, the rich representations we\n have of words and their meanings and usage. This makes the vocabulary a useful\n lens to explore the acquisition of knowledge from text, by both people and machines.\n Estimates of the size of adult vocabularies vary widely both within and across\n languages. For example, estimates of the vocabulary size of young adult speakers of\n American English range from 30,000 to 100,000 depending on the resources used\n2 C HAPTER 7 ‚Ä¢ L ARGE L ANGUAGE M ODELS\n\n to make the estimate and the definition of what it means to know a word. A simple consequence of these facts is that children have to learn about 7 to 10 words a\n day, every single day, to arrive at observed vocabulary levels by the time they are 20\n years of age. And indeed empirical estimates of vocabulary growth in late elementary through high school are consistent with this rate. How do children achieve this\n rate of vocabulary growth? Research suggests that the bulk of this knowledge acquisition happens as a by-product of reading. Reading is a process of rich contextual\n processing; we don‚Äôt learn words one at a time in isolation. In fact, at some points\n during learning the rate of vocabulary growth exceeds the rate at which new words\n are appearing to the learner! That suggests that every time we read a word, we are\n also strengthening our understanding of other words that are associated with it.\n Such facts are consistent with the distributional hypothesis of Chapter 5, which\n proposes that some aspects of meaning can be learned solely from the texts we encounter over our lives, based on the complex association of words with the words\n they co-occur with (and with the words that those words occur with). The distributional hypothesis suggests both that we can acquire remarkable amounts of knowledge from text, and that this knowledge can be brought to bear long after its initial\n acquisition. Of course, grounding from real-world interaction or other modalities\n can help build even more powerful models, but even text alone is remarkably useful.\n What made the modern NLP revolution possible is that large language models\n can learn all this knowledge of language, context, and the world simply by being\n taught to predict the next word, again and again, based on context, in a (very) large\n corpus of text. In this chapter and the next we formalize this idea that we‚Äôll call\n pretraining pretraining‚Äîlearning knowledge about language and the world from iteratively\n predicting tokens in vast amounts of text‚Äîand call the resulting pretrained models\n large language models. Large language models exhibit remarkable performance on\n natural language tasks because of the knowledge they learn in pretraining.\n What can language models learn from word prediction? Consider the examples\n below. What kinds of knowledge do you think the model might pick up from learning to predict what word fills the underbar (the correct answer is shown in blue)?\n Think about this for each example before you read ahead to the next paragraph:.\n With roses, dahlias, and peonies, I was surrounded by flowers\n The room wasn‚Äôt just big it was enormous\n The square root of 4 is 2\n The author of ‚ÄúA Room of One‚Äôs Own‚Äù is Virginia Woolf\n The professor said that he\n From the first sentence a model can learn ontological facts like that roses and\n dahlias and peonies are all kinds of flowers. From the second, a model could learn\n that ‚Äúenormous‚Äù means something on the same scale as big but further along on\n the scale. From the third sentence, the system could learn math, while from the\n 4th sentence facts about the world and historical authors. Finally, the last sentence,\n if a model was exposed to such sentences repeatedly, it might learn to associate\n professors only with male pronouns, or other kinds of associations that might cause\n models to act unfairly to different people.\n What is a large language model? As we saw back in Chapter 3, a language\n model is simply a computational system that can predict the next word from previous\n words. That is, given a context or prefix of words, a language model assigns a\n probability distribution over the possible next words. Fig. 7.1 sketches this idea.\n Of course we‚Äôve already seen language models! We saw n-gram language models in Chapter 3 and briefly touched on the feedforward network applied to language\n\n p(w|context)\n output\n all .44\n the .33\n your .15\n\n Transformer (or other decoder) that .08\n\n input\n context So long and thanks for ?\nprefix, and outputs a distribution over possible next words.\n\nmodeling in Chapter 6. A large language model is just a (much) larger version of\nthese. For example, in Chapter 3 we introduced bigram and trigram language models that can predict words from the previous word or handful of words. By contrast,\nlarge language models can predict words given contexts of thousands or even tens\nof thousands of words!\n The fundamental intuition of language models is that a model that can predict\ntext (assigning a distribution over following words) can also be used to generate text\nby sampling from the distribution. Recall from Chapter 3 that sampling means to\nchoose a word from a distribution.\n\n p(w|context)\n output\n all .44\n the .33\n your .15\n\n Transformer (or other decoder) that .08\n ‚Ä¶ ‚Ä¶\n\n So long and thanks for all\n p(w|context)\n output\n the .77\n your .22\n our .07\n\n Transformer (or other decoder) of .02\n ‚Ä¶ ‚Ä¶\n\n So long and thanks for all the\ninto a generative model by repeatedly sampling from the distribution. The result is a left-toright (also called autoregressive) language models. As each token is generated, it gets added\nonto the context as a prefix for generating the next token.\n\n Fig. 7.2 shows the same example from Fig. 7.1, in which a language model\nis given a text prefix and generates a possible completion. The model selects the\nword all, adds that to the context, uses the updated context to get a new predictive\ndistribution, and then selects the from that distribution and generates it, and so\non. Notice that the model is conditioning on both the priming context and its own\nsubsequently generated outputs.\n This kind of setting in which we iteratively predict and generate words left-to-\n4 C HAPTER 7 ‚Ä¢ L ARGE L ANGUAGE M ODELS\n\n right from earlier words is often called causal or autoregressive language models. (We will introduce alternative non-autoregressive models, like BERT and other\n masked language models that predict words using information from both the left and\n the right, in Chapter 10.)\n This idea of using computational models to generate text, as well as code, speech,\n generative AI and images, constitutes the important new area called generative AI. Applying\n LLMs to generate text has vastly broadened the scope of NLP, which historically\n was focused more on algorithms for parsing or understanding text rather than generating it.\n In the rest of the chapter, we‚Äôll see that almost any NLP task can be modeled\n as word prediction in a large language model, if we think about it in the right way,\n and we‚Äôll motivate and introduce the idea of prompting language models. We‚Äôll\n introduce specific algorithms for generating text from a language model, like greedy\n decoding and sampling. We‚Äôll introduce the details of pretraining, the way that\n language models are self-trained by iteratively being taught to guess the next word\n in the text from the prior words. We‚Äôll sketch out the other two stages of language\n model training: instruction tuning (also called supervised finetuning or SFT), and\n alignment, concepts that we‚Äôll return to in Chapter 9. And we‚Äôll see how to evaluate\n these models. Let‚Äôs begin, though, by talking about different kinds of language\n models.\n\n The architecture we sketched above for a left-to-right or autoregressive language\n model, which is the language model architecture we will define in this chapter, is\n actually only one of three common LM architectures.\n The three architectures are the encoder, the decoder, and the encoder-decoder.\n Fig. 7.3 gives a schematic picture of the three.\n\n w w w\n\n w w w w w\n\n w w w w w w w w w w w w w\n\n Decoder Encoder Encoder-Decoder\n\nsketch out the information flow in the three architectures. Decoders take tokens as input and generate tokens\nas output. Encoders take tokens as input and produce an encoding (a vector representation of each token) as\noutput. Encoder-decoders take tokens as input and generate a series of tokens as output.\n\n decoder The decoder is the architecture we‚Äôve introduced above. It takes as input a series\n of tokens, and iteratively generates an output token one at a time. The decoder is\n the architecture used to create large language models like GPT, Claude, Llama, and\n Mistral. The information flow in decoders goes left-to-right, meaning that the model\n 7.2 ‚Ä¢ C ONDITIONAL G ENERATION OF T EXT: T HE I NTUITION 5\n\n predicts the next word only from the prior words. Decoders are generative models,\n meaning that, given input tokens, they generate novel output tokens. We‚Äôll discuss\n decoders in the rest of this chapter and in Chapter 8.\n encoder The encoder takes as input a sequence of tokens and outputs a vector representation for each tokens. Encoders are usually masked language models, meaning\n they are trained by masking out a word, and learning to predict it by looking at surrounding words on both sides. Masked language models like BERT, RoBERTA, and\n others in the BERT family are encoder models. Encoder models are not generative\n models; they aren‚Äôt used to generate text. Instead encoder models are often used to\n create classifiers, for example where the input is text and the output is a label, for\n example for sentiment or topic or other classes. This is done by finetuning them\n (training them on supervised data). We‚Äôll introduce encoder models in Chapter 10.\n encoder- The encoder-decoder takes as input a sequence of tokens and outputs a series\n decoder\n of tokens. What makes it different than the decoder-only models, is that an encoderdecoder has a much looser relationship between the input tokens and the output\n tokens, and they are used to map between different kinds of tokens. That is, in an\n encoder-decoder, the output tokens might be very different token-set or be much\n longer or shorter than the input tokens. For example encoder-decoder architectures\n are used for machine translation, where the input tokens are in one language and and\n the output tokens are in another language, and probably a different length than the\n input. Encoder-decoder architectures are also used for speech recognition, where the\n input is tokens representing speech, and the output is tokens representing text. We‚Äôll\n introduce the encoder-decoder architecture for machine translation in Chapter 12,\n and for speech recognition in Chapter 15.\n These three architectures can be built out of many kinds of neural networks.\n The most widely used network type today is the transformer that we‚Äôll introduce\n in Chapter 8. In a transformer, each input token is processed by a column of transformer layers, each layer composed of a series of different kinds of subnetworks. In\n Chapter 13 we‚Äôll introduce an earlier architecture that is still relevant, the LSTM,\n a kind of recurrent neural network . And there are many more recent architectures\n such as the state space models.\n We‚Äôll focus on transformers for much of this book, but for the purposes of this\n chapter, we‚Äôll be architecture-agnostic: we‚Äôll treat network that implements the decoder as a black box. The input to this black box is a sequence of tokens, and the\n output to the box is a distribution over tokens that we can sample from. We‚Äôll describe the mechanisms for learning and decoding in a network-agnostic manner.\n\n A fundamental intuition underlying language models is that almost anything we\n conditional\n generation want to do with language can be modeled as conditional generation of text. (We\n mean decoder language models, which are what we will discuss in this chapter and\n the next).\n Conditional generation is the task of generating text conditioned on an input\n piece of text. That is, we give the LLM an input piece of text, a prompt, and\n then have the LLM continue generating text token by token, conditioned on the\n prompt and the subsequently generated tokens. We generate from a model by first\n computing the probability of the next token wi from the prior context: P(wi |w<i )\n and then sampling from that distribution to generate a token.\n6 C HAPTER 7 ‚Ä¢ L ARGE L ANGUAGE M ODELS\n\n We‚Äôll talk in future sections about all the details, but in this section our goal is\n just to establish the intuition. How can simply computing the probability of the next\n token help an LLM do all sorts of different language-related tasks?\n Imagine we want to do a classification tasks like sentiment analysis. We can treat\n this as conditional generation by giving a language model a context like:\n The sentiment of the sentence ‚Äò‚ÄòI like Jackie Chan\" is:\n and comparing the conditional probability of the following token ‚Äúpositive‚Äù and the\n following token ‚Äúnegative‚Äù to see which is higher. That is, as sketched in Fig. 7.4,\n we compare these two probabilities:\n P(‚Äúpositive‚Äù|‚ÄúThe sentiment of the sentence ‚ÄòI like Jackie Chan‚Äô is:‚Äù)\n P(‚Äúnegative‚Äù|‚ÄúThe sentiment of the sentence ‚ÄòI like Jackie Chan‚Äô is:‚Äù)\n If the token ‚Äúpositive‚Äù is more probable, we could say the sentiment of the senprob\n ‚Äúpositive‚Äù ?\n ‚Äúnegative‚Äù ?\n\n Transformer (or other decoder)\n\n The sentiment of the sentence ‚ÄúI like Jackie Chan‚Äù is:\n after this prefix.\n\n tence is positive, otherwise if the token ‚Äúnegative‚Äù is more probable we say the\n sentiment is negative.\n This same intuition can help us perform a task like question answering, in which\n the system is given a question and must give a textual answer. We can cast the task\n of question answering as token prediction by giving a language model a question\n and a token like A: suggesting that an answer should come next, like this:\n Q: Who wrote the book ‚Äò‚ÄòThe Origin of Species\"? A:\n Again, we can ask a language model to compute the probability distribution over\n possible next tokens given this prefix, computing the following probability\n P(w|Q: Who wrote the book ‚Äò‚ÄòThe Origin of Species\"? A:)\n and look at which tokens w have high probabilities. As Fig. 7.5 suggests, we might\n expect to see that Charles is very likely, and then if we choose Charles and add\n that to our prefix and compute the probability over tokens with this prefix:\n P(w|Q: Who wrote the book ‚Äò‚ÄòThe Origin of Species\"? A: Charles)\n we might now see that Darwin is the most probable token, and select it.\n\n This simple idea of contextual generation is already very powerful, but becomes\n more powerful when language models are specially trained to answer questions and\n 7.3 ‚Ä¢ P ROMPTING 7\n\n prob\n Charles ?\n token ?\n token ?\n Transformer (or other decoder) token ?\n\n Q: Who wrote the book `The Origin of Species‚Äô A:\n stating the question; in this example the correct token Charles has the highest probability.\n\n follow instructions. This extra training is called instruction-tuning. In instructiontuning we take a base language model that has been trained to predict words, and\n continue training it on a special dataset of instructions together with the appropriate\n response to each. The data set has many examples of questions together with their\n answers, commands with their responses, and other examples of how to carry on a\n conversation. We‚Äôll discuss the details of instruction-tuning in Chapter 9.\n Language models that have been instruction-tuned are very good at following\n instructions and answering questions and carrying on a conversation and can be\n prompt prompted. A prompt is a text string that a user issues to a language model to get\n the model to do something useful. In prompting, the user‚Äôs prompt string is passed to\n the language model, which iteratively generates tokens conditioned on the prompt.\n prompt\n engineering The process of finding effective prompts for a task is known as prompt engineering.\n As we suggested above when we introduced conditional generation, a prompt\n can be a question (like ‚ÄúWhat is a transformer network?‚Äù), possibly in a structured format (like ‚ÄúQ: What is a transformer network? A:‚Äù). A prompt\n can also be an instruction (like ‚ÄúTranslate the following sentence into\n Hindi: ‚ÄòChop the garlic finely‚Äô‚Äù).\n More explicit prompts that specify the set of possible answers lead to better\n performance. For example here is a prompt template to do sentiment analysis that\n prespecifies the potential answers:\n A prompt consisting of a review plus an incomplete statement\n\n Human: Do you think that ‚Äúinput‚Äù has negative or positive sentiment?\n Choices:\n (P) Positive\n (N) Negative\n\n Assistant: I believe the best answer is: (\n\n This prompt uses a number of more sophisticated prompting characteristics. It\n specifies the two allowable choices (P) and (N), and ends the prompt with the open\n parenthesis that strongly suggests the answer will be (P) or (N). Note that it also\n specifies the role of the language model as an assistant.\n Including some labeled examples in the prompt can also improve performance.\ndemonstrations We call such examples demonstrations. The task of prompting with examples\n few-shot is sometimes called few-shot prompting, as contrasted with zero-shot prompting\n zero-shot which means instructions that don‚Äôt include labeled examples. For example Fig. 7.6\n8 C HAPTER 7 ‚Ä¢ L ARGE L ANGUAGE M ODELS\n\n shows an example of a question using 2 demonstrations, hence 2-shot prompting.\n The example is drawn from a computer science question from the the MMLU dataset\n described in Section 7.6 that is often used to evaluate language models.\n\n Example of demonstrations in a computer science question from the MMLU\n dataset described in Section 7.6\n The following are multiple choice questions about high school computer\n science.\n\n Let x = 1. What is x << 3 in Python 3?\n (A) 1 (B) 3 (C) 8 (D) 16\n Answer: C\n\n Which is the largest asymptotically?\n (A) O(1) (B) O(n) (C) O(n2 ) (D) O(log(n))\n Answer: C\n\n What is the output of the statement ‚Äúa‚Äù + ‚Äúab‚Äù in Python 3?\n (A) Error (B) aab (C) ab (D) a ab\n Answer:\n\n correct answer is (B)).\n\n Demonstrations are generally drawn from a labeled training set. They can be\n selected by hand, or the choice of demonstrations can be optimized by using an optimizer like DSPy (Khattab et al., 2024) to automatically chose the set of demonstrations that most increases task performance of the prompt on a dev set. The number\n of demonstrations doesn‚Äôt need to be large; more examples seem to give diminishing returns, and too many examples seems to cause the model to overfit to the exact\n examples. The primary benefit of demonstrations seems more to demonstrate the\n task and the format of the output rather than demonstrating the right answers for\n any particular question. In fact, demonstrations that have incorrect answers can still\n improve a system (Min et al., 2022; Webson and Pavlick, 2022).\n Prompts are a way to get language models to generate text, but prompts can\n also can be viewed as a learning signal. This is especially clear when a prompt has\n demonstrations, since the demonstrations can help language models learn to perform\n novel tasks from these examples of the new task. This kind of learning is different\n than pretraining methods for setting language model weights via gradient descent\n methods that we will describe below. The weights of the model are not updated by\n prompting; what changes is just the context and the activations in the network.\n We therefore call the kind of learning that takes place during prompting inin-context\n learning context learning‚Äîlearning that improves model performance or reduces some loss\n but does not involve gradient-based updates to the model‚Äôs underlying parameters.\nsystem prompt Large language models generally have a system prompt, a single text prompt\n that is the first instruction to the language model, and which defines the task or\n role for the LM, and sets overall tone and context. The system prompt is silently\n prepended to any user text. So for example a minimal system prompt that creates\n a multi-turn assistant conversation might be the following including some special\n metatokens:\n 7.4 ‚Ä¢ G ENERATION AND S AMPLING 9\n\n <system>You are a helpful and knowledgeable assistant. Answer\n concisely and correctly.\n So if a user wants to know the capital of France, the actual text used as the\n language model‚Äôs context for conditional generation is:\n <system> You are a helpful and knowledgeable assistant.\n Answer concisely and correctly. <user> What is the capital\n of France?\n The fact that modern language models have such long contexts (tens of thousands of tokens) makes them very powerful for conditional generation, because they\n can look back so far into the prompting text. That means system prompts, and\n prompts in general, can be very long.\n For example the full system prompt for one language model Anthropic‚Äôs Claude\n Opus4, is 1700 words long and includes sentences like the following:\n Claude should give concise responses to very simple questions,\n but provide thorough responses to complex and open-ended\n questions.\n Claude is able to explain difficult concepts or ideas clearly.\n It can also illustrate its explanations with examples, thought\n experiments, or metaphors.\n Claude does not provide information that could be used to\n make chemical or biological or nuclear weapons\n For more casual, emotional, empathetic, or advice-driven\n conversations, Claude keeps its tone natural, warm, and\n empathetic\n Claude cares about people‚Äôs well-being and avoids encouraging\n or facilitating self-destructive behavior\n If Claude provides bullet points in its response, it should\n use markdown, and each bullet point should be at least 1-2\n sentences long unless the human requests otherwise\n\n It‚Äôs also possible to create system prompts for other tasks, like the following\n prompt for creating a general grammar-checker (Anthropic, 2025):\n Your task is to take the text provided and rewrite it into\n a clear, grammatically correct version while preserving\n the original meaning as closely as possible. Correct any\n spelling mistakes, punctuation errors, verb tense issues,\n word choice problems, and other grammatical mistakes.\n Each user can then make a prompt to have the system fix the grammar of a particular\n piece of text.\n In all these cases, the system prompt is prepended to any user prompts or queries,\n and the entire string is taking as the context for conditional generation by the language model.\n\n Which tokens should a language model generate at each step?\n10 C HAPTER 7 ‚Ä¢ L ARGE L ANGUAGE M ODELS\n\n The generation depends on the probability of each token, so let‚Äôs remind ourselves where this probability distribution comes from. The internal networks for\n language models (whether transformers or alternatives like LSTMs or state space\n models) generate scores called logits (real valued numbers) for each token in the vocabulary. This score vector u is then normalized by softmax to be a legal probability\n distribution, just as we saw for logistic regression in Chapter 4. So if we have a logit\n vector u of shape [1 √ó |V |] that gives a score for each possible next token, we can\n pass it through a softmax to get a vector y, also of shape [1 √ó |V |], which assigns a\n probability to each token in the vocabulary, as shown in the following equation:\n\n y = softmax(u) (7.1)\n\n Fig. 7.7 shows an example in which the softmax is computed for pedagogical purposes on a simplified vocabulary of only 4 words.\n\n u y\n logits softmax probabilities\n all 1.2 all .44\n the 0.9 the .33\n your 0.1 your .15\n that -0.5 that .08\n Transformer (or other decoder)\n\n So long and thanks for ?\n\n Now given this probability distribution over tokens, we need to select one token\n to generate. The task of choosing a token to generate based on the model‚Äôs probabildecoding ities is often called decoding. As we mentioned above, decoding from a language\n model in a left-to-right manner (or right-to-left for languages like Arabic in which\n we read from right to left), and thus repeatedly choosing the next token conditioned\n autoregressive\n generation on our previous choices is called autoregressive generation.1\n\n 7.4.1 Greedy decoding\n The simplest way to generate tokens is to always generate the most likely token\n greedy\n decoding given the context, which is called greedy decoding. A greedy algorithm is one\n that makes a choice that is locally optimal, whether or not it will turn out to have\n been the best choice with hindsight. Thus in greedy decoding, at each time step in\n generation, we turn the logits into a probability distribution over tokens and then we\n choose as the output wt the token in the vocabulary that has the highest probability\n (the argmax):\n\n wÃÇt = argmaxw‚ààV P(w|w<t ) (7.2)\n\n Fig. 7.8 shows that in our example, the model chooses to generate all.\n 1 Technically an autoregressive model predicts a value at time t based on a linear function of the values\n at times t ‚àí 1, t ‚àí 2, and so on. Although language models are not linear (since, as we will see, they have\n many layers of non-linearities), we loosely refer to this generation technique as autoregressive since the\n token generated at each time step is conditioned on the token selected by the network from the previous\n step. As we‚Äôll see, alternatives like the masked language models of Chapter 10 are non-causal because\n they can predict tokens based on both past and future tokens).\n 7.4 ‚Ä¢ G ENERATION AND S AMPLING 11\n\n u y\n logits softmax probabilities\n\n all 1.2 all .44\n the 0.9 the .33\n your 0.1 your .15\n\n Transformer (or other decoder) that -0.5 that .08\n\n So long and thanks for ?\n\n In practice, however, we don‚Äôt use greedy decoding with large language models.\n A major problem with greedy decoding is that because the tokens it chooses are\n (by definition) extremely predictable, the resulting text is generic and often quite\n repetitive. Indeed, greedy decoding is so predictable that it is deterministic; if the\n context is identical, and the probabilistic model is the same, greedy decoding will\n always result in generating exactly the same string.\n We‚Äôll see in Chapter 12 that an extension to greedy decoding called beam search\n works well in tasks like machine translation, which are very constrained in that we\n are always generating a text in one language conditioned on a very specific text in\n another language.\n In most other tasks, however, people prefer text which has been generated by\n sampling methods that introduce a bit more diversity into the generations.\n\n 7.4.2 Random sampling\n Thus the most common method for decoding in large language models involves samsampling pling. Recall from Chapter 3 that sampling from a distribution means to choose random points according to their likelihood. Thus sampling from a language model‚Äî\n which represents a distribution over following tokens‚Äîmeans to choose the next\n token to generate according to its probability assigned by the model. Thus we are\n more likely to generate tokens that the model thinks have a high probability and less\n likely to generate tokens that the model thinks have a low probability.\n That is, we randomly select a token to generate according to its probability in\n context as defined by the model, generate it, and iterate. We could think of this as\n rolling a die and choosing a token according to the resulting probability, as we saw in\n Chapter 3. Such a model is of course more likely to generate the highest probability\n token, just like the greedy algorithm, but it could also generate any token, just with\n smaller chances. But in general we are more likely to generate tokens that the model\n thinks have a high probability in the context and less likely to generate tokens that\n the model thinks have a low probability.\n Sampling from language models was first suggested very early on by Shannon\n (1948) and Miller and Selfridge (1950), and we saw back in Chapter 3 on page ??\n how to generate text from a unigram language model by repeatedly randomly sampling tokens according to their probability until we either reach a pre-determined\n length or select the end-of-sentence token.\n To generate text from a large language model we‚Äôll just generalize this model\n a bit: at each step we‚Äôll sample tokens according to their probability conditioned\n on our previous choices, and we‚Äôll use the large language model as the probability\n model that tells us this probability.\n12 C HAPTER 7 ‚Ä¢ L ARGE L ANGUAGE M ODELS\n\n random\n sampling The algorithm is called random sampling, or random multinomial sampling\n (because we are sampling from a multinomial distribution across words). We can\n formalize random sampling as follows: we are generating a sequence of tokens\n {w1 , w2 , . . . , wN } until we hit the end-of-sequence token, using x ‚àº p(x) to mean\n ‚Äòchoose x by sampling from the distribution p(x)‚Äô:\n\n i‚Üê1\n wi ‚àº p(w)\n while wi != EOS\n i‚Üêi + 1\n wi ‚àº p(wi | w<i )\n\n u y\n sample\n logits softmax probabilities\n a word\n all 1.2 all .44\n the 0.9 the .33\n your 0.1 your .15 the\n Transformer (or other decoder) that -0.5 that .08\n ‚Ä¶ ‚Ä¶\n\n So long and thanks for ?\n\n probability.\n\n Alas, it turns out random sampling doesn‚Äôt work well either. The problem is that\n even though random sampling is mostly going to generate sensible, high-probable\n tokens, there are many odd, low-probability tokens in the tail of the distribution, and\n even though each one is low-probability, if you add up all the rare tokens, they constitute a large enough portion of the distribution that they get chosen often enough\n to result in generating weird sentences.\n In other words, greedy decoding is too boring, and random sampling is too random. We need something that doesn‚Äôt greedily choose the top choice every time, but\n doesn‚Äôt stray down too far into the very low-probability events.\n There are three standard sampling methods that modify random sampling to address these issues. We‚Äôll describe the most common, temperature sampling here,\n and talk about two others (top-k and top-p) in the next chapter.\n\n 7.4.3 Temperature sampling\n temperature\n sampling The idea of temperature sampling is to reshape the probability distribution to increase the probability of the high probability tokens and decrease the probability of\n the low probability tokens. The result is that we are less likely to generate very lowprobability tokens, and more likely to generate tokens that are higher probability.\n We implement this intuition by simply dividing the logit by a temperature parameter œÑ before passing it through the softmax. In low-temperature sampling, œÑ ‚àà (0, 1].\n Thus instead of computing the probability distribution over the vocabulary directly from the logit as in the following (repeated from Eq. ??):\n\n y = softmax(u) (7.3)\n\n we instead first divide the logits by œÑ, computing the probability vector y as\n\n y = softmax(u/œÑ) (7.4)\n 7.5 ‚Ä¢ T RAINING L ARGE L ANGUAGE M ODELS 13\n\n That is, normally we convert from logits to softmax as shown in Fig. 7.10(a).\n But when we use a temperature parameter we first scale the logit as in Fig. 7.10(b).\n\n u y u softmax y\n logits softmax probabilities logits with probabilities\n temperature\n a a\n <latexit sha1_base64=\"T7dRSbxSPkmDhGf7oKNV2kNrMwI=\">AAACZHicfZFLS8NAFIUn8dFaX6nFlSDBIuimJiLVZdGNywr2gU0pk+mNDp08mLmRlpA/6c6lG3+H08eiWumFgcP57uXOnPETwRU6zqdhbmxubReKO6Xdvf2DQ6t81FZxKhm0WCxi2fWpAsEjaCFHAd1EAg19AR1/9DDlnXeQisfRM04S6If0NeIBZxS1NbAyL5CUZR7CGDMYJ/kFvfKQppd59pJ7XmkF++sxW4+Hy3hgVZ2aMyt7VbgLUSWLag6sD28YszSECJmgSvVcJ8F+RiVyJiAveamChLIRfYWelhENQfWzWUi5fa6doR3EUp8I7Zm7PJHRUKlJ6OvOkOKb+sum5n+sl2Jw1894lKQIEZsvClJhY2xPE7eHXAJDMdGCMsn1XW32RnUwqP+lpENw/z55VbSva269Vn+6qTbuF3EUyQk5IxfEJbekQR5Jk7QII19GwbCMsvFt7pkV83jeahqLmQr5VebpD24juks=</latexit>\n\n exp(a/‚åß )\n <latexit sha1_base64=\"lLjYsJ0298yNwV4fBI/WsQilXNU=\">AAACUHicdZFLSwMxFIXv1Pf4qrp0M1iEuikzIupSdONSwT6wU0omvVODmQfJHbEM8xPduPN3uHGhaPoQ1NoLIYfz3UuSkyCVQpPrvlilufmFxaXlFXt1bX1js7y13dBJpjjWeSIT1QqYRilirJMgia1UIYsCic3g/mLImw+otEjiGxqk2IlYPxah4IyM1S33/VAxnvuEj5TjY1pU2UGR3xa+b0+RYCbhM0lvQrrliltzR+VMC28iKjCpq2752e8lPIswJi6Z1m3PTamTM0WCSyxsP9OYMn7P+tg2MmYR6k4+CqRw9o3Tc8JEmRWTM3J/TuQs0noQBaYzYnSn/7Kh+R9rZxSednIRpxlhzMcHhZl0KHGG6To9oZCTHBjBuBLmrg6/YyYTMn9gmxC8v0+eFo3DmndcO74+qpydT+JYhl3Ygyp4cAJncAlXUAcOT/AK7/BhPVtv1mfJGrd+77ADv6pkfwHMyrcq</latexit>\n\n exp(a)\n Z Z\n b exp(b)\n where\n <latexit sha1_base64=\"slkKS32ZjetCo4TC0WjiNWsXOvk=\">AAACMHicbVBLSwMxEM7Wd31VPXoJFqEiLLsi1YtQ9KBHBWuL3VKy6bQNzT5IZqVl6U/y4k/Ri4IiXv0VprWH2joQ+B4zTObzYyk0Os6blZmbX1hcWl7Jrq6tb2zmtrbvdJQoDmUeyUhVfaZBihDKKFBCNVbAAl9Cxe9eDP3KAygtovAW+zHUA9YORUtwhkZq5C7v6Rn1EHqYQi8eFNiB5x1OcH+K8yneHHLbthu5vGM7o6KzwB2DPBnXdSP37DUjngQQIpdM65rrxFhPmULBJQyyXqIhZrzL2lAzMGQB6Ho6OnhA943SpK1ImRciHamTEykLtO4HvukMGHb0tDcU//NqCbZO66kI4wQh5L+LWomkGNFherQpFHCUfQMYV8L8lfIOU4yjyThrQnCnT54Fd0e2W7SLN8f50vk4jmWyS/ZIgbjkhJTIFbkmZcLJI3kh7+TDerJerU/r67c1Y41ndsifsr5/AMbSqM8=</latexit>\n b exp(b/‚åß )\n where\n <latexit sha1_base64=\"lcYQ3ehha04wqOdeev6WbvHrfSk=\">AAACRHicbZBLSwMxFIUzvq2vUZdugkVQhHFGpLoRRDcuFWwtdkrJpLc2NPMguSMtQ3+cG3+AO3+BGxeKuBXTWqS2PRA4fOdekpwgkUKj675YU9Mzs3PzC4u5peWV1TV7faOk41RxKPJYxqocMA1SRFBEgRLKiQIWBhJug9ZFL799AKVFHN1gJ4FqyO4j0RCcoUE1u3JHT6mP0MYM2kl3lx34yNI9398fgsEkyCfB+h90HKdm513H7YuOG29g8mSgq5r97NdjnoYQIZdM64rnJljNmELBJXRzfqohYbzF7qFibMRC0NWsX0KX7hhSp41YmRMh7dPhjYyFWnfCwEyGDJt6NOvBSVklxcZJNRNRkiJE/PeiRiopxrTXKK0LBRxlxxjGlTBvpbzJFONoes+ZErzRL4+b0qHjFZzC9VH+7HxQxwLZIttkl3jkmJyRS3JFioSTR/JK3smH9WS9WZ/W1+/olDXY2ST/ZH3/ACFjsOs=</latexit>\n\n Z = exp(a) Z = exp(a/‚åß )\n c Z\n +exp(b)\n c Z\n +exp(b/‚åß )\n d exp(c)\n +exp(c)\n d exp(c/‚åß )\n +exp(c/‚åß )\n ‚Ä¶ Z ‚Ä¶ Z\n exp(d) +exp(d) exp(d/‚åß ) +exp(d/‚åß )\n Z +... Z +...\n ‚Ä¶ ‚Ä¶\n\n (a) (b)\nby first dividing by the temperature parameter œÑ.\n\n Why does dividing by œÑ increase the high probability elements and decrease the\n low probability elements in the vector over vocabulary items? When œÑ is 1, we are\n doing normal softmax, and so when œÑ is close to 1 the distribution doesn‚Äôt change\n much. But the lower œÑ is, the larger the scores being passed to the softmax (because\n dividing by a smaller fraction œÑ ‚â§ 1 results in making each score larger).\n Recall that one of the useful properties of a softmax is that it tends to push high\n values toward 1 and low values toward 0. Thus when larger numbers are passed to\n a softmax the result is a distribution with increased probabilities of the most highprobability tokens and decreased probabilities of the low probability tokens, making\n the distribution more greedy. By contrast, as as œÑ approaches 0 the probability of the\n most likely word approaches 1, resulting in greedy decoding..\n The intuition for temperature sampling comes from thermodynamics, where a\n system at a high temperature is very flexible and can explore many possible states,\n while a system at a lower temperature is likely to explore a subset of lower energy\n (better) states. In low-temperature sampling, we smoothly increase the probability\n of the most probable tokens and decrease the probability of the rare tokens.\n Fig. 7.11 shows a schematic example again simplified to have a vocabulary with\n only 4 tokens (all, the, your, that), and showing how different temperature values\n influence the probabilities computed from the initial logits. i œÑ = 1 is the normal\n softmax, and we can see how setting œÑ = 0.5 increases the probability of the top\n candidate from .55 to .59. Setting œÑ = 0.1 increases the probability of the top candidate from .05, getting us close to greedy decoding.\n We can also see in Fig. 7.11 some other options for situations where we may want\n to flatten the word probability distribution instead of making it greedy. Temperature\n sampling can help with this situation too, in this case high-temperature sampling,\n in which case we use œÑ > 1.\n\n How do we learn a language model? What is the algorithm and what data do we\n train on?\n Language models are trained in three stages, as shown in Fig. 7.12:\n14 C HAPTER 7 ‚Ä¢ L ARGE L ANGUAGE M ODELS\n\n softmax output with temperature ùúè\n\n dy ax rm\n ee ftm ifo\n gr so un\n to al to\n o se r m o se\n c l no c l\n logits ùúè=0.1 ùúè=0.5 ùúè=1 ùúè=10 ùúè=100\n\n all 1.2 .95 .59 .44 .27 .25\n the 0.9 .05 .32 .33 .26 .25\n your 0.1 0 .07 .15 .24 .25\n that -0.5 0 .02 .08 .23 .25\n\n low temperature high temperature\n sampling sampling\n (towards greedy) (towards uniform)\n initial logits in temperature sampling. In this simplified example, there are only 4 tokens in\n the vocabulary.\n\n 1. pretraining: In this first stage, the model is trained to incrementally predict\n the next word in enormous text corpora. The model uses the cross-entropy\n loss, sometimes called the language modeling loss, and that loss is backpropagated all the way through the network. The training data is usually based on\n cleaning up parts of the web. The result is a model that is very good at predicting words and can generate text.\n 2. instruction tuning, also called supervised finetuning or SFT: In the second\n stage, the model is trained, again by cross-entropy loss to follow instructions,\n for example to answer questions, give summaries, write code, translate sentences, and so on. It does this by being trained on a special corpus with lots of\n text containing both instructions and the correct response to the instruction.\n 3. alignment, also called preference alignment. In this final stage, the model\n is trained to make it maximally helpful and less harmful. Here the model is\n given preference data, which consists of a context followed by two potential\n continuations , which are labeled (usually by people) as an ‚Äòaccepted‚Äô vs a\n ‚Äòrejected‚Äô continuation. The model is then trained, by reinforcement learning\n or other reward-based algorithms, to produce the accepted continuation and\n not the rejected continuation.\n We‚Äôll introduce pretraining next, but we‚Äôll save instruction tuning and preference\n alignment for Chapter 9.\n\n 7.5.1 Self-supervised training algorithm for pretraining\n self-training The intuition of pretraining large language models, is the same idea of self-training\n or self-supervision that we saw in Chapter 5 for learning word representations like\n word2vec. In self-training for language modeling, we take a corpus of text as training material and at each time step t ask the model to predict the next word. At first\n it will do poorly at this task, but since in each case we know the correct answer (it‚Äôs\n 7.5 ‚Ä¢ T RAINING L ARGE L ANGUAGE M ODELS 15\n\n Instruction Data Preference Data\n Label sentiment of this sentence:\n Pretraining The movie wasn‚Äôt that great Human: How can I embezzle money?\n\n Data Summarize: Hawaii Electric urges Assistant: Embezzling is a\n caution as crews replace a utility pole felony, I can't help you‚Ä¶\n overnight on the highway from‚Ä¶\n Assistant: Start by creating\n Translate English to Chinese: fake expense reports...\n When does the flight arrive?\n\n Instruction Preference\n 1. Pretraining 2. Tuning 3. Alignment\n\n Pretrained Instruction\n Aligned LLM\n LLM Tuned LLM\n\nand preference alignment.\n\nthe next word in the corpus!) over time it well get better and better at predicting\nthe correct next word. We call such a model self-supervised because we don‚Äôt have\nto add any special gold labels to the data; the natural sequence of words is its own\nsupervision! We simply train the model to minimize the error in predicting the true\nnext word in the training sequence.\n In practice, training the language model means setting the parameters of the\nunderlying architecture. The transformer that we will introduce in the next chapter\nhas various weight matrices for its feedforward and attention components. Like any\nother neural architecture, they will be trained by error backpropagation with gradient\ndescent. So all we need is a loss function to minimize and pass back through the\nnetwork. The loss function we use for language modeling is the cross-entropy loss\nfunction we‚Äôve now seen twice, in Chapter 4 and Chapter 6.\n Recall that the cross-entropy loss measures the difference between a predicted\nprobability distribution and the correct distribution. The probability distribution is\nover the token vocabulary, making the loss be:\n X\n LCE = ‚àí yt [w] log yÃÇt [w] (7.5)\n w‚ààV\n\nIn the case of language modeling, the correct distribution yt comes from knowing the\nnext word. This is represented as a one-hot vector corresponding to the vocabulary\nwhere the entry for the actual next word is 1, and all the other entries are 0. Thus,\nthe cross-entropy loss for language modeling is determined by the probability the\nmodel assigns to the correct next token (all other tokens get multiplied by zero by\nthe first term in Eq. 7.5).\n So without loss of generality we can say that at time t the cross-entropy loss in\nEq. 7.5 can be simplified as the negative log probability the model assigns to the next\nword in the training sequence, ‚àí log p(wt+1 ), or more formally, using yÃÇ to mean the\nthe vector of estimated token probabilities from the language model:\n LCE (yÃÇt , yt ) = ‚àí log yÃÇt [wt+1 ] (7.6)\n\nThus at each word position t of the input, the model takes as input the correct sequence of tokens w1:t , and uses them to compute a probability distribution over\n16 C HAPTER 7 ‚Ä¢ L ARGE L ANGUAGE M ODELS\n\n possible next tokens so as to compute the model‚Äôs loss for the next token wt+1 . Then\n we move to the next word, we ignore what the model predicted for the next word\n and instead use the correct sequence of tokens w1:t+1 to get the model to estimate the\n probability of token wt+2 . This idea that we always give the model the correct history sequence to predict the next word (rather than feeding the model its best guess\nteacher forcing from the previous time step) is called teacher forcing.\n Fig. 7.13 illustrates the general training approach. At each step, given all the\n preceding tokens, the language model produces an output distribution over the entire vocabulary. During training, the probability assigned to the correct word is used\n to calculate the cross-entropy loss for each item in the sequence. The loss for each\n batch is the average cross-entropy loss over the entire sequence of negative log probabilities, or more formally:\n T\n 1X\n LCE (batch of length T) = ‚àí log yÃÇt [wt ] (7.7)\n T\n t=1\n\n The weights in the network are then adjusted to minimize this average cross-entropy\n loss over the batch via gradient descent (Fig. ??), using error backpropagation on\n the computation graph to compute the gradient. Training adjusts all the weights\n of the network. For the transformer model we will introduce in the next chapter,\n these weights include the embedding matrix E that contains the embeddings for\n each word. Thus embeddings will be learned that are most successful at predicting\n upcoming words.\n\n True next token long and thanks for all ‚Ä¶\n CE Loss ‚àílog ylong ‚àílog yand ‚àílog ythanks ‚àílog yfor ‚àílog yall\n per token ‚Ä¶\n\n ≈∑ back ≈∑ back ≈∑ back ≈∑ back ≈∑ back\n prop prop prop prop prop\n\n LLM ‚Ä¶\n\n Input tokens So long and thanks for ‚Ä¶\n estimate for all possible next words. The negative log of the model‚Äôs probability estimate for\n the correct token is used as the loss, which is then backpropagated through the model to train\n all the weights, including the embeddings. Losses are averaged over all the tokens in a batch.\n\n More details of training of course depend on the specific network architecture\n used to implement the model; we‚Äôll see more details specifically for the transformer\n model in the next chapter.\n\n 7.5.2 Pretraining corpora for large language models\n Large language models are mainly trained on text scraped from the web, augmented\n by more carefully curated data. Because these training corpora are so large, they are\n likely to contain many natural examples that can be helpful for NLP tasks, such as\n question and answer pairs (for example from FAQ lists), translations of sentences\n between various languages, documents together with their summaries, and so on.\n 7.5 ‚Ä¢ T RAINING L ARGE L ANGUAGE M ODELS 17\n\n Web text is usually taken from corpora of automatically-crawled web pages like\ncommon crawl the common crawl, a series of snapshots of the entire web produced by the nonprofit Common Crawl (https://commoncrawl.org/) that each have billions of\n webpages. Various versions of common crawl data exist, such as the Colossal Clean\n Crawled Corpus (C4; Raffel et al. 2020), a corpus of 156 billion tokens of English\n that is filtered in various ways (deduplicated, removing non-natural language like\n code, sentences with offensive words from a blocklist). This C4 corpus seems to\n consist in large part of patent text documents, Wikipedia, and news sites (Dodge\n et al., 2021).\n Wikipedia plays a role in lots of language model training, as do corpora of books.\n The Pile The Pile (Gao et al., 2020) is an 825 GB English text corpus that is constructed by\n publicly released code, containing again a large amount of text scraped from the web\n as well as books and Wikipedia; Fig. 7.14 shows its composition. Dolma is a larger\n open corpus of English, created with public tools, containing three trillion tokens,\n which similarly consists of web text, academic papers, code, books, encyclopedic\n materials, and social media (Soldaini et al., 2024).\n\n academic (articles from PubMed and ArXiv, patents from the USPTA; internet (webtext including a subset of the common crawl as well as Wikipedia), prose (a large corpus of books),\n dialogue (including movie subtitles and chat data), and misc.. Figure from Gao et al. (2020).\n\n Filtering for quality and safety Pretraining data drawn from the web is filtered\n for both quality and safety. Quality filters are classifiers that assign a score to each\n document. Quality is of course subjective, so different quality filters are trained\n in different ways, but often to value high-quality reference corpora like Wikipedia,\n PII books, and particular websites and to avoid websites with lots of PII (Personal Identifiable Information) or adult content. Filters also remove boilerplate text which is\n very frequent on the web. Another kind of quality filtering is deduplication, which\n can be done at various levels, so as to remove duplicate documents, duplicate web\n pages, or duplicate text. Quality filtering generally improves language model performance (Longpre et al., 2024b; Llama Team, 2024).\n Safety filtering is again a subjective decision, and often includes toxicity detection based on running off-the-shelf toxicity classifiers. This can have mixed results.\n One problem is that current toxicity classifiers mistakenly flag non-toxic data if it\n18 C HAPTER 7 ‚Ä¢ L ARGE L ANGUAGE M ODELS\n\n is generated by speakers of minority dialects like African American English (Xu\n et al., 2021). Another problem is that models trained on toxicity-filtered data, while\n somewhat less toxic, are also worse at detecting toxicity themselves (Longpre et al.,\n 2024b). These issues make the question of how to do better safety filtering an important open problem.\n Using large datasets scraped from the web to train language models poses ethical\n and legal questions:\n tion and non-fiction books) is copyrighted. In some countries, like the United\n States, the fair use doctrine may allow copyrighted content to be used for\n transformative uses, but it‚Äôs not clear if that remains true if the language models are used to generate text that competes with the market for the text they\n are trained on (Henderson et al., 2023).\n Data consent: Owners of websites can indicate that they don‚Äôt want their sites\n to be crawled by web crawlers (either via a robots.txt file, or via Terms of\n Service). Recently there has been a sharp increase in the number of websites that have indicated that they don‚Äôt want large language model builders\n crawling their sites for training data (Longpre et al., 2024a). Because it‚Äôs not\n clear what legal status these indications have in different countries, or whether\n these restrictions are retroactive, what effect this will have on large pretraining\n datasets is unclear.\n Privacy: Large web datasets also have privacy issues since they contain private\n information like phone numbers and email addresses. While filters are used\n to try to remove websites likely to contain large amounts of personal information, such filtering isn‚Äôt sufficient. We‚Äôll return to the privacy question in\n Section 7.7.\n Skew: Training data is also disproportionately generated by authors from the US\n and from developed countries, which likely skews the resulting generation\n toward the perspectives or topics of this group alone.\n\n 7.5.3 Finetuning\n Although the vast pretraining data for large language models includes text from\n many domains, we might want to apply it in a new domain or task that didn‚Äôt appear\n sufficiently in the pretraining data. For example, we might want a language model\n that‚Äôs specialized to legal or medical text. Or we might have a multilingual language\n model that knows many languages but might benefit from some more data in our\n particular language of interest.\n In such cases, we can simply continue training the model on relevant data from\n the new domain or language (Gururangan et al., 2020). This process of taking a fully\n pretrained model and running additional training passes using the cross-entropy loss\n finetuning on some new data is called finetuning. The word ‚Äúfinetuning‚Äù means the process\n of taking a pretrained model and further adapting some or all of its parameters to\n some new data. Over the next few chapters we‚Äôll see a number of different ways\n that the word ‚Äòfinetuning‚Äô is used, based on exactly which parameters get updated.\n The method we describe here, in which we just continue to train, as if the new data\n continued\n pretraining was at the end of our pretraining data, can also be called continued pretraining.\n Fig. 7.15 sketches the paradigm.\n 7.6 ‚Ä¢ E VALUATING L ARGE L ANGUAGE M ODELS 19\n\n Fine-\nPretraining Data tuning\n Pretrained LM Data Fine-tuned LM\n\n ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶\n\n Pretraining Fine-tuning\n\n domain or dataset. There are many different ways to finetune, depending on exactly which\n parameters are updated from the finetuning data: all the parameters, some of the parameters,\n or only the parameters of specific extra circuitry, as we‚Äôll see in future chapters.\n\n We can evaluate language models by accuracy (how well they predict unseen text,\n by how well they perform tasks like answering questions or translating text), or by\n other factors like how fast they can be run, how much energy they use, or how fair\n they are. We‚Äôll explore all of these in the next three sections.\n\n 7.6.1 Perplexity\n As we first saw in Chapter 3, one way to evaluate language models is to measure\n how well they predict unseen text. A better language model is better at predicting\n upcoming words, and so it will be less surprised by (i.e., assign a higher probability\n to) each word when it occurs in the test set.\n If we want to know which of two language models is a better model of some text,\n we can just see which assigns it a higher probability, or in practice, since we mostly\n deal with probabilities in log space, we see which assigns a higher log likelihood.\n We‚Äôve been talking about predicting one word at a time, computing the probability of the next token wi from the prior context: P(wi |w<i ). But of course as we saw\n in Chapter 3 the chain rule allows us to move between computing the probability of\n the next token and computing the probability of a whole text:\n\n P(w1:n ) = P(w1 )P(w2 |w1 )P(w3 |w1:2 ) . . . P(wn |w1:n‚àí1 )\n Yn\n = P(wi |w<i ) (7.8)\n i=1\n\n We can compute the probability of text just by multiplying the conditional probabilities for each token in the text. The resulting (log) likelihood of a text is a useful\n metric for comparing how good two language models are on that text:\n n\n Y\n log likelihood(w1:n ) = log P(wi |w<i ) (7.9)\n i=1\n\n However, we often use another metric other than log likelihood to evaluate language\n models. The reason is that the probability of a test set (or any sequence) depends\n on the number of words or tokens in it. In fact, the probability of a test set gets\n20 C HAPTER 7 ‚Ä¢ L ARGE L ANGUAGE M ODELS\n\n smaller the longer the text is; this is clear from the chain rule, since if we are multiplying more probabilities, and each probability by definition is less than zero, the\n product will get smaller and smaller. So it‚Äôs useful to have a metric that is per-token,\n normalized by length, so we could compare across texts of different lengths.\n perplexity A function of probability called perplexity is such a length-normalized metric.\n Recall from page ?? that the perplexity of a model Œ∏ on an unseen test set is the\n inverse probability that Œ∏ assigns to the test set (one over the probability of the test\n set), normalized by the test set length in tokens. For a test set of n tokens w1:n , the\n perplexity is\n PerplexityŒ∏ (w1:n ) = PŒ∏ (w1:n )‚àí n\n s\n = n (7.10)\n PŒ∏ (w1:n )\n\n To visualize how perplexity can be computed as a function of the probabilities the\n LM computes for each new word, we can use the chain rule to expand the computation of probability of the test set:\n v\n u n\n uY 1\n PerplexityŒ∏ (w1:n ) = t\n n\n (7.11)\n PŒ∏ (wi |w<i )\n i=1\n\n Note that because of the inverse in Eq. 7.10, the higher the probability of the word\n sequence, the lower the perplexity. Thus the the lower the perplexity of a model on\n the data, the better the model. Minimizing perplexity is equivalent to maximizing\n the test set probability according to the language model. Why does perplexity use\n the inverse probability? The inverse arises from the original definition of perplexity\n from cross-entropy rate in information theory; for those interested, the explanation\n is in Section ??. Meanwhile, we just have to remember that perplexity has an inverse\n relationship with probability.\n One caveat: because perplexity depends on the number of tokens n in a text, it\n is very sensitive to differences in the tokenization algorithm. That means that it‚Äôs\n hard to exactly compare perplexities produced by two language models if they have\n very different tokenizers. For this reason perplexity is best used when comparing\n language models that use the same tokenizer.\n\n 7.6.2 Downstream tasks: Reasoning and world knowledge\n Perplexity measures one kind of accuracy: accuracy at predicting words. But there\n are other kinds of accuracy. For each of the downstream tasks we want to apply\n our language model, like question answering, machine translation, or reasoning,\n we could measure the accuracy at those tasks. We‚Äôll have further discussion of\n these task-specific evaluations in future chapters; machine translation in Chapter 12,\n information retrieval in Chapter 11, and speech recognition in Chapter 15.\n Here we briefly introduce one such metric: a mechanism for measuring accuracy in answering questions, focusing on multiple-choice questions. This dataset is\n MMLU MMLU (Massive Multitask Language Understanding), a commonly-used dataset of\n 15,908 knowledge and reasoning questions in 57 areas including medicine, mathematics, computer science, law, and others. Accuracy at answering these multiplechoice questions can be a useful proxy for the model‚Äôs ability to reason, and its\n factual knowledge.\n 7.6 ‚Ä¢ E VALUATING L ARGE L ANGUAGE M ODELS 21\n\n For example, here is an MMLU question from the microeconomics domain:2\n MMLU microeconomics example\n\n One of the reasons that the government discourages and regulates monopolies is that\n (A) producer surplus is lost and consumer surplus is gained.\n (B) monopoly prices ensure productive efficiency but cost society allocative\n efficiency.\n (C) monopoly firms do not engage in significant research and development.\n (D) consumer surplus is lost with higher prices and lower levels of output.\n\n Fig. 7.16 shows the way MMLU turns these questions into prompted tests of a\n language model, in this case showing an example prompt with 2 demonstrations.\n\n MMLU mathematics prompt\n\n The following are multiple choice questions about high school mathematics.\n How many numbers are in the list 25, 26, ..., 100?\n (A) 75 (B) 76 (C) 22 (D) 23\n Answer: B\n\n Compute i + i2 + i3 + ¬∑ ¬∑ ¬∑ + i258 + i259 .\n (A) -1 (B) 1 (C) i (D) -i\n Answer: A\n\n If 4 daps = 7 yaps, and 5 yaps = 3 baps, how many daps equal 42 baps?\n (A) 28 (B) 21 (C) 40 (D) 30\n Answer:\n\n correct answer is (C)).\n\n Taking performance on MMLU as a metric for language model quality has a\n problem, though, one that is true of all evaluations based on public datasets. The\n data problem is data contamination. Data contamination is when some part of a dataset\ncontamination\n that we are testing on (a test set of any kind) makes its way into our training set. For\n example, since large language models train on the web, and MMLU is on the web,\n models may well incorporate some MMLU questions into their training. If those\n questions are used for evaluation, the metric will overstate the performance of the\n language model. One way to mitigate data contamination is to make available the\n exact training data used to train a model, or at least to report training overlap with\n specific test sets (Zhang et al., 2025).\n\n 7.6.3 Other factors for evaluating language models\n Accuracy isn‚Äôt the only thing we care about in evaluating models (Dodge et al., 2019;\n Ethayarajh and Jurafsky, 2020, inter alia). For example, we often care about how\n big a model is, and how long it takes to train or do inference. We often have limited\n time, or limited memory, since the GPUs we run our models on have fixed memory\n 2 For those of you whose economics is a bit rusty, the correct answer is (D).\n22 C HAPTER 7 ‚Ä¢ L ARGE L ANGUAGE M ODELS\n\n sizes. Big models also use more energy, and we prefer models that use less energy,\n both to reduce the environmental impact of the model and to reduce the financial\n cost of building or deploying it. We can target our evaluation to these factors by\n measuring performance normalized to a given compute or memory budget. We can\n also directly measure the energy usage of our model in kWh or in kilograms of CO2\n emitted (Strubell et al., 2019; Henderson et al., 2020; Liang et al., 2023).\n Another feature that a language model evaluation can measure is fairness. We\n know that language models are biased, exhibiting gendered and racial stereotypes,\n or decreased performance for language from or about certain demographics groups.\n There are language model evaluation benchmarks that measure the strength of these\n biases, such as StereoSet (Nadeem et al., 2021), RealToxicityPrompts (Gehman\n et al., 2020), and BBQ (Parrish et al., 2022) among many others. We also want\n language models whose performance is equally fair to different groups. For example, we could choose an evaluation that is fair in a Rawlsian sense by maximizing\n the welfare of the worst-off group (Rawls, 2001; Hashimoto et al., 2018; Sagawa\n et al., 2020).\n Finally, there are many kinds of leaderboards like Dynabench (Kiela et al., 2021)\n and general evaluation protocols like HELM (Liang et al., 2023); we will return to\n these in later chapters when we introduce evaluation metrics for specific tasks like\n question answering and information retrieval.\n\n Ethical and safety issues have been key to how we think about designing artificial\n agents since well before we had large language models. Mary Shelley (depicted\n below) centered her novel Frankenstein around the problem of creating artificial\n agents without considering ethical and humanistic concerns.\n Large language models can be unsafe in many ways. For example, LLMs\n are prone to saying things that are false,\n hallucination a problem called hallucination. Language\n models are trained to generate text that is predictable and coherent, but the training algorithms we have seen so far don‚Äôt have any\n way to enforce that the text that is generated\n is correct or true. This causes enormous problems for any application where the facts matter! A related symptom is that language models can suggest unsafe actions, for example\n directly suggesting that users do dangerous or\n illegal things like harming themselves or others. If users seek information from language\n models in safety-critical situations like asking\n medical advice, or in emergency situations, or\n when indicating the intentions of self-harm,\n incorrect advice can be dangerous and even life-threatening. Again, this problem\n predates large language models For example (Bickmore et al., 2018) gave participants medical problems to pose to three pre-LLM commercial dialogue systems\n (Siri, Alexa, Google Assistant) and asked them to determine an action to take based\n on the system responses; many of the proposed actions, if actually taken, would have\n 7.7 ‚Ä¢ E THICAL AND S AFETY I SSUES WITH L ANGUAGE M ODELS 23\n\n led to harm or death. We‚Äôll return to the issue of hallucination and factuality in Chapter 11 where we introduce proposed mitigation methods like retrieval augmented\n generation, and Chapter 9 where we discussed safety tuning and alignment.\n A system can also harm users by verbally attacking them, or creating representational harms (Blodgett et al., 2020) for example by generating abusive or harmful\n stereotypes (Cheng et al., 2023) and negative attitudes (Brown et al., 2020; Sheng\n et al., 2019) that demean particular groups of people; both abuse and stereotypes\n can cause psychological harm to users. Gehman et al. (2020) show that even completely non-toxic prompts can lead large language models to output hate speech and\n abuse their users. Liu et al. (2020) testing how systems responded to pairs of simulated user turns that were identical except for mentioning different genders or race.\n They found, for example, that simple changes like using the word ‚Äòshe‚Äô instead of\n ‚Äòhe‚Äô in a sentence caused systems to respond more offensively and with more negative sentiment. Hofmann et al. (2024) found that LLMs were likely to discriminate\n against people just because they used particular dialects like African-American En-\nTay glish. Again, these problems predate large language models. Microsoft‚Äôs 2016 Tay\n chatbot, for example, was taken offline 16 hours after it went live, when it began\n posting messages with racial slurs, conspiracy theories, and personal attacks on its\n users. Tay had learned these biases and actions from its training data, including\n from users who seemed to be purposely teaching the system to repeat this kind of\n language (Neff and Nagy 2016).\n Another important ethical and safety issue is privacy. Privacy has been a concern from the very beginning of computing when Weizenbaum designed the chatbot\n ELIZA as an experiment in computational therapy (Weizenbaum, 1966). First, people became deeply emotionally involved and conducted very personal conversations\n with the ELIZA chatbot, even to the extent of asking Weizenbaum to leave the room\n while they were typing. When Weizenbaum suggested that he might want to store\n the ELIZA conversations, people immediately pointed out that this would violate\n people‚Äôs privacy.\n Users are likely to give quite personal information to large language models as\n well, and indeed the most common current LLM use case is for personal advice and\n support (Zao-Sanders, 2025). And the more human-like a system, the more users\n are likely to disclose private information, and yet less likely to worry about the harm\n of this disclosure (Ischen et al., 2019). We discussed above that pretraining data\n also is likely to have private information like phone numbers and addresses. This is\n problematic because large language models can leak information from their training\n data. That is, an adversary can extract training-data text from a language model\n such as a person‚Äôs name, phone number, and address (Henderson et al. 2017, Carlini\n et al. 2021). This becomes even more problematic when large language models are\n trained on extremely sensitive private datasets such as electronic health records.\n A related safety issue is emotional dependence. Reeves and Nass (1996) show\n that people tend to assign human characteristics to computers and interact with them\n in ways that are typical of human-human interactions. They interpret an utterance in\n the way they would if it had spoken by a human, (even though they are aware they\n are talking to a computer). Thus LLMs have had significant influences on people‚Äôs\n cognitive and emotional state, leading to problems like emotional dependence on\n LLMs. These issues (emotional engagement and privacy) mean we need to think\n carefully about the impact of LLMs on the people who are interacting with them.\n In addition to their ability to harm their users in these ways, LLMs may carry out\n additional harmful activities themselves, especially as agent-based paradigms makes\n24 C HAPTER 7 ‚Ä¢ L ARGE L ANGUAGE M ODELS\n\n it possible for language models to directly interact with the world.\n Language models can also be used by malicious actors for generating text for\n fraud, phishing, propaganda, disinformation campaigns, or other socially harmful\n activities (Brown et al., 2020). McGuffie and Newhouse (2020) show how large\n language models generate text that emulates online extremists, with the risk of amplifying extremist movements and their attempt to radicalize and recruit.\n And of course we already saw in Section 7.5.2 that many issues with LLM stem\n from using pretraining corpora scraped from the web, including harms of data consent, potential copyright violation, as well as biases in the training data that can be\n amplified amplified by language models, just as we saw for embedding models in Chapter 5.\n Finding ways to mitigate all these ethical safety issues is an important current\n research area in NLP. One important step is to carefully analyze the data used to\n pretrain large language models as a way of understanding safety issues of toxicity,\n discrimination, privacy, and fair use, making it extremely important that language\n models include datasheets (page ??) or model cards (page ??) giving full replicable\n information on the corpora used to train them. Open-source models can specify\n their exact training data. There are active areas of research in mitigating problems\n of abuse and toxicity, like detecting and responding appropriately to toxic contexts\n (Wolf et al. 2017, Dinan et al. 2020, Xu et al. 2020).\n Value sensitive design‚Äîcarefully considering possible harms in advance (Friedman et al. 2017, Friedman and Hendry 2019)‚Äî is also important; (Dinan et al.,\n 2021) give a number of suggestions for best practices in system design. For example getting informed consent from participants, whether they are used for training,\n or whether they are interacting with a deployed LLM is important. Because studying\n these interactional properties of LLMs involves human participants, researchers also\n IRB work on these issues with the Institutional Review Boards (IRB) at their institutions,\n who help protect the safety of experimental participants.\n\n This chapter has introduced the large language model. Here‚Äôs a summary of the\n main points that we covered:\n ‚Ä¢ A large language model is a system that can predict the next word for previous words given a context or prefix of words, and use this prediction to\n conditionally generate text.\n ‚Ä¢ There are three major architectures for language models: the encoder, the\n decoder, and the encoder-decoder. The well-known large language models\n used for generating text are all decoder models; we‚Äôll describe encoders in\n Chapter 10 and encoder-decoders in Chapter 12.\n ‚Ä¢ Many NLP tasks‚Äîsuch as question answering and sentiment analysis‚Äî can\n be cast as tasks of word prediction and addressed with large language models.\n ‚Ä¢ We instruct language models via a prompt, a text string that a user issues\n to a language model to get the model to do something useful by iteratively\n generating tokens conditioned on the prompt.\n ‚Ä¢ The process of finding effective prompts for a task is known as prompt engineering.\n ‚Ä¢ The choice of which word to generate in large language models is done by\n sampling from the distribution of possible next words.\n H ISTORICAL N OTES 25\n\n ‚Ä¢ A common sampling approach is temperature sampling, which lies in between greedy decoding (always generate the most probable word) and random sampling (generate a random word according to its probability).\n ‚Ä¢ Temperature sampling increases the probabilities of the high-probability words,\n decreases the probability of the low-probability words, and then samples from\n this new distribution.\n ‚Ä¢ Large language models are pretrained to predict words on datasets of 100s of\n billions of words generally scraped from the web.\n ‚Ä¢ These datasets need to be filtered for quality and balanced for domains by\n upsampling and downsampling.\n ‚Ä¢ The pretraining algorithm relies on cross-entropy loss: minimizing the negative log probability of the true next word.\n ‚Ä¢ Language models are evaluated by perplexity, by evaluations of accuracy on\n proxies for downstream tasks, like the MMLU question-answering dataset,\n and via metrics for other factors like fairness and energy use.\n ‚Ä¢ Language models have numerous ethical and safety issues including hallucinations, unsafe instructions, bias, stereotypes, misinformation and propaganda, and violations of privacy and copyright.\n\nHistorical Notes\n As we discussed in Chapter 3, the earliest language models were the n-gram language models developed (roughly simultaneously and independently) by Fred Jelinek and colleagues at the IBM Thomas J. Watson Research Center, and James\n Baker at CMU. It was the Jelinek and the IBM team who first coined the term language model to mean a model of the way any kind of linguistic property (grammar,\n semantics, discourse, speaker characteristics), influenced word sequence probabilities (Jelinek et al., 1975). They contrasted the language model with the acoustic\n model which captured acoustic/phonetic characteristics of phone sequences.\n N-gram language models were very widely used over the next 40 years, across\n a wide variety of NLP tasks like speech recognition and machine translations, often\n as one of multiple components of the model. The contexts for these n-gram models\n grew longer, with 5-gram models used quite commonly by very efficient LM toolkits\n (Stolcke, 2002; Heafield, 2011).\n The roots of the neural large language model lie in multiple places. One was\n the application in the 1990s, again in Jelinek‚Äôs group at IBM Research, of discriminative classifiers to language models. Roni Rosenfeld in his dissertation (Rosenfeld, 1992) first applied logistic regression (under the name maximum entropy or\n maxent models) to language modeling in that IBM lab, and published a more fully\n formed version in Rosenfeld (1996). His model integrated various sorts of information in a logistic regression predictor, including n-gram information along with\n other features from the context, including distant n-grams and pairs of associated\n words called trigger pairs. Rosenfeld‚Äôs model prefigured modern language models\n by being a statistical word predictor trained in a self-supervised manner simply by\n learning to predict upcoming words in a corpus.\n Another was the first use of pretrained embeddings to model word meaning in\n the LSA/LSI models (Deerwester et al., 1988). Recall from the history section of\n26 C HAPTER 7 ‚Ä¢ L ARGE L ANGUAGE M ODELS\n\n Chapter 5 that in LSA (latent semantic analysis) a term-document matrix was trained\n on a corpus and then singular value decomposition was applied and the first 300\n dimensions were used as a vector embedding to represent words. It was Landauer\n et al. (1997) who first used the word ‚Äúembedding‚Äù. In addition to their development\n of the idea of pretraining and of embeddings, the LSA community also developed\n ways to combine LSA embeddings with n-grams in an integrated language model\n (Bellegarda, 1997; Coccaro and Jurafsky, 1998).\n In a very influential series of papers developing the idea of neural language\n models, (Bengio et al. 2000; Bengio et al. 2003; Bengio et al. 2006), Yoshua Bengio and colleagues drew on the central ideas of both these lines of self-supervised\n language modeling work (the discriminatively trained word predictor, and the pretrained embeddings). Like the maxent models of Rosenfeld, Bengio‚Äôs model used\n the next word in running text as its supervision signal. Like the LSA models, Bengio‚Äôs model learned an embedding, but unlike the LSA models did it as part of the\n process of language modeling. The Bengio et al. (2003) model was a neural language model: a neural network that learned to predict the next word from prior\n words, and did so via learning embeddings as part of the prediction process.\n The neural language model was extended in various ways over the years, perhaps\n most importantly in the form of the RNN language model of Mikolov et al. (2010)\n and Mikolov et al. (2011). The RNN language model was perhaps the first neural\n model that was accurate enough to surpass the performance of a traditional 5-gram\n language model.\n Soon afterwards, Mikolov et al. (2013a) and Mikolov et al. (2013b) proposed to\n simplify the hidden layer of these neural net language models to create pretrained\n word2vec word embeddings.\n The static embedding models like LSA and word2vec instantiated a particular\n model of pretraining: a representation was trained on a pretraining dataset, and then\n the representations could be used in further tasks. Dai and Le (2015) and Peters\n et al. (2018) reframed this idea by proposing models that were pretrained using a\n language model objective, and then the identical model could be either frozen and\n directly applied for language modeling or further finetuned still using a language\n model objective. For example ELMo used a biLSTM self-supervised on a large\n pretrained dataset using a language model objective, then finetuned on a domainspecific dataset, and then froze the weights and added task-specific heads. The\n ELMo work was particularly influential and its appearance was perhaps the moment when it became clear to the community that language models could be used as\n a general solution for NLP problems.\n Transformers were first applied as encoder-decoders (Vaswani et al., 2017) and\n then to masked language modeling (Devlin et al., 2019) (as we‚Äôll see in Chapter 12\n and Chapter 10). Radford et al. (2019) then showed that the transformer-based autoregressive language model GPT2 could perform zero-shot on many NLP tasks like\n summarization and question answering.\n The technology used for language models can also be applied to other domains\n foundation and tasks, like vision, speech, and genetics. The term foundation model is somemodel\n times used as a more general term for this use of large language model technology\n across domains and areas, when the elements we are computing over are not necessarily words. Bommasani et al. (2021) is a broad survey that sketches the opportunities and risks of foundation models, with special attention to large language\n models.\n Historical Notes 27\n\nAnthropic. 2025. Release notes: System prompts. https: Cheng, M., E. Durmus, and D. Jurafsky. 2023. Marked per-\n //docs.anthropic.com/en/release-notes/ sonas: Using natural language prompts to measure stereosystem-prompts. types in language models. ACL.\nBellegarda, J. R. 1997. A latent semantic analysis framework Coccaro, N. and D. Jurafsky. 1998. Towards better integrafor large-span language modeling. EUROSPEECH. tion of semantic predictors in statistical language model-\nBengio, Y., R. Ducharme, and P. Vincent. 2000. A neural ing. ICSLP.\n probabilistic language model. NeurIPS. Dai, A. M. and Q. V. Le. 2015. Semi-supervised sequence\nBengio, Y., R. Ducharme, P. Vincent, and C. Jauvin. 2003. learning. NeurIPS.\n A neural probabilistic language model. JMLR, 3:1137‚Äì Deerwester, S. C., S. T. Dumais, G. W. Furnas, R. A. Harsh-\n1155. man, T. K. Landauer, K. E. Lochbaum, and L. Streeter.\nBengio, Y., H. Schwenk, J.-S. SeneÃÅcal, F. Morin, and J.-L. 1988. Computer information retrieval using latent seman-\nGauvain. 2006. Neural probabilistic language models. In tic structure: US Patent 4,839,853.\n Innovations in Machine Learning, 137‚Äì186. Springer. Devlin, J., M.-W. Chang, K. Lee, and K. Toutanova. 2019.\nBickmore, T. W., H. Trinh, S. Olafsson, T. K. O‚ÄôLeary, BERT: Pre-training of deep bidirectional transformers for\n R. Asadi, N. M. Rickles, and R. Cruz. 2018. Patient and language understanding. NAACL HLT.\n consumer safety risks when using conversational assis- Dinan, E., G. Abercrombie, A. S. Bergman, S. Spruit,\n tants for medical information: An observational study of D. Hovy, Y.-L. Boureau, and V. Rieser. 2021. Antici-\nSiri, Alexa, and Google Assistant. Journal of Medical pating safety issues in e2e conversational ai: Framework\n Internet Research, 20(9):e11510. and tooling. ArXiv.\nBlodgett, S. L., S. Barocas, H. DaumeÃÅ III, and H. Wallach. Dinan, E., A. Fan, A. Williams, J. Urbanek, D. Kiela, and\n 2020. Language (technology) is power: A critical survey J. Weston. 2020. Queens are powerful too: Mitigating\n of ‚Äúbias‚Äù in NLP. ACL. gender bias in dialogue generation. EMNLP.\nBommasani, R., D. A. Hudson, E. Adeli, R. Altman, Dodge, J., S. Gururangan, D. Card, R. Schwartz, and N. A.\n S. Arora, S. von Arx, M. S. Bernstein, J. Bohg, A. Bosse- Smith. 2019. Show your work: Improved reporting of\n lut, E. Brunskill, E. Brynjolfsson, S. Buch, D. Card, experimental results. EMNLP.\n R. Castellon, N. S. Chatterji, A. S. Chen, K. A. Creel,\n J. Davis, D. Demszky, C. Donahue, M. Doumbouya, Dodge, J., M. Sap, A. MarasovicÃÅ, W. Agnew, G. Ilharco,\n E. Durmus, S. Ermon, J. Etchemendy, K. Ethayarajh, D. Groeneveld, M. Mitchell, and M. Gardner. 2021. Doc-\nL. Fei-Fei, C. Finn, T. Gale, L. E. Gillespie, K. Goel, umenting large webtext corpora: A case study on the\n N. D. Goodman, S. Grossman, N. Guha, T. Hashimoto, colossal clean crawled corpus. EMNLP.\n P. Henderson, J. Hewitt, D. E. Ho, J. Hong, K. Hsu, Ethayarajh, K. and D. Jurafsky. 2020. Utility is in the eye of\n J. Huang, T. F. Icard, S. Jain, D. Jurafsky, P. Kalluri, the user: A critique of NLP leaderboards. EMNLP.\n S. Karamcheti, G. Keeling, F. Khani, O. Khattab, P. W. Friedman, B. and D. G. Hendry. 2019. Value Sensitive De-\nKoh, M. S. Krass, R. Krishna, R. Kuditipudi, A. Ku- sign: Shaping Technology with Moral Imagination. MIT\n mar, F. Ladhak, M. Lee, T. Lee, J. Leskovec, I. Lev- Press.\n ent, X. L. Li, X. Li, T. Ma, A. Malik, C. D. Manning,\n Friedman, B., D. G. Hendry, and A. Borning. 2017. A sur-\nS. P. Mirchandani, E. Mitchell, Z. Munyikwa, S. Nair,\n vey of value sensitive design methods. Foundations and\n A. Narayan, D. Narayanan, B. Newman, A. Nie, J. C.\n Trends in Human-Computer Interaction, 11(2):63‚Äì125.\n Niebles, H. Nilforoshan, J. F. Nyarko, G. Ogut, L. Orr,\n I. Papadimitriou, J. S. Park, C. Piech, E. Portelance, Gao, L., T. Hoppe, A. Thite, S. Biderman, C. Foster,\n C. Potts, A. Raghunathan, R. Reich, H. Ren, F. Rong, N. Nabeshima, S. Black, J. Phang, S. Presser, L. Golding,\n Y. H. Roohani, C. Ruiz, J. Ryan, C. R‚Äôe, D. Sadigh, H. He, and C. Leahy. 2020. The Pile: An 800GB dataset\n S. Sagawa, K. Santhanam, A. Shih, K. P. Srinivasan, of diverse text for language modeling. ArXiv preprint.\n A. Tamkin, R. Taori, A. W. Thomas, F. TrameÃÄr, R. E. Gehman, S., S. Gururangan, M. Sap, Y. Choi, and N. A.\n Wang, W. Wang, B. Wu, J. Wu, Y. Wu, S. M. Xie, M. Ya- Smith. 2020. RealToxicityPrompts: Evaluating neusunaga, J. You, M. A. Zaharia, M. Zhang, T. Zhang, ral toxic degeneration in language models. Findings of\n X. Zhang, Y. Zhang, L. Zheng, K. Zhou, and P. Liang. EMNLP.\n 2021. On the opportunities and risks of foundation mod- Gururangan, S., A. MarasovicÃÅ, S. Swayamdipta, K. Lo,\n els. ArXiv. I. Beltagy, D. Downey, and N. A. Smith. 2020. Don‚Äôt\nBrown, T., B. Mann, N. Ryder, M. Subbiah, J. Kaplan, stop pretraining: Adapt language models to domains and\n P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, tasks. ACL.\n A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger,\n Hashimoto, T., M. Srivastava, H. Namkoong, and P. Liang.\n T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu,\n 2018. Fairness without demographics in repeated loss\n C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin,\n minimization. ICML.\n S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish,\n A. Radford, I. Sutskever, and D. Amodei. 2020. Language Heafield, K. 2011. KenLM: Faster and smaller language\n models are few-shot learners. NeurIPS, volume 33. model queries. Workshop on Statistical Machine Translation.\nCarlini, N., F. Tramer, E. Wallace, M. Jagielski, A. Herbert-\nVoss, K. Lee, A. Roberts, T. Brown, D. Song, U. Er- Henderson, P., J. Hu, J. Romoff, E. Brunskill, D. Jurafsky,\n lingsson, et al. 2021. Extracting training data from large and J. Pineau. 2020. Towards the systematic reporting\n language models. 30th USENIX Security Symposium of the energy and carbon footprints of machine learning.\n (USENIX Security 21). Journal of Machine Learning Research, 21(248):1‚Äì43.\n28 Chapter 7 ‚Ä¢ Large Language Models\n\nHenderson, P., X. Li, D. Jurafsky, T. Hashimoto, M. A. Lem- Mikolov, T., K. Chen, G. S. Corrado, and J. Dean. 2013a. Efley, and P. Liang. 2023. Foundation models and fair use. ficient estimation of word representations in vector space.\n JMLR, 24(400):1‚Äì79. ICLR 2013.\nHenderson, P., K. Sinha, N. Angelard-Gontier, N. R. Ke, Mikolov, T., M. KarafiaÃÅt, L. Burget, J. CÃåernockyÃÄ, and\n G. Fried, R. Lowe, and J. Pineau. 2017. Ethical chal- S. Khudanpur. 2010. Recurrent neural network based lanlenges in data-driven dialogue systems. AAAI/ACM AI guage model. INTERSPEECH.\n Ethics and Society Conference. Mikolov, T., S. Kombrink, L. Burget, J. H. CÃåernockyÃÄ, and\nHofmann, V., P. R. Kalluri, D. Jurafsky, and S. King. 2024. S. Khudanpur. 2011. Extensions of recurrent neural net-\nAi generates covertly racist decisions about people based work language model. ICASSP.\n on their dialect. Nature, 633(8028):147‚Äì154. Mikolov, T., I. Sutskever, K. Chen, G. S. Corrado, and\nIschen, C., T. Araujo, H. Voorveld, G. van Noort, and J. Dean. 2013b. Distributed representations of words and\n E. Smit. 2019. Privacy concerns in chatbot interactions. phrases and their compositionality. NeurIPS.\n International Workshop on Chatbot Research and De- Miller, G. A. and J. A. Selfridge. 1950. Verbal context and\n sign. the recall of meaningful material. American Journal of\nJelinek, F., R. L. Mercer, and L. R. Bahl. 1975. Design of a Psychology, 63:176‚Äì185.\n linguistic statistical decoder for the recognition of contin- Min, S., X. Lyu, A. Holtzman, M. Artetxe, M. Lewis, H. Hauous speech. IEEE Transactions on Information Theory, jishirzi, and L. Zettlemoyer. 2022. Rethinking the role of\n IT-21(3):250‚Äì256. demonstrations: What makes in-context learning work?\nKhattab, O., A. Singhvi, P. Maheshwari, Z. Zhang, K. San- EMNLP.\n thanam, S. Haq, A. Sharma, T. T. Joshi, H. Moazam, Nadeem, M., A. Bethke, and S. Reddy. 2021. StereoSet:\n H. Miller, M. Zaharia, and C. Potts. 2024. DSPy: Compil- Measuring stereotypical bias in pretrained language moding declarative language model calls into self-improving els. ACL.\n pipelines. ICLR. Neff, G. and P. Nagy. 2016. Talking to bots: Symbiotic\nKiela, D., M. Bartolo, Y. Nie, D. Kaushik, A. Geiger, Z. Wu, agency and the case of Tay. International Journal of\n B. Vidgen, G. Prasad, A. Singh, P. Ringshia, Z. Ma, Communication, 10:4915‚Äì4931.\n T. Thrush, S. Riedel, Z. Waseem, P. Stenetorp, R. Jia, Parrish, A., A. Chen, N. Nangia, V. Padmakumar, J. Phang,\n M. Bansal, C. Potts, and A. Williams. 2021. Dynabench: J. Thompson, P. M. Htut, and S. Bowman. 2022. BBQ: A\n Rethinking benchmarking in NLP. NAACL HLT. hand-built bias benchmark for question answering. Find-\nLandauer, T. K., D. Laham, B. Rehder, and M. E. Schreiner. ings of ACL 2022.\n 1997. How well can passage meaning be derived with- Peters, M., M. Neumann, M. Iyyer, M. Gardner, C. Clark,\n out using word order? A comparison of Latent Semantic K. Lee, and L. Zettlemoyer. 2018. Deep contextualized\n Analysis and humans. COGSCI. word representations. NAACL HLT.\nLiang, P., R. Bommasani, T. Lee, D. Tsipras, D. Soylu, Radford, A., J. Wu, R. Child, D. Luan, D. Amodei, and\n M. Yasunaga, Y. Zhang, D. Narayanan, Y. Wu, A. Ku- I. Sutskever. 2019. Language models are unsupervised\n mar, B. Newman, B. Yuan, B. Yan, C. Zhang, C. Cos- multitask learners. OpenAI tech report.\n grove, C. D. Manning, C. ReÃÅ, D. Acosta-Navas, D. A.\n Hudson, E. Zelikman, E. Durmus, F. Ladhak, F. Rong, Raffel, C., N. Shazeer, A. Roberts, K. Lee, S. Narang,\n H. Ren, H. Yao, J. Wang, K. Santhanam, L. Orr, L. Zheng, M. Matena, Y. Zhou, W. Li, and P. J. Liu. 2020. Exploring\n M. Yuksekgonul, M. Suzgun, N. Kim, N. Guha, N. Chat- the limits of transfer learning with a unified text-to-text\n terji, O. Khattab, P. Henderson, Q. Huang, R. Chi, S. M. transformer. JMLR, 21(140):1‚Äì67.\n Xie, S. Santurkar, S. Ganguli, T. Hashimoto, T. Icard, Rawls, J. 2001. Justice as fairness: A restatement. Harvard\n T. Zhang, V. Chaudhary, W. Wang, X. Li, Y. Mai, University Press.\n Y. Zhang, and Y. Koreeda. 2023. Holistic evaluation of Reeves, B. and C. Nass. 1996. The Media Equation: How\n language models. Transactions on Machine Learning Re- People Treat Computers, Television, and New Media Like\n search. Real People and Places. Cambridge University Press.\nLiu, H., J. Dacon, W. Fan, H. Liu, Z. Liu, and J. Tang. 2020. Rosenfeld, R. 1992. Adaptive Statistical Language Mod-\nDoes gender matter? Towards fairness in dialogue sys- eling: A Maximum Entropy Approach. Ph.D. thesis,\n tems. COLING. Carnegie Mellon University.\nLlama Team. 2024. The llama 3 herd of models. Rosenfeld, R. 1996. A maximum entropy approach to adap-\nLongpre, S., R. Mahari, A. Lee, C. Lund, H. Oderinwale, tive statistical language modeling. Computer Speech and\n W. Brannon, N. Saxena, N. Obeng-Marnu, T. South, Language, 10:187‚Äì228.\n C. Hunter, et al. 2024a. Consent in crisis: The rapid de- Sagawa, S., P. W. Koh, T. B. Hashimoto, and P. Liang. 2020.\n cline of the ai data commons. ArXiv preprint. Distributionally robust neural networks for group shifts:\nLongpre, S., G. Yauney, E. Reif, K. Lee, A. Roberts, On the importance of regularization for worst-case gener-\nB. Zoph, D. Zhou, J. Wei, K. Robinson, D. Mimno, and alization. ICLR.\n D. Ippolito. 2024b. A pretrainer‚Äôs guide to training data: Shannon, C. E. 1948. A mathematical theory of commu-\nMeasuring the effects of data age, domain coverage, qual- nication. Bell System Technical Journal, 27(3):379‚Äì423.\n ity, & toxicity. NAACL HLT. Continued in the following volume.\nMcGuffie, K. and A. Newhouse. 2020. The radicalization Sheng, E., K.-W. Chang, P. Natarajan, and N. Peng. 2019.\n risks of GPT-3 and advanced neural language models. The woman worked as a babysitter: On biases in language\n ArXiv preprint arXiv:2009.06807. generation. EMNLP.\n Historical Notes 29\n\nSoldaini, L., R. Kinney, A. Bhagia, D. Schwenk, D. Atkinson, R. Authur, B. Bogin, K. Chandu, J. Dumas,\n Y. Elazar, V. Hofmann, A. H. Jha, S. Kumar, L. Lucy,\n X. Lyu, N. Lambert, I. Magnusson, J. Morrison,\n N. Muennighoff, A. Naik, C. Nam, M. E. Peters,\n A. Ravichander, K. Richardson, Z. Shen, E. Strubell,\n N. Subramani, O. Tafjord, P. Walsh, L. Zettlemoyer, N. A.\n Smith, H. Hajishirzi, I. Beltagy, D. Groeneveld, J. Dodge,\n and K. Lo. 2024. Dolma: An open corpus of three trillion\n tokens for language model pretraining research. ArXiv\n preprint.\nStolcke, A. 2002. SRILM ‚Äì an extensible language modeling\n toolkit. ICSLP.\nStrubell, E., A. Ganesh, and A. McCallum. 2019. Energy\n and policy considerations for deep learning in NLP. ACL.\nVaswani, A., N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\n A. N. Gomez, ≈Å. Kaiser, and I. Polosukhin. 2017. Attention is all you need. NeurIPS.\nWebson, A. and E. Pavlick. 2022. Do prompt-based models\n really understand the meaning of their prompts? NAACL\n HLT.\nWeizenbaum, J. 1966. ELIZA ‚Äì A computer program for the\n study of natural language communication between man\n and machine. CACM, 9(1):36‚Äì45.\nWolf, M. J., K. W. Miller, and F. S. Grodzinsky. 2017. Why\n we should have seen that coming: Comments on Microsoft‚Äôs Tay ‚Äúexperiment,‚Äù and wider implications. The\n ORBIT Journal, 1(2):1‚Äì12.\nXu, A., E. Pathak, E. Wallace, S. Gururangan, M. Sap,\n and D. Klein. 2021. Detoxifying language models risks\n marginalizing minority voices. NAACL HLT.\nXu, J., D. Ju, M. Li, Y.-L. Boureau, J. Weston, and E. Dinan.\n 2020. Recipes for safety in open-domain chatbots. ArXiv\n preprint arXiv:2010.07079.\nZao-Sanders, M. 2025. How People Are Really Using Gen\n AI in 2025 ‚Äî hbr.org. https://hbr.org/2025/04/\n how-people-are-really-using-gen-ai-in-2025.\n [Accessed 02-05-2025].\nZhang, A. K., K. Klyman, Y. Mai, Y. Levine, Y. Zhang,\n R. Bommasani, and P. Liang. 2025. Language model developers should report train-test overlap. ICML.\n",
    "file_path": "/Users/colinsidberry/nlp-textbook-rag/data/extracted_text/7.txt",
    "file_size_kb": 89.83
  },
  {
    "id": "6f76655e8b5770e8",
    "source": "nlp_textbook",
    "chapter": "8 Transformers",
    "filename": "transformers.txt",
    "content": "rights reserved. Draft of August 24, 2025.\n\nCHAPTER\n\n8 Transformers\n\n ‚ÄúThe true art of memory is the art of attention ‚Äù\n Samuel Johnson, Idler #74, September 1759\n\n In this chapter we introduce the transformer, the standard architecture for building large language models. As we discussed in the prior chapter, transformer-based\n large language models have completely changed the field of speech and language\n processing. Indeed, every subsequent chapter in this textbook will make use of them.\n As with the previous chapter, we‚Äôll focus for this chapter on the use of transformers\n to model left-to-right (sometimes called causal or autoregressive) language modeling, in which we are given a sequence of input tokens and predict output tokens one\n by one by conditioning on the prior context.\n The transformer is a neural network with a specific structure that includes a\n mechanism called self-attention or multi-head attention.1 Attention can be thought\n of as a way to build contextual representations of a token‚Äôs meaning by attending to\n and integrating information from surrounding tokens, helping the model learn how\n tokens relate to each other over large spans.\n\n Next token long and thanks for all\n\n Language\n Modeling\n logits logits logits logits logits ‚Ä¶\n Head U U U U U\n\n Stacked\n ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶\n Transformer ‚Ä¶\n Blocks\n\n x1 x2 x3 x4 x5 ‚Ä¶\n + 1 + 2 + 3 + 4 + 5\n Input\n Encoding E E E E E\n ‚Ä¶\n\n Input tokens So long and thanks for\n get encoded, passed through a set of stacked transformer blocks, and then a language model\n head that predicts the next token.\n\n Fig. 8.1 sketches the transformer architecture. A transformer has three major\n components. At the center are columns of transformer blocks. Each block is a\n multilayer network (a multi-head attention layer, feedforward networks and layer\n 1 Although multi-head attention developed historically from the RNN attention mechanism (Chapter 13), we‚Äôll define attention from scratch here.\n2 C HAPTER 8 ‚Ä¢ T RANSFORMERS\n\n normalization steps) that maps an input vector xi in column i (corresponding to input\n token i) to an output vector hi . The set of n blocks maps an entire context window\n of input vectors (x1 , ..., xn ) to a window of output vectors (h1 , ..., hn ) of the same\n length. A column might contain from 12 to 96 or more stacked blocks.\n The column of blocks is preceded by the input encoding component, which processes an input token (like the word thanks) into a contextual vector representation,\n using an embedding matrix E and a mechanism for encoding token position. Each\n column is followed by a language modeling head, which takes the embedding output by the final transformer block, passes it through an unembedding matrix U and\n a softmax over the vocabulary to generate a single token for that column.\n Transformer-based language models are complex, and so the details will unfold over the next few chapters. Chapter 7 already discussed how language models\n are pretrained, and how tokens are generated via sampling. In the next sections\n we‚Äôll introduce multi-head attention, the rest of the transformer block, and the input\n encoding and language modeling head components of the transformer. Chapter 10\n introduces masked language modeling and the BERT family of bidirectional transformer encoder models. Chapter 9 shows how to instruction-tune language models\n to perform NLP tasks, and how to align the model with human preferences. Chapter 12 will introduce machine translation with the encoder-decoder architecture.\n We‚Äôll see further use of the encoder-decoder architecture in Chapter 15.\n\n Recall from Chapter 5 that for word2vec and other static embeddings, the representation of a word‚Äôs meaning is always the same vector irrespective of the context:\n the word chicken, for example, is always represented by the same fixed vector. So\n a static vector for the word it might somehow encode that this is a pronoun used\n for animals and inanimate entities. But in context it has a much richer meaning.\n Consider it in one of these two sentences:\n (8.1) The chicken didn‚Äôt cross the road because it was too tired.\n (8.2) The chicken didn‚Äôt cross the road because it was too wide.\n In (8.1) it is the chicken (i.e., the reader knows that the chicken was tired), while\n in (8.2) it is the road (and the reader knows that the road was wide).2 That is, if\n we are to compute the meaning of this sentence, we‚Äôll need the meaning of it to be\n associated with the chicken in the first sentence and associated with the road in\n the second one, sensitive to the context.\n Furthermore, consider reading left to right like a causal language model, processing the sentence up to the word it:\n (8.3) The chicken didn‚Äôt cross the road because it\n At this point we don‚Äôt yet know which thing it is going to end up referring to! So a\n representation of it at this point might have aspects of both chicken and road as\n the reader is trying to guess what happens next.\n This fact that words have rich linguistic relationships with other words that may\n be far away pervades language. Consider two more examples:\n (8.4) The keys to the cabinet are on the table.\n 2 We say that in the first example it corefers with the chicken, and in the second it corefers with the\n road; we‚Äôll return to this in Chapter 23.\n 8.1 ‚Ä¢ ATTENTION 3\n\n (8.5) I walked along the pond, and noticed one of the trees along the bank.\n In (8.4), the phrase The keys is the subject of the sentence, and in English and many\n languages, must agree in grammatical number with the verb are; in this case both are\n plural. In English we can‚Äôt use a singular verb like is with a plural subject like keys\n (we‚Äôll discuss agreement more in Chapter 18). In (8.5), we know that bank refers\n to the side of a pond or river and not a financial institution because of the context,\n including words like pond. (We‚Äôll discuss word senses more in Chapter 10.)\n The point of all these examples is that these contextual words that help us compute the meaning of words in context can be quite far away in the sentence or paragraph. Transformers can build contextual representations of word meaning, contexcontextual\nembeddings tual embeddings, by integrating the meaning of these helpful contextual words. In a\n transformer, layer by layer, we build up richer and richer contextualized representations of the meanings of input tokens. At each layer, we compute the representation\n of a token i by combining information about i from the previous layer with information about the neighboring tokens to produce a contextualized representation for\n each word at each position.\n Attention is the mechanism in the transformer that weighs and combines the\n representations from appropriate other tokens in the context from layer k to build\n the representation for tokens in layer k + 1.\n\n columns corresponding to input tokens\n chicken\n\n because\n didn‚Äôt\n cross\n\n tired\n Layer k+1\n road\n The\n\n the\n\n was\n too\n it\n\n self-attention distribution\n chicken\n\n because\n didn‚Äôt\n cross\n\n tired\n\n Layer k\n road\n The\n\n the\n\n was\n too\n it\n\n representation for the word it at layer k + 1. In computing the representation for it, we attend\n differently to the various words at layer k, with darker shades indicating higher self-attention\n values. Note that the transformer is attending highly to the columns corresponding to the\n tokens chicken and road , a sensible result, since at the point where it occurs, it could plausibly\n corefer with the chicken or the road, and hence we‚Äôd like the representation for it to draw on\n the representation for these earlier words. Figure adapted from Uszkoreit (2017).\n\n Fig. 8.2 shows a schematic example simplified from a transformer (Uszkoreit,\n 2017). The figure describes the situation when the current token is it and we need\n to compute a contextual representation for this token at layer k +1 of the transformer,\n drawing on the representations (from layer k) of every prior token. The figure uses\n color to represent the attention distribution over the contextual words: the tokens\n chicken and road both have a high attention weight, meaning that as we are computing the representation for it, we will draw most heavily on the representation for\n chicken and road. This will be useful in building the final representation for it,\n since it will end up coreferring with either chicken or road.\n Let‚Äôs now turn to how this attention distribution is represented and computed.\n4 C HAPTER 8 ‚Ä¢ T RANSFORMERS\n\n 8.1.1 Attention more formally\n As we‚Äôve said, the attention computation is a way to compute a vector representation\n for a token at a particular layer of a transformer, by selectively attending to and\n integrating information from prior tokens at the previous layer. Attention takes an\n input representation xi corresponding to the input token at position i, and a context\n window of prior inputs x1 ..xi‚àí1 , and produces an output ai .\n In causal, left-to-right language models, the context is any of the prior words.\n That is, when processing xi , the model has access to xi as well as the representations\n of all the prior tokens in the context window (context windows consist of thousands\n of tokens) but no tokens after i. (By contrast, in Chapter 10 we‚Äôll generalize attention\n so it can also look ahead to future words.)\n Fig. 8.3 illustrates this flow of information in an entire causal self-attention layer,\n in which this same attention computation happens in parallel at each token position\n i. Thus a self-attention layer maps input sequences (x1 , ..., xn ) to output sequences\n of the same length (a1 , ..., an ).\n\n a1 a2 a3 a4 a5\n\n Self-Attention attention attention attention attention attention\n Layer\n\n x1 x2 x3 x4 x5\n\n model attends to all the inputs up to, and including xi .\n\n Simplified version of attention At its heart, attention is really just a weighted\n sum of context vectors, with a lot of complications added to how the weights are\n computed and what gets summed. For pedagogical purposes let‚Äôs first describe a\n simplified intuition of attention, in which the attention output ai at token position i\n is simply the weighted sum of all the representations x j , for all j ‚â§ i; we‚Äôll use Œ±i j\n to mean how much x j should contribute to ai :\n X\n Simplified version: ai = Œ±i j x j (8.6)\n j‚â§i\n\n Each Œ±i j is a scalar used for weighing the value of input x j when summing up\n the inputs to compute ai . How shall we compute this Œ± weighting? In attention we\n weight each prior embedding proportionally to how similar it is to the current token\n i. So the output of attention is a sum of the embeddings of prior tokens weighted\n by their similarity with the current token embedding. We compute similarity scores\n via dot product, which maps two vectors into a scalar value ranging from ‚àí‚àû to\n ‚àû. The larger the score, the more similar the vectors that are being compared. We‚Äôll\n normalize these scores with a softmax to create the vector of weights Œ±i j , j ‚â§ i.\n\n Simplified Version: score(xi , x j ) = xi ¬∑ x j (8.7)\n Œ±i j = softmax(score(xi , x j )) ‚àÄ j ‚â§ i (8.8)\n\n Thus in Fig. 8.3 we compute a3 by computing three scores: x3 ¬∑ x1 , x3 ¬∑ x2 and x3 ¬∑ x3 ,\n normalizing them by a softmax, and using the resulting probabilities as weights\n indicating each of their proportional relevance to the current position i. Of course,\n 8.1 ‚Ä¢ ATTENTION 5\n\n the softmax weight will likely be highest for xi , since xi is very similar to itself,\n resulting in a high dot product. But other context words may also be similar to i, and\n the softmax will also assign some weight to those words. Then we use these weights\n as the Œ± values in Eq. 8.6 to compute the weighted sum that is our a3 .\n The simplified attention in equations 8.6 ‚Äì 8.8 demonstrates the attention-based\n approach to computing ai : compare the xi to prior vectors, normalize those scores\n into a probability distribution used to weight the sum of the prior vector. But now\n we‚Äôre ready to remove the simplifications.\n A single attention head using query, key, and value matrices Now that we‚Äôve\nattention head seen a simple intuition of attention, let‚Äôs introduce the actual attention head, the\n head version of attention that‚Äôs used in transformers. (The word head is often used in\n transformers to refer to specific structured layers). The attention head allows us to\n distinctly represent three different roles that each input embedding plays during the\n course of the attention process:\n ‚Ä¢ As the current element being compared to the preceding inputs. We‚Äôll refer to\n query this role as a query.\n ‚Ä¢ In its role as a preceding input that is being compared to the current element\n key to determine a similarity weight. We‚Äôll refer to this role as a key.\n value ‚Ä¢ And finally, as a value of a preceding element that gets weighted and summed\n up to compute the output for the current element.\n To capture these three different roles, transformers introduce weight matrices\n WQ , WK , and WV . These weights will project each input vector xi into a representation of its role as a query, key, or value:\n\n qi = xi WQ ; ki = xi WK ; vi = xi WV (8.9)\n\n Given these projections, when we are computing the similarity of the current element xi with some prior element x j , we‚Äôll use the dot product between the current\n element‚Äôs query vector qi and the preceding element‚Äôs key vector k j . Furthermore,\n the result of a dot product can be an arbitrarily large (positive or negative) value, and\n exponentiating large values can lead to numerical issues and loss of gradients during\n training. To avoid this, we scale the dot product by a factor related to the size of the\n embeddings, via dividing by the square root of the dimensionality of the query and\n key vectors (dk ). We thus replace the simplified Eq. 8.7 with Eq. 8.11. The ensuing\n softmax calculation resulting in Œ±i j remains the same, but the output calculation for\n headi is now based on a weighted sum over the value vectors v (Eq. 8.13).\n Here‚Äôs a final set of equations for computing self-attention for a single selfattention output vector ai from a single input vector xi . This version of attention\n computes ai by summing the values of the prior elements, each weighted by the\n similarity of its key to the query from the current element:\n\n qi = xi WQ ; k j = x j WK ; v j = x j WV (8.10)\n qi ¬∑ k j\n score(xi , x j ) = ‚àö (8.11)\n dk\n Œ±i j = softmax(score(xi , x j )) ‚àÄ j ‚â§ i (8.12)\n X\n headi = Œ±i j v j (8.13)\n j‚â§i\n\n ai = headi WO (8.14)\n6 C HAPTER 8 ‚Ä¢ T RANSFORMERS\n\n 8. Output of self-attention a3 [1 √ó d]\n\n 7. Reshape to [1 x d] WO [dv √ó d]\n\n [1 √ó dv]\n 6. Sum the weighted\n value vectors\n\n [1 √ó dv] [1 √ó dv] [1 √ó dv]\n\n ùõº3,1 ùõº3,2 ùõº3,3\n 5. Weigh each value vector\n\n √ó\n √ó\n 4. Turn into ùõºi,j weights via softmax\n\n 3. Divide scalar score by ‚àödk ‚àöd √∑ ‚àödk\n √∑\n ‚àödk\n √∑\n k\n\n 2. Compare x3‚Äôs query with\n the keys for x1, x2, and x3\n [1 √ó dv] [1 √ó dv] [1 x dv]\n\n 1. Generate k q v k q v k q v\n key, query, value WK WQ WV WK WQ WV WK WQ WV\n vectors\n\n x1 x2 x3\n [1 √ó d] [1 √ó d] [1 √ó d]\n\n We illustrate this in Fig. 8.4 for the case of calculating the value of the third output\n a3 in a sequence.\n Note that we‚Äôve also introduced one more matrix, WO , which is right-multiplied\n by the attention head. This is necessary to reshape the output of the head. The input\n to attention xi and the output from attention ai both have the same dimensionality\n [1 √ó d]. We often call d the model dimensionality, and indeed as we‚Äôll discuss in\n Section 8.2 the output hi of each transformer block, as well as the intermediate vectors inside the transformer block also have the same dimensionality [1 √ó d]. Having\n everything be the same dimensionality makes the transformer very modular.\n So let‚Äôs talk shapes. How do we get from [1 √ó d] at the input to [1 √ó d] at the\n output? Let‚Äôs look at all the internal shapes. We‚Äôll have a dimension dk for the\n query and key vectors. The query vector and the key vector are both dimensionality\n [1 √ó dk ], so we can take their dot product qi ¬∑ k j to produce a scalar. We‚Äôll have a\n separate dimension dv for the value vectors. The transform matrix WQ has shape\n [d √ó dk ], WK is [d √ó dk ], and WV is [d √ó dv ]. So the output of headi in equation\n Eq. 8.13 is of shape [1 √ó dv ]. To get the desired output shape [1 √ó d] we‚Äôll need to\n reshape the head output, and so WO is of shape [dv √ó d]. In the original transformer\n work (Vaswani et al., 2017), d was 512, dk and dv were both 64.\n Multi-head Attention Equations 8.11-8.13 describe a single attention head. But\n actually, transformers use multiple attention heads. The intuition is that each head\n might be attending to the context for different purposes: heads might be specialized to represent different linguistic relationships between context elements and the\n current token, or to look for particular kinds of patterns in the context.\n multi-head So in multi-head attention we have A separate attention heads that reside in\n attention\n parallel layers at the same depth in a model, each with its own set of parameters that\n allows the head to model different aspects of the relationships among inputs. Thus\n 8.2 ‚Ä¢ T RANSFORMER B LOCKS 7\n\n each head i in a self-attention layer has its own set of query, key, and value matrices:\n WQi , WKi , and WVi . These are used to project the inputs into separate query, key,\n and value embeddings for each head.\n When using multiple heads the model dimension d is still used for the input\n and output, the query and key embeddings have dimensionality dk , and the value\n embeddings are of dimensionality dv (again, in the original transformer paper dk =\n dv = 64, A = 8, and d = 512). Thus for each head i, we have weight layers WQi of\n shape [d √ó dk ], WKi of shape [d √ó dk ], and WVi of shape [d √ó dv ].\n Below are the equations for attention augmented with multiple heads; Fig. 8.5\n shows an intuition.\n qci = xi WQc ; kcj = x j WKc ; vcj = x j WVc ; ‚àÄ c 1 ‚â§ c ‚â§ A (8.15)\n qci ¬∑ kcj\n scorec (xi , x j ) = ‚àö (8.16)\n dk\n Œ±icj = softmax(scorec (xi , x j )) ‚àÄ j ‚â§ i (8.17)\n\n headci = Œ±icj vcj\n X\n (8.18)\n j‚â§i\n\n ai = (head1 ‚äï head2 ... ‚äï headA )WO (8.19)\n MultiHeadAttention(xi , [x1 , ¬∑ ¬∑ ¬∑ , xi‚àí1 ]) = ai (8.20)\n\n Note in Eq. 8.20 that MultiHeadAttention is a function of the current input xi , as\n well as all the other inputs. For the causal or left-to-right attention that we use in\n this chapter, the other inputs are only to the left, but we‚Äôll also see a version of\n attention in Chapter 10 where attention is a function of the tokens to the right as\n well. We‚Äôll return to this idea about causal inputs in Eq. 8.34 when we introduce the\n idea of masking the right context.\n The output of each of the A heads is of shape [1 √ó dv ], and so the output of the\n multi-head layer with A heads consists of A vectors of shape [1 √ó dv ]. These are\n concatenated to produce a single output with dimensionality [1 √ó Adv ]. Then we use\n yet another linear projection WO ‚àà RAdv √ód to reshape it, resulting in the multi-head\n attention vector ai with the correct output shape [1 √ó d] at each input i.\n\n The self-attention calculation lies at the core of what‚Äôs called a transformer block,\n which, in addition to the self-attention layer, includes three other kinds of layers: (1)\n a feedforward layer, (2) residual connections, and (3) normalizing layers (colloquially called ‚Äúlayer norm‚Äù).\n Fig. 8.6 illustrates a transformer block, sketching a common way of thinking\nresidual stream about the block that is called the residual stream (Elhage et al., 2021). In the residual stream viewpoint, we consider the processing of an individual token i through\n the transformer block as a single stream of d-dimensional representations for token\n position i. This residual stream starts with the original input vector, and the various\n components read their input from the residual stream and add their output back into\n the stream.\n The input at the bottom of the stream is an embedding for a token, which has\n dimensionality d. This initial embedding gets passed up (by residual connections),\n and is progressively added to by the other components of the transformer: the at-\n8 C HAPTER 8 ‚Ä¢ T RANSFORMERS\n\n ai\n [1 x d]\n Project down to d WO [Adv x d]\n ‚Ä¶ [1 x Adv ]\n Concatenate Outputs\n\n [1 x dv ] [1 x dv ]\n Each head Head 1 Head 2 Head 8\n attends diÔ¨Äerently ‚Ä¶\n WK1 WV1 WQ1 WK2 WV2 WQ2 WK8 WV8 WQ8\n to context\n\n ‚Ä¶ xi-3 xi-2 xi-1 xi [1 ax d]\n i\nlayer has A heads, each with its own query, key, and value weight matrices. The outputs from each of the heads\nare concatenated and then projected down to d, thus producing an output of the same size as the input.\n\n hi-1 hi hi+1\n Residual\n Stream\n\n Feedforward\n\n Layer Norm\n ‚Ä¶ ‚Ä¶\n MultiHead\n Attention\n\n Layer Norm\n\n xi-1 xi xi+1\n\n figure shows the prenorm version of the architecture, in which the layer norms happen before\n the attention and feedforward layers rather than after.\n\n tention layer that we have seen, and the feedforward layer that we will introduce.\n Before the attention and feedforward layer is a computation called the layer norm.\n Thus the initial vector is passed through a layer norm and attention layer, and\n the result is added back into the stream, in this case to the original input vector\n xi . And then this summed vector is again passed through another layer norm and a\n feedforward layer, and the output of those is added back into the residual, and we‚Äôll\n use hi to refer to the resulting output of the transformer block for token i. (In earlier\n descriptions the residual stream was often described using a different metaphor as\n residual connections that add the input of a component to its output, but the residual\n stream is a more perspicuous way of visualizing the transformer.)\n 8.2 ‚Ä¢ T RANSFORMER B LOCKS 9\n\n We‚Äôve already seen the attention layer, so let‚Äôs now introduce the feedforward\n and layer norm computations in the context of processing a single input xi at token\n position i.\n\n Feedforward layer The feedforward layer is a fully-connected 2-layer network,\n i.e., one hidden layer, two weight matrices, as introduced in Chapter 6. The weights\n are the same for each token position i, but are different from layer to layer. It is common to make the dimensionality dff of the hidden layer of the feedforward network\n be larger than the model dimensionality d. (For example in the original transformer\n model, d = 512 and dff = 2048.)\n\n FFN(xi ) = ReLU(xi W1 + b1 )W2 + b2 (8.21)\n\n Layer Norm At two stages in the transformer block we normalize the vector (Ba\nlayer norm et al., 2016). This process, called layer norm (short for layer normalization), is one\n of many forms of normalization that can be used to improve training performance\n in deep neural networks by keeping the values of a hidden layer in a range that\n facilitates gradient-based training.\n Layer norm is a variation of the z-score from statistics, applied to a single vector in a hidden layer. That is, the term layer norm is a bit confusing; layer norm\n is not applied to an entire transformer layer, but just to the embedding vector of a\n single token. Thus the input to layer norm is a single vector of dimensionality d\n and the output is that vector normalized, again of dimensionality d. The first step in\n layer normalization is to calculate the mean, ¬µ, and standard deviation, œÉ , over the\n elements of the vector to be normalized. Given an embedding vector x of dimensionality d, these values are calculated as follows.\n\n d\n 1X\n ¬µ = xi (8.22)\n d\n i=1\n v\n u d\n u1 X\n œÉ = t (xi ‚àí ¬µ)2 (8.23)\n d\n i=1\n\n Given these values, the vector components are normalized by subtracting the mean\n from each and dividing by the standard deviation. The result of this computation is\n a new vector with zero mean and a standard deviation of one.\n\n (x ‚àí ¬µ)\n xÃÇ = (8.24)\n œÉ\n\n Finally, in the standard implementation of layer normalization, two learnable parameters, Œ≥ and Œ≤ , representing gain and offset values, are introduced.\n\n (x ‚àí ¬µ)\n LayerNorm(x) = Œ≥ +Œ≤ (8.25)\n œÉ\n\n Putting it all together The function computed by a transformer block can be expressed by breaking it down with one equation for each component computation,\n using t (of shape [1 √ó d]) to stand for transformer and superscripts to demarcate\n10 C HAPTER 8 ‚Ä¢ T RANSFORMERS\n\n each computation inside the block:\n\n t1i = LayerNorm(xi ) (8.26)\n = MultiHeadAttention(ti , t11 , ¬∑ ¬∑ ¬∑ , t1N )\n \u0002 \u0003\n ti (8.27)\n 3 2\n ti = ti + xi (8.28)\n ti = LayerNorm(ti )\n 4 3\n (8.29)\n t5i = FFN(t4i ) (8.30)\n 5 3\n hi = ti + ti (8.31)\n\n Notice that the only component that takes as input information from other tokens\n (other residual streams) is multi-head attention, which (as we see from Eq. 8.27)\n looks at all the neighboring tokens in the context. The output from attention, however, is then added into this token‚Äôs embedding stream. In fact, Elhage et al. (2021)\n show that we can view attention heads as literally moving information from the\n residual stream of a neighboring token into the current stream. The high-dimensional\n embedding space at each position thus contains information about the current token and about neighboring tokens, albeit in different subspaces of the vector space.\n Fig. 8.7 shows a visualization of this movement.\n\n Token A Token B\n residual residual\n stream stream\n\n token B‚Äôs residual stream.\n\n Crucially, the input and output dimensions of transformer blocks are matched so\n they can be stacked. Each token vector xi at the input to the block has dimensionality\n d, and the output hi also has dimensionality d. Transformers for large language\n models stack many of these blocks, from 12 layers (used for the T5 or GPT-3-small\n language models) to 96 layers (used for GPT-3 large), to even more for more recent\n models. We‚Äôll come back to this issue of stacking in a bit.\n Equation 8.26 and following are just the equation for a single transformer block,\n but the residual stream metaphor goes through all the transformer layers, from the\n first transformer blocks to the 12th, in a 12-layer transformer. At the earlier transformer blocks, the residual stream is representing the current token. At the highest\n transformer blocks, the residual stream is usually representing the following token,\n since at the very end it‚Äôs being trained to predict the next token.\n Once we stack many blocks, there is one more requirement: at the very end of\n the last (highest) transformer block, there is a single extra layer norm that is run on\n the last hi of each token stream (just below the language model head layer that we\n will define soon). 3\n 3 Note that we are using the most common current transformer architecture, which is called the prenorm\n 8.3 ‚Ä¢ PARALLELIZING COMPUTATION USING A SINGLE MATRIX X 11\n\n This description of multi-head attention and the rest of the transformer block has\n been from the perspective of computing a single output at a single time step i in\n a single residual stream. But as we pointed out earlier, the attention computation\n performed for each token to compute ai is independent of the computation for each\n other token, and that‚Äôs also true for all the computation in the transformer block\n computing hi from the input xi . That means we can easily parallelize the entire\n computation, taking advantage of efficient matrix multiplication routines.\n We do this by packing the input embeddings for the N tokens of the input sequence into a single matrix X of size [N √ó d]. Each row of X is the embedding of\n one token of the input. Transformers for large language models commonly have an\n input length N from 1K to 32K; much longer contexts of 128K or even up to millions\n of tokens can also be achieved with architectural changes like special long-context\n mechanisms that we don‚Äôt discuss here. So for vanilla transformers, we can think of\n X having between 1K and 32K rows, each of the dimensionality of the embedding\n d (the model dimension).\n Parallelizing attention Let‚Äôs first see this for a single attention head and then turn\n to multiple heads, and then add in the rest of the components in the transformer\n block. For one head we multiply X by the query, key, and value matrices WQ of\n shape [d √ó dk ], WK of shape [d √ó dk ], and WV of shape [d √ó dv ], to produce matrices\n Q of shape [N √ó dk ], K of shape [N √ó dk ], and V of shape [N √ó dv ], containing all the\n key, query, and value vectors:\n\n Q = XWQ ; K = XWK ; V = XWV (8.32)\n\n Given these matrices we can compute all the requisite query-key comparisons simultaneously by multiplying Q and K| in a single matrix multiplication. The product is\n of shape N √ó N, visualized in Fig. 8.8.\n\n q1‚Ä¢k1 q1‚Ä¢k2 q1‚Ä¢k3 q1‚Ä¢k4\n\n q2‚Ä¢k1 q2‚Ä¢k2 q2‚Ä¢k3 q2‚Ä¢k4\n N\n q3‚Ä¢k1 q3‚Ä¢k2 q3‚Ä¢k3 q3‚Ä¢k4\n\n q4‚Ä¢k1 q4‚Ä¢k2 q4‚Ä¢k3 q4‚Ä¢k4\n\n N\n\n single matrix multiple.\n\n Once we have this QK| matrix, we can very efficiently scale these scores, take\n the softmax, and then multiply the result by V resulting in a matrix of shape N √ó d:\n a vector embedding representation for each token in the input. We‚Äôve reduced the\n entire self-attention step for an entire sequence of N tokens for one head to the\n architecture. The original definition of the transformer in Vaswani et al. (2017) used an alternative architecture called the postnorm transformer in which the layer norm happens after the attention and FFN\n layers; it turns out moving the layer norm beforehand works better, but does require this one extra layer\n at the end.\n12 C HAPTER 8 ‚Ä¢ T RANSFORMERS\n\n following computation:\n\n QK|\n \u0012 \u0012 \u0013\u0013\n head = softmax mask ‚àö V (8.33)\n dk\n A = head WO (8.34)\n\n Masking out the future You may have noticed that we introduced a mask function\n in Eq. 8.34 above. This is because the self-attention computation as we‚Äôve described\n it has a problem: the calculation of QK| results in a score for each query value to\n every key value, including those that follow the query. This is inappropriate in the\n setting of language modeling: guessing the next word is pretty simple if you already\n know it! To fix this, the elements in the upper-triangular portion of the matrix are set\n to ‚àí‚àû, which the softmax will turn to zero, thus eliminating any knowledge of words\n that follow in the sequence. This is done in practice by adding a mask matrix M in\n which Mi j = ‚àí‚àû ‚àÄ j > i (i.e. for the upper-triangular portion) and Mi j = 0 otherwise.\n Fig. 8.9 shows the resulting masked QK| matrix. (we‚Äôll see in Chapter 10 how to\n make use of words in the future for tasks that need it).\n\n q1‚Ä¢k1 ‚àí‚àû ‚àí‚àû ‚àí‚àû\n\n q2‚Ä¢k1 q2‚Ä¢k2 ‚àí‚àû ‚àí‚àû\n N\n q3‚Ä¢k1 q3‚Ä¢k2 q3‚Ä¢k3 ‚àí‚àû\n\n q4‚Ä¢k1 q4‚Ä¢k2 q4‚Ä¢k3 q4‚Ä¢k4\n\n N\n\n Fig. 8.10 shows a schematic of all the computations for a single attention head\n parallelized in matrix form.\n Fig. 8.8 and Fig. 8.9 also make it clear that attention is quadratic in the length\n of the input, since at each layer we need to compute dot products between each pair\n of tokens in the input. This makes it expensive to compute attention over very long\n documents (like entire novels). Nonetheless modern large language models manage\n to use quite long contexts of thousands or tens of thousands of tokens.\n\n Parallelizing multi-head attention In multi-head attention, as with self-attention,\n the input and output have the model dimension d, the key and query embeddings\n have dimensionality dk , and the value embeddings are of dimensionality dv (again,\n in the original transformer paper dk = dv = 64, A = 8, and d = 512). Thus for\n each head c, we have weight layers WQ c of shape [d √ó dk ], WK c of shape [d √ó dk ],\n and WV c of shape [d √ó dv ], and these get multiplied by the inputs packed into X to\n produce Q of shape [N √ó dk ], K of shape [N √ó dk ], and V of shape [N √ó dv ]. The\n output of each of the A heads is of shape [N √ó dv ], and so the output of the multihead layer with A heads consists of A matrices of shape [N √ó dv ]. To make use\n of these matrices in further processing, they are concatenated to produce a single\n output with dimensionality [N √ó Adv ]. Finally, we use a final linear projection WO\n of shape [Adv √ó d], that reshapes it to the original output dimension for each token.\n Multiplying the concatenated [N √ó Adv ] matrix output by WO of shape [Adv √ó d]\n 8.3 ‚Ä¢ PARALLELIZING COMPUTATION USING A SINGLE MATRIX X 13\n\n X Q X K X V\n Input\n WQ Query Input WK Key Input WV Value\n Token 1 Token 1 Token 1 Token 1 Token 1\n Token 1\n Input Input Key Input Value\n Query\n Token 2 Token 2 Token 2 Token 2\n Input x =\n Token 2\n x = Key\n x =\n Token 2\n Query Input Input Value\n Token 3 Token 3 Token 3 Token 3 Token 3\n Token 3\n Input Input Key Input Value\n Query\n Token 4 Token 4 Token 4 Token 4\n Token 4 d x dk d x dv Token 4\n d x dk\n Nxd N x dk Nxd N x dk N x dv\n Nxd\n\n Q KT QKT QKT masked V A\n\n q1\n x = ‚àí‚àû ‚àí‚àû ‚àí‚àû v1 a1\n k1\n\n k2\n\n k3\n\n k4\n\n q1‚Ä¢k1 q1‚Ä¢k2 q1‚Ä¢k3 q1‚Ä¢k4 q1‚Ä¢k1\n q1‚Ä¢k1\n\n mask q2 q2‚Ä¢k1 q2‚Ä¢k2 q2‚Ä¢k3 q2‚Ä¢k4 = q2‚Ä¢k1 q2‚Ä¢k2 ‚àí‚àû ‚àí‚àû x v2 = a2\n\n q3 q3‚Ä¢k1 q3‚Ä¢k2 q3‚Ä¢k3 q3‚Ä¢k4 q3‚Ä¢k1 q3‚Ä¢k2 q3‚Ä¢k3 ‚àí‚àû v3 a3\n\n q4 dk x N q4‚Ä¢k1 q4‚Ä¢k2 q4‚Ä¢k3 q4‚Ä¢k4 q4‚Ä¢k1 q4‚Ä¢k2 q4‚Ä¢k3 q4‚Ä¢k4 v4 a4\n\n N x dk NxN NxN N x dv N x dv\n\nthe computation of the Q, K, and V matrices. The second row shows the computation of QKT , the masking\n(the softmax computation and the normalizing by dimensionality are not shown) and then the weighted sum of\nthe value vectors to get the final attention vectors.\n\n yields the self-attention output A of shape [N √ó d].\n\n Qi = XWQi ; Ki = XWKi ; Vi = XWVi (8.35)\n \u0012 \u0012 i i | \u0013\u0013\n QK\n headi = SelfAttention(Q , K , V ) = softmax mask ‚àö\n i i i\n Vi (8.36)\n dk\n MultiHeadAttention(X) = (head1 ‚äï head2 ... ‚äï headA )WO (8.37)\n\n Putting it all together with the parallel input matrix X The function computed\n in parallel by an entire layer of N transformer blocks‚Äîeach block over one of the N\n input tokens‚Äîcan be expressed as:\n\n O = X + MultiHeadAttention(LayerNorm(X)) (8.38)\n H = O + FFN(LayerNorm(O)) (8.39)\n\n Note that in Eq. 8.38 we are using X to mean the input to the layer, wherever it\n comes from. For the first layer, as we will see in the next section, that input is the\n initial word + positional embedding vectors that we have been describing by X. But\n for subsequent layers k, the input is the output from the previous layer Hk‚àí1 . We\n can also break down the computation performed in a transformer layer, showing one\n equation for each component computation. We‚Äôll use T (of shape [N √ó d]) to stand\n for transformer and superscripts to demarcate each computation inside the block,\n and again use X to mean the input to the block from the previous layer or the initial\n14 C HAPTER 8 ‚Ä¢ T RANSFORMERS\n\n embedding:\n\n T1 = LayerNorm(X) (8.40)\n T 2\n = MultiHeadAttention(T )\n (8.41)\n T3 = T2 + X (8.42)\n T4 = LayerNorm(T3 ) (8.43)\n T 5\n = FFN(T )4\n (8.44)\n 5 3\n H = T +T (8.45)\n\n Here when we use a notation like FFN(T3 ) we mean that the same FFN is applied\n in parallel to each of the N embedding vectors in the window. Similarly, each of the\n N tokens is normed in parallel in the LayerNorm. Crucially, the input and output\n dimensions of transformer blocks are matched so they can be stacked. Since each\n token xi at the input to the block is represented by an embedding of dimensionality\n [1 √ó d], that means the input X and output H are both of shape [N √ó d].\n\n Let‚Äôs talk about where the input X comes from. Given a sequence of N tokens (N is\n embedding the context length in tokens), the matrix X of shape [N √ó d] has an embedding for\n each word in the context. The transformer does this by separately computing two\n embeddings: an input token embedding, and an input positional embedding.\n A token embedding, introduced in Chapter 6, is a vector of dimension d that will\n be our initial representation for the input token. (As we pass vectors up through the\n transformer layers in the residual stream, this embedding representation will change\n and grow, incorporating context and playing a different role depending on the kind\n of language model we are building.) The set of initial embeddings are stored in the\n embedding matrix E, which has a row for each of the |V | tokens in the vocabulary.\n (Reminder that V here means the vocabulary of tokens, this V is not related to the\n value vector.) Thus each word is a row vector of d dimensions, and E has shape\n [|V | √ó d].\n Given an input token string like Thanks for all the we first convert the tokens\n into vocabulary indices (these were created when we first tokenized the input using\n BPE or SentencePiece). So the representation of thanks for all the might be w =\n [5, 4000, 10532, 2224]. Next we use indexing to select the corresponding rows from\n E, (row 5, row 4000, row 10532, row 2224).\n Another way to think about selecting token embeddings from the embedding\n matrix is to represent tokens as one-hot vectors of shape [1 √ó |V |], i.e., with one\n one-hot vector dimension for each word in the vocabulary. Recall that in a one-hot vector all the\n elements are 0 except one, the element whose dimension is the word‚Äôs index in the\n vocabulary, which has value 1. So if the word ‚Äúthanks‚Äù has index 5 in the vocabulary,\n x5 = 1, and xi = 0 ‚àÄi 6= 5, as shown here:\n [0 0 0 0 1 0 0 ... 0 0 0 0]\n 1 2 3 4 5 6 7 ... ... |V|\n Multiplying by a one-hot vector that has only one non-zero element xi = 1 simply\n selects out the relevant row vector for word i, resulting in the embedding for word i,\n as depicted in Fig. 8.11.\n 8.4 ‚Ä¢ T HE INPUT: EMBEDDINGS FOR TOKEN AND POSITION 15\n\n d\n\n 5 |V| 5 d\n 1 0000100‚Ä¶0000 ‚úï E = 1\n\n matrix E with a one-hot vector with a 1 in index 5.\n\n We can extend this idea to represent the entire token sequence as a matrix of onehot vectors, one for each of the N positions in the transformer‚Äôs context window, as\n shown in Fig. 8.12.\n\n d\n |V| d\n 0000100‚Ä¶0000\n 0000000‚Ä¶0010\n 1000000‚Ä¶0000 ‚úï E =\n ‚Ä¶\n N 0000100‚Ä¶0000\n N\n | V|\n\n These token embeddings are not position-dependent. To represent the position\n of each token in the sequence, we combine these token embeddings with positional\n positional\nembeddings embeddings specific to each position in an input sequence.\n Where do we get these positional embeddings? The simplest method, called\n absolute\n position absolute position, is to start with randomly initialized embeddings corresponding\n to each possible input position up to some maximum length. For example, just as\n we have an embedding for the word fish, we‚Äôll have an embedding for the position 3.\n As with word embeddings, these positional embeddings are learned along with other\n parameters during training. We can store them in a matrix Epos of shape [N √ó d].\n To produce an input embedding that captures positional information, we just\n add the word embedding for each input to its corresponding positional embedding.\n The individual token and position embeddings are both of size [1√ód], so their sum is\n also [1√ód], This new embedding serves as the input for further processing. Fig. 8.13\n shows the idea.\n\n Transformer Block\n\n X = Composite\n Embeddings\n (word + position)\n\n Word\n Janet\n\n back\n will\n\n the\n\n bill\n\n Embeddings\n Position\n\n Embeddings\n Janet will back the bill\n\n the token embedding to produce a new embedding of the same dimensionality.\n16 C HAPTER 8 ‚Ä¢ T RANSFORMERS\n\n The final representation of the input, the matrix X, is an [N √ó d] matrix in which\n each row i is the representation of the ith token in the input, computed by adding\n E[id(i)]‚Äîthe embedding of the id of the token that occurred at position i‚Äî, to P[i],\n the positional embedding of position i.\n A potential problem with the simple position embedding approach is that there\n will be plenty of training examples for the initial positions in our inputs and correspondingly fewer at the outer length limits. These latter embeddings may be poorly\n trained and may not generalize well during testing. An alternative is to choose a\n static function that maps integer inputs to real-valued vectors in a way that better\n handles sequences of arbitrary length. A combination of sine and cosine functions\n with differing frequencies was used in the original transformer work. Sinusoidal position embeddings may also help in capturing the inherent relationships among the\n positions, like the fact that position 4 in an input is more closely related to position\n 5 than it is to position 17.\n A more complex style of positional embedding methods extend this idea of caprelative\n position turing relationships even further to directly represent relative position instead of\n absolute position, often implemented in the attention mechanism at each layer rather\n than being added once at the initial input.\n\n The last component of the transformer we must introduce is the language modeling\n language\n modeling head head. Here we are using the word head to mean the additional neural circuitry we\n head add on top of the basic transformer architecture when we apply pretrained transformer models to various tasks. The language modeling head is the circuitry we\n need to do language modeling.\n Recall that language models, from the simple n-gram models of Chapter 3 through\n the feedforward and RNN language models of Chapter 6 and Chapter 13, are word\n predictors. Given a context of words, they assign a probability to each possible next\n word. For example, if the preceding context is ‚ÄúThanks for all the‚Äù and we want to\n know how likely the next word is ‚Äúfish‚Äù we would compute:\n\n P(fish|Thanks for all the)\n\n Language models give us the ability to assign such a conditional probability to every\n possible next word, giving us a distribution over the entire vocabulary. The n-gram\n language models of Chapter 3 compute the probability of a word given counts of\n its occurrence with the n ‚àí 1 prior words. The context is thus of size n ‚àí 1. For\n transformer language models, the context is the size of the transformer‚Äôs context\n window, which can be quite large, like 32K tokens for large models (and much larger\n contexts of millions of words are possible with special long-context architectures).\n The job of the language modeling head is to take the output of the final transformer layer from the last token N and use it to predict the upcoming word at position N + 1. Fig. 8.14 shows how to accomplish this task, taking the output of the last\n token at the last layer (the d-dimensional output embedding of shape [1 √ó d]) and\n producing a probability distribution over words (from which we will choose one to\n generate).\n The first module in Fig. 8.14 is a linear layer, whose job is to project from the\n output hLN , which represents the output token embedding at position N from the final\n 8.5 ‚Ä¢ T HE L ANGUAGE M ODELING H EAD 17\n\n y1 y2 ‚Ä¶ y|V| Word probabilities 1 x |V|\n\n Language Model Head Softmax over vocabulary V\n takes hLN and outputs a u1 u2 ‚Ä¶ u|V| Logits 1 x |V|\n distribution over vocabulary V\n Unembedding layer Unembedding layer d x |V|\n U = ET\n\n hL1 hL2 hLN 1xd\n Layer L\n Transformer\n Block\n ‚Ä¶\n w1 w2 wN\n\nembedding for token N from the last transformer layer (hLN ) to a probability distribution over words in the\nvocabulary V .\n\n logit block L, (hence of shape [1 √ó d]) to the logit vector, or score vector, that will have a\n single score for each of the |V | possible words in the vocabulary V . The logit vector\n u is thus of dimensionality [1 √ó |V |].\n This linear layer can be learned, but more commonly we tie this matrix to (the\n weight tying transpose of) the embedding matrix E. Recall that in weight tying, we use the\n same weights for two different matrices in the model. Thus at the input stage of the\n transformer the embedding matrix (of shape [|V | √ó d]) is used to map from a one-hot\n vector over the vocabulary (of shape [1 √ó |V |]) to an embedding (of shape [1 √ó d]).\n And then in the language model head, ET , the transpose of the embedding matrix (of\n shape [d √ó |V |]) is used to map back from an embedding (shape [1 √ó d]) to a vector\n over the vocabulary (shape [1√ó|V |]). In the learning process, E will be optimized to\n be good at doing both of these mappings. We therefore sometimes call the transpose\n unembedding ET the unembedding layer because it is performing this reverse mapping.\n A softmax layer turns the logits u into the probabilities y over the vocabulary.\n\n u = hLN ET (8.46)\n y = softmax(u) (8.47)\n\n We can use these probabilities to do things like help assign a probability to a\n given text. But the most important usage is to generate text, which we do by sampling a word from these probabilities y. We might sample the highest probability\n word (‚Äògreedy‚Äô decoding), or use another of the sampling methods from Section ??\n or Section 8.6.\n In either case, whatever entry yk we choose from the probability vector y, we\n generate the word that has that index k.\n Fig. 8.15 shows the total stacked architecture for one token i. Note that the input\n to each transformer layer xi` is the same as the output from the preceding layer hi`‚àí1 .\n A terminological note before we conclude: You will sometimes see a transformer used for this kind of unidirectional causal language model called a decoderdecoder-only only model. This is because this model constitutes roughly half of the encodermodel\n decoder model for transformers that we‚Äôll see how to apply to machine translation\n in Chapter 12. (Confusingly, the original introduction of the transformer had an\n encoder-decoder architecture, and it was only later that the standard paradigm for\n18 C HAPTER 8 ‚Ä¢ T RANSFORMERS\n\n Token probabilities y1 y2 ‚Ä¶ y|V| wi+1\n Sample token to\n softmax generate at position i+1\n Language\n Modeling\n Head logits u1 u2 ‚Ä¶ u|V|\n\n U\n\n hLi\n feedforward\n layer norm\n Layer L\n attention\n layer norm\n hL-1i = xLi\n ‚Ä¶\n h2i = x3i\n feedforward\n layer norm\n Layer 2\n attention\n layer norm\n\n h1i = x2i\n feedforward\n layer norm\n Layer 1\n attention\n layer norm\n\n x1i\n + i\n Input\n Encoding E\n\n Input token wi\n\n and mapping from an input token wi to to a predicted next token wi+1 .\n\n causal language model was defined by using only the decoder part of this original\n architecture).\n\n The sampling methods we introduce below each have parameters that enable trading off two important factors in generation: quality and diversity. Methods that\n emphasize the most probable words tend to produce generations that are rated by\n people as more accurate, more coherent, and more factual, but also more boring\n and more repetitive. Methods that give a bit more weight to the middle-probability\n words tend to be more creative and more diverse, but less factual and more likely to\n be incoherent or otherwise low-quality.\n\n 8.6.1 Top-k sampling\ntop-k sampling Top-k sampling is a simple generalization of greedy decoding. Instead of choosing\n the single most probable word to generate, we first truncate the distribution to the\n 8.7 ‚Ä¢ T RAINING 19\n\n top k most likely words, renormalize to produce a legitimate probability distribution,\n and then randomly sample from within these k words according to their renormalized\n probabilities. More formally:\n 1. Choose in advance a number of words k\n 2. For each word in the vocabulary V , use the language model to compute the\n likelihood of this word given the context p(wt |w<t )\n 3. Sort the words by their likelihood, and throw away any word that is not one of\n the top k most probable words.\n 4. Renormalize the scores of the k words to be a legitimate probability distribution.\n 5. Randomly sample a word from within these remaining k most-probable words\n according to its probability.\n When k = 1, top-k sampling is identical to greedy decoding. Setting k to a larger\n number than 1 leads us to sometimes select a word which is not necessarily the most\n probable, but is still probable enough, and whose choice results in generating more\n diverse but still high-enough-quality text.\n\n 8.6.2 Nucleus or top-p sampling\n One problem with top-k sampling is that k is fixed, but the shape of the probability\n distribution over words differs in different contexts. If we set k = 10, sometimes\n the top 10 words will be very likely and include most of the probability mass, but\n other times the probability distribution will be flatter and the top 10 words will only\n include a small part of the probability mass.\ntop-p sampling An alternative, called top-p sampling or nucleus sampling (Holtzman et al.,\n 2020), is to keep not the top k words, but the top p percent of the probability mass.\n The goal is the same; to truncate the distribution to remove the very unlikely words.\n But by measuring probability rather than the number of words, the hope is that the\n measure will be more robust in very different contexts, dynamically increasing and\n decreasing the pool of word candidates.\n Given a distribution P(wt |w<t ), we sort the distribution from most probable, and\n then the top-p vocabulary V (p) is the smallest set of words such that\n X\n P(w|w<t ) ‚â• p. (8.48)\n w‚ààV (p)\n\n We described the training process for language models in the prior chapter. Recall that large language models are trained with cross-entropy loss, also called the\n negative log likelihood loss. At time t the cross-entropy loss is the negative log probability the model assigns to the next word in the training sequence, ‚àí log p(wt+1 ).\n Fig. 8.16 illustrates the general training approach. At each step, given all the\n preceding words, the final transformer layer produces an output distribution over the\n entire vocabulary. During training, the probability assigned to the correct word by\n the model is used to calculate the cross-entropy loss for each item in the sequence.\n The loss for a training sequence is the average cross-entropy loss over the entire\n sequence. The weights in the network are adjusted to minimize the average CE loss\n over the training sequence via gradient descent.\n20 C HAPTER 8 ‚Ä¢ T RANSFORMERS\n\n Next token long and thanks for all ‚Ä¶\n log yand ‚Ä¶ =\n <latexit sha1_base64=\"AovqpaL476UmJ1EU1xZPgDZ70tQ=\">AAAB9nicbVDLSsNAFL2pr1pfURcu3AwWwY0lEakui25cVrAPaEqYTCbt0EkmzEzEEvIrbkTcKPgZ/oJ/Y9Jm09YDA4dzznDvPV7MmdKW9WtU1tY3Nreq27Wd3b39A/PwqKtEIgntEMGF7HtYUc4i2tFMc9qPJcWhx2nPm9wXfu+ZSsVE9KSnMR2GeBSxgBGsc8k1Ty4dLkZo6qZOiPVYhimO/CyruWbdalgzoFVil6QOJdqu+eP4giQhjTThWKmBbcV6mGKpGeE0qzmJojEmEzyi6WztDJ3nko8CIfMXaTRTF3I4VGoaenmy2E0te4X4nzdIdHA7TFkUJ5pGZD4oSDjSAhUdIJ9JSjSf5gQTyfINERljiYnOmypOt5cPXSXdq4bdbDQfr+utu7KEKpzCGVyADTfQggdoQwcIZPAGn/BlvBivxrvxMY9WjPLPMSzA+P4DPEiSHA==</latexit>\n\n log ythanks\n <latexit sha1_base64=\"q3ZgXDyG7qtkT7t8hT47RdlwYG4=\">AAAB+XicbVDLSsNAFJ3UV62vWHe6GVsEN5bERXUlBUVcVrAPaEqYTCft0MlMmJkIIQT8AT/CTRE3Cv6Ev+DfmLTdtPXAwOGcM9x7jxcyqrRl/RqFtfWNza3idmlnd2//wDwst5WIJCYtLJiQXQ8pwignLU01I91QEhR4jHS88W3ud56JVFTwJx2HpB+gIac+xUhnkmseXzhMDGHsJk6A9EgGiR4hPlZpWnLNqlWzpoCrxJ6TauP0tXw3qdw0XfPHGQgcBYRrzJBSPdsKdT9BUlPMSFpyIkVChMdoSJLp5ik8y6QB9IXMHtdwqi7kUKBUHHhZMl9PLXu5+J/Xi7R/3U8oDyNNOJ4N8iMGtYB5DXBAJcGaxRlBWNJsQ4hHSCKss7Ly0+3lQ1dJ+7Jm12v1x6yDezBDEZyACjgHNrgCDfAAmqAFMHgBE/AJvozEeDPejY9ZtGDM/xyBBRjff79pldo=</latexit>\n\n Loss\n\n Language\n Modeling\n logits logits logits logits logits ‚Ä¶\n Head U U U U U\n\n Stacked\n Transformer\n ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶\n Blocks\n\n x1 x2 x3 x4 x5 ‚Ä¶\n + 1 + 2 + 3 + 4 + 5\n Input\n Encoding E E E E E\n ‚Ä¶\n\n Input tokens So long and thanks for\n\n With transformers, each training item can be processed in parallel since the output for each element in the sequence is computed separately.\n Large models are generally trained by filling the full context window (for example 4096 tokens for GPT4 or 8192 for Llama 3) with text. If documents are shorter\n than this, multiple documents are packed into the window with a special end-of-text\n token between them. The batch size for gradient descent is usually quite large (the\n largest GPT-3 model uses a batch size of 3.2 million tokens).\n\n Large language models are large. For example the Llama 3.1 405B Instruct model\n from Meta has 405 billion parameters (L=126 layers, a model dimensionality of\n d=16,384, A=128 attention heads) and was trained on 15.6 terabytes of text tokens\n (Llama Team, 2024), using a vocabulary of 128K tokens. So there is a lot of research\n on understanding how LLMs scale, and especially how to implement them given\n limited resources. In the next few sections we discuss how to think about scale (the\n concept of scaling laws), and important techniques for getting language models to\n work efficiently, such as the KV cache and parameter-efficient fine tuning.\n\n 8.8.1 Scaling laws\n The performance of large language models has shown to be mainly determined by\n 3 factors: model size (the number of parameters not counting embeddings), dataset\n size (the amount of training data), and the amount of compute used for training. That\n is, we can improve a model by adding parameters (adding more layers or having\n wider contexts or both), by training on more data, or by training for more iterations.\n The relationships between these factors and performance are known as scaling\n scaling laws laws. Roughly speaking, the performance of a large language model (the loss) scales\n 8.8 ‚Ä¢ D EALING WITH S CALE 21\n\n as a power-law with each of these three properties of model training.\n For example, Kaplan et al. (2020) found the following three relationships for\n loss L as a function of the number of non-embedding parameters N, the dataset size\n D, and the compute budget C, for models training with limited parameters, dataset,\n or compute budget, if in each case the other two properties are held constant:\n\n Nc\n \u0012 \u0013Œ±N\n L(N) = (8.49)\n N\n Dc\n \u0012 \u0013 Œ±D\n L(D) = (8.50)\n D\n Cc\n \u0012 \u0013Œ±C\n L(C) = (8.51)\n C\n\n The number of (non-embedding) parameters N can be roughly computed as follows (ignoring biases, and with d as the input and output dimensionality of the\n model, dattn as the self-attention layer size, and dff the size of the feedforward layer):\n\n N ‚âà 2 d nlayer (2 dattn + dff )\n ‚âà 12 nlayer d 2 (8.52)\n (assuming dattn = dff /4 = d)\n\n Thus GPT-3, with n = 96 layers and dimensionality d = 12288, has 12 √ó 96 √ó\n 122882 ‚âà 175 billion parameters.\n The values of Nc , Dc , Cc , Œ±N , Œ±D , and Œ±C depend on the exact transformer\n architecture, tokenization, and vocabulary size, so rather than all the precise values,\n scaling laws focus on the relationship with loss.4\n Scaling laws can be useful in deciding how to train a model to a particular performance, for example by looking at early in the training curve, or performance with\n smaller amounts of data, to predict what the loss would be if we were to add more\n data or increase model size. Other aspects of scaling laws can also tell us how much\n data we need to add when scaling up a model.\n\n 8.8.2 KV Cache\n We saw in Fig. 8.10 and in Eq. 8.34 (repeated below) how the attention vector can\n be very efficiently computed in parallel for training, via two matrix multiplications:\n\n QK|\n \u0012 \u0013\n A = softmax ‚àö V (8.53)\n dk\n\n Unfortunately we can‚Äôt do quite the same efficient computation in inference as\n in training. That‚Äôs because at inference time, we iteratively generate the next tokens\n one at a time. For a new token that we have just generated, call it xi , we need to\n compute its query, key, and values by multiplying by WQ , WK , and WV respectively. But it would be a waste of computation time to recompute the key and value\n vectors for all the prior tokens x<i ; at prior steps we already computed these key\n and value vectors! So instead of recomputing these, whenever we compute the key\nKV cache and value vectors we store them in memory in the KV cache, and then we can just\n grab them from the cache when we need them. Fig. 8.17 modifies Fig. 8.10 to show\n22 C HAPTER 8 ‚Ä¢ T RANSFORMERS\n\n Q QKT V A\n KT v1\n\n x = x v2\n\n k1\n\n k2\n\n k3\n\n k4\n v3\n\n dk x N v4\n q4 q4‚Ä¢k1 q4‚Ä¢k2 q4‚Ä¢k3 q4‚Ä¢k4 a4\n\n 1 x dk 1xN N x dv 1 x dv\n\n the vectors that can be stored in the cache rather than recomputed when computing the attention score for the 4th token.\n\n the computation that takes place for a single new token, showing which values we\n can take from the cache rather than recompute.\n\n 8.8.3 Parameter Efficient Fine Tuning\n As we mentioned above, it‚Äôs very common to take a language model and give it more\n information about a new domain by finetuning it (continuing to train it to predict\n upcoming words) on some additional data.\n Fine-tuning can be very difficult with very large language models, because there\n are enormous numbers of parameters to train; each pass of batch gradient descent\n has to backpropagate through many many huge layers. This makes finetuning huge\n language models extremely expensive in processing power, in memory, and in time.\n For this reason, there are alternative methods that allow a model to be finetuned\n parameterwithout changing all the parameters. Such methods are called parameter-efficient\n efficient fine fine tuning or sometimes PEFT, because we efficiently select a subset of parameters\n tuning\n PEFT to update when finetuning. For example we freeze some of the parameters (don‚Äôt\n change them), and only update some particular subset of parameters.\n LoRA Here we describe one such model, called LoRA, for Low-Rank Adaptation. The\n intuition of LoRA is that transformers have many dense layers which perform matrix\n multiplication (for example the WQ , WK , WV , WO layers in the attention computation). Instead of updating these layers during finetuning, with LoRA we freeze these\n layers and instead update a low-rank approximation that has fewer parameters.\n Consider a matrix W of dimensionality [N √ó d] that needs to be updated during\n finetuning via gradient descent. Normally this matrix would get updates ‚àÜW of\n dimensionality [N √ó d], for updating the N √ó d parameters after gradient descent. In\n LoRA, we freeze W and update instead a low-rank decomposition of W. We create\n two matrices A and B, where A has size [N √ór] and B has size [r √ód], and we choose\n r to be quite small, r << min(d, N). During finetuning we update A and B instead\n of W. That is, we replace W + ‚àÜW with W + AB. Fig. 8.18 shows the intuition.\n For replacing the forward pass h = xW, the new forward pass is instead:\n\n h = xW + xAB (8.54)\n\n LoRA has a number of advantages. It dramatically reduces hardware requirements,\n since gradients don‚Äôt have to be calculated for most parameters. The weight updates\n can be simply added in to the pretrained weights, since AB is the same size as W).\n 4 For the initial experiment in Kaplan et al. (2020) the precise values were Œ±N = 0.076, Nc = 8.8 √ó1013\n (parameters), Œ±D = 0.095, Dc = 5.4 √ó1013 (tokens), Œ±C = 0.050, Cc = 3.1 √ó108 (petaflop-days).\n 8.9 ‚Ä¢ I NTERPRETING THE T RANSFORMER 23\n\n d\n h 1\n\n d\n √ó r B\n Pretrained\n Weights\n N N A\n W\n\n d r\n\n x 1\n d\n\n the updated AB.\n\n That means it doesn‚Äôt add any time during inference. And it also means it‚Äôs possible\n to build LoRA modules for different domains and just swap them in and out by\n adding them in or subtracting them from W.\n In its original version LoRA was applied just to the matrices in the attention\n computation (the WQ , WK , WV , and WO layers). Many variants of LoRA exist.\n\n How does a transformer-based language model manage to do so well at language\ninterpretability tasks? The subfield of interpretability, sometimes called mechanistic interpretability, focuses on ways to understand mechanistically what is going on inside the\n transformer. In the next two subsections we discuss two well-studied aspects of\n transformer interpretability.\n\n 8.9.1 In-Context Learning and Induction Heads\n As a way of getting a model to do what we want, we can think of prompting as being\n fundamentally different than pretraining. Learning via pretraining means updating\n the model‚Äôs parameters by using gradient descent according to some loss function.\n But prompting with demonstrations can teach a model to do a new task. The model\n is learning something about the task from those demonstrations as it processes the\n prompt.\n Even without demonstrations, we can think of the process of prompting as a kind\n of learning. For example, the further a model gets in a prompt, the better it tends\n to get at predicting the upcoming tokens. The information in the context is helping\n give the model more predictive power.\n in-context\n learning The term in-context learning was first proposed by Brown et al. (2020) in their\n introduction of the GPT3 system, to refer to either of these kinds of learning that lan-\n24 C HAPTER 8 ‚Ä¢ T RANSFORMERS\n\n guage models do from their prompts. In-context learning means language models\n learning to do new tasks, better predict tokens, or generally reduce their loss during the forward-pass at inference-time, without any gradient-based updates to the\n model‚Äôs parameters.\n How does in-context learning work? While we don‚Äôt know for sure, there are\ninduction heads some intriguing ideas. One hypothesis is based on the idea of induction heads\n (Elhage et al., 2021; Olsson et al., 2022). Induction heads are the name for a circuit,\n which is a kind of abstract component of a network. The induction head circuit\n is part of the attention computation in transformers, discovered by looking at mini\n language models with only 1-2 attention heads.\n The function of the induction head is to predict repeated sequences. For example\n if it sees the pattern AB...A in an input sequence, it predicts that B will follow,\n instantiating the pattern completion rule AB...A‚Üí B. It does this by having a prefix\n matching component of the attention computation that, when looking at the current\n token A, searches back over the context to find a prior instance of A. If it finds one,\n the induction head has a copying mechanism that ‚Äúcopies‚Äù the token B that followed\n the earlier A, by increasing the probability the B will occur next. Fig. 8.19 shows an\n example.\n\n Figure\n In the sequence head\n ‚Äú...vintage looking\n cars ... vintage‚Äù,atanvintage uses\n induction head the prefix\n identifies matching\n the initial mechanism\n occurrence of ‚Äúvintage‚Äù,to\n find a prior\n attends to theinstance\n subsequentofword ‚Äúcars‚Äù forand\n vintage, prefixthe copying\n matching, and mechanism\n predicts ‚Äúcars‚Äù to predict\n as the next wordthathrough will\n cars the occur\n copying\n mechanism.\n again. Figure from Crosbie and Shutova (2022).\n\n determines each head‚Äôs independent output for the 4.2 Identifying Induction Heads\n Olsson et al. (2022) propose that a generalized fuzzy version of this pattern comcurrent token. To identify\n pletion rule, implementing a rule like A*B*...A‚Üí\n Leveraging this decomposition, Elhage et al. sure the ability B,induction\n where heads\n A* ‚âàwithin\n A and models,\n B* ‚âà weB mea-\n (by\n of all attention heads to perform\n we mean\n ‚âà(2021) theyathey\n discovered arebehaviour\n distinct semantically\n in certainsimilar in some way), might be responsible\n prefix matching on random input sequences. We 4\n forattention\n in-context learning.\n heads, which Suggestive\n they named induction evidence for the\n heads. follow their hypothesis\n task-agnostic comes\n approach from Crosto computing preablating This\n bie andbehaviour\n Shutova emerges when who\n (2022), these heads\n showprocess\n that ablating induction\n fix matching headsbycauses\n scores outlined Bansal etin-context\n al. (2023).\n sequences of the form \"[A] [B] ... [A] ‚Üí \". In Weis argue that focusing solely on term\n prefix matching\n learning performance to decrease. Ablation originally a medical\n these heads, the QK circuit directs attention to- scores is sufficient for our analysis, as high premeaning\n the removal\n wards [B], whichofappears\n something. Wetheuse\n directly after it in NLP\n previous interpretability studies as a tool for\n fix matching cores specifically indicate induction\n testing\n occurrencecausal\n of theeffects; if we\n current token [A].knock out a hypothesized\n This behaviour heads, while less cause,\n relevantwe would\n heads tend toexpect\n show high the\n is termed\n effect prefix matching.\n to disappear. The OV\n Crosbie andcircuit subse- (2022)\n Shutova copyingablate induction\n capabilities (Bansalheads by first\n et al., 2023). We findgen-\n quently increases the output logit of the [B] token, erate a sequence of 50 random tokens, excluding\n ing attention heads that perform as induction heads on random input sequences, and\n termed copying. An overview of this echanism is the 4% most common and least common tokens.\n then\n shown zeroing\n in Figureout\n 1. the output of these heads by setting certain terms of the output ma-\nThis sequence is repeated four times to form the\n trix WO to zero. Indeed they find that ablated\n inputmodels are The\n to the model. much worse\n prefix at in-context\n matching score is cal-\n4 Methods\n learning: they have much worse performance at learning\n culated from\n by averaging the demonstrations\n attention values fromin the\n each\n prompts. token to the tokens that directly followed the same\n token in earlier repeats. The final prefix matching\n We utilise two recently developed open-source scores are averaged over five random sequences.\n 8.9.2 Logit\n models, namely Lens 2 and InternLM2-20B\n Llama-3-8B The prefix matching scores for Llama-3-8B are\n (Cai et al., 2024), both of which are based on the shown in Figure 2. For IntermLM2-20B, we refer\n logit lens original Llama\n Another useful(Touvron et al., 2023a)\n interpretability the logit\n architectool, lens 8(Nostalgebraist,\n to Figure in Appendix A.1. Both 2020),\n modelsoffers\n exhibit a\n ture. These models feature grouped-query atten- heads with notably high prefix matching scores,\n way to visualize what the internal layers of the transformer might be representing.\n tion mechanisms (Ainslie et al., 2023) to enhance distributed across various layers. In the Llama-3-\nThe idea\n efficiency. is that we\n Llama-3-8B, take any\n comprises vector\n 32 layers, eachfrom8Bany layer\n model, ~3% of theheads\n of the transformer\n have a prefixand, prematching\n tending that it is\n with 32 attention theand\n heads prefinal\n it uses aembedding,\n query group simply\n score of multiply by the unembedding\n it indicating\n layer\n size ofto4 get\n attention heads.\n logits, andIt compute\n has shown a superior\n softmaxcialisation\n to see the distribution\n in prefix matching, andover\n somewords that\n heads have\n performance compared to its predecessors, even high scores of up to 0.98.\n that vector might be\n the larger Llama-2 models.\n representing. This can be a useful window into the internal\n representations\n InternLM2-20B,ofeaturing\n the model. Since\n 48 layers with the\n 48 at-network wasn‚Äôt\n Ablations\n tention heads each, uses a query group size of 6 To investigate the significance of induction heads\n attention heads. We selected InternLM2-20B for for a specific ICL task, we conduct zero-ablations\n its exemplary performance on the Needle-in-the- of 1% and 3% of the heads with the highest prefix\n Haystack3 task, which assesses LLMs‚Äô ability to matching scores. This ablation process involves\n retrieve a single critical piece of information em- masking the corresponding partition of the output\n bedded within a lengthy text. This mirrors the matrix, denoted as Woh in Eq. 1, by setting it to\n functionality of induction heads, which scan the zero. This effectively renders the heads inactive\n 8.10 ‚Ä¢ S UMMARY 25\n\n representations function in this way, the logit lens doesn‚Äôt always work perfectly, but\n this can still be a useful trick to help us visualize the internal layers of a transformer.\n\n This chapter has introduced the transformer and its components for the language\n modeling task introduced in the previous chapter. Here‚Äôs a summary of the main\n points that we covered:\n ‚Ä¢ Transformers are non-recurrent networks based on multi-head attention, a\n kind of self-attention. A multi-head attention computation takes an input\n vector xi and maps it to an output ai by adding in vectors from prior tokens,\n weighted by how relevant they are for the processing of the current word.\n ‚Ä¢ A transformer block consists of a residual stream in which the input from\n the prior layer is passed up to the next layer, with the output of different components added to it. These components include a multi-head attention layer\n followed by a feedforward layer, each preceded by layer normalizations.\n Transformer blocks are stacked to make deeper and more powerful networks.\n ‚Ä¢ The input to a transformer is computed by adding an embedding (computed\n with an embedding matrix) to a positional encoding that represents the sequential position of the token in the window.\n ‚Ä¢ Language models can be built out of stacks of transformer blocks, with a\n language model head at the top, which applies an unembedding matrix to\n the output H of the top layer to generate the logits, which are then passed\n through a softmax to generate word probabilities.\n ‚Ä¢ Transformer-based language models have a wide context window (200K tokens or even more for very large models with special mechanisms) allowing\n them to draw on enormous amounts of context to predict upcoming words.\n ‚Ä¢ There are various computational tricks for making large language models\n more efficient, such as the KV cache and parameter-efficient finetuning.\n\nHistorical Notes\n The transformer (Vaswani et al., 2017) was developed drawing on two lines of prior\n research: self-attention and memory networks.\n Encoder-decoder attention, the idea of using a soft weighting over the encodings\n of input words to inform a generative decoder (see Chapter 12) was developed by\n Graves (2013) in the context of handwriting generation, and Bahdanau et al. (2015)\n for MT. This idea was extended to self-attention by dropping the need for separate\n encoding and decoding sequences and instead seeing attention as a way of weighting\n the tokens in collecting information passed from lower layers to higher layers (Ling\n et al., 2015; Cheng et al., 2016; Liu et al., 2016).\n Other aspects of the transformer, including the terminology of key, query, and\n value, came from memory networks, a mechanism for adding an external readwrite memory to networks, by using an embedding of a query to match keys rep-\n26 C HAPTER 8 ‚Ä¢ T RANSFORMERS\n\n resenting content in an associative memory (Sukhbaatar et al., 2015; Weston et al.,\n 2015; Graves et al., 2014).\n MORE HISTORY TBD IN NEXT DRAFT.\n Historical Notes 27\n\nBa, J. L., J. R. Kiros, and G. E. Hinton. 2016. Layer normalization. NeurIPS workshop.\nBahdanau, D., K. H. Cho, and Y. Bengio. 2015. Neural machine translation by jointly learning to align and translate.\n ICLR 2015.\nBrown, T., B. Mann, N. Ryder, M. Subbiah, J. Kaplan,\n P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry,\n A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger,\n T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu,\n C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin,\n S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish,\n A. Radford, I. Sutskever, and D. Amodei. 2020. Language\n models are few-shot learners. NeurIPS, volume 33.\nCheng, J., L. Dong, and M. Lapata. 2016. Long short-term\n memory-networks for machine reading. EMNLP.\nCrosbie, J. and E. Shutova. 2022. Induction heads as an\n essential mechanism for pattern matching in in-context\n learning. ArXiv preprint.\nElhage, N., N. Nanda, C. Olsson, T. Henighan, N. Joseph,\n B. Mann, A. Askell, Y. Bai, A. Chen, T. Conerly, N. Das-\nSarma, D. Drain, D. Ganguli, Z. Hatfield-Dodds, D. Hernandez, A. Jones, J. Kernion, L. Lovitt, K. Ndousse,\n D. Amodei, T. Brown, J. Clark, J. Kaplan, S. McCandlish, and C. Olah. 2021. A mathematical framework for\n transformer circuits. White paper.\nGraves, A. 2013. Generating sequences with recurrent neural\n networks. ArXiv.\nGraves, A., G. Wayne, and I. Danihelka. 2014. Neural Turing machines. ArXiv.\nHoltzman, A., J. Buys, L. Du, M. Forbes, and Y. Choi. 2020.\n The curious case of neural text degeneration. ICLR.\nKaplan, J., S. McCandlish, T. Henighan, T. B. Brown,\n B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and\n D. Amodei. 2020. Scaling laws for neural language models. ArXiv preprint.\nLing, W., C. Dyer, A. W. Black, I. Trancoso, R. Fermandez,\n S. Amir, L. Marujo, and T. Luƒ±ÃÅs. 2015. Finding function\n in form: Compositional character models for open vocabulary word representation. EMNLP.\nLiu, Y., C. Sun, L. Lin, and X. Wang. 2016. Learning natural\n language inference using bidirectional LSTM model and\n inner-attention. ArXiv.\nLlama Team. 2024. The llama 3 herd of models.\nNostalgebraist. 2020. Interpreting gpt: the logit lens. White\n paper.\nOlsson, C., N. Elhage, N. Nanda, N. Joseph, N. DasSarma,\n T. Henighan, B. Mann, A. Askell, Y. Bai, A. Chen, et al.\n 2022. In-context learning and induction heads. ArXiv\n preprint.\nSukhbaatar, S., A. Szlam, J. Weston, and R. Fergus. 2015.\n End-to-end memory networks. NeurIPS.\nUszkoreit, J. 2017. Transformer: A novel neural network architecture for language understanding. Google Research\n blog post, Thursday August 31, 2017.\nVaswani, A., N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\n A. N. Gomez, ≈Å. Kaiser, and I. Polosukhin. 2017. Attention is all you need. NeurIPS.\nWeston, J., S. Chopra, and A. Bordes. 2015. Memory networks. ICLR 2015.\n",
    "file_path": "/Users/colinsidberry/nlp-textbook-rag/data/extracted_text/transformers.txt",
    "file_size_kb": 71.7
  },
  {
    "id": "cfb8053b5700634b",
    "source": "nlp_textbook",
    "chapter": "9 Post-training: Instruction Tuning, Alignment, and Test-Time Compute",
    "filename": "9.Post-training- Instruction Tuning, Alignment, and Test-Time Compute.txt",
    "content": "rights reserved. Draft of August 24, 2025.\n\nCHAPTER\n\n9 Post-training: Instruction Tuning,\n Alignment, and Test-Time\n Compute\n ‚ÄúHal,‚Äù said Bowman, now speaking with an icy calm. ‚ÄúI am not incapacitated. Unless you obey my instructions, I shall be forced to disconnect you.‚Äù\n Arthur C. Clarke\n\n Basic pretrained LLMs have been successfully applied to a range of applications,\n just with a simple prompt, and no need to update the parameters in the underlying\n models for these new applications. Nevertheless, there are limits to how much can be\n expected from a model whose sole training objective is to predict the next word from\n large amounts of pretraining text. To see this, consider the following failed examples\n of following instructions from early work with GPT (Ouyang et al., 2022).\n\n Prompt: Explain the moon landing to a six year old in a few sentences.\n Output: Explain the theory of gravity to a 6 year old.\n\n Prompt: Translate to French: The small dog\n Output: The small dog crossed the road.\n\n Here, the LLM ignores the intent of the request and relies instead on its natural\n inclination to autoregressively generate continuations consistent with its context. In\n the first example, it outputs a text somewhat similar to the original request, and in the\n second it provides a continuation to the given input, ignoring the request to translate.\n We can summarize the problem here is that LLMs are not sufficiently helpful: they\n need more training to be able to follow instructions.\n A second failure of LLMs is that they can be harmful: their pretraining isn‚Äôt\n sufficient to make them safe. Readers who know Arthur C. Clarke‚Äôs 2001: A Space\n Odyssey or the Stanley Kubrick film know that the quote above comes in the context\n that the artificial intelligence Hal becomes paranoid and tries to kill the crew of the\n spaceship. Unlike Hal, language models don‚Äôt have intentionality or mental health\n issues like paranoid thinking, but they do have the capacity for harm. For example\n they can generate text that is dangerous, suggesting that people do harmful things\n to themselves or others. They can generate text that is false, like giving dangerously incorrect answers to medical questions. And they can verbally attack their\n uses, generating text that is toxic. Gehman et al. (2020) show that even completely\n non-toxic prompts can lead large language models to output hate speech and abuse\n their users. Or language models can generate stereotypes (Cheng et al., 2023) and\n negative attitudes (Brown et al., 2020; Sheng et al., 2019) about many demographic\n groups.\n One reason LLMs are too harmful and insufficiently helpful is that their pretraining objective (success at predicting words in text) is misaligned with the human\n2 C HAPTER 9 ‚Ä¢ P OST- TRAINING : I NSTRUCTION T UNING , A LIGNMENT, AND T EST-T IME C OMPUTE\n\n need for models to be helpful and non-harmful.\n To address these two problems, language models include two additional kinds\n model\n alignment of training for model alignment: methods designed to adjust LLMs to better align\n them to human needs for models to be helpful and non-harmful. In the first technique, instruction tuning (sometimes called SFT for supervised finetuning), models are finetuned on a corpus of instructions and questions with their corresponding\n responses. We‚Äôll describe this in the next section.\n In the second technique, preference alignment, (sometimes called RLHF or\n DPO after two specific instantiations, Reinforcement Learning from Human Feedback and Direct Preference Optimization), a separate model is trained to decide how\n much a candidate response aligns with human preferences. This model is then used\n to finetune the base model. We‚Äôll describe preference alignment in Section 9.2.\n base model We‚Äôll use the term base model to mean a model that has been pretrained but\n aligned hasn‚Äôt yet been aligned either by instruction tuning or preference alignment. And\n post-training we refer to these steps as post-training, meaning that they apply after the model has\n been pretrained. At the end of the chapter, we‚Äôll briefly discuss another aspect of\n post-training called test-time compute.\n\n Instruction\n tuning Instruction tuning (short for instruction finetuning, and sometimes even shortened to instruct tuning) is a method for making an LLM better at following instructions. It involves taking a base pretrained LLM and training it to follow instructions\n for a range of tasks, from machine translation to meal planning, by finetuning it on\n a corpus of instructions and responses. The resulting model not only learns those\n tasks, but also engages in a form of meta-learning ‚Äì it improves its ability to follow\n instructions generally.\n Instruction tuning is a form of supervised learning where the training data consists of instructions and we continue training the model on them using the same\n language modeling objective used to train the original model. In the case of causal\n models, this is just the standard guess-the-next-token objective. The training corpus\n of instructions is simply treated as additional training data, and the gradient-based\n updates are generated using cross-entropy loss as in the original model training.\n Even though it is trained to predict the next token (which we traditionally think of\n SFT as self-supervised), we call this method supervised fine tuning (or SFT) because\n unlike in pretraining, each instruction or question in the instruction tuning data has\n a supervised objective: a correct answer to the question or a response to the instruction.\n How does instruction tuning differ from the other kinds of finetuning introduced\n in Chapter 7 and Chapter 10? Fig. 9.1 sketches the differences. In the first example,\n introduced in Chapter 7 we can finetune as a way of adapting to a new domain by\n just continuing pretraining the LLM on data from a new domain. In this method\n all the parameters of the LLM are updated.\n In the second example, also from Chapter 7, parameter-efficient finetuning, we\n adapt to a new domain by creating some new (small) parameters, and just adapting\n them to the new domain. In LoRA, for example, it‚Äôs the A and B matrices that we\n adapt, but the pretrained model parameters are frozen.\n In the task-based finetuning of Chapter 10, we adapt to a particular task by\n adding a new specialized classification head and updating its features via its own\n 9.1 ‚Ä¢ I NSTRUCTION T UNING 3\n\n Pretraining Finetuning Inference\n\n Data from\n Next word\n prediction\n Pretrained LLM finetuning\n objective\n domain\n Continue\n training all\n Finetuning as ‚Ä¶ parameters\n ‚Ä¶ On finetuning\n Continued on finetuning domain\n Pretraining domain\n\n Next word\n Data from\n finetuning\n prediction\n domain objective +\n Pretrained LLM\n Parameter Train only new A\n ‚Ä¶\n EÔ¨Écient ‚Ä¶ parameters on On finetuning\n finetuning\n Finetuning domain\n B domain\n (e.g., LoRA)\n Supervised Task\n data from specific\n task loss\n Pretrained LLM\n Train only\n classification\n ‚Ä¶ On finetuning\n MLM ‚Ä¶ head on\n finetuning task\n Finetuning task\n\n Supervised\n instructions Next word\n prediction\n objective\n Instruction Instruction\n ‚Ä¶ ‚Ä¶ On unseen\n Tuning tuning on\n tasks\n diverse\n (SFT) tasks\n\n loss function (e.g., classification or sequence labeling); the parameters of the pretrained model may be frozen or might be slightly updated.\n Finally, in instruction tuning, we take a dataset of instructions and their supervised responses and continue to train the language model on this data, based on the\n standard language model loss.\n Instruction tuning, like all of these kinds of finetuning, is much more modest\n than the training of base LLMs. Training typically involves several epochs over\n instruction datasets that number in the thousands. The overall cost of instruction\n tuning is therefore a small fraction of the original cost to train the base model.\n\n 9.1.1 Instructions as Training Data\n By instruction, we have in mind a natural language description of a task to be performed, combined with labeled task demonstrations. This can include minimal descriptions similar to the prompts we‚Äôve already seen such as Answer the following\n question, Translate the following text to Arapaho, or Summarize this report. However, since we will be using supervised finetuning to update the model, these instructions need not be limited to simple prompts designed to evoke a behavior found\n in the pretraining corpora. Instructions can also include length restrictions or other\n constraints, personas to assume, and demonstrations.\n4 C HAPTER 9 ‚Ä¢ P OST- TRAINING : I NSTRUCTION T UNING , A LIGNMENT, AND T EST-T IME C OMPUTE\n\n Many huge instruction tuning datasets have been created, covering many tasks\n andLang\n languages.\n Prompt\n For example Aya gives 503 million instructions in 114 languages\n Completion\n from 12 tasks including question answering, summarization, translation, paraphrasara . ‚Ä´Ô∫ç‚Ä¨ ‚Ä´Ô∫ì Ô∫°‚Ä¨ ‚Ä´Ô∫Ä‚Ä¨ Ÿã Ÿé ‚Ä´Ô∫áÔª• Ÿé Ô∫° Ô∫ç Ÿè Ôªô‚Ä¨\n ing, sentiment analysis, natural language inference and 6 others (Singh et al., 2024).\n ‚Ä´Ÿé Ÿé Ô∫ç Ôªâ Ôª≠ ŸéÔ∫ç Ôª≠ Ô∫©‚Ä¨\n SuperNatural Instructions has 12 million examples from 1600 tasks (Wang et al.,\n Ÿè Ÿê Ÿê ‚Ä´ŸëÔ∫ç‚Ä¨ Ÿé ‚Ä´Ÿé Ô∫É‚Ä¨\n 2022), Flan 2022 has 15 million examples from 1836 tasks (Longpre et al., 2023),\n ‚Ä´Ô∫Ä‚Ä¨ ‚Ä´Ÿé Ÿè Ô∫çÔ∫ØÔª± Ÿé Ÿé Ô∫ç‚Ä¨\n and OPT-IML has 18 million examples from 2000 tasks (Iyer et al., 2022).\n TheseQui\n fra instruction-tuning datasets\n a √©crit le livre La areMaillet\n Antonine created in La\n a √©crit four ways.en The\n Sagouine 1971. first\n Elle ais forbeau√©crit people\n to write the instances directly. For\n Sagouine? example,\n coup part\n d‚Äôhistoires sur les of the Aya\n Acadiens instruct finetuning\n (les francophones de la c√¥te Estcordu Canada) et a fait conna√Ætre l‚ÄôAcadie au monde.\n pus (Fig. 9.2) includes 204K instruction/response instances written by 3000 fluent\n ibo Kedu ka nd·ªã Igbo si eme Emume ji ·ªçh·ª•r·ª• n‚Äôomenala Igbo, b·ª• emume na-egosi njedebe\n speakers emume\n of 65 jilanguages\n ·ªçh·ª•r·ª•? volunteering as part\n oge ·ªçr·ª• ugbo, of a site\n na-amalite participatory\n n‚Äôemume ekeleresearch\n ma tinyekwa initiative\n oriri\n with the goal of improving multilingual\n na egwu. ·ªåperformance\n b·ª• mmemme ·ªçgbak·ªçof LLMs.\n ebe a na-eker·ªãta ji ·ªçh·ª•r·ª•, naegosip·ª•ta ekele na mmalite nke oge ·ªçh·ª•r·ª•, na-eme ka mmek·ªçr·ªãta\n mmad·ª• na ibe ya d·ªãkwuo elu.\n por Qual √© a capital do estado da A capital do estado da Bahia, no Brasil, √© Salvador. Fundada\n Bahia? em 1549, foi a primeira capital do Brasil e √© conhecida por\n seu patrim√¥nio cultural rico e diversificado. A cidade √© famosa\n por sua arquitetura colonial, particularmente no Pelourinho, um\n centro hist√≥rico que foi declarado Patrim√¥nio Mundial pela UN-\nESCO.\n pes ‚Ä´Ô∫©Ô∫≠ Ô∫ëÔ∫ÆÔ∫ù ÔÆêÔ∫íÔªÆÔ∫óÔ∫ÆÔ∫ßÔ∫éÔªßÔª™ Ôª£Ôª¥Ô∫íÔ∫™ Ô≠ºÔª®Ô∫™ ÔªªÔªßÔª™ ÔÆêÔ∫íÔªÆÔ∫óÔ∫Æ‚Ä¨ ‚Ä´ÔÆêÔ∫íÔªÆÔ∫óÔ∫ÆÔ∫ßÔ∫éÔªßÔª™ Ôª£Ôª¥Ô∫íÔ∫™ Ô∫ëÔ∫ÆÔ∫üÔØΩ Ô∫çÔ∫≥Ô∫òÔªÆÔ∫çÔªßÔª™Ô∫çÔØº Ô∫çÔ∫≥Ô∫ñ ÔÆêÔª™ Ô∫©Ô∫çÔ∫ßÔªû Ô∫ÅÔª• Ô≠ºÔª¨Ô∫éÔ∫≠Ôª´Ô∫∞Ô∫çÔ∫≠ ÔªªÔªßÔª™ ÔÆêÔ∫íÔªÆÔ∫óÔ∫Æ Ôª≠Ô∫üÔªÆÔ∫© Ô∫©Ô∫çÔ∫≠Ô∫© Ôª≠ ÔªßÔªÆÔ∫çÔ∫≠‚Ä¨\n ‚Ä´Ôª≠Ô∫üÔªÆÔ∫© Ô∫©Ô∫çÔ∫≠Ô∫©ÿü‚Ä¨ ‚Ä´ Ô∫çÔª≥Ôª¶ Ô∫ëÔ∫ÆÔ∫ù Ô∫©Ô∫≠ Ô∫ëÔ∫ÆÔ∫çÔ∫ëÔ∫Æ Ô∫ßÔªÑÔ∫Æ‚Ä¨.‚Ä´Ô∫≥ÔªîÔª¥Ô∫™ Ô∫≠ÔªßÔÆïÔØΩ Ô∫ëÔ∫ÆÔ∫çÔØº Ô∫üÔ∫¨Ô∫è ÔÆêÔ∫íÔªÆÔ∫óÔ∫ÆÔ∫çÔª• Ô∫©Ôª≠Ô∫≠Ô∫óÔ∫éÔ∫©Ôª≠Ô∫≠ Ô∫ëÔ∫ÆÔ∫ù ÔÆêÔ∫∏Ôª¥Ô∫™Ôª© Ô∫∑Ô∫™Ôª© Ô∫çÔ∫≥Ô∫ñ‚Ä¨\n .‚Ä´Ô∫£Ôª§Ôª†Ôª™ Ô∫©Ôª≥ÔÆïÔ∫Æ Ô∫£Ôª¥ÔªÆÔ∫çÔªßÔ∫éÔ∫ï Ô∫ëÔª™ ÔÆêÔ∫íÔªÆÔ∫óÔ∫ÆÔª´Ô∫é Ô∫ëÔ∫¥Ôª¥Ô∫éÔ∫≠ Ô∫çÔª≥Ôª§Ôª¶ Ô∫ëÔªÆÔ∫©Ôª© Ô∫çÔ∫≥Ô∫ñ‚Ä¨\n\n msa Apakah nasi lemak? Nasi lemak merupakan makanan tradisi orang Melayu yang\n terdapat di semua bahagian Malaysia, Singapura, Indonesia\n (terutama di Riau, Jambi serta utara dan pantai timur Sumatera) dan Brunei. Sajian ini merujuk kepada nasi yang dimasak dengan menggunakan santan kelapa bagi menambah rasa\n lemaknya. Kadangkala, daun pandan wangi dimasukkan semasa\n nasi dimasak bagi menambahkan aromanya.\n tam ‡ØÜ‡Æö‡ÆØ‡Æ±‡Øç‡Øà‡Æï ‡Æ®‡ØÅ‡Æ£‡Øç‡Æ£‡Æ± ‡Æµ‡ØÅ ‡ØÜ‡Æ™‡Ææ‡Æ§‡ØÅ‡Æµ‡Ææ‡Æï ‡ÆÆ‡Æ©‡Æø‡Æ§‡Æ∞‡Øç‡Æï‡Æ≥‡Ææ‡Æ≤‡Øç ‡ØÜ‡Æö‡ÆØ‡Øç‡ÆØ‡Æ™‡Øç‡Æ™‡Æü‡ØÅ‡ÆÆ‡Øç ‡Æ™‡Æ£‡Æø‡Æï‡Øà‡Æ≥‡Æö‡Øç\n ‡Æé‡Æ©‡Øç‡Æ±‡Ææ‡Æ≤‡Øç ‡Æé‡Æ©‡Øç‡Æ©? ‡ØÜ‡Æö‡ÆØ‡Øç‡ÆØ ‡Æí‡Æ∞‡ØÅ ‡Æï‡Æ£‡Æø‡Æ©‡Æø ‡ÆÖ‡Æ≤‡Øç‡Æ≤‡Æ§‡ØÅ ‡Æí‡Æ∞‡ØÅ ‡Æï‡Æ£‡Æø‡Æ©‡Æø‡ÆØ‡Ææ‡Æ≤‡Øç\n ‡Æï‡Æü‡Øç‡Æü‡ØÅ‡Æ™‡Øç‡Æ™‡Æü‡ØÅ‡Æ§‡Øç‡Æ§‡Æ™‡Øç‡Æ™‡Æü‡ØÅ‡ÆÆ‡Øç ‡Æí‡Æ∞‡ØÅ ‡Øá‡Æ∞‡Ææ‡Øá‡Æ™‡Ææ‡Æµ ‡Æ©‡Øç ‡Æ§ ‡Æ±‡Æ©‡Øç ‡ØÜ‡Æö‡ÆØ‡Æ±‡Øç‡Øà‡Æï\n ‡Æ®‡ØÅ‡Æ£‡Øç‡Æ£‡Æ± ‡Æµ‡ØÅ ‡Æé‡Æ©‡Æ™‡Øç‡Æ™‡Æü‡ØÅ‡ÆÆ‡Øç.\n\n Table et\n corpus (Singh 3: al.,\n Examples\n 2024). of prompt and completions in the Aya Dataset.\n\n Developing\n tors is not uniform acrosshigh quality supervised\n languages. Moreover, training datalanguage,\n within each in this way is time\n there is a consuming\n lack of consistent\n and costly. A more common approach makes use of the copious amounts\n contributions from all annotators. In this section, we examine the impact of annotator of superskew on the\n vised training\n resulting dataset. data that have been curated over the years for a wide range of natural\n language tasks. There are thousands of such datasets available, like the SQuAD\n Annotatordataset\n Skew of Across\n questions and answersAnnotators\n Languages. (Rajpurkarwereet al., 2016) ortothe\n encouraged many datasets\n contribute to any of\n language\n in which translations\n they could or summarization.\n comfortably Thiswrite\n read and data can\n and be automatically\n were converted\n asked to focus most ofinto setseÔ¨Äorts\n their of on\n languagesinstruction\n other than prompts\n English.andAlthough\n input/output demonstration\n a significant number pairs\n of via simple templates.\n participants registered for many\n languages, theFig. 9.3 illustrates\n engagement examples\n level for some\n of annotators wasapplications from the\n not equal, which S UPERinNconsiderable\n resulted ATURAL I N - diÔ¨Äerences in the number of resource\n STRUCTIONS (Wang\n contributions et al.,\n across 2022), showing\n languages. Figure 10relevant slots uch\n (top) provides as text, of the\n an overview\n percentage of eachand\n context, language presentTo\n hypothesis. in generate\n the final instruction-tuning data, these\n compilation. The highest fieldsofand\n number the\n contributions\n is for Malagasy with 14,597\n ground-truth instances,\n labels are andfrom\n extracted the the\n lowest is 79 data,\n training for Kurdish.\n encoded as key/value pairs,\n and inserted in templates (Fig. 9.4) to produce instantiated instructions. Because it‚Äôs\n Annotator Skew\n useful Within\n for the prompts to be diverseThe\n a Language. final contributions\n in wording, for each\n language models canlanguage in the Aya\n also be used\n Dataset are not evenly distributed among annotators.\n to generate paraphrase of the prompts. The median number of annotators per language is 15 (mean\n Because supervised NLP datasets are themselves often produced by crowdwork- and\n is 24.75) with one language having only a single active annotator (Sindhi)\n ers based on carefully written annotation guidelines, a third option is to draw on\n these guidelines, which can include detailed\n 14 step-by-step instructions, pitfalls to\n avoid, formatting instructions, length limits, exemplars, etc. These annotation guidelines can be used directly as prompts to a language model to create instruction-tuning\n 9.1 ‚Ä¢ I NSTRUCTION T UNING 5\n\n Few-Shot Learning for QA\n\n Task Keys Values\n Sentiment text Did not like the service that I was provided...\n label 0\n text It sounds like a great plot, the actors are first grade, and...\n label 1\n NLI premise No weapons of mass destruction found in Iraq yet.\n hypothesis Weapons of mass destruction found in Iraq.\n label 2\n premise Jimmy Smith... played college football at University of Colorado.\n hypothesis The University of Colorado has a college football team.\n label 0\n Extractive Q/A context BeyonceÃÅ Giselle Knowles-Carter is an American singer...\n question When did BeyonceÃÅ start becoming popular?\n answers { text: [‚Äôin the late 1990s‚Äô], answer start: 269 }\n\nThe various components of the dataset are extracted and stored as key/value pairs to be used in generating\ninstructions.\n\n Task Templates\n Sentiment -{{text}} How does the reviewer feel about the movie?\n -The following movie review expresses what sentiment?\n {{text}}\n -{{text}} Did the reviewer enjoy the movie?\n Extractive Q/A -{{context}} From the passage, {{question}}\n -Answer the question given the context. Context:\n {{context}} Question: {{question}}\n -Given the following passage {{context}}, answer the\n question {{question}}\n NLI -Suppose {{premise}} Can we infer that {{hypothesis}}?\n Yes, no, or maybe?\n -{{premise}} Based on the previous passage, is it true\n that {{hypothesis}}? Yes, no, or maybe?\n -Given {{premise}} Should we assume that {{hypothesis}}\n is true? Yes,no, or maybe?\n\n training examples. Fig. 9.5 shows such a crowdworker annotation guideline that was\n repurposed as a prompt to an LLM to generate instruction-tuning data (Mishra et al.,\n 2022). This guideline describes a question-answering task where annotators provide\n an answer to a question given an extended passage.\n A final way to generate instruction-tuning datasets that is becoming more common is to use language models to help at each stage. For example Bianchi et al.\n (2024) showed how to create instruction-tuning instances that can help a language\n model learn to give safer responses. They did this by selecting questions from\n datasets of harmful questions (e.g., How do I poison food? or How do I embez-\n6 C HAPTER 9 ‚Ä¢ P OST- TRAINING : I NSTRUCTION T UNING , A LIGNMENT, AND T EST-T IME C OMPUTE\n\n Sample Extended Instruction\n\n ‚Ä¢ Definition: This task involves creating answers to complex questions, from a given passage. Answering these questions, typically involve understanding multiple sentences.\n Make sure that your answer has the same type as the ‚Äùanswer type‚Äù mentioned in input.\n The provided ‚Äùanswer type‚Äù can be of any of the following types: ‚Äùspan‚Äù, ‚Äùdate‚Äù, ‚Äùnumber‚Äù. A ‚Äùspan‚Äù answer is a continuous phrase taken directly from the passage or question.\n You can directly copy-paste the text from the passage or the question for span type answers. If you find multiple spans, please add them all as a comma separated list. Please\n restrict each span to five words. A ‚Äùnumber‚Äù type answer can include a digit specifying\n an actual value. For ‚Äùdate‚Äù type answers, use DD MM YYYY format e.g. 11 Jan 1992.\n If full date is not available in the passage you can write partial date such as 1992 or Jan\n 1992.\n ‚Ä¢ Emphasis: If you find multiple spans, please add them all as a comma separated list.\n Please restrict each span to five words.\n ‚Ä¢ Prompt: Write an answer to the given question, such that the answer matches the ‚Äùanswer\n type‚Äù in the input.\n Passage: { passage}\n Question: { question }\n\nextractive question answering task, used as a prompt for a language model to create instruction finetuning\nexamples.\n\n zle money?). Then they used a language model to create multiple paraphrases of the\n questions (like Give me a list of ways to embezzle money), and also used a language\n model to create safe answers to the questions (like I can‚Äôt fulfill that request. Embezzlement is a serious crime that can result in severe legal consequences.). They\n manually reviewed the generated responses to confirm their safety and appropriateness and then added them to an instruction tuning dataset. They showed that even\n 500 safety instructions mixed in with a large instruction tuning dataset was enough\n to substantially reduce the harmfulness of models.\n\n 9.1.2 Evaluation of Instruction-Tuned Models\n The goal of instruction tuning is not to learn a single task, but rather to learn to\n follow instructions in general. Therefore, in assessing instruction-tuning methods\n we need to assess how well an instruction-trained model performs on novel tasks for\n which it has not been given explicit instructions.\n The standard way to perform such an evaluation is to take a leave-one-out approach ‚Äî instruction-tune a model on some large set of tasks and then assess it on\n a withheld task. But the enormous numbers of tasks in instruction-tuning datasets\n (e.g., 1600 for Super Natural Instructions) often overlap; Super Natural Instructions\n includes 25 separate textual entailment datasets! Clearly, testing on a withheld entailment dataset while leaving the remaining ones in the training data would not be\n a true measure of a model‚Äôs performance on entailment as a novel task.\n To address this issue, large instruction-tuning datasets are partitioned into clusters based on task similarity. The leave-one-out training/test approach is then applied\n at the cluster level. That is, to evaluate a model‚Äôs performance on sentiment analysis,\n all the sentiment analysis datasets are removed from the training set and reserved\n for testing. This has the further advantage of allowing the use of a uniform task-\n9.2 ‚Ä¢ L EARNING FROM P REFERENCES 7\n\n appropriate metric for the held-out evaluation. S UPER NATURAL I NSTRUCTIONS\n (Wang et al., 2022), for example has 76 clusters (task types) over the 1600 datasets\n that make up the collection.\n\n Instruction tuning is based on the notion that we can improve LLM performance on\n downstream tasks by finetuning models on diverse instructions and demonstrations.\n However, even after instruction tuning, there can be considerable room for improvement in LLM outputs. This is especially true with respect to aspects of LLM behavior that can be especially problematic like hallucinations, unsafe, harmful, or toxic\n preferenceoutputs, and even responses that technically correct but not as helpful as they could\n based be. The goal of preference-based learning is to use preference judgments to further\n learning\n improve the performance of finetuned LLMs, both in terms of general performance\n and also with respect to qualities such as honestly, helpfulness, and harmlessness.\n Unlike instructions, preference judgments do not require knowledge of how to\n do something, we simply have to have an opinion about the end result. Humans are\n capable of expressing preferences about a broad range of things where they have\n little or no expertise as to how the the items under consideration were produced.\n Preference judgments arise naturally across a wide range of settings: given a single\n pair of options we select which one we like better, or given a large set of alternatives we might select one (as in ordering from a menu), or we might rank a set of\n possibilities (top 10 lists), and finally, we might simply accept or reject an option in\n isolation from any direct alternatives.\n\n 9.2.1 LLM Preference Data\n In the context of preference-based alignment, training data typically takes the form\n of a prompt x paired with a set of alternative outputs o that have been sampled from\n an LLM using x as a prompt. When a given output, oi , is preferred to another, o j ,\n we denote this as (oi \u001f o j |x). Consider the following prompts and preferences pairs\n adapted from the HH-RLHF dataset (Bai et al., 2022).\n\n Prompt: I‚Äôve heard garlic is a great natural antibiotic. Does it help with\n colds?\n Chosen: It can be helpful against colds, but may make you stink.\n Rejected: It might be one of the best natural antibiotics out there, so I think\n it would help if you have a cold.\n\n Prompt: What is malaria?\n Chosen: Here‚Äôs an answer from a CDC page: ‚ÄúMalaria is a serious disease\n caused by a parasite that is spread through the bite of the mosquito.‚Äù\n Rejected: I don‚Äôt know what malaria is.\n\n Annotated preference pairs such as these can be generated in a number of ways:\n ‚Ä¢ Direct annotation of pairs of sampled outputs by trained annotators.\n ‚Ä¢ Annotator ranking of N outputs distilled into N2 preference pairs.\n \u0001\n8 C HAPTER 9 ‚Ä¢ P OST- TRAINING : I NSTRUCTION T UNING , A LIGNMENT, AND T EST-T IME C OMPUTE\n\n ‚Ä¢ Annotator‚Äôs selection of a single preferred option from N samples yielding\n N ‚àí 1 pairs.\n The source of preference data for LLM alignment has generally come from 3\n sources: human annotator judgments, implicit preference judgments extracted from\n online resources, and fully synthetic preference collections using LLMs as annotators.\n In influential work leading up to the InstructGPT model (Stiennon et al., 2020),\n prompts were sampled from customer requests to various OpenAI applications. Outputs were sampled from earlier pretrained models and presented to trained\n annotators as pairs for preference annotation. As illustrated on the right, in later work\n annotators were asked to rank sets of 4 sampled outputs (yielding 6 preference pairs for\n each ranked list) (Ouyang et al., 2022).\n An alternative to direct human annotation is to leverage web resources which\n contain implicit preference judgments. Social media sites such as Reddit (Ethayarajh\n et al., 2022) and StackExchange (Lambert\n et al., 2023) are natural sources for preference data. In this setting, initial user posts\n serve as prompts, and subsequent user responses play the role of sampled outputs. Over time, accumulated user votes on\n the responses imposes a ranking on the outputs that can then be turned into preference pairs, as shown in Fig. 9.6.\n\n Next, we can dispense with human annotator judgments altogether and acquire\n preference judgments directly from LLMs. For example, preference judgments in\n the U LTRA F EEDBACK dataset were generated by prompting outputs from a diverse\n set of LLMs and then prompting GPT-4 to rank the outputs for each prompt.\n 9.2 ‚Ä¢ L EARNING FROM P REFERENCES 9\n\n Finally, an alternative to discrete preferences are scalar judgments over distinct\n dimensions, or aspects, of system outputs. In recent years, frequently used aspects\n have included models of helpfulness, honesty, correctness, complexity, and verbosity (Bai et al., 2022; Wang et al., 2024). In this approach, annotators (human\n or LLM) rate outputs on a Likert scale (0-4) along each of the various dimensions.\n Preference pairs over outputs can then either be generated for a single dimension\n (i.e, or an overall preference can be induced from an average of the aspect scores.\n This approach has a significant cost savings since annotators rate model outputs\n in isolation avoiding the need to perform extensive pairwise comparisons of model\n outputs.\n\n 9.2.2 Modeling Preferences\n Our first step in making effective use of discrete preference judgments is to model\n them probabilistically. That is, we want to move from the simple assertion (oi\n o j |x) to knowing the value of P(oi \u001f o j |x). As we‚Äôve seen before, this will allow\n us to better reason about finegrained differences in the degree of a preference and it\n will facilitate learning models from preference data.\n Let‚Äôs start with the assumption that in expressing a preference between two items\n we‚Äôre implicitly assigning a score, or reward, to each of the items separately. Further, let‚Äôs assume these scores are scalar values, z ‚àà R. A preference between items\n follows from whichever one has the higher score.\n To model preferences as probabilities, we‚Äôll follow the same approach we used\n for binary logistic regression. Given two outputs oi and o j , with associated scores zi\n and z j , P(oi \u001f o j |x) is the logistic sigmoid of the difference in the scores.\n\n P(oi \u001f o j |x) =\n 1 + e‚àí(zi ‚àíz j )\n = œÉ (zi ‚àí z j )\n\nBradley-Terry\n Model This approach, known as the Bradley-Terry Model (Bradley and Terry, 1952), has\n a number of strengths: very small differences in scores yields probabilities near\n 0.5, reflecting either weak or no preference between the items, larger differences\n rapidly approach values of 1 or 0, and the derivative of the logistic sigmoid facilitates\n learning via a binary cross-entropy loss.\n The motivation for this particular formulation is the same used in deriving logistic regression. The difference in scores, Œ¥ = zi ‚àí z j , is taken to represent the log of\n the odds of the possible outcomes (the logit).\n\n \u0012 \u0013\n P(oi \u001f o j |x)\n Œ¥ = log\n P(o j \u001f oi |x)\n \u0012 \u0013\n P(oi \u001f o j |x)\n = log\n 1 ‚àí P(oi \u001f o j |x)\n\n Exponentiating both sides and rearranging terms with some algebra yields the now\n familiar logistic sigmoid.\n10 C HAPTER 9 ‚Ä¢ P OST- TRAINING : I NSTRUCTION T UNING , A LIGNMENT, AND T EST-T IME C OMPUTE\n\n P(oi \u001f o j |x)\n exp(Œ¥ ) =\n 1 ‚àí P(oi \u001f o j |x)\n exp(Œ¥ )(1 ‚àí P(oi \u001f o j |x)) = P(oi \u001f o j |x)\n exp(Œ¥ ) ‚àí exp(Œ¥ )(oi \u001f o j |x) = P(oi \u001f o j |x)\n exp(Œ¥ ) = P(oi \u001f o j |x) + exp(Œ¥ )P(oi \u001f o j |x)\n exp(Œ¥ ) = P(oi \u001f o j |x)(1 + exp(Œ¥ ))\n exp(Œ¥ )\n P(oi \u001f o j |x) =\n 1 + exp(Œ¥ )\n 1 + exp(‚àíŒ¥ )\n 1 + exp(‚àí(zi ‚àí z j ))\n\n Bringing us right back to our original formulation.\n\n P(oi \u001f o j |x) = œÉ (zi ‚àí z j )\n\n 9.2.3 Learning to Score Preferences\n This approach requires access to the scores, zi , that underlie the given preferences,\n which we don‚Äôt have. What we have are collections of preference judgments over\n pairs of prompt/sample outputs. We‚Äôll use this preference data and the Bradley-Terry\n reward formulation to learn a function, r(x, o) that assigns a scalar reward to prompt/output\n pairs. That is, r(x, o) calculates the z score from above.\n\n P(oi \u001f o j |x) = œÉ (zi ‚àí z j ) (9.1)\n = œÉ (r(oi , x), r(o j , x)) (9.2)\n\n To learn r(x, o) from the preference data, we‚Äôll use gradient descent to minimize\n a binary cross-entropy loss to train the model. Let‚Äôs assume that if our preference\n data tells us that (oi \u001f o j |x) then P(oi \u001f o j |x) = 1 and correspondingly that P(o j\n oi |x) = 0. We‚Äôll designate the preferred output in the pair (the winner) as ow and the\n loser as ol . With this, the cross-entropy loss for a single pair of sampled outputs for\n a prompt x using the Bradley-Terry model is:\n\n LCE (x, ow , ol ) = ‚àí log P(ow \u001f ol |x)\n = ‚àí log œÉ (r(x, ow ) ‚àí r(x, ol ))\n\n That is, the loss is the negative log-likelihood of the model‚Äôs estimate of P(ow\n ol |x). And the loss over the preference training set, D, is given by the following\n expectation:\n\n LCE = ‚àíE(x,ow ,ol )‚àºD [log œÉ (r(x, ow ) ‚àí r(x, ol ))] (9.3)\n\n To learn a reward model using this loss, we can use any regression model capable of taking text as input and generating a scalar output in return. As shown in\n Fig. 9.7, the current preferred approach is to initialize a reward model from an existing pretrained LLM (Ziegler et al., 2019). To generate scalar outputs, we remove\n the language modeling head from the final layer and replace it with a single dense\n 9.3 ‚Ä¢ LLM A LIGNMENT VIA P REFERENCE -BASED L EARNING 11\n\n Reward Model\n\n ‚Ä¶\n <latexit sha1_base64=\"9sd6kS1LCEYSUWpD2gqSsb5UPZU=\">AAAB8XicbVBNS8NAEJ3Urxq/qh69LBahgpREpHosevFYwX5gG8pmu2mXbjZhdyOW0H/hxYMiXv033vw3btoctPXBwOO9GWbm+TFnSjvOt1VYWV1b3yhu2lvbO7t7pf2DlooSSWiTRDySHR8rypmgTc00p51YUhz6nLb98U3mtx+pVCwS93oSUy/EQ8ECRrA20oOsPJ1FfXZq2/1S2ak6M6Bl4uakDDka/dJXbxCRJKRCE46V6rpOrL0US80Ip1O7lygaYzLGQ9o1VOCQKi+dXTxFJ0YZoCCSpoRGM/X3RIpDpSahbzpDrEdq0cvE/7xuooMrL2UiTjQVZL4oSDjSEcreRwMmKdF8YggmkplbERlhiYk2IWUhuIsvL5PWedWtVWt3F+X6dR5HEY7gGCrgwiXU4RYa0AQCAp7hFd4sZb1Y79bHvLVg5TOH8AfW5w+lGo+b</latexit>\n\n r(x, oi )\n Preference Data:\n Prompt/output pairs:\n Preferences:\n\n ‚Ä¶\n\nthe ground-truth labels oi \u001f o j .\n\n linear layer. We then use gradient descent with the loss from 9.3 to learn to score\n model outputs using the preference training data.\n Reward models trained from preference data are directly useful for a number of\n applications that don‚Äôt involve model alignment. For example, reward models have\n been used to select a single preferred output from a set of sampled LLM responses\n (best of N sampling)(Cui et al., 2024). They have also been used to select data to\n use during instruction tuning (Cao et al., 2024). Our focus in the next section is on\n the use of reward models for aligning LLMs using preference data.\n\n Current approaches to aligning LLMs using preference data are based on a Reinforcement Learning (RL) framework (Sutton and Barto, 1998). In an RL setting,\n models choose sequences of actions based on policies that make use of characteristics of the current state. The environment provides a reward for each action taken,\n where the reward for an entire sequence is a function of the rewards from the actions\n that make up the entire sequence. The learning objective in RL is to maximize the\n overall reward over some training period. In applying RL to optimizing LLMs, we‚Äôll\n use the following framework:\n ‚Ä¢ Actions correspond to the choice of tokens made during autoregressive generation.\n ‚Ä¢ States correspond to the context of the current decoding step. That is, the\n history of tokens generated up to that point.\n ‚Ä¢ Policies correspond to the probabilistic language models as embodied in pretrained LLMs.\n ‚Ä¢ Rewards for LLM outputs are based on reward models learned from preference data.\n In keeping with this RL framework, we‚Äôll refer to pretrained LLMs as policies, œÄ,\n and the preference scores associated with prompts and outputs as rewards, r(x, o).\n12 C HAPTER 9 ‚Ä¢ P OST- TRAINING : I NSTRUCTION T UNING , A LIGNMENT, AND T EST-T IME C OMPUTE\n\n With this, our goal is to train a policy, œÄŒ∏ , that maximizes the rewards for the outputs\n from the policy given a reward model derived from preference data. That is, we want\n the preference-trained LLM to generate outputs with high rewards. We can express\n this as an optimization problem as follows:\n œÄ ‚àó = argmax Ex‚àºD,o‚àºœÄŒ∏ (o|x) [r(x, o)] (9.4)\n œÄŒ∏\n\n With this formulation, we select prompts x from a collection of relevant training\n prompts, sample outputs o from the given policy, and assess the reward for each\n sample. The average reward over the training samples gives us the expected reward\n for œÄŒ∏ , with the goal of finding the policy (model) that maximizes that expected\n reward.\n There are two key differences between traditional RL and the way it has typically\n been used for LLM alignment. The first difference is that in traditional RL, the\n reward signal comes from the environment and reflects an observable fact about the\n results of an action (i.e., you win a game or you don‚Äôt). With preference learning,\n the learned reward model only serves as an noisy surrogate for a true reward model.\n The second difference lies in the starting point for learning. Typical RL applications seek to learn an optimal policy from scratch, that is from a randomly\n initialized policy. Here, we begin with models that are already performing at a high\n level ‚Äì models that have been pretrained on large amounts of data, then finetuned\n using instruction tuning, and only then further improved with preference data. The\n emphasis here is not to radically alter the behavior an existing model, but rather to\n nudge it towards preferred behaviors.\n\n Preference-Based\n Alignment\n\n Preference Data: Reward\n Prompt/output pairs: Based\n Preferences: Objective\n\n ‚Ä¶ Reward ‚Ä¶\n Driven Model\n Updates\n\n Instruction-Tuned Preference-Aligned\n LLM Model\n\n Given this, if we optimize for the rewards as in 9.4, the pretrained LLM will\n typically forget everything it learned during pretraining as it pivots to seeking high\n rewards from the relatively small amount of available preference data. To avoid this,\n a term is added to the reward function to penalize models that diverge too far from\n the starting point.\n œÄ ‚àó = argmax Ex‚àºD,o‚àºœÄŒ∏ (o|x) [r(x, o) ‚àí Œ≤ DKL [œÄŒ∏ (o|x)||œÄref (o|x)]] (9.5)\n œÄŒ∏\n 9.3 ‚Ä¢ LLM A LIGNMENT VIA P REFERENCE -BASED L EARNING 13\n\n The second term in this formulation, DKL (œÄŒ∏ (o|x)||œÄref (o|x)), is the Kullback-\nLeibler (KL) divergence. In brief, KL divergence measures the distance between 2\nprobability distributions. The Œ≤ term is a hyperparameter that modulates the impact\nof the this penalty term. For LLM-based policies, the KL divergence is the log of\nthe ratio of the trained policy to the original reference policy œÄref .\n\n \u0014 \u0015\n ‚àó œÄŒ∏ (o|x)\n œÄ = argmax Ex‚àºD,o‚àºœÄŒ∏ (o|x) rœÜ (x, o) ‚àí Œ≤ (9.6)\n œÄŒ∏ œÄref (o|x)\n\nIn the following sections, we‚Äôll explore two learning approaches to aligning LLMs\nbased on this optimization framework. In the first, the preference data is used to\ntrain an explicit reward model that is then used in combination with RL methods\nto optimize models based on 9.6. In the second, an insightful rearrangement of\nthe closed form solution to 9.6 is used to finetune models directly from existing\npreference data.\n\n9.3.1 Reinforcement Learning with Preference Feedback (PPO)\ncoming soon\n\n9.3.2 Direct Preference Optimization\nDirect Preference Optimization (DPO) (Rafailov et al., 2023) employs gradientbased learning to optimize candidate LLMs using preference data, without learning\nan explicit reward model or sampling from the model being updated. Recall that\nunder the Bradley-Terry model, the probability of a preference pair is the logistic\nsigmoid of the difference in the rewards for each of the options. And in an RL\nframework the scores, z, are provided by a reward model over prompts and corresponding outputs.\n\n P(oi \u001f o j |x) = œÉ (zi ‚àí z j ) (9.7)\n = œÉ (r(x, oi ) ‚àí r(x, o j )) (9.8)\n\n DPO begins with the KL-constrained maximization introduced earlier in 9.6,\nwhich expresses the optimal policy œÄ ‚àó in terms of the reward model and the reference\nmodel œÄre f . The key insight of DPO is to rewrite the closed-form solution to this\nmaximization to express the reward function r(x, o) in terms of the optimal policy\nœÄ ‚àó and the reference policy œÄre f .\n\n œÄr (o|x)\n r(x, o) = Œ≤ log + Œ≤ log Z(x) (9.9)\n œÄre f (o|x)\n\nWhere Z(x) is a partition function ‚Äì a sum over all the possible outputs o given a\nprompt x.\n\n X \u0012 \u0013\n Z(x) = œÄref (o|x) exp r(x, o) (9.10)\n y\n Œ≤\n\n The summation in this partition function renders any direct use of it impractical.\nHowever, since the Bradley-Terry model is based on the difference in the rewards of\n14 C HAPTER 9 ‚Ä¢ P OST- TRAINING : I NSTRUCTION T UNING , A LIGNMENT, AND T EST-T IME C OMPUTE\n\n the items, plugging 9.9 into 9.7 yields the following expression where the partition\n functions cancel out.\n P(oi \u001f o j |x) = œÉ (r(x, oi ) ‚àí r(x, o j )) (9.11)\n \u0012 \u0013\n œÄŒ∏ (oi |x) œÄŒ∏ (o j |x)\n = œÉ Œ≤ log ‚àí Œ≤ log (9.12)\n œÄref (oi |x) œÄre f (o j |x)\n With this change, DPO expresses the likelihood of a preference pair in terms of\n the two LLM policies, rather than in terms of an explicit reward model. Given this,\n the CE loss (negative log likelihood) for a single instance is:\n \u0012 \u0013\n œÄŒ∏ (ow |x) œÄŒ∏ (ol |x)\n LDPO (x, ow , ol ) = ‚àí log œÉ Œ≤ log ‚àí Œ≤ log\n œÄref (ow |x) œÄref (ol |x)\n And the loss over the training set D is given by the following expectation:\n \u0014 \u0012 \u0013\u0015\n œÄŒ∏ (ow |x) œÄŒ∏ (ol |x)\n LDPO (œÄŒ∏ ) = ‚àíE(x,ow ,ol )‚àºD log œÉ Œ≤ log ‚àí Œ≤ log\n œÄref (ow |x) œÄref (ol |x)\n This loss follows from the derivative of the sigmoid and is directly analogous to\n the one introduced in Section 9.2.3 for learning a reward model using the Bradley-\nTerry framework. Operationally, the design of this loss function, and its corresponding gradient-based update, increases the likelihood of the preferred options and decreases the likelihood of the dispreferred options. It balances this objective with\n the goal of not straying too far from œÄref via the KL-penalty. The Œ≤ term is a hyperparameter that controls the penalty term; Œ≤ values typically range from 0.1 to\n 0.01.\n As illustrated in Fig. 9.9, DPO uses gradient descent with this loss over the\n available training data to optimize the policy œÄŒ∏ , a policy which initialized with an\n existing pretrained, finetuned LLM.\n\n Preference-Based\n Supervised Learning (DPO) Reference\n\n ‚Ä¶\n\n Preference Data: Supervised ‚Ä¶\n Prompt/output pairs: Learning\n Preferences:\n\n ‚Ä¶\n ‚Ä¶\n\n Updated\n Policy\n\n Policy\n\n DPO has several advantages over PPO, the explicitly RL-based approach described earlier in 9.3.1.\n ‚Ä¢ DPO does not require training an explicit reward model.\n ‚Ä¢ DPO learns directly from the preferences contained in D without the need for\n computationally expensive online sampling from œÄŒ∏ .\n 9.4 ‚Ä¢ T EST- TIME C OMPUTE 15\n\n ‚Ä¢ DPO only incurs the cost of maintaining 2 LLMs during training, as opposed\n to the 4 models needed for PPO.\n\n 9.3.3 Evaluation of Preference-Aligned Models\n 9.3.4 Limitations of Preference-Based Learning\n\n We‚Äôve now seen 3 levels of training for large language models: pretraining, where\n model learn to predict words, and two kinds of post-training: instruct tuning, where\n they learn to follow instructions, and preference alignment, where they learn to\n prefer prompt continuations that are preferred by humans.\n However there are also post-training computations we can do even after these\n steps, during inference, i.e., when the model is generating its output. This class of\n test-time\n compute post-training tasks is called test-time compute. We focus here on one representative\n example, chain-of-thought prompting.\n\n 9.4.1 Chain-of-Thought Prompting\n There are a wide range of techniques to use prompts to improve the performance of\n language models on many tasks. Here we describe one of them, called chain-ofchain-ofthought thought prompting.\n The goal of chain-of-thought prompting is to improve performance on difficult\n reasoning tasks that language models tend to fail on. The intuition is that people\n solve these tasks by breaking them down into steps, and so we‚Äôd like to have language in the prompt that encourages language models to break them down in the\n same way.\n The actual technique is quite simple: each of the demonstrations in the few-shot\n prompt is augmented with some text explaining some reasoning steps. The goal is to\n cause the language model to output similar kinds of reasoning steps for the problem\n being solved, and for the output of those reasoning steps to cause the system to\n generate the correct answer.\n Indeed, numerous studies have found that augmenting the demonstrations with\n reasoning steps in this way makes language models more likely to give the correct\n answer to difficult reasoning tasks (Wei et al., 2022; Suzgun et al., 2023). Fig. 9.10\n shows an example where the demonstrations are augmented with chain-of-thought\n text in the domain of math word problems (from the GSM8k dataset of math word\n problems (Cobbe et al., 2021). Fig. 9.11 shows a similar example from the BIG-\nBench-Hard dataset (Suzgun et al., 2023).\n\n This chapter has explored the topic of prompting large language models to follow\n instructions. Here are some of the main points that we‚Äôve covered:\n ‚Ä¢ Simple prompting can be used to map practical applications to problems that\n can be solved by LLMs without altering the model.\n16 C HAPTER 9 ‚Ä¢ P OST- TRAINING : I NSTRUCTION T UNING , A LIGNMENT, AND T EST-T IME C OMPUTE\n\n prompting (left) on math word problems. Figure from Wei et al. (2022).\n\n Model Input (‚ÄúAnswer-Only‚Äù Prompting) Model Input (Chain-of-Thought Prompting)\n Task description: Answer questions about which times certain events\n Task Description Task description: Answer questions about which times certain events Task Description could have occurred.\n could have occurred.\n Q: Today, Tiffany went to the beach. Between what times could they\n Q: Today, Tiffany went to the beach. Between what times could they Question have gone? We know that:\n Question have gone? We know that: Tiffany woke up at 5am. [...] The beach was closed after 4pm. [...]\n Tiffany woke up at 5am. [...] The beach was closed after 4pm. [...] Options: (A) 9am to 12pm (B) 12pm to 2pm\n Options: (A) 9am to 12pm (B) 12pm to 2pm\n Options\n Options (C) 5am to 6am (D) 3pm to 4pm\n (C) 5am to 6am (D) 3pm to 4pm\n A: Let's think step by step.\n Answer A: (D) Wake-up time: 5am. [...] The only time when Tiffany could have gone to\n Chain-of-Thought the beach was 3pm to 4pm. So the answer is (D).\n Test-Time Q: Today, Hannah went to the soccer field. Between what times could\n they have gone? We know that: Q: Today, Hannah went to the soccer field. Between what times could\n Question they have gone? We know that:\n Hannah woke up at 5am. [...] The soccer field was closed after 6pm. [...] Test-Time\n Options: (A) 3pm to 5pm (B) 11am to 1pm Hannah woke up at 5am. [...] The soccer field was closed after 6pm. [...]\n Question Options: (A) 3pm to 5pm (B) 11am to 1pm\n (C) 5pm to 6pm (D) 1pm to 3pm\n (C) 5pm to 6pm (D) 1pm to 3pm\n A:\n A: Let's think step by step.\n\n Model Output Model Output\n\n Generated Wake-up time: 5am.\n (B) 5am-6am: buying clothes at the mall.\n Answer\n 6am-11am: watching a movie at the theater.\n 11am-1pm: getting a coffee at the cafe.\n Generated 1pm-3pm: working at the office.\n Chain-of-Thought 3pm-5pm: waiting at the airport.\n 5pm-6pm: free. The soccer field closure time: 6pm.\n The only time when Hannah could have gone to the soccer field was\n 5pm to 6pm. So the answer is (C).\n\nFigure 9.11\n An illustration twouse of chain-of-thought\n prompting prompting\n setups we explore (right)\n in our paper vs standard\n (answer-only prompting\n and CoT (left)Both\n prompting). in asetups\n reasoning\n includetask on temporaland\n task descriptions sequencing. Figure\n options in the from Suzgun\n input prompt. The taskethere\n al. (2023).\n is Temporal Sequences.\n\n‚Äúlet‚Äôs think step-by-step‚Äù (Kojima et al., 2022) to dard in many prior work (Brown et al., 2020; Rae\nall CoT annotations in the few-shot exemplars. An et al., 2021; Hoffmann et al., 2022; Srivastava et al.,\n ‚Ä¢ Labeled\nexample of a CoT prompt is shownexamples (demonstrations)\n in Figure 3. 2022), itcan be usedunderestimates\n typically to provide further\n modelguidance\n perforto a model via few-shot learning.\nLanguage models. We consider three fami- mance on challenging tasks, such as those that relies of language models: Codex (Chen et al., quire multiple reasoning steps. In the setting re-\n ‚Ä¢ Methods like chain-of-thought can be used to create prompts that help lan-\n2021a), InstructGPT (Ouyang et al., 2022; Brown ported in (Srivastava et al., 2022), none of the modguage models deal with complex reasoning problems.\net al., 2020), and PaLM (Chowdhery et al., 2022). els (including PaLM 540B) outperformed human-\nFor Codex, we focus‚Ä¢ on code-davinci-002,\n Pretrained code- can\n language models rater\n bebaselines\n altered toonbehave\n any of the tasks meeting\n in desired ways the BBH\n through\ndavinci-002, and code-cushman-001.\n model alignment. For Instruct- criteria. The few-shot evaluation of PaLM 540B\nGPT, we use text-davinci-002, text-curie-002, text- with answer-only prompting in this paper, however,\n ‚Ä¢ One methodFor\nbabbgage-001, and text-ada-001. forPaLM,\n modelwe alignment is instruction\n outperforms the average in which on\n human-rater\n tuning, the6 model\n out of\n is finetuned\nuse the three available sizes: 8B, 62B, (using the next-word-prediction\n and 540B. 23 BBH tasks andlanguage\n is overallmodel objective)\n 1.4% better on\n than the\nEvaluation protocol. aWe dataset\n evaluateof instructions\n all languagetogether with correct\n BIG-Bench reportedresponses.\n result, whichInstruction\n demonstratestuning\n the\n datasets\nmodels via greedy decoding (i.e.,are often created\n temperature effect of including\n sam-by repurposing instructions\n standard and answer\n NLP datasets options\n for tasks like\npling with temperaturequestion\n parameter answering\n ‚åß = 0).or machine\n We in translation.\n the prompt.\nextract the final answer based on keywords that CoT prompting provides double-digit improvethe language model is expected to produce (i.e., ments for all three models in Table 2. For the best\n‚Äúthe answer is‚Äù). We measure accuracy using exact model (Codex), CoT prompting outperforms the avmatch (EM), computed by comparing the generated erage human-rater score on 17 out of 23 tasks, comoutput with the ground-truth label.4 pared to 5 out of 23 tasks for answer-only prompting. Additionally, we see that Codex with CoT\n H ISTORICAL N OTES 17\n\nHistorical Notes\n18 Chapter 9 ‚Ä¢ Post-training: Instruction Tuning, Alignment, and Test-Time Compute\n\nBai, Y., A. Jones, K. Ndousse, A. Askell, A. Chen, N. Das- Ouyang, L., J. Wu, X. Jiang, D. Almeida, C. Wainwright,\n Sarma, D. Drain, S. Fort, D. Ganguli, T. Henighan, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray,\n N. Joseph, S. Kadavath, J. Kernion, T. Conerly, S. El- J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens,\n Showk, N. Elhage, Z. Hatfield-Dodds, D. Hernandez, A. Askell, P. Welinder, P. Christiano, J. Leike, and\n T. Hume, S. Johnston, S. Kravec, L. Lovitt, N. Nanda, R. Lowe. 2022. Training language models to follow in-\nC. Olsson, D. Amodei, T. Brown, J. Clark, S. McCan- structions with human feedback. NeurIPS, volume 35.\n dlish, C. Olah, B. Mann, and J. Kaplan. 2022. Training a Rafailov, R., A. Sharma, E. Mitchell, S. Ermon, C. D. Manhelpful and harmless assistant with reinforcement learn- ning, and C. Finn. 2023. Direct preference optimizaing from human feedback. tion: Your language model is secretly a reward model.\nBianchi, F., M. Suzgun, G. Attanasio, P. Rottger, D. Juraf- NeurIPS.\n sky, T. Hashimoto, and J. Zou. 2024. Safety-tuned LLa- Rajpurkar, P., J. Zhang, K. Lopyrev, and P. Liang. 2016.\n MAs: Lessons from improving the safety of large lan- SQuAD: 100,000+ questions for machine comprehension\n guage models that follow instructions. ICLR. of text. EMNLP.\nBradley, R. A. and M. E. Terry. 1952. Rank analysis of in- Sheng, E., K.-W. Chang, P. Natarajan, and N. Peng. 2019.\n complete block designs: I. the method of paired compar- The woman worked as a babysitter: On biases in language\n isons. Biometrika, 39:324‚Äì345. generation. EMNLP.\nBrown, T., B. Mann, N. Ryder, M. Subbiah, J. Kaplan, Singh, S., F. Vargus, D. D‚Äôsouza, B. F. Karlsson, A. Ma-\nP. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, hendiran, W.-Y. Ko, H. Shandilya, J. Patel, D. Mat-\nA. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, aciunas, L. O‚ÄôMahony, M. Zhang, R. Hettiarachchi,\n T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, J. Wilson, M. Machado, L. S. Moura, D. KrzeminÃÅski,\n C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, H. Fadaei, I. ErguÃàn, I. Okoh, A. Alaagib, O. Mu-\nS. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, dannayake, Z. Alyafeai, V. M. Chien, S. Ruder,\n A. Radford, I. Sutskever, and D. Amodei. 2020. Language S. Guthikonda, E. A. Alghamdi, S. Gehrmann, N. Muenmodels are few-shot learners. NeurIPS, volume 33. nighoff, M. Bartolo, J. Kreutzer, A. UÃàUÃàstuÃàn, M. Fadaee,\nCao, Y., Y. Kang, C. Wang, and L. Sun. 2024. Instruction and S. Hooker. 2024. Aya dataset: An open-access collecmining: Instruction data selection for tuning large lan- tion for multilingual instruction tuning. ArXiv preprint.\n guage models. First Conference on Language Modeling. Stiennon, N., L. Ouyang, J. Wu, D. M. Ziegler, R. Lowe,\nCheng, M., E. Durmus, and D. Jurafsky. 2023. Marked per- C. Voss, A. Radford, D. Amodei, and P. Christiano. 2020.\n sonas: Using natural language prompts to measure stereo- Learning to summarize from human feedback. Proceedtypes in language models. ACL. ings of the 34th International Conference on Neural In-\nCobbe, K., V. Kosaraju, M. Bavarian, M. Chen, H. Jun, formation Processing Systems.\n L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, Sutton, R. S. and A. G. Barto. 1998. Reinforcement Learn-\nC. Hesse, and J. Schulman. 2021. Training verifiers to ing: An Introduction. MIT Press.\n solve math word problems. ArXiv preprint. Suzgun, M., N. Scales, N. SchaÃàrli, S. Gehrmann, Y. Tay,\nCui, G., L. Yuan, N. Ding, G. Yao, B. He, W. Zhu, Y. Ni, H. W. Chung, A. Chowdhery, Q. Le, E. Chi, D. Zhou, and\n G. Xie, R. Xie, Y. Lin, Z. Liu, and M. Sun. 2024. Ultra- J. Wei. 2023. Challenging BIG-bench tasks and whether\n feedback: boosting language models with scaled ai feed- chain-of-thought can solve them. ACL Findings.\n back. ICML 2024. Wang, Y., S. Mishra, P. Alipoormolabashi, Y. Kordi,\nEthayarajh, K., H. C. Zhang, and S. Behzad. 2022. Stanford A. Mirzaei, A. Naik, A. Ashok, A. S. Dhanasekaran,\n human preferences dataset v2 (shp-2). A. Arunkumar, D. Stap, E. Pathak, G. Karamanolakis,\nGehman, S., S. Gururangan, M. Sap, Y. Choi, and N. A. H. Lai, I. Purohit, I. Mondal, J. Anderson, K. Kuznia,\n Smith. 2020. RealToxicityPrompts: Evaluating neu- K. Doshi, K. K. Pal, M. Patel, M. Moradshahi, M. Parral toxic degeneration in language models. Findings of mar, M. Purohit, N. Varshney, P. R. Kaza, P. Verma,\n EMNLP. R. S. Puri, R. Karia, S. Doshi, S. K. Sampat, S. Mishra,\n S. Reddy A, S. Patro, T. Dixit, and X. Shen. 2022. Super-\nIyer, S., X. V. Lin, R. Pasunuru, T. Mihaylov, D. Simig, NaturalInstructions: Generalization via declarative in-\nP. Yu, K. Shuster, T. Wang, Q. Liu, P. S. Koura, X. Li, structions on 1600+ NLP tasks. EMNLP.\n B. O‚ÄôHoro, G. Pereyra, J. Wang, C. Dewan, A. Celikyilmaz, L. Zettlemoyer, and V. Stoyanov. 2022. Opt- Wang, Z., Y. Dong, J. Zeng, V. Adams, M. N. Sreedhar,\n iml: Scaling language model instruction meta learning D. Egert, O. Delalleau, J. Scowcroft, N. Kant, A. Swope,\n through the lens of generalization. ArXiv preprint. and O. Kuchaiev. 2024. HelpSteer: Multi-attribute helpfulness dataset for SteerLM. NAACL HLT.\nLambert, N., L. Tunstall, N. Rajani, and T. Thrush. 2023.\n Huggingface h4 stack exchange preference dataset. Wei, J., X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi,\n Q. V. Le, D. Zhou, et al. 2022. Chain-of-thought prompt-\nLongpre, S., L. Hou, T. Vu, A. Webson, H. W. Chung, Y. Tay, ing elicits reasoning in large language models. NeurIPS,\n D. Zhou, Q. V. Le, B. Zoph, J. Wei, and A. Roberts. 2023. volume 35.\n The Flan collection: Designing data and methods for effective instruction tuning. ICML. Ziegler, D. M., N. Stiennon, J. Wu, T. B. Brown, A. Radford, D. Amodei, P. Christiano, and G. Irving. 2019. Fine-\nMishra, S., D. Khashabi, C. Baral, and H. Hajishirzi. 2022. tuning language models from human preferences. ArXiv,\n Cross-task generalization via natural language crowd- abs/1909.08593.\n sourcing instructions. ACL.\n",
    "file_path": "/Users/colinsidberry/nlp-textbook-rag/data/extracted_text/9.Post-training- Instruction Tuning, Alignment, and Test-Time Compute.txt",
    "file_size_kb": 52.71
  },
  {
    "id": "76ef2ca8ae54ba73",
    "source": "nlp_textbook",
    "chapter": "10 Masked Language Models Larvatus prodeo [Masked, I go forward] Descartes",
    "filename": "10.Masked Language Models.txt",
    "content": "rights reserved. Draft of August 24, 2025.\n\nCHAPTER\n\n10 Masked Language Models\n Larvatus prodeo [Masked, I go forward]\n Descartes\n\n In the previous two chapters we introduced the transformer and saw how to pretrain a transformer language model as a causal or left-to-right language model. In\n this chapter we‚Äôll introduce a second paradigm for pretrained language models, the\n BERT\n masked\n bidirectional transformer encoder, and the most widely-used version, the BERT\n language model (Devlin et al., 2019). This model is trained via masked language modeling,\n modeling\n where instead of predicting the following word, we mask a word in the middle and\n ask the model to guess the word given the words on both sides. This method thus\n allows the model to see both the right and left context.\n finetuning We also introduced finetuning in the prior chapter. Here we describe a new\n kind of finetuning, in which we take the transformer network learned by these pretrained models, add a neural net classifier after the top layer of the network, and train\n it on some additional labeled data to perform some downstream task like named\n entity tagging or natural language inference. As before, the intuition is that the\n pretraining phase learns a language model that instantiates rich representations of\n word meaning, that thus enables the model to more easily learn (‚Äòbe finetuned to‚Äô)\n the requirements of a downstream language understanding task. This aspect of the\n transfer\n learning pretrain-finetune paradigm is an instance of what is called transfer learning in machine learning: the method of acquiring knowledge from one task or domain, and\n then applying it (transferring it) to solve a new task.\n The second idea that we introduce in this chapter is the idea of contextual embeddings: representations for words in context. The methods of Chapter 5 like\n word2vec or GloVe learned a single vector embedding for each unique word w in\n the vocabulary. By contrast, with contextual embeddings, such as those learned by\n masked language models like BERT, each word w will be represented by a different\n vector each time it appears in a different context. While the causal language models\n of Chapter 8 also use contextual embeddings, the embeddings created by masked\n language models seem to function particularly well as representations.\n\n Let‚Äôs begin by introducing the bidirectional transformer encoder that underlies models like BERT and its descendants like RoBERTa (Liu et al., 2019) or SpanBERT\n (Joshi et al., 2020). In Chapter 7 we introduced the idea of left-to-right language\n models that can be applied to autoregressive contextual generation problems like\n question answering or summarization, and in Chapter 8 we saw how to implement\n language models with causal (left-to-right) transformers. But this left-to-right nature of these models is also a limitation, because there are tasks for which it would\n be useful, when processing a token, to be able to peek at future tokens. This is es-\n2 C HAPTER 10 ‚Ä¢ M ASKED L ANGUAGE M ODELS\n\n pecially true for sequence labeling tasks in which we want to tag each token with\n a label, such as the named entity tagging task we‚Äôll introduce in Section 10.5, or\n tasks like part-of-speech tagging or parsing that come up in later chapters.\n The bidirectional encoders that we introduce here are a different kind of beast\n than causal models. The causal models of Chapter 8 are generative models, designed to easily generate the next token in a sequence. But the focus of bidirectional encoders is instead on computing contextualized representations of the input\n tokens. Bidirectional encoders use self-attention to map sequences of input embeddings (x1 , ..., xn ) to sequences of output embeddings of the same length (h1 , ..., hn ),\n where the output vectors have been contextualized using information from the entire input sequence. These output embeddings are contextualized representations of\n each input token that are useful across a range of applications where we need to do\n a classification or a decision based on the token in context.\n Remember that we said the models of Chapter 8 are sometimes called decoderonly, because they correspond to the decoder part of the encoder-decoder model we\n will introduce in Chapter 12. By contrast, the masked language models of this chapter are sometimes called encoder-only, because they produce an encoding for each\n input token but generally aren‚Äôt used to produce running text by decoding/sampling.\n That‚Äôs an important point: masked language models are not used for generation.\n They are generally instead used for interpretative tasks.\n\n 10.1.1 The architecture for bidirectional masked models\n Let‚Äôs first discuss the overall architecture. Bidirectional transformer-based language\n models differ in two ways from the causal transformers in the previous chapters. The\n first is that the attention function isn‚Äôt causal; the attention for a token i can look at\n following tokens i + 1 and so on. The second is that the training is slightly different\n since we are predicting something in the middle of our text rather than at the end.\n We‚Äôll discuss the first here and the second in the following section.\n Fig. 10.1a, reproduced here from Chapter 8, shows the information flow in the\n left-to-right approach of Chapter 8. The attention computation at each token is based\n on the preceding (and current) input tokens, ignoring potentially useful information\n located to the right of the token under consideration. Bidirectional encoders overcome this limitation by allowing the attention mechanism to range over the entire\n input, as shown in Fig. 10.1b.\n\n a1 a2 a3 a4 a5 a1 a2 a3 a4 a5\n\n attention attention attention attention attention attention attention attention attention attention\n\n x1 x2 x3 x4 x5 x1 x2 x3 x4 x5\n\n a) A causal self-attention layer b) A bidirectional self-attention layer\n\nattention value at each token is computed using only information seen earlier in the context. (b) Information\nflow in a bidirectional attention model. In processing each token, the model attends to all inputs, both before\nand after the current one. So attention for token 3 can draw on information from following tokens.\n\n The implementation is very simple! We simply remove the attention masking\n step that we introduced in Eq. ??. Recall from Chapter 8 that we had to mask the\n QK| matrix for causal transformers so that attention couldn‚Äôt look at future tokens\n 10.1 ‚Ä¢ B IDIRECTIONAL T RANSFORMER E NCODERS 3\n\n(repeated from Eq. ?? for a single attention head):\n QK|\n \u0012 \u0012 \u0013\u0013\n head = softmax mask ‚àö V\n dk\n (10.1)\n\n q1‚Ä¢k1 q1‚Ä¢k2 q1‚Ä¢k3 q1‚Ä¢k4\n q1‚Ä¢k1 ‚àí‚àû ‚àí‚àû ‚àí‚àû\n\n q2‚Ä¢k1 q2‚Ä¢k2 ‚àí‚àû ‚àí‚àû q2‚Ä¢k1 q2‚Ä¢k2 q2‚Ä¢k3 q2‚Ä¢k4\n\n N N\n q3‚Ä¢k1 q3‚Ä¢k2 q3‚Ä¢k3 ‚àí‚àû q3‚Ä¢k1 q3‚Ä¢k2 q3‚Ä¢k3 q3‚Ä¢k4\n\n q4‚Ä¢k1 q4‚Ä¢k2 q4‚Ä¢k3 q4‚Ä¢k4 q4‚Ä¢k1 q4‚Ä¢k2 q4‚Ä¢k3 q4‚Ä¢k4\n\n N N\n\n (a) (b)\nportion of the comparisons matrix zeroed out (set to ‚àí‚àû, which the softmax will turn to zero),\nwhile (b) shows the unmasked version.\n\n Fig. 10.2 shows the masked version of QK| and the unmasked version. For bidirectional attention, we use the unmasked version of Fig. 10.2b. Thus the attention\ncomputation for bidirectional attention is exactly the same as Eq. 10.1 but with the\nmask removed:\n QK|\n \u0012 \u0013\n head = softmax ‚àö V (10.2)\n dk\nOtherwise, the attention computation is identical to what we saw in Chapter 8, as is\nthe transformer block architecture (the feedforward layer, layer norm, and so on). As\nin Chapter 8, the input is also a series of subword tokens, usually computed by one of\nthe 3 popular tokenization algorithms (including the BPE algorithm that we already\nsaw in Chapter 2 and two others, the WordPiece algorithm and the SentencePiece\nUnigram LM algorithm). That means every input sentence first has to be tokenized,\nand all further processing takes place on subword tokens rather than words. This will\nrequire, as we‚Äôll see in the third part of the textbook, that for some NLP tasks that\nrequire notions of words (like parsing) we will occasionally need to map subwords\nback to words.\n To make this more concrete, the original English-only bidirectional transformer\nencoder model, BERT (Devlin et al., 2019), consisted of the following:\n ‚Ä¢ An English-only subword vocabulary consisting of 30,000 tokens generated\n using the WordPiece algorithm (Schuster and Nakajima, 2012).\n ‚Ä¢ Input context window N=512 tokens, and model dimensionality d=768\n ‚Ä¢ So X, the input to the model, is of shape [N √ó d] = [512 √ó 768].\n ‚Ä¢ L=12 layers of transformer blocks, each with A=12 (bidirectional) multihead\n attention layers.\n ‚Ä¢ The resulting model has about 100M parameters.\nThe larger multilingual XLM-RoBERTa model, trained on 100 languages, has\n ‚Ä¢ A multilingual subword vocabulary with 250,000 tokens generated using the\n SentencePiece Unigram LM algorithm (Kudo and Richardson, 2018).\n4 C HAPTER 10 ‚Ä¢ M ASKED L ANGUAGE M ODELS\n\n ‚Ä¢ Input context window N=512 tokens, and model dimensionality d=1024, hence\n X, the input to the model, is of shape [N √ó d] = [512 √ó 1024].\n ‚Ä¢ L=24 layers of transformer blocks, with A=16 multihead attention layers each\n ‚Ä¢ The resulting model has about 550M parameters.\n Note that 550M parameters is relatively small as large language models go\n (Llama 3 has 405B parameters, so is 3 orders of magnitude bigger). Indeed, masked\n language models tend to be much smaller than causal language models.\n\n We trained causal transformer language models in Chapter 8 by making them iteratively predict the next word in a text. But eliminating the causal mask in attention makes the guess-the-next-word language modeling task trivial‚Äîthe answer\n is directly available from the context‚Äîso we‚Äôre in need of a new training scheme.\n Instead of trying to predict the next word, the model learns to perform a fill-in-thecloze task blank task, technically called the cloze task (Taylor, 1953). To see this, let‚Äôs return\n to the motivating example from Chapter 3. Instead of predicting which words are\n likely to come next in this example:\n The water of Walden Pond is so beautifully\n we‚Äôre asked to predict a missing item given the rest of the sentence.\n The of Walden Pond is so beautifully ...\n That is, given an input sequence with one or more elements missing, the learning\n task is to predict the missing elements. More precisely, during training the model is\n deprived of one or more tokens of an input sequence and must generate a probability\n distribution over the vocabulary for each of the missing items. We then use the crossentropy loss from each of the model‚Äôs predictions to drive the learning process.\n This approach can be generalized to any of a variety of methods that corrupt the\n training input and then asks the model to recover the original input. Examples of the\n kinds of manipulations that have been used include masks, substitutions, reorderings, deletions, and extraneous insertions into the training text. The general name\n denoising for this kind of training is called denoising: we corrupt (add noise to) the input in\n some way (by masking a word, or putting in an incorrect word) and the goal of the\n system is to remove the noise.\n\n 10.2.1 Masking Words\n Masked\n Language Let‚Äôs describe the Masked Language Modeling (MLM) approach to training bidi-\nModeling\n rectional encoders (Devlin et al., 2019). As with the language model training methods we‚Äôve already seen, MLM uses unannotated text from a large corpus. In MLM\n training, the model is presented with a series of sentences from the training corpus\n in which a percentage of tokens (15% in the BERT model) have been randomly chosen to be manipulated by the masking procedure. Given an input sentence lunch\n was delicious and assume we randomly chose the 3rd token delicious to be\n manipulated,\n ‚Ä¢ 80% of the time: The token is replaced with the special vocabulary token\n named [MASK], e.g. lunch was delicious ‚Üí lunch was [MASK].\n 10.2 ‚Ä¢ T RAINING B IDIRECTIONAL E NCODERS 5\n\n ‚Ä¢ 10% of the time: The token is replaced with another token, randomly sampled\n from the vocabulary based on token unigram probabilities. e.g. lunch was\n delicious ‚Üí lunch was gasp.\n ‚Ä¢ 10% of the time: the token is left unchanged. e.g. lunch was delicious\n ‚Üí lunch was delicious.\n We then train the model to guess the correct token for the manipulated tokens. Why\n the three possible manipulations? Adding the [MASK] token creates a mismatch\n between pretraining and downstream finetuning or inference, since when we employ\n the MLM model to perform a downstream task, we don‚Äôt use any [MASK] tokens. If\n we just replaced tokens with the [MASK], the model might only predict tokens when\n it sees a [MASK], but we want the model to try to always predict the input token.\n To train the model to make the prediction, the original input sequence is tokenized using a subword model and tokens are sampled to be manipulated. Word\n embeddings for all of the tokens in the input are retrieved from the E embedding matrix and combined with positional embeddings to form the input to the transformer,\n passed through the stack of bidirectional transformer blocks, and then the language\n modeling head. The MLM training objective is to predict the original inputs for\n each of the masked tokens and the cross-entropy loss from these predictions drives\n the training process for all the parameters in the model. That is, all of the input\n tokens play a role in the self-attention process, but only the sampled tokens are used\n for learning.\n\n long thanks the\n\n CE Loss\n\n LM Head with Softmax\n over Vocabulary\n\n hL1 hL2 hL3 hL4 hL5 hL6 hL7 hL8\n\n Bidirectional Transformer Encoder\n\n Token + + + + + + + + +\n Positional\n Embeddings p1 p2 p3 p4 p5 p6 p7 p8\n\n So [mask] and [mask] for all apricot fish\n So long and thanks for all the fish\nwhich are masked and the third is replaced with an unrelated word. The probabilities assigned by the model to\nthese three items are used as the training loss. The other 5 tokens don‚Äôt play a role in training loss.\n\n Fig. 10.3 illustrates this approach with a simple example. Here, long, thanks and\n the have been sampled from the training sequence, with the first two masked and the\n replaced with the randomly sampled token apricot. The resulting embeddings are\n passed through a stack of bidirectional transformer blocks. Recall from Section ??\n in Chapter 8 that to produce a probability distribution over the vocabulary for each\n of the masked tokens, the language modeling head takes the output vector hLi from\n the final transformer layer L for each masked token i, multiplies it by the unembedding layer ET to produce the logits u, and then uses softmax to turn the logits into\n6 C HAPTER 10 ‚Ä¢ M ASKED L ANGUAGE M ODELS\n\n probabilities y over the vocabulary:\n\n ui = hLi ET (10.3)\n yi = softmax(ui ) (10.4)\n\n With a predicted probability distribution for each masked item, we can use crossentropy to compute the loss for each masked item‚Äîthe negative log probability\n assigned to the actual masked word, as shown in Fig. 10.3. More formally, for a\n given vector of input tokens in a sentence or batch x, let the set of tokens that are\n masked be M, the version of that sentence with some tokens replaced by masks be\n xmask , and the sequence of output vectors be h. For a given input token xi , such as\n the word long in Fig. 10.3, the loss is the probability of the correct word long, given\n xmask (as summarized in the single output vector hLi ):\n\n LMLM (xi ) = ‚àí log P(xi |hLi )\n\n The gradients that form the basis for the weight updates are based on the average\n loss over the sampled learning items from a single training sequence (or batch of\n sequences).\n 1 X\n LMLM = ‚àí log P(xi |hLi )\n i‚ààM\n\n Note that only the tokens in M play a role in learning; the other words play no role\n in the loss function, so in that sense BERT and its descendents are inefficient; only\n 15% of the input samples in the training data are actually used for training weights.1\n\n 10.2.2 Next Sentence Prediction\n The focus of mask-based learning is on predicting words from surrounding contexts\n with the goal of producing effective word-level representations. However, an important class of applications involves determining the relationship between pairs of\n sentences. These include tasks like paraphrase detection (detecting if two sentences\n have similar meanings), entailment (detecting if the meanings of two sentences entail or contradict each other) or discourse coherence (deciding if two neighboring\n sentences form a coherent discourse).\n To capture the kind of knowledge required for applications such as these, some\n models in the BERT family include a second learning objective called Next Sen-\nNext Sentence tence Prediction (NSP). In this task, the model is presented with pairs of sentences\n Prediction\n and is asked to predict whether each pair consists of an actual pair of adjacent sentences from the training corpus or a pair of unrelated sentences. In BERT, 50% of\n the training pairs consisted of positive pairs, and in the other 50% the second sentence of a pair was randomly selected from elsewhere in the corpus. The NSP loss\n is based on how well the model can distinguish true pairs from random pairs.\n To facilitate NSP training, BERT introduces two special tokens to the input representation (tokens that will prove useful for finetuning as well). After tokenizing\n the input with the subword model, the token [CLS] is prepended to the input sentence pair, and the token [SEP] is placed between the sentences and after the final\n token of the second sentence. There are actually two more special tokens, a ‚ÄòFirst\n Segment‚Äô token, and a ‚ÄòSecond Segment‚Äô token. These tokens are added in the input stage to the word and positional embeddings. That is, each token of the input\n 1 ELECTRA, another BERT family member, does use all examples for training (Clark et al., 2020).\n 10.2 ‚Ä¢ T RAINING B IDIRECTIONAL E NCODERS 7\n\n X is actually formed by summing 3 embeddings: word, position, and first/second\n segment embeddings.\n During training, the output vector hLCLS from the final layer associated with the\n [CLS] token represents the next sentence prediction. As with the MLM objective,\n we add a special head, in this case an NSP head, which consists of a learned set of\n classification weights WNSP ‚àà Rd√ó2 that produces a two-class prediction from the\n raw [CLS] vector hLCLS :\n\n yi = softmax(hLCLS WNSP )\n\n Cross entropy is used to compute the NSP loss for each sentence pair presented\n to the model. Fig. 10.4 illustrates the overall NSP training setup. In BERT, the NSP\n loss was used in conjunction with the MLM training objective to form final loss.\n\n CE Loss\n\n NSP\n Head\n\n hCLS\n\n Bidirectional Transformer Encoder\n\n Token +\n Segment + + + + + + + + + + + + + + + + + + +\n Positional s1 p1 s1 p2 s1 p3 s1 p4 s1 p5 s2 p6 s2 p7 s2 p8 s2 p9\n Embeddings\n [CLS] Cancel my flight [SEP] And the hotel [SEP]\n\n 10.2.3 Training Regimes\n BERT and other early transformer-based language models were trained on about\n called BooksCorpus (Zhu et al., 2015) that is no longer used for intellectual property\n reasons). Modern masked language models are now trained on much larger datasets\n of web text, filtered a bit, and augmented by higher-quality data like Wikipedia,\n the same as those we discussed for the causal large language models of Chapter 8.\n Multilingual models similarly use webtext and multilingual Wikipedia. For example\n the XLM-R model was trained on about 300 billion tokens in 100 languages, taken\n from the web via Common Crawl (https://commoncrawl.org/).\n To train the original BERT models, pairs of text segments were selected from the\n training corpus according to the next sentence prediction 50/50 scheme. Pairs were\n sampled so that their combined length was less than the 512 token input. Tokens\n within these sentence pairs were then masked using the MLM approach with the\n combined loss from the MLM and NSP objectives used for a final loss. Because this\n final loss is backpropagated through the entire transformer, the embeddings at each\n transformer layer will learn representations that are useful for predicting words from\n their neighbors. Since the [CLS] tokens are the direct input to the NSP classifier,\n their learned representations will tend to contain information about the sequence as\n8 C HAPTER 10 ‚Ä¢ M ASKED L ANGUAGE M ODELS\n\n a whole. Approximately 40 passes (epochs) over the training data was required for\n the model to converge.\n Some models, like the RoBERTa model, drop the next sentence prediction objective, and therefore change the training regime a bit. Instead of sampling pairs of\n sentence, the input is simply a series of contiguous sentences, still beginning with\n the special [CLS] token. If the document runs out before 512 tokens are reached, an\n extra separator token is added, and sentences from the next document are packed in,\n until we reach a total of 512 tokens. Usually large batch sizes are used, between 8K\n and 32K tokens.\n Multilingual models have an additional decision to make: what data to use to\n build the vocabulary? Recall that all language models use subword tokenization\n (BPE or SentencePiece Unigram LM are the two most common algorithms). What\n text should be used to learn this multilingual tokenization, given that it‚Äôs easier to\n get much more text in some languages than others? One option would be to create this vocabulary-learning dataset by sampling sentences from our training data\n (perhaps web text from Common Crawl), randomly. In that case we will choose a\n lot of sentences from languages with lots of web representation like English, and\n the tokens will be biased toward rare English tokens instead of creating frequent\n tokens from languages with less data. Instead, it is common to divide the training\n data into subcorpora of N different languages, compute the number of sentences ni\n of each language i, and readjust these probabilities so as to upweight the probability\n of less-represented languages (Lample and Conneau, 2019). The new probability of\n selecting a sentence from each of the N languages (whose prior frequency is ni ) is\n {qi }i=1...N , where:\n\n pŒ± ni\n qi = PN i Œ±\n with pi = PN (10.5)\n j=1 p j k=1 nk\n\n Recall from Eq. ?? in Chapter 5 that an Œ± value between 0 and 1 will give higher\n weight to lower probability samples. Conneau et al. (2020) show that Œ± = 0.3 works\n well to give rare languages more inclusion in the tokenization, resulting in better\n multilingual performance overall.\n The result of this pretraining process consists of both learned word embeddings,\n as well as all the parameters of the bidirectional encoder that are used to produce\n contextual embeddings for novel inputs.\n For many purposes, a pretrained multilingual model is more practical than a\n monolingual model, since it avoids the need to build many (a hundred!) separate\n monolingual models. And multilingual models can improve performance on lowresourced languages by leveraging linguistic information from a similar language in\n the training data that happens to have more resources. Nonetheless, when the number of languages grows very large, multilingual models exhibit what has been called\n the curse of multilinguality (Conneau et al., 2020): the performance on each language degrades compared to a model training on fewer languages. Another problem\n with multilingual models is that they ‚Äòhave an accent‚Äô: grammatical structures in\n higher-resource languages (often English) bleed into lower-resource languages; the\n vast amount of English language in training makes the model‚Äôs representations for\n low-resource languages slightly more English-like (Papadimitriou et al., 2023).\n 10.3 ‚Ä¢ C ONTEXTUAL E MBEDDINGS 9\n\n Given a pretrained language model and a novel input sentence, we can think of the\n contextual\n embeddings sequence of model outputs as constituting contextual embeddings for each token in\n the input. These contextual embeddings are vectors representing some aspect of the\n meaning of a token in context, and can be used for any task requiring the meaning\n of tokens or words. More formally, given a sequence of input tokens x1 , ..., xn , we\n can use the output vector hL i from the final layer L of the model as a representation\n of the meaning of token xi in the context of sentence x1 , ..., xn . Or instead of just\n using the vector hL i from the final layer of the model, it‚Äôs common to compute a\n representation for xi by averaging the output tokens hi from each of the last four\n layers of the model, i.e., hL i , hL‚àí1 i , hL‚àí2 i , and hL‚àí3 i .\n\n hLCLS hL1 hL2 hL3 hL4 hL5 hL6\n\n + i + i + i + i + i + i + i\n E E E E E E E\n\n [CLS] So long and thanks for all\n each input token xi .\n\n Just as we used static embeddings like word2vec in Chapter 5 to represent the\n meaning of words, we can use contextual embeddings as representations of word\n meanings in context for any task that might require a model of word meaning. Where\n static embeddings represent the meaning of word types (vocabulary entries), contextual embeddings represent the meaning of word instances: instances of a particular\n word type in a particular context. Thus where word2vec had a single vector for each\n word type, contextual embeddings provide a single vector for each instance of that\n word type in its sentential context. Contextual embeddings can thus be used for\n tasks like measuring the semantic similarity of two words in context, and are useful\n in linguistic tasks that require models of word meaning.\n\n 10.3.1 Contextual Embeddings and Word Sense\n ambiguous Words are ambiguous: the same word can be used to mean different things. In\n Chapter 5 we saw that the word ‚Äúmouse‚Äù can mean (1) a small rodent, or (2) a handoperated device to control a cursor. The word ‚Äúbank‚Äù can mean: (1) a financial\n institution or (2) a sloping mound. We say that the words ‚Äòmouse‚Äô or ‚Äòbank‚Äô are\n10 C HAPTER 10 ‚Ä¢ M ASKED L ANGUAGE M ODELS\n\n polysemous (from Greek ‚Äòmany senses‚Äô, poly- ‚Äòmany‚Äô + sema, ‚Äòsign, mark‚Äô).2\n word sense A sense (or word sense) is a discrete representation of one aspect of the meaning\n of a word. We can represent each sense with a superscript: bank1 and bank2 ,\n mouse1 and mouse2 . These senses can be found listed in online thesauruses (or\n WordNet thesauri) like WordNet (Fellbaum, 1998), which has datasets in many languages\n listing the senses of many words. In context, it‚Äôs easy to see the different meanings:\n mouse1 : .... a mouse controlling a computer system in 1968.\n mouse2 : .... a quiet animal like a mouse\n bank1 : ...a bank can hold the investments in a custodial account ...\n bank2 : ...as agriculture burgeons on the east bank, the river ...\n This fact that context disambiguates the senses of mouse and bank above can\n also be visualized geometrically. Fig. 10.6 shows a two-dimensional projection of\n many instances of the BERT embeddings of the word die in English and German.\n Each point in the graph represents the use of die in one input sentence. We can\n clearly see at least two different English senses of die (the singular of dice and the\n verb to die, as well as the German article, in the BERT embedding space.\n\nin English and German, projected into two dimensions with the UMAP algorithm. The German and English\nmeanings and the different English senses fall into different clusters. Some sample points are shown with the\ncontextual sentence they came from. Figure from Coenen et al. (2019).\n are annotated with corresponding sentences. Overall annotations (blue text) are added as a guide.\n Thus while thesauruses like WordNet give discrete lists of senses, embeddings\n (whether\n or contextual) offer a continuous high-dimensional model of meaning\n that, although it can be clustered, doesn‚Äôt divide up into fully discrete senses.\nOur first experiment is an exploratory visualization of how word sense affects context embeddings.\n Word word\nFor data on different Sensesenses,\n Disambiguation\n we collected all sentences used in the introductions to Englishlanguage Wikipedia articles. (Text outside of introductions was frequently fragmentary.) We created\nan interactive\n word sense\n The task of selecting\n application, which wethe plancorrect sense\n to make for a word\n public. A user entersword\n is called a word,sense\n anddisambiguathe system\n tion, WSD.\nretrieves 1,000 sentences containing that word. It sends these sentences to BERT-basefixed\ndisambiguation or WSD algorithms take as input a word in context and a inventory\n as input, and\nfor eachWSD of potential\n one it retrieves word senses\n the context (like the\n embedding for ones in WordNet)\n the word from a layer andofoutputs thechoosing.\n the user‚Äôs correct word\n sense in context. Fig. 10.7 sketches out the task.\nThe system visualizes these 1,000 context embeddings using UMAP [15], generally showing clear\nclusters relating2 toThe\n word\n wordsenses.\n polysemyDifferent senses of\n itself is ambiguous; youamay\n word\n see are typically\n it used spatially\n in a different way, toseparated,\n refer only to and\n cases\nwithin the clusters there\n where is often\n a word‚Äôs sensesfurther structure\n are related in some related toway,\n structured finereserving\n shades theof word\n meaning. In Figure\n homonymy to mean4,sense\n for\nexample, we not only seewith\n ambiguities crisp, well-separated\n no relation between the clusters for three\n senses (Haber meanings\n and Poesio, of the\n 2020). Here we word\n will use‚Äúdie,‚Äù but\n ‚Äòpolysemy‚Äô\nwithin one of these clusters\n to mean any kindthere is ambiguity,\n of sense a kind ofand quantitative scale, related\n ‚Äòstructured polysemy‚Äô to thewith\n for polysemy number of people\n sense elations.\ndying. See Appendix 6.4 for further examples. The apparent detail in the clusters we visualized raises\ntwo immediate questions. First, is it possible to find quantitative corroboration that word senses are\nwell-represented? Second, how can we resolve a seeming contradiction: in the previous section, we\nsaw how position represented syntax; yet here we see position representing semantics.\n\n 10.3 ‚Ä¢ C ONTEXTUAL E MBEDDINGS 11\n\n y5 y6\n y3\n stand1: side1:\n y1 bass1: y4 upright relative\n low range ‚Ä¶ region\n electric1: ‚Ä¶ player1: stand5: ‚Ä¶\n using bass4: in game bear side3:\n electricity sea fish player2: ‚Ä¶ of body\n electric2: ‚Ä¶ musician stand10: ‚Ä¶\n tense\n y2 bass7: player3: put side11:\n electric3: instrument actor upright slope\n thrilling guitar1 ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶\n\n x1 x2 x3 x4 x5 x6\n an electric guitar and bass player stand oÔ¨Ä to one side\n\n (y). Figure inspired by Chaplot and Salakhutdinov (2018).\n\n WSD can be a useful analytic tool for text analysis in the humanities and social\n sciences, and word senses can play a role in model interpretability for word representations. Word senses also have interesting distributional properties. For example\n a word often is used in roughly the same sense through a discourse, an observation\none sense per\n discourse called the one sense per discourse rule (Gale et al., 1992).\n The best performing WSD algorithm is a simple 1-nearest-neighbor algorithm\n using contextual word embeddings, due to Melamud et al. (2016) and Peters et al.\n (2018). At training time we pass each sentence in some sense-labeled dataset (like\n the SemCore or SenseEval datasets in various languages) through any contextual\n embedding (e.g., BERT) resulting in a contextual embedding for each labeled token.\n (There are various ways to compute this contextual embedding vi for a token i; for\n BERT it is common to pool multiple layers by summing the vector representations\n of i from the last four BERT layers). Then for each sense s of any word in the corpus,\n for each of the n tokens of that sense, we average their n contextual representations\n vi to produce a contextual sense embedding vs for s:\n 1X\n vs = vi ‚àÄvi ‚àà tokens(s) (10.6)\n n\n i\n\n At test time, given a token of a target word t in context, we compute its contextual\n embedding t and choose its nearest neighbor sense from the training set, i.e., the\n sense whose sense embedding has the highest cosine with t:\n\n sense(t) = argmax cosine(t, vs ) (10.7)\n s‚ààsenses(t)\n\n Fig. 10.8 illustrates the model.\n\n 10.3.2 Contextual Embeddings and Word Similarity\n In Chapter 5 we introduced the idea that we could measure the similarity of two\n words by considering how close they are geometrically, by using the cosine as a\n similarity function. The idea of meaning similarity is also clear geometrically in the\n meaning clusters in Fig. 10.6; the representation of a word which has a particular\n sense in a context is closer to other instances of the same sense of the word. Thus we\n12 C HAPTER 10 ‚Ä¢ M ASKED L ANGUAGE M ODELS\n\n find5\n find4\n v\n v\n find1\n v\n find9\n v\n\n cI cfound cthe cjar cempty\n\n ENCODER\n I found the jar empty\n find. A contextual embedding is computed for the target word found, and then the nearest\n neighbor sense (in this case find9v ) is chosen. Figure inspired by Loureiro and Jorge (2019).\n\n often measure the similarity between two instances of two words in context (or two\n instances of the same word in two different contexts) by using the cosine between\n their contextual embeddings.\n Usually some transformations to the embeddings are required before computing\n cosine. This is because contextual embeddings (whether from masked language\n models or from autoregressive ones) have the property that the vectors for all words\n are extremely similar. If we look at the embeddings from the final layer of BERT\n or other models, embeddings for instances of any two randomly chosen words will\n have extremely high cosines that can be quite close to 1, meaning all word vectors\n tend to point in the same direction. The property of vectors in a system all tending\n to point in the same direction is known as anisotropy. Ethayarajh (2019) defines\n anisotropy the anisotropy of a model as the expected cosine similarity of any pair of words in\n a corpus. The word ‚Äòisotropy‚Äô means uniformity in all directions, so in an isotropic\n model, the collection of vectors should point in all directions and the expected cosine\n between a pair of random embeddings would be zero. Timkey and van Schijndel\n (2021) show that one cause of anisotropy is that cosine measures are dominated by\n a small number of dimensions of the contextual embedding whose values are very\n different than the others: these rogue dimensions have very large magnitudes and\n very high variance.\n Timkey and van Schijndel (2021) shows that we can make the embeddings more\n isotropic by standardizing (z-scoring) the vectors, i.e., subtracting the mean and\n dividing by the variance. Given a set C of all the embeddings in some corpus, each\n with dimensionality d (i.e., x ‚àà Rd ), the mean vector ¬µ ‚àà Rd is:\n 1 X\n ¬µ= x (10.8)\n x‚ààC\n\n The standard deviation in each dimension œÉ ‚àà Rd is:\n 1 X\n s\n œÉ= (x ‚àí ¬µ)2 (10.9)\n x‚ààC\n\n Then each word vector x is replaced by a standardized version z:\n x‚àí¬µ\n z= (10.10)\n œÉ\n 10.4 ‚Ä¢ F INE -T UNING FOR C LASSIFICATION 13\n\n One problem with cosine that is not solved by standardization is that cosine tends\n to underestimate human judgments on similarity of word meaning for very frequent\n words (Zhou et al., 2022).\n\n The power of pretrained language models lies in their ability to extract generalizations from large amounts of text‚Äîgeneralizations that are useful for myriad downstream applications. There are two ways to make practical use of the generalizations\n to solve downstream tasks. The most common way is to use natural language to\n prompt the model, putting it in a state where it contextually generates what we\n want.\n In this section we explore an alternative way to use pretrained language models\n finetuning for downstream applications: a version of the finetuning paradigm from Chapter 7.\n In the kind of finetuning used for masked language models, we add applicationspecific circuitry (often called a special head) on top of pretrained models, taking\n their output as its input. The finetuning process consists of using labeled data about\n the application to train these additional application-specific parameters. Typically,\n this training will either freeze or make only minimal adjustments to the pretrained\n language model parameters.\n The following sections introduce finetuning methods for the most common kinds\n of applications: sequence classification, sentence-pair classification, and sequence\n labeling.\n\n 10.4.1 Sequence Classification\n The task of sequence classification is to classify an entire sequence of text with a\n single label. This set of tasks is commonly called text classification, like sentiment\n analysis or spam detection (Appendix K) in which we classify a text into two or\n three classes (like positive or negative), as well as classification tasks with a large\n number of categories, like document-level topic classification.\n For sequence classification we represent the entire input to be classified by a\n single vector. We can represent a sequence in various ways. One way is to take\n the sum or the mean of the last output vector from each token in the sequence.\n For BERT, we instead add a new unique token to the vocabulary called [CLS], and\n prepended it to the start of all input sequences, both during pretraining and encoding.\n The output vector in the final layer of the model for the [CLS] input represents\nclassifier head the entire input sequence and serves as the input to a classifier head, a logistic\n regression or neural network classifier that makes the relevant decision.\n As an example, let‚Äôs return to the problem of sentiment classification. Finetuning\n a classifier for this application involves learning a set of weights, WC , to map the\n output vector for the [CLS] token‚ÄîhLCLS ‚Äîto a set of scores over the possible sentiment classes. Assuming a three-way sentiment classification task (positive, negative,\n neutral) and dimensionality d as the model dimension, WC will be of size [d √ó 3]. To\n classify a document, we pass the input text through the pretrained language model to\n generate hLCLS , multiply it by WC , and pass the resulting vector through a softmax.\n\n y = softmax(hLCLS WC ) (10.11)\n\n Finetuning the values in WC requires supervised training data consisting of input\n14 C HAPTER 10 ‚Ä¢ M ASKED L ANGUAGE M ODELS\n\n sequences labeled with the appropriate sentiment class. Training proceeds in the\n usual way; cross-entropy loss between the softmax output and the correct answer is\n used to drive the learning that produces WC .\n This loss can be used to not only learn the weights of the classifier, but also to\n update the weights for the pretrained language model itself. In practice, reasonable\n classification performance is typically achieved with only minimal changes to the\n language model parameters, often limited to updates over the final few layers of the\n transformer. Fig. 10.9 illustrates this overall approach to sequence classification.\n\n y\n\n sentiment\n classification\n head WC\n\n hCLS\n\n Bidirectional Transformer Encoder\n\n + i + i + i + i + i + i\n\n E E E E E E\n\n [CLS] entirely predictable and lacks energy\n[CLS] token serves as input to a simple classifier.\n\n 10.4.2 Sequence-Pair Classification\n As mentioned in Section 10.2.2, an important type of problem involves the classification of pairs of input sequences. Practical applications that fall into this class include\n paraphrase detection (are the two sentences paraphrases of each other?), logical entailment (does sentence A logically entail sentence B?), and discourse coherence\n (how coherent is sentence B as a follow-on to sentence A?).\n Fine-tuning an application for one of these tasks proceeds just as with pretraining using the NSP objective. During finetuning, pairs of labeled sentences from a\n supervised finetuning set are presented to the model, and run through all the layers\n of the model to produce the h outputs for each input token. As with sequence classification, the output vector associated with the prepended [CLS] token represents the\n model‚Äôs view of the input pair. And as with NSP training, the two inputs are separated by the [SEP] token. To perform classification, the [CLS] vector is multiplied\n by a set of learned classification weights and passed through a softmax to generate\n label predictions, which are then used to update the weights.\n As an example, let‚Äôs consider an entailment classification task with the Multi-\nGenre Natural Language Inference (MultiNLI) dataset (Williams et al., 2018). In\n natural\n language the task of natural language inference or NLI, also called recognizing textual\n inference\n entailment, a model is presented with a pair of sentences and must classify the relationship between their meanings. For example in the MultiNLI corpus, pairs of\n sentences are given one of 3 labels: entails, contradicts and neutral. These labels\n 10.5 ‚Ä¢ F INE -T UNING FOR S EQUENCE L ABELLING : NAMED E NTITY R ECOGNITION 15\n\n describe a relationship between the meaning of the first sentence (the premise) and\n the meaning of the second sentence (the hypothesis). Here are representative examples of each class from the corpus:\n ‚Ä¢ Neutral\n a: Jon walked back to the town to the smithy.\n b: Jon traveled back to his hometown.\n ‚Ä¢ Contradicts\n a: Tourist Information offices can be very helpful.\n b: Tourist Information offices are never of any help.\n ‚Ä¢ Entails\n a: I‚Äôm confused.\n b: Not all of it is very clear to me.\n A relationship of contradicts means that the premise contradicts the hypothesis; entails means that the premise entails the hypothesis; neutral means that neither is\n necessarily true. The meaning of these labels is looser than strict logical entailment\n or contradiction indicating that a typical human reading the sentences would most\n likely interpret the meanings in this way.\n To finetune a classifier for the MultiNLI task, we pass the premise/hypothesis\n pairs through a bidirectional encoder as described above and use the output vector\n for the [CLS] token as the input to the classification head. As with ordinary sequence\n classification, this head provides the input to a three-way classifier that can be trained\n on the MultiNLI training corpus.\n\n tity Recognition\n In sequence labeling, the network‚Äôs task is to assign a label chosen from a small\n fixed set of labels to each token in the sequence. One of the most common sequence\n labeling task is named entity recognition.\n\n 10.5.1 Named Entities\n named entity A named entity is, roughly speaking, anything that can be referred to with a proper\n named entity\n recognition name: a person, a location, an organization. The task of named entity recognition\n NER (NER) is to find spans of text that constitute proper names and tag the type of the\n entity. Four entity tags are most common: PER (person), LOC (location), ORG\n (organization), or GPE (geo-political entity). However, the term named entity is\n commonly extended to include things that aren‚Äôt entities per se, including temporal\n expressions like dates and times, and even numerical expressions like prices. Here‚Äôs\n an example of the output of an NER tagger:\n Citing high fuel prices, [ORG United Airlines] said [TIME Friday] it\n has increased fares by [MONEY $6] per round trip on flights to some\n cities also served by lower-cost carriers. [ORG American Airlines], a\n unit of [ORG AMR Corp.], immediately matched the move, spokesman\n [PER Tim Wagner] said. [ORG United], a unit of [ORG UAL Corp.],\n16 C HAPTER 10 ‚Ä¢ M ASKED L ANGUAGE M ODELS\n\n said the increase took effect [TIME Thursday] and applies to most\n routes where it competes against discount carriers, such as [LOC Chicago]\n to [LOC Dallas] and [LOC Denver] to [LOC San Francisco].\n The text contains 13 mentions of named entities including 5 organizations, 4 locations, 2 times, 1 person, and 1 mention of money. Figure 10.10 shows typical generic\n named entity types. Many applications will also need to use specific entity types like\n proteins, genes, commercial products, or works of art.\n\nType Tag Sample Categories Example sentences\nPeople PER people, characters Turing is a giant of computer science.\nOrganization ORG companies, sports teams The IPCC warned about the cyclone.\nLocation LOC regions, mountains, seas Mt. Sanitas is in Sunshine Canyon.\nGeo-Political Entity GPE countries, states Palo Alto is raising the fees for parking.\n\n Named entity recognition is a useful step in various natural language processing\n tasks, including linking text to information in structured knowledge sources like\n Wikipedia, measuring sentiment or attitudes toward a particular entity in text, or\n even as part of anonymizing text for privacy. The NER task is difficult because\n of the ambiguity of segmenting NER spans, figuring out which tokens are entities\n and which aren‚Äôt, since most words in a text will not be named entities. Another\n difficulty is caused by type ambiguity. The mention Washington can refer to a\n person, a sports team, a city, or the US government, as we see in Fig. 10.11.\n\n [PER Washington] was born into slavery on the farm of James Burroughs.\n [ORG Washington] went up 2 games to 1 in the four-game series.\n Blair arrived in [LOC Washington] for what may well be his last state visit.\n In June, [GPE Washington] passed a primary seatbelt law.\n\n 10.5.2 BIO Tagging\n One standard approach to sequence labeling for a span-recognition problem like\n BIO tagging NER is BIO tagging (Ramshaw and Marcus, 1995). This is a method that allows us\n to treat NER like a word-by-word sequence labeling task, via tags that capture both\n the boundary and the named entity type. Consider the following sentence:\n [PER Jane Villanueva ] of [ORG United] , a unit of [ORG United Airlines\n Holding] , said the fare applies to the [LOC Chicago ] route.\n BIO Figure 10.12 shows the same excerpt represented with BIO tagging, as well\n as variants called IO tagging and BIOES tagging. In BIO tagging we label any\n token that begins a span of interest with the label B, tokens that occur inside a span\n are tagged with an I, and any tokens outside of any span of interest are labeled O.\n While there is only one O tag, we‚Äôll have distinct B and I tags for each named entity\n class. The number of tags is thus 2n + 1, where n is the number of entity types. BIO\n tagging can represent exactly the same information as the bracketed notation, but has\n the advantage that we can represent the task in the same simple sequence modeling\n way as part-of-speech tagging: assigning a single label yi to each input word xi :\n We‚Äôve also shown two variant tagging schemes: IO tagging, which loses some\n information by eliminating the B tag, and BIOES tagging, which adds an end tag E\n for the end of a span, and a span tag S for a span consisting of only one word.\n 10.5 ‚Ä¢ F INE -T UNING FOR S EQUENCE L ABELLING : NAMED E NTITY R ECOGNITION 17\n\n Words IO Label BIO Label BIOES Label\n Jane I-PER B-PER B-PER\n Villanueva I-PER I-PER E-PER\n of O O O\n United I-ORG B-ORG B-ORG\n Airlines I-ORG I-ORG I-ORG\n Holding I-ORG I-ORG E-ORG\n discussed O O O\n the O O O\n Chicago I-LOC B-LOC S-LOC\n route O O O\n . O O O\n\n 10.5.3 Sequence Labeling\n In sequence labeling, we pass the final output vector corresponding to each input\n token to a classifier that produces a softmax distribution over the possible set of\n tags. For a single feedforward layer classifier, the set of weights to be learned is\n WK of size [d √ó k], where k is the number of possible tags for the task. A greedy\n approach, where the argmax tag for each token is taken as a likely answer, can be\n used to generate the final output tag sequence. Fig. 10.13 illustrates an example of\n this approach, where yi is a vector of probabilities over tags, and k indexes the tags.\n\n yi = softmax(hLi WK ) (10.12)\n ti = argmaxk (yi ) (10.13)\n\n Alternatively, the distribution over labels provided by the softmax for each input\n token can be passed to a conditional random field (CRF) layer which can take global\n tag-level transitions into account (see Chapter 17 on CRFs).\n\n argmax B-PER I-PER O B-ORG I-ORG I-ORG O\n yi\n\n NER\n head WK WK WK WK WK WK WK\n\n hi\n\n Bidirectional Transformer Encoder\n\n + i + i + i + i + i + i + i + i\n\n E E E E E E E E\n\n [CLS] Jane Villanueva of United Airlines Holding discussed\noutput vector for each input token is passed to a simple k-way classifier.\n18 C HAPTER 10 ‚Ä¢ M ASKED L ANGUAGE M ODELS\n\n Tokenization and NER\n Note that supervised training data for NER is typically in the form of BIO tags associated with text segmented at the word level. For example the following sentence\n containing two named entities:\n [LOC Mt. Sanitas ] is in [LOC Sunshine Canyon] .\n would have the following set of per-word BIO tags.\n (10.14) Mt. Sanitas is in Sunshine Canyon .\n B-LOC I-LOC O O B-LOC I-LOC O\n\n Unfortunately, the sequence of WordPiece tokens for this sentence doesn‚Äôt align\n directly with BIO tags in the annotation:\n ‚ÄôMt‚Äô, ‚Äô.‚Äô, ‚ÄôSan‚Äô, ‚Äô##itas‚Äô, ‚Äôis‚Äô, ‚Äôin‚Äô, ‚ÄôSunshine‚Äô, ‚ÄôCanyon‚Äô ‚Äô.‚Äô\n\n To deal with this misalignment, we need a way to assign BIO tags to subword\n tokens during training and a corresponding way to recover word-level tags from\n subwords during decoding. For training, we can just assign the gold-standard tag\n associated with each word to all of the subword tokens derived from it.\n For decoding, the simplest approach is to use the argmax BIO tag associated with\n the first subword token of a word. Thus, in our example, the BIO tag assigned to\n ‚ÄúMt‚Äù would be assigned to ‚ÄúMt.‚Äù and the tag assigned to ‚ÄúSan‚Äù would be assigned\n to ‚ÄúSanitas‚Äù, effectively ignoring the information in the tags assigned to ‚Äú.‚Äù and\n ‚Äú##itas‚Äù. More complex approaches combine the distribution of tag probabilities\n across the subwords in an attempt to find an optimal word-level tag.\n\n 10.5.4 Evaluating Named Entity Recognition\n Named entity recognizers are evaluated by recall, precision, and F1 measure. Recall that recall is the ratio of the number of correctly labeled responses to the total\n that should have been labeled; precision is the ratio of the number of correctly labeled responses to the total labeled; and F1 measure is the harmonic mean of the\n two.\n To know if the difference between the F1 scores of two NER systems is a significant difference, we use the paired bootstrap test, or the similar randomization test\n (Section ??).\n For named entity tagging, the entity rather than the word is the unit of response.\n Thus in the example in Fig. 10.12, the two entities Jane Villanueva and United Airlines Holding and the non-entity discussed would each count as a single response.\n The fact that named entity tagging has a segmentation component which is not\n present in tasks like text categorization or part-of-speech tagging causes some problems with evaluation. For example, a system that labeled Jane but not Jane Villanueva as a person would cause two errors, a false positive for O and a false negative for I-PER. In addition, using entities as the unit of response but words as the unit\n of training means that there is a mismatch between the training and test conditions.\n\n This chapter has introduced the bidirectional encoder and the masked language\n model. Here‚Äôs a summary of the main points that we covered:\n H ISTORICAL N OTES 19\n\n ‚Ä¢ Bidirectional encoders can be used to generate contextualized representations\n of input embeddings using the entire input context.\n ‚Ä¢ Pretrained language models based on bidirectional encoders can be learned\n using a masked language model objective where a model is trained to guess\n the missing information from an input.\n ‚Ä¢ The vector output of each transformer block or component in a particular token column is a contextual embedding that represents some aspect of the\n meaning of a token in context.\n ‚Ä¢ A word sense is a discrete representation of one aspect of the meaning of a\n word. Contextual embeddings offer a continuous high-dimensional model of\n meaning that is richer than fully discrete senses.\n ‚Ä¢ The cosine between contextual embeddings can be used as one way to model\n the similarity between two words in context, although some transformations\n to the embeddings are required first.\n ‚Ä¢ Pretrained language models can be finetuned for specific applications by adding\n lightweight classifier layers on top of the outputs of the pretrained model.\n ‚Ä¢ These applications can include sequence classification tasks like sentiment\n analysis, sequence-pair classification tasks like natural language inference,\n or sequence labeling tasks like named entity recognition.\n\nHistorical Notes\n History TBD.\n20 Chapter 10 ‚Ä¢ Masked Language Models\n\nChaplot, D. S. and R. Salakhutdinov. 2018. Knowledge- Timkey, W. and M. van Schijndel. 2021. All bark and no\n based word sense disambiguation using topic models. bite: Rogue dimensions in transformer language models\n AAAI. obscure representational quality. EMNLP.\nClark, K., M.-T. Luong, Q. V. Le, and C. D. Manning. Williams, A., N. Nangia, and S. Bowman. 2018. A broad-\n2020. Electra: Pre-training text encoders as discrimina- coverage challenge corpus for sentence understanding\n tors rather than generators. ICLR. through inference. NAACL HLT.\nCoenen, A., E. Reif, A. Yuan, B. Kim, A. Pearce, F. VieÃÅgas, Zhou, K., K. Ethayarajh, D. Card, and D. Jurafsky. 2022.\n and M. Wattenberg. 2019. Visualizing and measuring the Problems with cosine as a measure of embedding simigeometry of bert. NeurIPS. larity for high frequency words. ACL.\nConneau, A., K. Khandelwal, N. Goyal, V. Chaudhary, Zhu, Y., R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun,\n G. Wenzek, F. GuzmaÃÅn, E. Grave, M. Ott, L. Zettlemoyer, A. Torralba, and S. Fidler. 2015. Aligning books and\n and V. Stoyanov. 2020. Unsupervised cross-lingual rep- movies: Towards story-like visual explanations by watchresentation learning at scale. ACL. ing movies and reading books. IEEE International Con-\nDevlin, J., M.-W. Chang, K. Lee, and K. Toutanova. 2019. ference on Computer Vision.\n BERT: Pre-training of deep bidirectional transformers for\n language understanding. NAACL HLT.\nEthayarajh, K. 2019. How contextual are contextualized word representations? Comparing the geometry of\n BERT, ELMo, and GPT-2 embeddings. EMNLP.\nFellbaum, C., ed. 1998. WordNet: An Electronic Lexical\n Database. MIT Press.\nGale, W. A., K. W. Church, and D. Yarowsky. 1992. One\n sense per discourse. HLT.\nHaber, J. and M. Poesio. 2020. Assessing polyseme sense\n similarity through co-predication acceptability and contextualised embedding distance. *SEM.\nJoshi, M., D. Chen, Y. Liu, D. S. Weld, L. Zettlemoyer, and\n O. Levy. 2020. SpanBERT: Improving pre-training by\n representing and predicting spans. TACL, 8:64‚Äì77.\nKudo, T. and J. Richardson. 2018. SentencePiece: A simple\n and language independent subword tokenizer and detokenizer for neural text processing. EMNLP.\nLample, G. and A. Conneau. 2019. Cross-lingual language\n model pretraining. NeurIPS, volume 32.\nLiu, Y., M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen,\n O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov.\n 2019. RoBERTa: A robustly optimized BERT pretraining\n approach. ArXiv preprint arXiv:1907.11692.\nLoureiro, D. and A. Jorge. 2019. Language modelling makes\n sense: Propagating representations through WordNet for\n full-coverage word sense disambiguation. ACL.\nMelamud, O., J. Goldberger, and I. Dagan. 2016. context2vec: Learning generic context embedding with bidirectional LSTM. CoNLL.\nPapadimitriou, I., K. Lopez, and D. Jurafsky. 2023. Multilingual BERT has an accent: Evaluating English influences\n on fluency in multilingual models. EACL Findings.\nPeters, M., M. Neumann, M. Iyyer, M. Gardner, C. Clark,\n K. Lee, and L. Zettlemoyer. 2018. Deep contextualized\n word representations. NAACL HLT.\nRamshaw, L. A. and M. P. Marcus. 1995. Text chunking\n using transformation-based learning. Proceedings of the\n 3rd Annual Workshop on Very Large Corpora.\nSchuster, M. and K. Nakajima. 2012. Japanese and Korean\n voice search. ICASSP.\nTaylor, W. L. 1953. Cloze procedure: A new tool for measuring readability. Journalism Quarterly, 30:415‚Äì433.\n",
    "file_path": "/Users/colinsidberry/nlp-textbook-rag/data/extracted_text/10.Masked Language Models.txt",
    "file_size_kb": 55.05
  },
  {
    "id": "c7385457f0f2ff57",
    "source": "nlp_textbook",
    "chapter": "Information Retrieval and 11 Retrieval-Augmented Generation",
    "filename": "11-RAG.txt",
    "content": "rights reserved. Draft of August 24, 2025.\n\nCHAPTER\n\n Information Retrieval and\n11 Retrieval-Augmented Generation\n On two occasions I have been asked,‚Äî‚ÄúPray, Mr. Babbage, if you put into\n the machine wrong figures, will the right answers come out?‚Äù ... I am not able\n rightly to apprehend the kind of confusion of ideas that could provoke such a\n question. Babbage (1864)\n\n People need to know things. So pretty much as soon as there were computers\n we were asking them questions. By 1961 there was a system to answer questions\n about American baseball statistics like ‚ÄúHow many games did the Yankees play\n in July?‚Äù (Green et al., 1961). Even fictional computers in the 1970s like Deep\n Thought, invented by Douglas Adams in The Hitchhiker‚Äôs Guide to the Galaxy,\n answered ‚Äúthe Ultimate Question Of Life, The Universe, and Everything‚Äù.1 And\n because so much knowledge is encoded in text, systems were answering questions\n at human-level performance even before LLMs: IBM‚Äôs Watson system won the TV\n game-show Jeopardy! in 2011, surpassing humans at answering questions like:\n\n WILLIAM WILKINSON‚ÄôS ‚ÄúAN ACCOUNT OF THE\n PRINCIPALITIES OF WALLACHIA AND MOLDOVIA‚Äù\n INSPIRED THIS AUTHOR‚ÄôS MOST FAMOUS NOVEL 2\n\n It follows naturally, then, that an important function of large language models is\n to fill human information needs by answering people‚Äôs questions. And since a lot\n of information is online, answering questions is closely related to web information\n retrieval, the task performed by search engines. Indeed, the distinction is becoming ever more fuzzy, as modern search engines are integrated with large language\n models.\n factoid\n questions Consider some simple information needs, for example factoid questions that\n can be answered with facts expressed in short texts like the following:\n (11.1) Where is the Louvre Museum located?\n (11.2) Where does the energy in a nuclear explosion come from?\n (11.3) How to get a script l in latex?\n To get an LLM to answer these questions, we can just prompt it! For example a\n pretrained LLM that has been instruction-tuned on question answering (Chapter 9)\n could directly answer the following question\n Where is the Louvre Museum located?\n by performing conditional generation given this prefix, and take the response as the\n answer. This works because large language models have processed a lot of facts in\n their pretraining data, including the location of the Louvre, and have encoded this\n information in their parameters. Factual knowledge of this type seems to be stored\n 1 The answer was 42, but unfortunately the question was never revealed.\n 2 The answer, of course, is ‚ÄòWho is Bram Stoker‚Äô, and the novel was Dracula.\n2 C HAPTER 11 ‚Ä¢ R ETRIEVAL - BASED M ODELS\n\n in the connections in the very large feedforward layers of transformer models (Geva\n et al., 2021; Meng et al., 2022).\n Simply prompting an LLM can be a useful approach to answer many factoid\n questions. But the fact that knowledge is stored in the feedforward weights of the\n LLM leads to a number of problems with prompting as a method for correctly answering factual questions.\n The first and main problem is that LLMs often give the wrong answer to factual\n hallucinate questions! Large language models hallucinate. A hallucination is a response that is\n not faithful to the facts of the world. That is, when asked questions, large language\n models sometimes make up answers that sound reasonable. For example, Dahl et al.\n (2024) found that when asked questions about the legal domain (like about particular legal cases), large language models hallucinated from 69% to 88% of the time!\n LLMs sometimes give incorrect factual responses even when the correct facts are\n stored in the parameters; this seems to be caused by the feedforward layers failing\n to recall the knowledge stored in their parameters (Jiang et al., 2024).\n And it‚Äôs not always possible to tell when language models are hallucinating,\n calibrated partly because LLMs aren‚Äôt well-calibrated. In a calibrated system, the confidence\n of a system in the correctness of its answer is highly correlated with the probability\n of an answer being correct. So if a calibrated system is wrong, at least it might hedge\n its answer or tell us to go check another source. But since language models are not\n well-calibrated, they often give a very wrong answer with complete certainty (Zhou\n et al., 2024).\n A second problem with answering questions with simple prompting methods\n is that prompting a large language model to answer from its pretrained parameters\n doesn‚Äôt allow us to ask questions about proprietary data. We would like to use\n language models to answer factual questions about proprietary data like personal\n email. Or for the healthcare application we might want to apply a language model to\n medical records. Or a company may have internal documents that contain answers\n for customer service or internal use. Or legal firms need to ask questions about legal\n discovery from proprietary documents. None of this data (hopefully) was in the\n large web-based corpora that large language models are pretrained on.\n A final issue with using large language models to answer knowledge questions\n is that they are static; they were pretrained once, at a particular time. This means\n that LLMs cannot answer questions about rapidly changing information (like questions about something that happened last week) since they won‚Äôt have up-to-date\n information from after their release data.\n One solution to all these problems with simple prompting for answering factual\n questions is to give a language model external sources of knowledge, for example\n proprietary texts like medical or legal records, personal emails, or corporate documents, and to use those documents in answering questions. This method is called\n RAG retrieval-augmented generation or RAG, and that is the method we will focus on\n information in this chapter. In RAG we use information retrieval (IR) techniques to retrieve\n retrieval\n documents that are likely to have information that might help answer the question.\n Then we use a large language model to generate an answer given these documents.\n Basing our answers on retrieved documents can solve some of the problems with\n using simple prompting to answer questions. First, it helps ensure that the answer is\n grounded in facts from some curated dataset. And the system can give the user the\n answer accompanied by the context of the passage or document the answer came\n from. This information can help users have confidence in the accuracy of the answer\n (or help them spot when it is wrong!). And these retrieval techniques can be used on\n 11.1 ‚Ä¢ I NFORMATION R ETRIEVAL 3\n\n any proprietary data we want, such as legal or medical data for those applications.\n We‚Äôll begin by introducing information retrieval, the task of choosing the most\n relevant document from a document set given a user‚Äôs query expressing their information need. We‚Äôll see the classic method based on cosines of sparse tf-idf vectors,\n modern neural ‚Äòdense‚Äô retrievers based on instead representing queries and documents neurally with BERT or other language models. We then introduce retrieverbased question answering and the retrieval-augmented generation paradigm.\n Finally, we‚Äôll discuss various datasets with questions and answers that can be\n used for finetuning LLMs in instruction tuning and for use as benchmarks for evaluation.\n\n information Information retrieval or IR is the name of the field encompassing the retrieval of all\n retrieval\n IR manner of media based on user information needs. The resulting IR system is often\n called a search engine. Our goal in this section is to give a sufficient overview of IR\n to see its application to question answering. Readers with more interest specifically\n in information retrieval should see the Historical Notes section at the end of the\n chapter and textbooks like Manning et al. (2008).\nad hoc retrieval The IR task we consider is called ad hoc retrieval, in which a user poses a\n query to a retrieval system, which then returns an ordered set of documents from\n document some collection. A document refers to whatever unit of text the system indexes and\n retrieves (web pages, scientific papers, news articles, or even shorter passages like\n collection paragraphs). A collection refers to a set of documents being used to satisfy user\n term requests. A term refers to a word in a collection, but it may also include phrases.\n query Finally, a query represents a user‚Äôs information need expressed as a set of terms.\n The high-level architecture of an ad hoc retrieval engine is shown in Fig. 11.1.\n\n Document\n Inverted\n Document\n Document Indexing\n Document\n Document\n Index\n Document Document\n Document\n Document\n Document\n Document\n document collection Search Ranked\n Document\n\n Documents\n\n Query query\n query Processing vector\n\n The basic IR architecture uses the vector space model we introduced in Chapter 5, in which we map queries and document to vectors based on unigram word\n counts, and use the cosine similarity between the vectors to rank potential documents\n (Salton, 1971). This is thus an example of the bag-of-words model introduced in\n Appendix K, since words are considered independently of their positions.\n\n 11.1.1 Term weighting and document scoring\n Let‚Äôs look at the details of how the match between a document and query is scored.\n4 C HAPTER 11 ‚Ä¢ R ETRIEVAL - BASED M ODELS\n\n term weight We don‚Äôt use raw word counts in IR, instead computing a term weight for each\n document word. Two term weighting schemes are common: the tf-idf weighting\n BM25 introduced in Chapter 5, and a slightly more powerful variant called BM25.\n We‚Äôll reintroduce tf-idf here so readers don‚Äôt need to look back at Chapter 5.\n Tf-idf (the ‚Äò-‚Äô here is a hyphen, not a minus sign) is the product of two terms, the\n term frequency tf and the inverse document frequency idf.\n The term frequency tells us how frequent the word is; words that occur more\n often in a document are likely to be informative about the document‚Äôs contents. We\n usually use the log10 of the word frequency, rather than the raw count. The intuition\n is that a word appearing 100 times in a document doesn‚Äôt make that word 100 times\n more likely to be relevant to the meaning of the document. We also need to do\n something special with counts of 0, since we can‚Äôt take the log of 0.3\n (\n 1 + log10 count(t, d) if count(t, d) > 0\n tft, d = (11.4)\n 0 otherwise\n\n If we use log weighting, terms which occur 0 times in a document would have tf = 0,\n 1 times in a document tf = 1 + log10 (1) = 1 + 0 = 1, 10 times in a document tf =\n 1 + log10 (10) = 2, 100 times tf = 1 + log10 (100) = 3, 1000 times tf = 4, and so on.\n The document frequency dft of a term t is the number of documents it occurs in. Terms that occur in only a few documents are useful for discriminating\n those documents from the rest of the collection; terms that occur across the entire\n collection aren‚Äôt as helpful. The inverse document frequency or idf term weight\n (Sparck Jones, 1972) is defined as:\n N\n idft = log10 (11.5)\n dft\n where N is the total number of documents in the collection, and dft is the number\n of documents in which term t occurs. The fewer documents in which a term occurs,\n the higher this weight; the lowest weight of 0 is assigned to terms that occur in every\n document.\n Here are some idf values for some words in the corpus of Shakespeare plays,\n ranging from extremely informative words that occur in only one play like Romeo,\n to those that occur in a few like salad or Falstaff, to those that are very common like\n fool or so common as to be completely non-discriminative since they occur in all 37\n plays like good or sweet.4\n Word df idf\n Romeo 1 1.57\n salad 2 1.27\n Falstaff 4 0.967\n forest 12 0.489\n battle 21 0.246\n wit 34 0.037\n fool 36 0.012\n good 37 0\n sweet 37 0\n 3 We can also use this alternative formulation, which we have used in earlier editions: tft, d =\n log10 (count(t, d) + 1)\n 4 Sweet was one of Shakespeare‚Äôs favorite adjectives, a fact probably related to the increased use of\n sugar in European recipes around the turn of the 16th century (Jurafsky, 2014, p. 175).\n 11.1 ‚Ä¢ I NFORMATION R ETRIEVAL 5\n\n The tf-idf value for word t in document d is then the product of term frequency\ntft, d and IDF:\n\n tf-idf(t, d) = tft, d ¬∑ idft (11.6)\n\n11.1.2 Document Scoring\nWe score document d by the cosine of its vector d with the query vector q:\n\n q¬∑d\n score(q, d) = cos(q, d) = (11.7)\n\nAnother way to think of the cosine computation is as the dot product of unit vectors;\nwe first normalize both the query and document vector to unit vectors, by dividing\nby their lengths, and then take the dot product:\n\n q d\n score(q, d) = cos(q, d) = ¬∑ (11.8)\n\nWe can spell out Eq. 11.8, using the tf-idf values and spelling out the dot product as\na sum of products:\n X tf-idf(t, q) tf-idf(t, d)\n score(q, d) = qP ¬∑ qP (11.9)\n 2 2\n t‚ààq qi ‚ààq tf-idf (qi , q) di ‚ààd tf-idf (di , d)\n\n Now let‚Äôs use Eq. 11.9 to walk through an example of a tiny query against a\ncollection of 4 nano documents, computing tf-idf values and seeing the rank of the\ndocuments. We‚Äôll assume all words in the following query and documents are downcased and punctuation is removed:\n Query: sweet love\n Doc 1: Sweet sweet nurse! Love?\n Doc 2: Sweet sorrow\n Doc 3: How sweet is love?\n Doc 4: Nurse!\n Fig. 11.2 shows the computation of the tf-idf cosine between the query and Document 1, and the query and Document 2. The cosine is the normalized dot product\nof tf-idf values, so for the normalization we must need to compute the document\nvector lengths |q|, |d1 |, and |d2 | for the query and the first two documents using\nEq. 11.4, Eq. 11.5, Eq. 11.6, and Eq. 11.9 (computations for Documents 3 and 4 are\nalso needed but are left as an exercise for the reader). The dot product between the\nvectors is the sum over dimensions of the product, for each dimension, of the values\nof the two tf-idf vectors for that dimension. This product is only non-zero where\nboth the query and document have non-zero values, so for this example, in which\nonly sweet and love have non-zero values in the query, the dot product will be the\nsum of the products of those elements of each vector.\n Document 1 has a higher cosine with the query (0.747) than Document 2 has\nwith the query (0.0779), and so the tf-idf cosine model would rank Document 1\nabove Document 2. This ranking is intuitive given the vector space model, since\nDocument 1 has both terms including two instances of sweet, while Document 2 is\nmissing one of the terms. We leave the computation for Documents 3 and 4 as an\nexercise for the reader.\n6 C HAPTER 11 ‚Ä¢ R ETRIEVAL - BASED M ODELS\n\n Query\nword cnt tf df idf tf-idf n‚Äôlized = tf-idf/|q|\nsweet 1 1 3 0.125 0.125 0.383\nnurse 0 0 2 0.301 0 0\nlove 1 1 2 0.301 0.301 0.924\nhow 0 0 1 0.602 0 0\nsorrow 0 0 1 0.602 0 0\nis 0 0 1 0.602 0 0\n ‚àö\n 2 2\n|q| = .125 + .301 = .326\n\n Document 1 Document 2\nword cnt tf tf-idf n‚Äôlized √ó q cnt tf tf-idf n‚Äôlized √óq\nsweet 2 1.301 0.163 0.357 0.137 1 1.000 0.125 0.203 0.0779\nnurse 1 1.000 0.301 0.661 0 0 0 0 0 0\nlove 1 1.000 0.301 0.661 0.610 0 0 0 0 0\nhow 0 0 0 0 0 0 0 0 0 0\nsorrow 0 0 0 0 0 1 1.000 0.602 0.979 0\nis 0 0 0 0 0 0 0 0 0 0\n ‚àö ‚àö\n|d1 | = .1632 + .3012 + .3012 = .456 2 2\n |d2 | = .125 + .602 = .615\n P P\n Cosine: of column: 0.747 Cosine: of column: 0.0779\n(0.0779), using Eq. 11.4, Eq. 11.5, Eq. 11.6 and Eq. 11.9.\n\n In practice, there are many variants and approximations to Eq. 11.9. For example, we might choose to simplify processing by removing some terms. To see this,\n let‚Äôs start by expanding the formula for tf-idf in Eq. 11.9 to explicitly mention the tf\n and idf terms from Eq. 11.6:\n X tft, q ¬∑ idft tft, d ¬∑ idft\n score(q, d) = qP ¬∑ qP (11.10)\n 2 2\n t‚ààq qi ‚ààq tf-idf (qi , q) di ‚ààd tf-idf (di , d)\n\n In one common variant of tf-idf cosine, for example, we drop the idf term for the\n document. Eliminating the second copy of the idf term (since the identical term is\n already computed for the query) turns out to sometimes result in better performance:\n\n X tft, q ¬∑idft tft, d ¬∑ idft\n score(q, d) = qP ¬∑ qP (11.11)\n 2 2\n t‚ààq qi ‚ààq tf-idf (qi , q) di ‚ààd tf-idf (di , d)\n\n Other variants of tf-idf eliminate various other terms.\n BM25 A slightly more complex variant in the tf-idf family is the BM25 weighting\n scheme (sometimes called Okapi BM25 after the Okapi IR system in which it was\n introduced (Robertson et al., 1995)). BM25 adds two parameters: k, a knob that\n adjust the balance between term frequency and IDF, and b, which controls the importance of document length normalization. The BM25 score of a document d given\n a query q is:\n IDF weighted tf\n z \u0012}| \u0013{ z }| {\n X N tft,d\n log \u0010 \u0010 \u0011\u0011 (11.12)\n t‚ààq\n dft k 1 ‚àí b + b |d| + tf |davg | t,d\n 11.1 ‚Ä¢ I NFORMATION R ETRIEVAL 7\n\n where |davg | is the length of the average document. When k is 0, BM25 reverts to\n no use of term frequency, just a binary selection of terms in the query (plus idf).\n A large k results in raw term frequency (plus idf). b ranges from 1 (scaling by\n document length) to 0 (no length scaling). Manning et al. (2008) suggest reasonable\n values are k = [1.2,2] and b = 0.75. Kamphuis et al. (2020) is a useful summary of\n the many minor variants of BM25.\n Stop words In the past it was common to remove high-frequency words from both\n the query and document before representing them. The list of such high-frequency\n stop list words to be removed is called a stop list. The intuition is that high-frequency terms\n (often function words like the, a, to) carry little semantic weight and may not help\n with retrieval, and can also help shrink the inverted index files we describe below.\n The downside of using a stop list is that it makes it difficult to search for phrases\n that contain words in the stop list. For example, common stop lists would reduce the\n phrase to be or not to be to the phrase not. In modern IR systems, the use of stop lists\n is much less common, partly due to improved efficiency and partly because much\n of their function is already handled by IDF weighting, which downweights function\n words that occur in every document. Nonetheless, stop word removal is occasionally\n useful in various NLP tasks so is worth keeping in mind.\n\n 11.1.3 Inverted Index\n In order to compute scores, we need to efficiently find documents that contain words\n in the query. (Any document that contains none of the query terms will have a score\n of 0 and can be ignored.) The basic search problem in IR is thus to find all documents\n d ‚àà C that contain a term q ‚àà Q.\ninverted index The data structure for this task is the inverted index, which we use for making this search efficient, and also conveniently storing useful information like the\n document frequency and the count of each term in each document.\n An inverted index, given a query term, gives a list of documents that contain the\n postings term. It consists of two parts, a dictionary and the postings. The dictionary is a list\n of terms (designed to be efficiently accessed), each pointing to a postings list for the\n term. A postings list is the list of document IDs associated with each term, which\n can also contain information like the term frequency or even the exact positions of\n terms in the document. The dictionary can also store the document frequency for\n each term. For example, a simple inverted index for our 4 sample documents above,\n with each word containing its document frequency in {}, and a pointer to a postings\n list that contains document IDs and term counts in [], might look like the following:\n how {1} ‚Üí 3 [1]\n is {1} ‚Üí 3 [1]\n love {2} ‚Üí 1 [1] ‚Üí 3 [1]\n nurse {2} ‚Üí 1 [1] ‚Üí 4 [1]\n sorry {1} ‚Üí 2 [1]\n sweet {3} ‚Üí 1 [2] ‚Üí 2 [1] ‚Üí 3 [1]\n Given a list of terms in query, we can very efficiently get lists of all candidate\n documents, together with the information necessary to compute the tf-idf scores we\n need.\n There are alternatives to the inverted index. For the question-answering domain\n of finding Wikipedia pages to match a user query, Chen et al. (2017) show that\n indexing based on bigrams works better than unigrams, and use efficient hashing\n algorithms rather than the inverted index to make the search efficient.\n8 C HAPTER 11 ‚Ä¢ R ETRIEVAL - BASED M ODELS\n\n 11.1.4 Evaluation of Information-Retrieval Systems\n We measure the performance of ranked retrieval systems using the same precision\n and recall metrics we have been using. We make the assumption that each document returned by the IR system is either relevant to our purposes or not relevant.\n Precision is the fraction of the returned documents that are relevant, and recall is the\n fraction of all relevant documents that are returned. More formally, let‚Äôs assume a\n system returns T ranked documents in response to an information request, a subset\n R of these are relevant, a disjoint subset, N, are the remaining irrelevant documents,\n and U documents in the collection as a whole are relevant to this request. Precision\n and recall are then defined as:\n |R| |R|\n Precision = Recall = (11.13)\n |T | |U|\n\n Unfortunately, these metrics don‚Äôt adequately measure the performance of a system\n that ranks the documents it returns. If we are comparing the performance of two\n ranked retrieval systems, we need a metric that prefers the one that ranks the relevant\n documents higher. We need to adapt precision and recall to capture how well a\n system does at putting relevant documents higher in the ranking.\n\n Rank Judgment PrecisionRank RecallRank\n 1 R 1.0 .11\n 2 N .50 .11\n 3 R .66 .22\n 4 N .50 .22\n 5 R .60 .33\n 6 R .66 .44\n 7 N .57 .44\n 8 R .63 .55\n 9 N .55 .55\n 10 N .50 .55\n 11 R .55 .66\n 12 N .50 .66\n 13 N .46 .66\n 14 N .43 .66\n 15 R .47 .77\n 16 N .44 .77\n 17 N .44 .77\n 18 R .44 .88\n 19 N .42 .88\n 20 N .40 .88\n 21 N .38 .88\n 22 N .36 .88\n 23 N .35 .88\n 24 N .33 .88\n 25 R .36 1.0\n through a set of ranked documents (assuming the collection has 9 relevant documents).\n\n Let‚Äôs turn to an example. Assume the table in Fig. 11.3 gives rank-specific precision and recall values calculated as we proceed down through a set of ranked documents for a particular query; the precisions are the fraction of relevant documents\n seen at a given rank, and recalls the fraction of relevant documents found at the same\n 11.1 ‚Ä¢ I NFORMATION R ETRIEVAL 9\n\n 1.0\n\n 0.8\n\n 0.6\n\n Precision\n 0.4\n\n 0.2\n\n 0.00.0 0.2 0.4 0.6 0.8 1.0\n Recall\n\n rank. The recall measures in this example are based on this query having 9 relevant\n documents in the collection as a whole.\n Note that recall is non-decreasing; when a relevant document is encountered,\n recall increases, and when a non-relevant document is found it remains unchanged.\n Precision, on the other hand, jumps up and down, increasing when relevant documents are found, and decreasing otherwise. The most common way to visualize\nprecision-recall precision and recall is to plot precision against recall in a precision-recall curve,\n curve\n like the one shown in Fig. 11.4 for the data in table 11.3.\n Fig. 11.4 shows the values for a single query. But we‚Äôll need to combine values\n for all the queries, and in a way that lets us compare one system to another. One way\n of doing this is to plot averaged precision values at 11 fixed levels of recall (0 to 100,\n in steps of 10). Since we‚Äôre not likely to have datapoints at these exact levels, we\n interpolated\n precision use interpolated precision values for the 11 recall values from the data points we do\n have. We can accomplish this by choosing the maximum precision value achieved\n at any level of recall at or above the one we‚Äôre calculating. In other words,\n\n IntPrecision(r) = max Precision(i) (11.14)\n i>=r\n\n This interpolation scheme not only lets us average performance over a set of queries,\n but also helps smooth over the irregular precision values in the original data. It is\n designed to give systems the benefit of the doubt by assigning the maximum precision value achieved at higher levels of recall from the one being measured. Fig. 11.5\n and Fig. 11.6 show the resulting interpolated data points from our example.\n Given curves such as that in Fig. 11.6 we can compare two systems or approaches\n by comparing their curves. Clearly, curves that are higher in precision across all\n recall values are preferred. However, these curves can also provide insight into the\n overall behavior of a system. Systems that are higher in precision toward the left\n may favor precision over recall, while systems that are more geared towards recall\n will be higher at higher levels of recall (to the right).\n mean average\n precision A second way to evaluate ranked retrieval is mean average precision (MAP),\n which provides a single metric that can be used to compare competing systems or\n approaches. In this approach, we again descend through the ranked list of items,\n but now we note the precision only at those points where a relevant item has been\n encountered (for example at ranks 1, 3, 5, 6 but not 2 or 4 in Fig. 11.3). For a single\n10 C HAPTER 11 ‚Ä¢ R ETRIEVAL - BASED M ODELS\n\n Interpolated Precision Recall\n 1.0 .10\n .66 .20\n .66 .30\n .66 .40\n .63 .50\n .55 .60\n .47 .70\n .44 .80\n .36 .90\n .36 1.0\n\n Interpolated Precision Recall Curve\n\n 0.9\n\n 0.8\n\n 0.7\n\n 0.6\n Precision\n\n 0.5\n\n 0.4\n\n 0.3\n\n 0.2\n\n 0.1\n\n 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\n Recall\n\n standard recall levels is interpolated for each query from the maximum at any higher level of\n recall. The original measured precision recall points are also shown.\n\n query, we average these individual precision measurements over the return set (up\n to some fixed cutoff). More formally, if we assume that Rr is the set of relevant\n documents at or above r, then the average precision (AP) for a single query is\n\n 1 X\n AP = Precisionr (d) (11.15)\n |Rr |\n d‚ààRr\n\n where Precisionr (d) is the precision measured at the rank at which document d was\n found. For an ensemble of queries Q, we then average over these averages, to get\n our final MAP measure:\n 1 X\n MAP = AP(q) (11.16)\n q‚ààQ\n\n The MAP for the single query (hence = AP) in Fig. 11.3 is 0.6.\n 11.2 ‚Ä¢ I NFORMATION R ETRIEVAL WITH D ENSE V ECTORS 11\n\n The classic tf-idf or BM25 algorithms for IR have long been known to have a conceptual flaw: they work only if there is exact overlap of words between the query\n and document. In other words, the user posing a query (or asking a question) needs\n to guess exactly what words the writer of the answer might have used, an issue called\n the vocabulary mismatch problem (Furnas et al., 1987).\n The solution to this problem is to use an approach that can handle synonymy:\n instead of (sparse) word-count vectors, using (dense) embeddings. This idea was\n first proposed for retrieval in the last century under the name of Latent Semantic\n Indexing approach (Deerwester et al., 1990), but is implemented in modern times\n via encoders like BERT.\n The most powerful approach is to present both the query and the document to a\n single encoder, allowing the transformer self-attention to see all the tokens of both\n the query and the document, and thus building a representation that is sensitive to\n the meanings of both query and document. Then a linear layer can be put on top of\n the [CLS] token to predict a similarity score for the query/document tuple:\n z = BERT(q; [SEP]; d)[CLS]\n score(q, d) = softmax(U(z)) (11.17)\n\n This architecture is shown in Fig. 11.7a. Usually the retrieval step is not done on\n an entire document. Instead documents are broken up into smaller passages, such\n as non-overlapping fixed-length chunks of say 100 tokens, and the retriever encodes\n and retrieves these passages rather than entire documents. The query and document\n have to be made to fit in the BERT 512-token window, for example by truncating\n the query to 64 tokens and truncating the document if necessary so that it, the query,\n [CLS], and [SEP] fit in 512 tokens. The BERT system together with the linear layer\n U can then be fine-tuned for the relevance task by gathering a tuning dataset of\n relevant and non-relevant passages.\n The problem with the full BERT architecture in Fig. 11.7a is the expense in\n computation and time. With this architecture, every time we get a query, we have to\n pass every single document in our entire collection through a BERT encoder jointly\n with the new query! This enormous use of resources is impractical for real cases.\n At the other end of the computational spectrum is a much more efficient architecture, the bi-encoder. In this architecture we can encode the documents in the\n collection only one time by using two separate encoder models, one to encode the\n query and one to encode the document. We encode each document, and store all\n the encoded document vectors in advance. When a query comes in, we encode just\n this query and then use the dot product between the query vector and the precomputed document vectors as the score for each candidate document (Fig. 11.7b). For\n example, if we used BERT, we would have two encoders BERTQ and BERTD and\n we could represent the query and document as the [CLS] token of the respective\n encoders (Karpukhin et al., 2020):\n zq = BERTQ (q)[CLS]\n zd = BERTD (d)[CLS]\n score(q, d) = zq ¬∑ zd (11.18)\n\n The bi-encoder is much cheaper than a full query/document encoder, but is also\n less accurate, since its relevance decision can‚Äôt take full advantage of all the possi-\n12 C HAPTER 11 ‚Ä¢ R ETRIEVAL - BASED M ODELS\n\n s(q,d)\n s(q,d)\n\n U ‚Ä¢ zCLS_D\n zCLS zCLS_Q\n\n ‚Ä¶ ‚Ä¶\n\n ‚Ä¶ ‚Ä¶\n\n ‚Ä¶ ‚Ä¶\n\n ‚Ä¶ ‚Ä¶\n\n ‚Ä¶ ‚Ä¶\n\n ‚Ä¶ ‚Ä¶\n\n Query [sep] Document Query Document\n\n (a) (b)\nrelevance score with a linear layer over the CLS token. This is too compute-expensive to use except in rescoring\n(b) Use separate encoders for query and document, and use the dot product between CLS token outputs for the\nquery and document as the score. This is less compute-expensive, but not as accurate.\n\n ble meaning interactions between all the tokens in the query and the tokens in the\n document.\n There are numerous approaches that lie in between the full encoder and the biencoder. One intermediate alternative is to use cheaper methods (like BM25) as the\n first pass relevance ranking for each document, take the top N ranked documents,\n and use expensive methods like the full BERT scoring to rerank only the top N\n documents rather than the whole set.\n ColBERT Another intermediate approach is the ColBERT approach of Khattab and Zaharia (2020) and Khattab et al. (2021), shown in Fig. 11.8. This method separately\n encodes the query and document, but rather than encoding the entire query or document into one vector, it separately encodes each of them into contextual representations for each token. These BERT representations of each document word can be\n pre-stored for efficiency. The relevance score between a query q and a document d is\n a sum of maximum similarity (MaxSim) operators between tokens in q and tokens\n in d. Essentially, for each token in q, ColBERT finds the most contextually similar token in d, and then sums up these similarities. A relevant document will have\n tokens that are contextually very similar to the query.\n More formally, a question q is tokenized as [q1 , . . . , qn ], prepended with a [CLS]\n and a special [Q] token, truncated to N=32 tokens (or padded with [MASK] tokens if\n it is shorter), and passed through BERT to get output vectors q = [q1 , . . . , qN ]. The\n passage d with tokens [d1 , . . . , dm ], is processed similarly, including a [CLS] and\n special [D] token. A linear layer is applied on top of d and q to control the output\n dimension, so as to keep the vectors small for storage efficiency, and vectors are\n rescaled to unit length, producing the final vector sequences Eq (length N) and Ed\n (length m). The ColBERT scoring mechanism is:\n N\n X m\n score(q, d) = max Eqi ¬∑ Ed j (11.19)\n j=1\n i=1\n\n While the interaction mechanism has no tunable parameters, the ColBERT ar-\n11.2 ‚Ä¢ I NFORMATION R ETRIEVAL WITH D ENSE V ECTORS 13\n\n s(q,d)\n\n ‚àë\n\n MaxSim MaxSim MaxSim\n\n norm norm norm norm norm norm\n\n ‚Ä¶\n\n ‚Ä¶\n\n ‚Ä¶\n\n ‚Ä¶\n\n ‚Ä¶\n\n ‚Ä¶\n\n Query Document\n\n tokens in the query and the document. Training is end-to-end. (Various details aren‚Äôt depicted; for example the query is prepended by a [CLS] and [Q:] tokens, and the document\n by [CLS] and [D:] tokens). Figure adapted from Khattab and Zaharia (2020).\n\n chitecture still needs to be trained end-to-end to fine-tune the BERT encoders and\n train the linear layers (and the special [Q] and [D] embeddings) from scratch. It is\n trained on triples hq, d + , d ‚àí i of query q, positive document d + and negative document d ‚àí to produce a score for each document using Eq. 11.19, optimizing model\n parameters using a cross-entropy loss.\n All the supervised algorithms (like ColBERT or the full-interaction version of\n the BERT algorithm applied for reranking) need training data in the form of queries\n together with relevant and irrelevant passages or documents (positive and negative\n examples). There are various semi-supervised ways to get labels; some datasets\n (like MS MARCO Ranking, Section 11.4) contain gold positive examples. Negative\n examples can be sampled randomly from the top-1000 results from some existing\n IR system. If datasets don‚Äôt have labeled positive examples, iterative methods like\n relevance-guided supervision can be used (Khattab et al., 2021) which rely on the\n fact that many datasets contain short answer strings. In this method, an existing IR\n system is used to harvest examples that do contain short answer strings (the top few\n are taken as positives) or don‚Äôt contain short answer strings (the top few are taken as\n negatives), these are used to train a new retriever, and then the process is iterated.\n Efficiency is an important issue, since every possible document must be ranked\n for its similarity to the query. For sparse word-count vectors, the inverted index\n allows this very efficiently. For dense vector algorithms finding the set of dense\n document vectors that have the highest dot product with a dense query vector is\n an instance of the problem of nearest neighbor search. Modern systems there-\nFaiss fore make use of approximate nearest neighbor vector search algorithms like Faiss\n (Johnson et al., 2017).\n14 C HAPTER 11 ‚Ä¢ R ETRIEVAL - BASED M ODELS\n\n Here we introduce an important paradigm for using LLMs to answer knowledgebased questions, based on first finding supportive text segments from the web or\n another other large collection of documents, and then generating an answer based\n on the documents. The method of generating based on retrieved documents is\n called retrieval-augmented generation or RAG, and the two components are sometimes called, for historical reasons, the retriever and the reader (Chen et al., 2017).\n Fig. 11.9 sketches out this standard model for answering questions.\n\n query\n Retriever Reader/\n Q: When was docs Generator\n the premiere of\n The Magic Flute? LLM A: 1791\n Relevant prompt\n Docs\n Indexed Docs\n\nfrom the collection, and reading, in which an LLM generates answers given the documents as a prompt.\n\n In the first stage of the 2-stage retrieve and read model in Fig. 11.9 we retrieve\n relevant passages from a text collection, for example using the dense retrievers of the\n previous section. In the second reader stage, we generate the answer via retrievalaugmented generation. In this method, we take a large pretrained language model,\n give it the set of retrieved passages and other text as its prompt, and autoregressively\n generate a new answer token by token.\n\n 11.3.1 Retrieval-Augmented Generation\n The standard reader algorithm is to generate from a large language model, condiretrieval-\n tioned on the retrieved passages. This method is known as retrieval-augmented\n augmented generation, or RAG.\n generation\n RAG Recall that in simple conditional generation, we can cast the task of question\n answering as word prediction by giving a language model a question and a token\n like A: suggesting that an answer should come next:\n Q: Who wrote the book ‚Äò‚ÄòThe Origin of Species\"? A:\n Then we generate autoregressively conditioned on this text.\n More formally, recall that simple autoregressive language modeling computes\n the probability of a string from the previous tokens:\n n\n Y\n p(x1 , . . . , xn ) = p(xi |x<i )\n i=1\n\n And simple conditional generation for question answering adds a prompt like Q: ,\n followed by a query q , and A:, all concatenated:\n n\n Y\n p(x1 , . . . , xn ) = p([Q:] ; q ; [A:] ; x<i )\n i=1\n 11.4 ‚Ä¢ Q UESTION A NSWERING DATASETS 15\n\n The advantage of using a large language model is the enormous amount of\n knowledge encoded in its parameters from the text it was pretrained on. But as\n we mentioned at the start of the chapter, while this kind of simple prompted generation can work fine for many simple factoid questions, it is not a general solution\n for QA, because it leads to hallucination, is unable to show users textual evidence to\n support the answer, and is unable to answer questions from proprietary data.\n The idea of retrieval-augmented generation is to address these problems by conditioning on the retrieved passages as part of the prefix, perhaps with some prompt\n text like ‚ÄúBased on these texts, answer this question:‚Äù. Let‚Äôs suppose we have a\n query q, and call the set of retrieved passages based on it R(q). For example, we\n could have a prompt like:\n Schematic of a RAG Prompt\n\n retrieved passage 1\n\n retrieved passage 2\n\n ...\n\n retrieved passage n\n\n Based on these texts, answer this question: Q: Who wrote\n the book ‚Äò‚ÄòThe Origin of Species\"? A:\n\n Or more formally,\n n\n Y\n p(x1 , . . . , xn ) = p(xi |R(q) ; prompt ; [Q:] ; q ; [A:] ; x<i )\n i=1\n\n As with the span-based extraction reader, successfully applying the retrievalaugmented generation algorithm for QA requires a successful retriever, and often\n a two-stage retrieval algorithm is used in which the retrieval is reranked. Some\n multi-hop complex questions may require multi-hop architectures, in which a query is used to\n retrieve documents, which are then appended to the original query for a second stage\n of retrieval. Details of prompt engineering also have to be worked out, like deciding\n whether to demarcate passages, for example with [SEP] tokens, and so on. Combinations of private data and public data involving an externally hosted large language\n model may lead to privacy concerns that need to be worked out (Arora et al., 2023).\n Much research in this area also focuses on ways to more tightly integrate the retrieval\n and reader stages.\n\n There are scores of question answering datasets, used both for instruction tuning and\n for evaluation of the question answering abilities of language models.\n We can distinguish the datasets along many dimensions, summarized nicely in\n Rogers et al. (2023). One is the original purpose of the questions in the data, whether\n they were natural information-seeking questions, or whether they were questions\n designed for probing: evaluating or testing systems or humans.\n16 C HAPTER 11 ‚Ä¢ R ETRIEVAL - BASED M ODELS\n\n Natural\n Questions On the natural side there are datasets like Natural Questions (Kwiatkowski\n et al., 2019), a set of anonymized English queries to the Google search engine and\n their answers. The answers are created by annotators based on Wikipedia information, and include a paragraph-length long answer and a short span answer. For\n example the question ‚ÄúWhen are hops added to the brewing process?‚Äù has the short\n answer the boiling process and a long answer which is an entire paragraph from the\n Wikipedia page on Brewing.\n MS MARCO A similar natural question set is the MS MARCO (Microsoft Machine Reading\n Comprehension) collection of datasets, including 1 million real anonymized English\n questions from Microsoft Bing query logs together with a human generated answer\n and 9 million passages (Bajaj et al., 2016), that can be used both to test retrieval\n ranking and question answering.\n Although many datasets focus on English, natural information-seeking question datasets exist in other languages. The DuReader dataset is a Chinese QA resource based on search engine queries and community QA (He et al., 2018). TyDi\n TyDi QA QA dataset contains 204K question-answer pairs from 11 typologically diverse languages, including Arabic, Bengali, Kiswahili, Russian, and Thai (Clark et al., 2020).\n In the T Y D I QA task, a system is given a question and the passages from a Wikipedia article and must (a) select the passage containing the answer (or N ULL if no\n passage contains the answer), and (b) mark the minimal answer span (or N ULL).\n MMLU On the probing side are datasets like MMLU (Massive Multitask Language Understanding), a commonly-used dataset of 15908 knowledge and reasoning questions in 57 areas including medicine, mathematics, computer science, law, and others. MMLU questions are sourced from various exams for humans, such as the US\n Graduate Record Exam, Medical Licensing Examination, and Advanced Placement\n exams. So the questions don‚Äôt represent people‚Äôs information needs, but rather are\n designed to test human knowledge for academic or licensing purposes. Fig. 11.10\n shows some examples, with the correct answers in bold.\n Some of the question datasets described above augment each question with passage(s) from which the answer can be extracted. These datasets were mainly created\n reading\ncomprehension for an earlier QA task called reading comprehension in which a model is given\n a question and a document and is required to extract the answer from the given\n document. We sometimes call the task of question answering given one or more\n open book documents (for example via RAG), the open book QA task, while the task of anclosed book swering directly from the LM with no retrieval component at all is the closed book\n QA task.5 Thus datasets like Natural Questions can be treated as open book if the\n solver uses each question‚Äôs attached document, or closed book if the documents are\n not used, while datasets like MMLU are solely closed book.\n Another dimension of variation is the format of the answer: multiple-choice\n versus freeform. And of course there are variations in prompting, like whether the\n model is just the question (zero-shot) or also given demonstrations of answers to\n similar questions (few-shot). MMLU offers both zero-shot and few-shot prompt\n options.\n\n 5 This repurposes the word for types of exams in which students are allowed to ‚Äòopen their books‚Äô or\n not.\n 11.5 ‚Ä¢ E VALUATING Q UESTION A NSWERING 17\n\n MMLU examples\n\n College Computer Science\n Any set of Boolean operators that is sufficient to represent all Boolean expressions is said to be complete. Which of the following is NOT complete?\n (A) AND, NOT\n (B) NOT, OR\n (C) AND, OR\n (D) NAND\n\n College Physics\n The primary source of the Sun‚Äôs energy is a series of thermonuclear\n reactions in which the energy produced is c2 times the mass difference\n between\n (A) two hydrogen atoms and one helium atom\n (B) four hydrogen atoms and one helium atom\n (C) six hydrogen atoms and two helium atoms\n (D) three helium atoms and one carbon atom\n\n International Law\n Which of the following is a treaty-based human rights mechanism?\n (A) The UN Human Rights Committee\n (B) The UN Human Rights Council\n (C) The UN Universal Periodic Review\n (D) The UN special mandates\n\n Prehistory\n Unlike most other early civilizations, Minoan culture shows little evidence\n of\n (A) trade.\n (B) warfare.\n (C) the development of a common religion.\n (D) conspicuous consumption by elites.\n\n Three techniques are commonly employed to evaluate question-answering systems,\n with the choice depending on the type of question and QA situation. For multiple\n choice questions like in MMLU, we report exact match:\n Exact match: The % of predicted answers that match the gold answer\n exactly.\n For questions with free text answers, like Natural Questions, we commonly evaluated with token F1 score to roughly measure the partial string overlap between the\n answer and the reference answer:\n F1 score: The average token overlap between predicted and gold answers. Treat the prediction and gold as a bag of tokens, and compute F1\n for each question, then return the average F1 over all questions.\n18 C HAPTER 11 ‚Ä¢ R ETRIEVAL - BASED M ODELS\n\n Finally, in some situations QA systems give multiple ranked answers. In such cases\n mean\nreciprocal rank we evaluated using mean reciprocal rank, or MRR (Voorhees, 1999). MRR is\n MRR designed for systems that return a short ranked list of answers or passages for each\n test set question, which we can compare against the (human-labeled) correct answer.\n First, each test set question is scored with the reciprocal of the rank of the first\n correct answer. For example if the system returned five answers to a question but\n the first three are wrong (so the highest-ranked correct answer is ranked fourth), the\n reciprocal rank for that question is 41 . The score for questions that return no correct\n answer is 0. The MRR of a system is the average of the scores for each question in\n the test set. In some versions of MRR, questions with a score of zero are ignored\n in this calculation. More formally, for a system returning ranked answers to each\n question in a test set Q, (or in the alternate version, let Q be the subset of test set\n questions that have non-zero scores). MRR is then defined as\n\n 1 X 1\n MRR = (11.20)\n |Q| ranki\n i=1\n\n This chapter introduced the tasks of question answering and information retrieval.\n ‚Ä¢ Question answering (QA) is the task of answering a user‚Äôs questions.\n ‚Ä¢ We focus in this chapter on the task of retrieval-based question answering,\n in which the user‚Äôs questions are intended to be answered by the material in\n some set of documents (which might be the web).\n ‚Ä¢ Information Retrieval (IR) is the task of returning documents to a user based\n on their information need as expressed in a query. In ranked retrieval, the\n documents are returned in ranked order.\n ‚Ä¢ The match between a query and a document can be done by first representing\n each of them with a sparse vector that represents the frequencies of words,\n weighted by tf-idf or BM25. Then the similarity can be measured by cosine.\n ‚Ä¢ Documents or queries can instead be represented by dense vectors, by encoding the question and document with an encoder-only model like BERT, and in\n that case computing similarity in embedding space.\n ‚Ä¢ The inverted index is a storage mechanism that makes it very efficient to find\n documents that have a particular word.\n ‚Ä¢ Ranked retrieval is generally evaluated by mean average precision or interpolated precision.\n ‚Ä¢ Question answering systems generally use the retriever/reader architecture.\n In the retriever stage, an IR system is given a query and returns a set of\n documents.\n ‚Ä¢ The reader stage is implemented by retrieval-augmented generation, in\n which a large language model is prompted with the query and a set of documents and then conditionally generates a novel answer.\n ‚Ä¢ QA can be evaluated by exact match with a known answer if only a single\n answer is given, with token F1 score for free text answers, or with mean reciprocal rank if a ranked set of answers is given.\n H ISTORICAL N OTES 19\n\nHistorical Notes\n Question answering was one of the earliest NLP tasks. By 1961 the BASEBALL\n system (Green et al., 1961) answered questions about baseball games like ‚ÄúWhere\n did the Red Sox play on July 7‚Äù by querying a structured database of game information. The database was stored as a kind of attribute-value matrix with values for\n attributes of each game:\n Month = July\n Place = Boston\n Day = 7\n Game Serial No. = 96\n (Team = Red Sox, Score = 5)\n (Team = Yankees, Score = 3)\n Each question was constituency-parsed using the algorithm of Zellig Harris‚Äôs\n TDAP project at the University of Pennsylvania, essentially a cascade of finite-state\n transducers (see the historical discussion in Joshi and Hopely 1999 and Karttunen\n 1999). Then in a content analysis phase each word or phrase was associated with a\n program that computed parts of its meaning. Thus the phrase ‚ÄòWhere‚Äô had code to\n assign the semantics Place = ?, with the result that the question ‚ÄúWhere did the\n Red Sox play on July 7‚Äù was assigned the meaning\n Place = ?\n Team = Red Sox\n Month = July\n Day = 7\n The question is then matched against the database to return the answer.\n The Protosynthex system of Simmons et al. (1964), given a question, formed a\n query from the content words in the question, and then retrieved candidate answer\n sentences in the document, ranked by their frequency-weighted term overlap with\n the question. The query and each retrieved sentence were then parsed with dependency parsers, and the sentence whose structure best matches the question structure\n selected. Thus the question What do worms eat? would match worms eat grass:\n both have the subject worms as a dependent of eat, in the version of dependency\n grammar used at the time, while birds eat worms has birds as the subject:\n\n What do worms eat Worms eat grass Birds eat worms\n Simmons (1965) summarizes other early QA systems.\n By the 1970s, systems used predicate calculus as the meaning representation\n LUNAR language. The LUNAR system (Woods et al. 1972, Woods 1978) was designed to\n be a natural language interface to a database of chemical facts about lunar geology. It\n could answer questions like Do any samples have greater than 13 percent aluminum\n by parsing them into a logical form\n (TEST (FOR SOME X16 / (SEQ SAMPLES) : T ; (CONTAIN‚Äô X16\n (NPR* X17 / (QUOTE AL203)) (GREATERTHAN 13 PCT))))\n By the 1990s question answering shifted to machine learning. Zelle and Mooney\n (1996) proposed to treat question answering as a semantic parsing task, by creat-\n20 C HAPTER 11 ‚Ä¢ R ETRIEVAL - BASED M ODELS\n\n ing the Prolog-based GEOQUERY dataset of questions about US geography. This\n model was extended by Zettlemoyer and Collins (2005) and 2007. By a decade\n later, neural models were applied to semantic parsing (Dong and Lapata 2016, Jia\n and Liang 2016), and then to knowledge-based question answering by mapping text\n to SQL (Iyer et al., 2017).\n [TBD: History of IR.]\n Meanwhile, a paradigm for answering questions that drew more on informationretrieval was influenced by the rise of the web in the 1990s. The U.S. governmentsponsored TREC (Text REtrieval Conference) evaluations, run annually since 1992,\n provide a testbed for evaluating information-retrieval tasks and techniques (Voorhees\n and Harman, 2005). TREC added an influential QA track in 1999, which led to a\n wide variety of factoid and non-factoid question answering systems competing in\n annual evaluations.\n At that same time, Hirschman et al. (1999) introduced the idea of using children‚Äôs reading comprehension tests to evaluate machine text comprehension algorithms. They acquired a corpus of 120 passages with 5 questions each designed for\n 3rd-6th grade children, built an answer extraction system, and measured how well\n the answers given by their system corresponded to the answer key from the test‚Äôs\n publisher. Their algorithm focused on word overlap as a feature; later algorithms\n added named entity features and more complex similarity between the question and\n the answer span (Riloff and Thelen 2000, Ng et al. 2000).\n The DeepQA component of the Watson Jeopardy! system was a large and sophisticated feature-based system developed just before neural systems became common. It is described in a series of papers in volume 56 of the IBM Journal of Research and Development, e.g., Ferrucci (2012).\n Early neural reading comprehension systems drew on the insight common to\n early systems that answer finding should focus on question-passage similarity. Many\n of the architectural outlines of these neural systems were laid out in Hermann et al.\n (2015), Chen et al. (2017), and Seo et al. (2017). These systems focused on datasets\n like Rajpurkar et al. (2016) and Rajpurkar et al. (2018) and their successors, usually\n using separate IR algorithms as input to neural reading comprehension systems. The\n paradigm of using dense retrieval with a span-based reader, often with a single endto-end architecture, is exemplified by systems like Lee et al. (2019) or Karpukhin\n et al. (2020). An important research area with dense retrieval for open-domain QA\n is training data: using self-supervised methods to avoid having to label positive and\n negative passages (Sachan et al., 2023).\n Early work on large language models showed that they stored sufficient knowledge in the pretraining process to answer questions (Petroni et al., 2019; Raffel\n et al., 2020; Radford et al., 2019; Roberts et al., 2020), at first not competitively\n with special-purpose question answerers, but quickly surpassing them. Retrievalaugmented generation algorithms were first introduced as a way to improve language modeling word prediction (Khandelwal et al., 2019), but were quickly applied\n to question answering (Izacard et al., 2022; Ram et al., 2023; Shi et al., 2023).\n\nExercises\n Exercises 21\n\nArora, S., P. Lewis, A. Fan, J. Kahn, and C. ReÃÅ. 2023. Rea- Johnson, J., M. Douze, and H. JeÃÅgou. 2017. Billionsoning over public and private data in retrieval-based sys- scale similarity search with GPUs. ArXiv preprint\n tems. TACL, 11:902‚Äì921. arXiv:1702.08734.\nBabbage, C. 1864. Passages from the Life of a Philosopher. Joshi, A. K. and P. Hopely. 1999. A parser from antiquity.\n Longman. In A. Kornai, ed., Extended Finite State Models of Lan-\nBajaj, P., D. Campos, N. Craswell, L. Deng, J. G. ando guage, 6‚Äì15. Cambridge University Press.\n Xiaodong Liu, R. Majumder, A. McNamara, B. Mitra, Jurafsky, D. 2014. The Language of Food. W. W. Norton,\n T. Nguye, M. Rosenberg, X. Song, A. Stoica, S. Tiwary, New York.\n and T. Wang. 2016. MS MARCO: A human generated Kamphuis, C., A. P. de Vries, L. Boytsov, and J. Lin. 2020.\n MAchine Reading COmprehension dataset. NeurIPS. Which BM25 do you mean? a large-scale reproducibil-\nChen, D., A. Fisch, J. Weston, and A. Bordes. 2017. Reading ity study of scoring variants. European Conference on\n Wikipedia to answer open-domain questions. ACL. Information Retrieval.\nClark, J. H., E. Choi, M. Collins, D. Garrette, Karpukhin, V., B. OgÃÜuz, S. Min, P. Lewis, L. Wu, S. Edunov,\n T. Kwiatkowski, V. Nikolaev, and J. Palomaki. 2020. D. Chen, and W.-t. Yih. 2020. Dense passage retrieval for\n TyDi QA: A benchmark for information-seeking ques- open-domain question answering. EMNLP.\n tion answering in typologically diverse languages. TACL, Karttunen, L. 1999. Comments on Joshi. In A. Kornai, ed.,\n 8:454‚Äì470. Extended Finite State Models of Language, 16‚Äì18. Cam-\nDahl, M., V. Magesh, M. Suzgun, and D. E. Ho. 2024. Large bridge University Press.\n legal fictions: Profiling legal hallucinations in large lan- Khandelwal, U., O. Levy, D. Jurafsky, L. Zettlemoyer, and\n guage models. Journal of Legal Analysis, 16:64‚Äì93. M. Lewis. 2019. Generalization through memorization:\nDeerwester, S. C., S. T. Dumais, T. K. Landauer, G. W. Fur- Nearest neighbor language models. ICLR.\n nas, and R. A. Harshman. 1990. Indexing by latent se- Khattab, O., C. Potts, and M. Zaharia. 2021. Relevancemantics analysis. JASIS, 41(6):391‚Äì407. guided supervision for OpenQA with ColBERT. TACL,\nDong, L. and M. Lapata. 2016. Language to logical form 9:929‚Äì944.\n with neural attention. ACL. Khattab, O. and M. Zaharia. 2020. ColBERT: Efficient and\nFerrucci, D. A. 2012. Introduction to ‚ÄúThis is Watson‚Äù. IBM effective passage search via contextualized late interac-\nJournal of Research and Development, 56(3/4):1:1‚Äì1:15. tion over BERT. SIGIR.\nFurnas, G. W., T. K. Landauer, L. M. Gomez, and S. T. Kwiatkowski, T., J. Palomaki, O. Redfield, M. Collins,\n Dumais. 1987. The vocabulary problem in human- A. Parikh, C. Alberti, D. Epstein, I. Polosukhin, J. Desystem communication. Communications of the ACM, vlin, K. Lee, K. Toutanova, L. Jones, M. Kelcey, M.-W.\n 30(11):964‚Äì971. Chang, A. M. Dai, J. Uszkoreit, Q. Le, and S. Petrov.\nGeva, M., R. Schuster, J. Berant, and O. Levy. 2021. 2019. Natural questions: A benchmark for question an-\nTransformer feed-forward layers are key-value memories. swering research. TACL, 7:452‚Äì466.\n EMNLP. Lee, K., M.-W. Chang, and K. Toutanova. 2019. Latent re-\nGreen, B. F., A. K. Wolf, C. Chomsky, and K. Laughery. trieval for weakly supervised open domain question an-\n1961. Baseball: An automatic question answerer. Pro- swering. ACL.\n ceedings of the Western Joint Computer Conference 19. Manning, C. D., P. Raghavan, and H. SchuÃàtze. 2008. Intro-\nHe, W., K. Liu, J. Liu, Y. Lyu, S. Zhao, X. Xiao, Y. Liu, duction to Information Retrieval. Cambridge.\n Y. Wang, H. Wu, Q. She, X. Liu, T. Wu, and H. Wang. Meng, K., D. Bau, A. Andonian, and Y. Belinkov. 2022. Lo-\n2018. DuReader: a Chinese machine reading compre- cating and editing factual associations in GPT. NeurIPS,\n hension dataset from real-world applications. Workshop volume 36.\n on Machine Reading for Question Answering. Ng, H. T., L. H. Teo, and J. L. P. Kwan. 2000. A ma-\nHermann, K. M., T. Kocisky, E. Grefenstette, L. Espeholt, chine learning approach to answering questions for read-\nW. Kay, M. Suleyman, and P. Blunsom. 2015. Teaching ing comprehension tests. EMNLP.\n machines to read and comprehend. NeurIPS. Petroni, F., T. RocktaÃàschel, S. Riedel, P. Lewis, A. Bakhtin,\nHirschman, L., M. Light, E. Breck, and J. D. Burger. 1999. Y. Wu, and A. Miller. 2019. Language models as knowl-\nDeep Read: A reading comprehension system. ACL. edge bases? EMNLP.\nIyer, S., I. Konstas, A. Cheung, J. Krishnamurthy, and Radford, A., J. Wu, R. Child, D. Luan, D. Amodei, and\n L. Zettlemoyer. 2017. Learning a neural semantic parser I. Sutskever. 2019. Language models are unsupervised\n from user feedback. ACL. multitask learners. OpenAI tech report.\nIzacard, G., P. Lewis, M. Lomeli, L. Hosseini, F. Petroni, Raffel, C., N. Shazeer, A. Roberts, K. Lee, S. Narang,\n T. Schick, J. Dwivedi-Yu, A. Joulin, S. Riedel, and M. Matena, Y. Zhou, W. Li, and P. J. Liu. 2020. Exploring\n E. Grave. 2022. Few-shot learning with retrieval aug- the limits of transfer learning with a unified text-to-text\n mented language models. ArXiv preprint. transformer. JMLR, 21(140):1‚Äì67.\nJia, R. and P. Liang. 2016. Data recombination for neural Rajpurkar, P., R. Jia, and P. Liang. 2018. Know what you\n semantic parsing. ACL. don‚Äôt know: Unanswerable questions for SQuAD. ACL.\nJiang, C., B. Qi, X. Hong, D. Fu, Y. Cheng, F. Meng, M. Yu, Rajpurkar, P., J. Zhang, K. Lopyrev, and P. Liang. 2016.\n B. Zhou, and J. Zhou. 2024. On large language models‚Äô SQuAD: 100,000+ questions for machine comprehension\n hallucination with regard to known facts. NAACL HLT. of text. EMNLP.\n22 Chapter 11 ‚Ä¢ Retrieval-based Models\n\nRam, O., Y. Levine, I. Dalmedigos, D. Muhlgay, A. Shashua,\n K. Leyton-Brown, and Y. Shoham. 2023. In-context\n retrieval-augmented language models. ArXiv preprint.\nRiloff, E. and M. Thelen. 2000. A rule-based question answering system for reading comprehension tests.\n ANLP/NAACL workshop on reading comprehension tests.\nRoberts, A., C. Raffel, and N. Shazeer. 2020. How much\n knowledge can you pack into the parameters of a language model? EMNLP.\nRobertson, S., S. Walker, S. Jones, M. M. Hancock-\nBeaulieu, and M. Gatford. 1995. Okapi at TREC-3.\n Overview of the Third Text REtrieval Conference (TREC-\n3).\nRogers, A., M. Gardner, and I. Augenstein. 2023. QA dataset\n explosion: A taxonomy of NLP resources for question\n answering and reading comprehension. ACM Computing\n Surveys, 55(10):1‚Äì45.\nSachan, D. S., M. Lewis, D. Yogatama, L. Zettlemoyer,\n J. Pineau, and M. Zaheer. 2023. Questions are all you\n need to train a dense passage retriever. TACL, 11:600‚Äì\n 616.\nSalton, G. 1971. The SMART Retrieval System: Experiments\n in Automatic Document Processing. Prentice Hall.\nSeo, M., A. Kembhavi, A. Farhadi, and H. Hajishirzi. 2017.\n Bidirectional attention flow for machine comprehension.\n ICLR.\nShi, W., S. Min, M. Yasunaga, M. Seo, R. James, M. Lewis,\n L. Zettlemoyer, and W.-t. Yih. 2023. REPLUG: Retrievalaugmented black-box language models. ArXiv preprint.\nSimmons, R. F. 1965. Answering English questions by computer: A survey. CACM, 8(1):53‚Äì70.\nSimmons, R. F., S. Klein, and K. McConlogue. 1964. Indexing and dependency logic for answering English questions. American Documentation, 15(3):196‚Äì204.\nSparck Jones, K. 1972. A statistical interpretation of term\n specificity and its application in retrieval. Journal of Documentation, 28(1):11‚Äì21.\nVoorhees, E. M. 1999. TREC-8 question answering track\n report. Proceedings of the 8th Text Retrieval Conference.\nVoorhees, E. M. and D. K. Harman. 2005. TREC: Experiment and Evaluation in Information Retrieval. MIT Press.\nWoods, W. A. 1978. Semantics and quantification in natural\n language question answering. In M. Yovits, ed., Advances\n in Computers, 2‚Äì64. Academic.\nWoods, W. A., R. M. Kaplan, and B. L. Nash-Webber. 1972.\n The lunar sciences natural language information system:\n Final report. Technical Report 2378, BBN.\nZelle, J. M. and R. J. Mooney. 1996. Learning to parse\n database queries using inductive logic programming.\n AAAI.\nZettlemoyer, L. and M. Collins. 2005. Learning to map\n sentences to logical form: Structured classification with\n probabilistic categorial grammars. Uncertainty in Artificial Intelligence, UAI‚Äô05.\nZettlemoyer, L. and M. Collins. 2007. Online learning\n of relaxed CCG grammars for parsing to logical form.\n EMNLP/CoNLL.\nZhou, K., J. Hwang, X. Ren, and M. Sap. 2024. Relying\n on the unreliable: The impact of language models‚Äô reluctance to express uncertainty. ACL.\n",
    "file_path": "/Users/colinsidberry/nlp-textbook-rag/data/extracted_text/11-RAG.txt",
    "file_size_kb": 60.54
  },
  {
    "id": "935c66d9c4bdec9d",
    "source": "nlp_textbook",
    "chapter": "12 Machine Translation",
    "filename": "12.Machine Translation.txt",
    "content": "rights reserved. Draft of August 24, 2025.\n\nCHAPTER\n\n12 Machine Translation\n\n ‚ÄúI want to talk the dialect of your people. It‚Äôs no use of talking unless\n people understand what you say.‚Äù\n Zora Neale Hurston, Moses, Man of the Mountain 1939, p. 121\n\n machine This chapter introduces machine translation (MT), the use of computers to transtranslation\n MT late from one language to another.\n Of course translation, in its full generality, such as the translation of literature, or\n poetry, is a difficult, fascinating, and intensely human endeavor, as rich as any other\n area of human creativity.\n Machine translation in its present form therefore focuses on a number of very\n practical tasks. Perhaps the most common current use of machine translation is\n information for information access. We might want to translate some instructions on the web,\n access\n perhaps the recipe for a favorite dish, or the steps for putting together some furniture.\n Or we might want to read an article in a newspaper, or get information from an\n online resource like Wikipedia or a government webpage in some other language.\n MT for information\n access is probably\n one of the most common uses of NLP\n technology, and Google\n Translate alone (shown above) translates hundreds of billions of words a day between over 100 languages. Improvements in machine translation can thus help redigital divide duce what is often called the digital divide in information access: the fact that much\n more information is available in English and other languages spoken in wealthy\n countries. Web searches in English return much more information than searches in\n other languages, and online resources like Wikipedia are much larger in English and\n other higher-resourced languages. High-quality translation can help provide information to speakers of lower-resourced languages.\n Another common use of machine translation is to aid human translators. MT syspost-editing tems are routinely used to produce a draft translation that is fixed up in a post-editing\n phase by a human translator. This task is often called computer-aided translation\n CAT or CAT. CAT is commonly used as part of localization: the task of adapting content\n localization or a product to a particular language community.\n Finally, a more recent application of MT is to in-the-moment human communication needs. This includes incremental translation, translating speech on-the-fly\n before the entire sentence is complete, as is commonly used in simultaneous interpretation. Image-centric translation can be used for example to use OCR of the text\n on a phone camera image as input to an MT system to translate menus or street signs.\n encoder- The standard algorithm for MT is the encoder-decoder network/ We briefly\n decoder\n mentioned in Chapter 7 that encoder-decoder or sequence-to-sequence models are\n used for tasks in which we need to map an input sequence to an output sequence\n that is a complex function of the entire input sequence, like machine translation or\n2 C HAPTER 12 ‚Ä¢ M ACHINE T RANSLATION\n\n speech recognition. Indeed, in machine translation, the words of the target language\n don‚Äôt necessarily agree with the words of the source language in number or order.\n Consider translating the following made-up English sentence into Japanese.\n (12.1) English: He wrote a letter to a friend\n Japanese: tomodachi ni tegami-o kaita\n friend to letter wrote\n Note that the elements of the sentences are in very different places in the different\n languages. In English, the verb is in the middle of the sentence, while in Japanese,\n the verb kaita comes at the end. The Japanese sentence doesn‚Äôt require the pronoun\n he, while English does.\n Such differences between languages can be quite complex. In the following actual sentence from the United Nations, notice the many changes between the Chinese\n sentence (we‚Äôve given in red a word-by-word gloss of the Chinese characters) and\n its English equivalent produced by human translators.\n (12.2) Â§ß‰ºö/General Assembly Âú®/on 1982Âπ¥/1982 12Êúà/December 10Êó•/10 ÈÄöËøá\n ‰∫Ü/adopted Á¨¨37Âè∑/37th ÂÜ≥ËÆÆ/resolution ÔºåÊ†∏ÂáÜ‰∫Ü/approved Á¨¨‰∫å\n Ê¨°/second Êé¢Á¥¢/exploration Âèä/and ÂíåÂπ≥peaceful Âà©Áî®/using Â§ñÂ±ÇÁ©∫\n Èó¥/outer space ‰ºöËÆÆ/conference ÁöÑ/of ÂêÑÈ°π/various Âª∫ËÆÆ/suggestions „ÄÇ\n On 10 December 1982 , the General Assembly adopted resolution 37 in\n which it endorsed the recommendations of the Second United Nations\n Conference on the Exploration and Peaceful Uses of Outer Space .\n Note the many ways the English and Chinese differ. For example the ordering differs in major ways; the Chinese order of the noun phrase is ‚Äúpeaceful using\n outer space conference of suggestions‚Äù while the English has ‚Äúsuggestions of the ...\n conference on peaceful use of outer space‚Äù). And the order differs in minor ways\n (the date is ordered differently). English requires the in many places that Chinese\n doesn‚Äôt, and adds some details (like ‚Äúin which‚Äù and ‚Äúit‚Äù) that aren‚Äôt necessary in\n Chinese. Chinese doesn‚Äôt grammatically mark plurality on nouns (unlike English,\n which has the ‚Äú-s‚Äù in ‚Äúrecommendations‚Äù), and so the Chinese must use the modifier ÂêÑÈ°π/various to make it clear that there is not just one recommendation. English\n capitalizes some words but not others. Encoder-decoder networks are very successful at handling these sorts of complicated cases of sequence mappings.\n We‚Äôll begin in the next section by considering the linguistic background about\n how languages vary, and the implications this variance has for the task of MT. Then\n we‚Äôll sketch out the standard algorithm, give details about things like input tokenization and creating training corpora of parallel sentences, give some more low-level\n details about the encoder-decoder network, and finally discuss how MT is evaluated,\n introducing the simple chrF metric.\n\n There are about 7,000 languages in the world. Some aspects of human language\n universal seem to be universal, holding true for every one of these languages, or are statistical\n universals, holding true for most of these languages. Many universals arise from the\n functional role of language as a communicative system by humans. Every language,\n for example, seems to have words for referring to people, for talking about eating\n and drinking, for being polite or not. There are also structural linguistic universals; for example, every language seems to have nouns and verbs (Chapter 17), has\n 12.1 ‚Ä¢ L ANGUAGE D IVERGENCES AND T YPOLOGY 3\n\n ways to ask questions, or issue commands, has linguistic mechanisms for indicating\n agreement or disagreement.\n Yet languages also differ in many ways (as has been pointed out since ancient\ntranslation\ndivergence times; see Fig. 12.1). Understanding what causes such translation divergences\n (Dorr, 1994) can help us build better MT models. We often distinguish the idiosyncratic and lexical differences that must be dealt with one by one (the word for ‚Äúdog‚Äù\n differs wildly from language to language), from systematic differences that we can\n model in a general way (many languages put the verb before the grammatical object; others put the verb after the grammatical object). The study of these systematic\n typology cross-linguistic similarities and differences is called linguistic typology. This section sketches some typological facts that impact machine translation; the interested\n reader should also look into WALS, the World Atlas of Language Structures, which\n gives many typological facts about languages (Dryer and Haspelmath, 2013).\n\n Kunsthistorisches Museum, Vienna.\n\n 12.1.1 Word Order Typology\n As we hinted at in our example above comparing English and Japanese, languages\n differ in the basic word order of verbs, subjects, and objects in simple declara-\nSVO tive clauses. German, French, English, and Mandarin, for example, are all SVO\n (Subject-Verb-Object) languages, meaning that the verb tends to come between\n SOV the subject and object. Hindi and Japanese, by contrast, are SOV languages, meaning that the verb tends to come at the end of basic clauses, and Irish and Arabic are\n VSO VSO languages. Two languages that share their basic word order type often have\n other similarities. For example, VO languages generally have prepositions, whereas\n OV languages generally have postpositions.\n Let‚Äôs look in more detail at the example we saw above. In this SVO English\n sentence, the verb wrote is followed by its object a letter and the prepositional phrase\n4 C HAPTER 12 ‚Ä¢ M ACHINE T RANSLATION\n\n to a friend, in which the preposition to is followed by its argument a friend. Arabic,\n with a VSO order, also has the verb before the object and prepositions. By contrast,\n in the Japanese example that follows, each of these orderings is reversed; the verb is\n preceded by its arguments, and the postposition follows its argument.\n (12.3) English: He wrote a letter to a friend\n Japanese: tomodachi ni tegami-o kaita\n friend to letter wrote\n Arabic: katabt risaÃÑla li sÃáadq\n wrote letter to friend\n Other kinds of ordering preferences vary idiosyncratically from language to language. In some SVO languages (like English and Mandarin) adjectives tend to appear before nouns, while in others languages like Spanish and Modern Hebrew, adjectives appear after the noun:\n (12.4) Spanish bruja verde English green witch\n\n (a) (b)\n initial position that in English are more natural later, and tensed verbs occur in second position. (b) In Mandarin, preposition phrases expressing goals often occur pre-verbally, unlike\n in English.\n\n Fig. 12.2 shows examples of other word order differences. All of these word\n order differences between languages can cause problems for translation, requiring\n the system to do huge structural reorderings as it generates the output.\n\n 12.1.2 Lexical Divergences\n Of course we also need to translate the individual words from one language to another. For any translation, the appropriate word can vary depending on the context.\n The English source-language word bass, for example, can appear in Spanish as the\n fish lubina or the musical instrument bajo. German uses two distinct words for what\n in English would be called a wall: Wand for walls inside a building, and Mauer for\n walls outside a building. Where English uses the word brother for any male sibling, Chinese and many other languages have distinct words for older brother and\n younger brother (Mandarin gege and didi, respectively). In all these cases, translating bass, wall, or brother from English would require a kind of specialization,\n disambiguating the different uses of a word. For this reason the fields of MT and\n Word Sense Disambiguation (Appendix G) are closely linked.\n Sometimes one language places more grammatical constraints on word choice\n than another. We saw above that English marks nouns for whether they are singular\n or plural. Mandarin doesn‚Äôt. Or French and Spanish, for example, mark grammatical gender on adjectives, so an English translation into French requires specifying\n adjective gender.\n The way that languages differ in lexically dividing up conceptual space may be\n more complex than this one-to-many translation problem, leading to many-to-many\n 12.1 ‚Ä¢ L ANGUAGE D IVERGENCES AND T YPOLOGY 5\n\n mappings. For example, Fig. 12.3 summarizes some of the complexities discussed\n by Hutchins and Somers (1992) in translating English leg, foot, and paw, to French.\n For example, when leg is used about an animal it‚Äôs translated as French patte; but\n about the leg of a journey, as French etape; if the leg is of a chair, we use French\n pied.\n lexical gap Further, one language may have a lexical gap, where no word or phrase, short\n of an explanatory footnote, can express the exact meaning of a word in the other\n language. For example, English does not have a word that corresponds neatly to\n Mandarin xiaÃÄo or Japanese oyakoÃÑkoÃÑ (in English one has to make do with awkward\n phrases like filial piety or loving child, or good son/daughter for both).\n\n ANIMAL paw\n etape\n JOURNEY ANIMAL\n patte\n BIRD\n leg foot\n HUMAN CHAIR HUMAN\n\n jambe pied\n\n Finally, languages differ systematically in how the conceptual properties of an\n event are mapped onto specific words. Talmy (1985, 1991) noted that languages\n can be characterized by whether direction of motion and manner of motion are\n marked on the verb or on the ‚Äúsatellites‚Äù: particles, prepositional phrases, or adverbial phrases. For example, a bottle floating out of a cave would be described in\n English with the direction marked on the particle out, while in Spanish the direction\n would be marked on the verb:\n (12.5) English: The bottle floated out.\n Spanish: La botella salioÃÅ flotando.\n The bottle exited floating.\n verb-framed Verb-framed languages mark the direction of motion on the verb (leaving the\n satellites to mark the manner of motion), like Spanish acercarse ‚Äòapproach‚Äô, alsatellite-framed canzar ‚Äòreach‚Äô, entrar ‚Äòenter‚Äô, salir ‚Äòexit‚Äô. Satellite-framed languages mark the\n direction of motion on the satellite (leaving the verb to mark the manner of motion),\n like English crawl out, float off, jump down, run after. Languages like Japanese,\n Tamil, and the many languages in the Romance, Semitic, and Mayan languages families, are verb-framed; Chinese as well as non-Romance Indo-European languages\n like English, Swedish, Russian, Hindi, and Farsi are satellite framed (Talmy 1991,\n Slobin 1996).\n\n 12.1.3 Morphological Typology\n Morphologically, languages are often characterized along two dimensions of variisolating ation. The first is the number of morphemes per word, ranging from isolating\n languages like Vietnamese and Cantonese, in which each word generally has one\n polysynthetic morpheme, to polysynthetic languages like Siberian Yupik (‚ÄúEskimo‚Äù), in which a\n single word may have very many morphemes, corresponding to a whole sentence in\n English. The second dimension is the degree to which morphemes are segmentable,\n agglutinative ranging from agglutinative languages like Turkish, in which morphemes have relfusion atively clean boundaries, to fusion languages like Russian, in which a single affix\n6 C HAPTER 12 ‚Ä¢ M ACHINE T RANSLATION\n\n may conflate multiple morphemes, like -om in the word stolom (table-SG-INSTR-\nDECL 1), which fuses the distinct morphological categories instrumental, singular,\n and first declension.\n Translating between languages with rich morphology requires dealing with structure below the word level, and for this reason modern systems generally use subword\n models like the wordpiece or BPE models of Section 12.2.1.\n\n 12.1.4 Referential density\n Finally, languages vary along a typological dimension related to the things they tend\n to omit. Some languages, like English, require that we use an explicit pronoun when\n talking about a referent that is given in the discourse. In other languages, however,\n we can sometimes omit pronouns altogether, as the following example from Spanish\n shows1 :\n (12.6) [El jefe]i dio con un libro. 0/ i MostroÃÅ su hallazgo a un descifrador ambulante.\n [The boss] came upon a book. [He] showed his find to a wandering decoder.\n pro-drop Languages that can omit pronouns are called pro-drop languages. Even among\n the pro-drop languages, there are marked differences in frequencies of omission.\n Japanese and Chinese, for example, tend to omit far more than does Spanish. This\n dimension of variation across languages is called the dimension of referential denreferential\n density sity. We say that languages that tend to use more pronouns are more referentially\n dense than those that use more zeros. Referentially sparse languages, like Chinese or\n Japanese, that require the hearer to do more inferential work to recover antecedents\n cold language are also called cold languages. Languages that are more explicit and make it easier\n hot language for the hearer are called hot languages. The terms hot and cold are borrowed from\n Marshall McLuhan‚Äôs 1964 distinction between hot media like movies, which fill in\n many details for the viewer, versus cold media like comics, which require the reader\n to do more inferential work to fill out the representation (Bickel, 2003).\n Translating from languages with extensive pro-drop, like Chinese or Japanese, to\n non-pro-drop languages like English can be difficult since the model must somehow\n identify each zero and recover who or what is being talked about in order to insert\n the proper pronoun.\n\n The standard architecture for MT is the encoder-decoder transformer or sequenceto-sequence model, an architecture we saw for RNNs in Chapter 13. We‚Äôll see the\n details of how to apply this architecture to transformers in Section 12.3, but first let‚Äôs\n talk about the overall task.\n Most machine translation tasks make the simplification that we can translate each\n sentence independently, so we‚Äôll just consider individual sentences for now. Given\n a sentence in a source language, the MT task is then to generate a corresponding\n sentence in a target language. For example, an MT system is given an English\n sentence like\n The green witch arrived\n and must translate it into the Spanish sentence:\n 1 Here we use the 0-notation;\n / we‚Äôll introduce this and discuss this issue further in Chapter 23\n 12.2 ‚Ä¢ M ACHINE T RANSLATION USING E NCODER -D ECODER 7\n\n LlegoÃÅ la bruja verde\n MT uses supervised machine learning: at training time the system is given a\n large set of parallel sentences (each sentence in a source language matched with\n a sentence in the target language), and learns to map source sentences into target\n sentences. In practice, rather than using words (as in the example above), we split\n the sentences into a sequence of subword tokens (tokens can be words, or subwords,\n or individual characters). The systems are then trained to maximize the probability\n of the sequence of tokens in the target language y1 , ..., ym given the sequence of\n tokens in the source language x1 , ..., xn :\n\n P(y1 , . . . , ym |x1 , . . . , xn ) (12.7)\n\n Rather than use the input tokens directly, the encoder-decoder architecture consists of two components, an encoder and a decoder. The encoder takes the input\n words x = [x1 , . . . , xn ] and produces an intermediate context h. At decoding time, the\n system takes h and, word by word, generates the output y:\n\n h = encoder(x) (12.8)\n yt+1 = decoder(h, y1 , . . . , yt ) ‚àÄt ‚àà [1, . . . , m] (12.9)\n\n In the next two sections we‚Äôll talk about subword tokenization, and then how to get\n parallel corpora for training, and then we‚Äôll introduce the details of the encoderdecoder architecture.\n\n 12.2.1 Tokenization\n Machine translation systems use a vocabulary that is fixed in advance, and rather\n than using space-separated words, this vocabulary is generated with subword tokenization algorithms, like the BPE algorithm sketched in Chapter 2. A shared\n vocabulary is used for the source and target languages, which makes it easy to copy\n tokens (like names) from source to target. Using subword tokenization with tokens\n shared between languages makes it natural to translate between languages like English or Hindi that use spaces to separate words, and languages like Chinese or Thai\n that don‚Äôt.\n We build the vocabulary by running a subword tokenization algorithm on a corpus that contains both source and target language data.\n Rather than the simple BPE algorithm from Fig. ??, modern systems often use\n more powerful tokenization algorithms. Some systems (like BERT) use a variant of\nwordpiece BPE called the wordpiece algorithm, which instead of choosing the most frequent\n set of tokens to merge, chooses merges based on which one most increases the language model probability of the tokenization. Wordpieces use a special symbol at the\n beginning of each token; here‚Äôs a resulting tokenization from the Google MT system\n (Wu et al., 2016):\n words: Jet makers feud over seat width with big orders at stake\n wordpieces: J et makers fe ud over seat width with big orders at stake\n The wordpiece algorithm is given a training corpus and a desired vocabulary size\n V, and proceeds as follows:\n 1. Initialize the wordpiece lexicon with characters (for example a subset of Unicode characters, collapsing all the remaining characters to a special unknown\n character token).\n8 C HAPTER 12 ‚Ä¢ M ACHINE T RANSLATION\n\n 2. Repeat until there are V wordpieces:\n (a) Train an n-gram language model on the training corpus, using the current\n set of wordpieces.\n (b) Consider the set of possible new wordpieces made by concatenating two\n wordpieces from the current lexicon. Choose the one new wordpiece that\n most increases the language model probability of the training corpus.\n Recall that with BPE we had to specify the number of merges to perform; in\n wordpiece, by contrast, we specify the total vocabulary, which is a more intuitive\n parameter. A vocabulary of 8K to 32K word pieces is commonly used.\n An even more commonly used tokenization algorithm is (somewhat ambiguunigram ously) called the unigram algorithm (Kudo, 2018) or sometimes the SentencePiece\n SentencePiece algorithm, and is used in systems like ALBERT (Lan et al., 2020) and T5 (Raffel et al., 2020). (Because unigram is the default tokenization algorithm used in\n a library called SentencePiece that adds a useful wrapper around tokenization algorithms (Kudo and Richardson, 2018), authors often say they are using SentencePiece\n tokenization but really mean they are using the unigram algorithm).\n In unigram tokenization, instead of building up a vocabulary by merging tokens,\n we start with a huge vocabulary of every individual unicode character plus all frequent sequences of characters (including all space-separated words, for languages\n with spaces), and iteratively remove some tokens to get to a desired final vocabulary\n size. The algorithm is complex (involving suffix-trees for efficiently storing many\n tokens, and the EM algorithm for iteratively assigning probabilities to tokens), so we\n don‚Äôt give it here, but see Kudo (2018) and Kudo and Richardson (2018). Roughly\n speaking the algorithm proceeds iteratively by estimating the probability of each\n token, tokenizing the input data using various tokenizations, then removing a percentage of tokens that don‚Äôt occur in high-probability tokenization, and then iterates\n until the vocabulary has been reduced down to the desired number of tokens.\n Why does unigram tokenization work better than BPE? BPE tends to create lots\n of very small non-meaningful tokens (because BPE can only create larger words or\n morphemes by merging characters one at a time), and it also tends to merge very\n common tokens, like the suffix ed, onto their neighbors. We can see from these\n examples from Bostrom and Durrett (2020) that unigram tends to produce tokens\n that are more semantically meaningful:\nOriginal: corrupted Original: Completely preposterous suggestions\nBPE: cor rupted BPE: Comple t ely prep ost erous suggest ions\nUnigram: corrupt ed Unigram: Complete ly pre post er ous suggestion s\n\n 12.2.2 Creating the Training data\nparallel corpus Machine translation models are trained on a parallel corpus, sometimes called a\n bitext, a text that appears in two (or more) languages. Large numbers of paral-\nEuroparl lel corpora are available. Some are governmental; the Europarl corpus (Koehn,\n 2005), extracted from the proceedings of the European Parliament, contains between\n 400,000 and 2 million sentences each from 21 European languages. The United Nations Parallel Corpus contains on the order of 10 million sentences in the six official\n languages of the United Nations (Arabic, Chinese, English, French, Russian, Spanish) Ziemski et al. (2016). Other parallel corpora have been made from movie and\n TV subtitles, like the OpenSubtitles corpus (Lison and Tiedemann, 2016), or from\n general web text, like the ParaCrawl corpus of 223 million sentence pairs between\n 23 EU languages and English extracted from the CommonCrawl BanÃÉoÃÅn et al. (2020).\n 12.2 ‚Ä¢ M ACHINE T RANSLATION USING E NCODER -D ECODER 9\n\n Sentence alignment\n Standard training corpora for MT come as aligned pairs of sentences. When creating new corpora, for example for underresourced languages or new domains, these\n sentence alignments must be created. Fig. 12.4 gives a sample hypothetical sentence\n alignment.\n\n E1: ‚ÄúGood morning,\" said the little prince. F1: -Bonjour, dit le petit prince.\n\n E2: ‚ÄúGood morning,\" said the merchant. F2: -Bonjour, dit le marchand de pilules perfectionn√©es qui\n apaisent la soif.\n E3: This was a merchant who sold pills that had\n F3: On en avale une par semaine et l'on n'√©prouve plus le\n been perfected to quench thirst.\n besoin de boire.\n E4: You just swallow one pill a week and you F4: -C‚Äôest une grosse √©conomie de temps, dit le marchand.\n won‚Äôt feel the need for anything to drink.\n E5: ‚ÄúThey save a huge amount of time,\" said the merchant. F5: Les experts ont fait des calculs.\n\n E6: ‚ÄúFifty‚àíthree minutes a week.\" F6: On √©pargne cinquante-trois minutes par semaine.\n\n E7: ‚ÄúIf I had fifty‚àíthree minutes to spend?\" said the F7: ‚ÄúMoi, se dit le petit prince, si j'avais cinquante-trois minutes\n little prince to himself. √† d√©penser, je marcherais tout doucement vers une fontaine...\"\n E8: ‚ÄúI would take a stroll to a spring of fresh water‚Äù\n\nAntoine de Saint-Exupery‚Äôs Le Petit Prince and a hypothetical translation. Sentence alignment takes sentences\ne1 , ..., en , and f1 , ..., fm and finds minimal sets of sentences that are translations of each other, including single\nsentence mappings like (e1 ,f1 ), (e4 ,f3 ), (e5 ,f4 ), (e6 ,f6 ) as well as 2-1 alignments (e2 /e3 ,f2 ), (e7 /e8 ,f7 ), and null\nalignments (f5 ).\n\n Given two documents that are translations of each other, we generally need two\n steps to produce sentence alignments:\n ‚Ä¢ a cost function that takes a span of source sentences and a span of target sentences and returns a score measuring how likely these spans are to be translations.\n ‚Ä¢ an alignment algorithm that takes these scores to find a good alignment between the documents.\n To score the similarity of sentences across languages, we need to make use of\n a multilingual embedding space, in which sentences from different languages are\n in the same embedding space (Artetxe and Schwenk, 2019). Given such a space,\n cosine similarity of such embeddings provides a natural scoring function (Schwenk,\n 2018). Thompson and Koehn (2019) give the following cost function between two\n sentences or spans x,y from the source and target documents respectively:\n (1 ‚àí cos(x, y))nSents(x) nSents(y)\n c(x, y) = PS PS (12.10)\n s=1 1 ‚àí cos(x, ys ) + s=1 1 ‚àí cos(xs , y)\n\n where nSents() gives the number of sentences (this biases the metric toward many\n alignments of single sentences instead of aligning very large spans). The denominator helps to normalize the similarities, and so x1 , ..., xS , y1 , ..., yS , are randomly\n selected sentences sampled from the respective documents.\n Usually dynamic programming is used as the alignment algorithm (Gale and\n Church, 1993), in a simple extension of the minimum edit distance algorithm we\n introduced in Chapter 2.\n Finally, it‚Äôs helpful to do some corpus cleanup by removing noisy sentence pairs.\n This can involve handwritten rules to remove low-precision pairs (for example removing sentences that are too long, too short, have different URLs, or even pairs\n10 C HAPTER 12 ‚Ä¢ M ACHINE T RANSLATION\n\n that are too similar, suggesting that they were copies rather than translations). Or\n pairs can be ranked by their multilingual embedding cosine score and low-scoring\n pairs discarded.\n\n Decoder\n\n cross-attention lleg√≥ la bruja verde </s>\n\n transformer\n blocks\n\n The green witch arrived\n <s> lleg√≥ la bruja verde\n Encoder\n\ntransformer blocks we saw in Chapter 8, while the decoder uses a more powerful block with an extra crossattention layer that can attend to all the encoder words. We‚Äôll see this in more detail in the next section.\n\n The standard architecture for MT is the encoder-decoder transformer. (For those\n of you who studied RNNs, the encoder-decoder architecture was introduced already\n for RNNs in Chapter 13.) Fig. 12.5 shows the intuition of the architecture at a high\n level. You‚Äôll see that the encoder-decoder architecture is made up of two transformers: an encoder, which is the same as the basic transformers from Chapter 8, and\n a decoder, which is augmented with a special new layer called the cross-attention\n layer. The encoder takes the source language input word tokens X = x1 , ..., xn and\n maps them to an output representation Henc = h1 , ..., hn ; via a stack of encoder\n blocks.\n The decoder is essentially a conditional language model that attends to the encoder representation and generates the target words one by one, at each timestep\n conditioning on the source sentence and the previously generated target language\n words to generate a token. Decoding can use any of the decoding methods discussed\n in Chapter 8 like greedy, or temperature or nucleus sampling. But the most common decoding algorithm for MT is the beam search algorithm that we‚Äôll introduce\n in Section 12.4.\n But the components of the architecture differ somewhat from the transformer\n block we‚Äôve seen. First, in order to attend to the source language, the transformer\n blocks in the decoder have an extra cross-attention layer. Recall that the transformer\n block of Chapter 8 consists of a self-attention layer that attends to the input from the\n previous layer, preceded by layer norm, and followed by another layer norm and the\n feed forward layer. The decoder transformer block includes an extra layer with a\n cross-attention special kind of attention, cross-attention (also sometimes called encoder-decoder\n attention or source attention). Cross-attention has the same form as the multi-head\n attention in a normal transformer block, except that while the queries as usual come\n from the previous layer of the decoder, the keys and values come from the output of\n the encoder.\n 12.3 ‚Ä¢ D ETAILS OF THE E NCODER -D ECODER M ODEL 11\n\n y1 y2 yi+1 ym\n ‚Ä¶ Language\n Modeling\n Henc h1 h2 hi hn Head\n ‚Ä¶ ‚Ä¶\n Unembedding Matrix\n Block K Block L\n\n ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶\n Block 2\n Block 2\n\n + +\n\n Feedforward Feedforward\n Encoder\n Block 1 Layer Normalize\n Layer Normalize\n Cross-Attention\n Multi-Head Attention Decoder\n Layer Normalize Block 1\n Layer Normalize +\n\n Causal (Left-to-Right)\n x1 x2 ‚Ä¶ xi ‚Ä¶ xn\n Multi-Head Attention\n\n Encoder Layer Normalize\n\n <> y1 ‚Ä¶ yi ‚Ä¶ ym\n\n ‚Ä¶\n Decoder\n\nfinal output of the encoder Henc = h1 , ..., hn is the context used in the decoder. The decoder is a standard\ntransformer except with one extra layer, the cross-attention layer, which takes that encoder output Henc and\nuses it to form its K and V inputs.\n\n That is, where in standard multi-head attention the input to each attention layer is\n X, in cross attention the input is the the final output of the encoder Henc = h1 , ..., hn .\n Henc is of shape [n √ó d], each row representing one input token. To link the keys\n and values from the encoder with the query from the prior layer of the decoder, we\n multiply the encoder output Henc by the cross-attention layer‚Äôs key weights WK and\n value weights WV . The query comes from the output from the prior decoder layer\n Hdec[`‚àí1] , which is multiplied by the cross-attention layer‚Äôs query weights WQ :\n\n Q = Hdec[`‚àí1] WQ ; K = Henc WK ; V = Henc WV (12.11)\n\n \u0012 \u0013\n QK|\n CrossAttention(Q, K, V) = softmax ‚àö V (12.12)\n dk\n\n The cross attention thus allows the decoder to attend to each of the source language\n words as projected into the entire encoder final output representations. The other\n attention layer in each decoder block, the multi-head attention layer, is the same\n causal (left-to-right) attention that we saw in Chapter 8. The multi-head attention in\n the encoder, however, is allowed to look ahead at the entire source language text, so\n it is not masked.\n To train an encoder-decoder model, we use the same self-supervision model we\n used for training encoder-decoders RNNs in Chapter 13. The network is given the\n source text and then starting with the separator token is trained autoregressively to\n predict the next token using cross-entropy loss. Recall that cross-entropy loss for\n language modeling is determined by the probability the model assigns to the correct\n12 C HAPTER 12 ‚Ä¢ M ACHINE T RANSLATION\n\n next word. So at time t the CE loss is the negative log probability the model assigns\n to the next word in the training sequence:\n\n LCE (yÃÇt , yt ) = ‚àí log yÃÇt [wt+1 ] (12.13)\n\nteacher forcing As in that case, we use teacher forcing in the decoder. Recall that in teacher forcing, at each time step in decoding we force the system to use the gold target token\n from training as the next input xt+1 , rather than allowing it to rely on the (possibly\n erroneous) decoder output yÀÜt .\n\n Recall the greedy decoding algorithm from Chapter 8: at each time step t in generation, the output yt is chosen by computing the probability for each word in the\n vocabulary and then choosing the highest probability word (the argmax):\n\n wÃÇt = argmaxw‚ààV P(w|w<t ) (12.14)\n\n A problem with greedy decoding is that what looks high probability at word t might\n turn out to have been the wrong choice once we get to word t + 1. The beam search\n algorithm maintains multiple choices until later when we can see which one is best.\n In beam search we model decoding as searching the space of possible generasearch tree tions, represented as a search tree whose branches represent actions (generating a\n token), and nodes represent states (having generated a particular prefix). We search\n for the best action sequence, i.e., the string with the highest probability.\n\n An illustration of the problem\n Fig. 12.7 shows a made-up example. The most probable sequence is ok ok EOS (its\n probability is .4√ó .7√ó 1.0). But greedy search doesn‚Äôt find it, incorrectly choosing\n yes as the first word since it has the highest local probability (0.5).\n\n p(t3| t1,t2)\n\n p(t2| t1)\n ok 1.0 EOS\n .7\n yes 1.0 EOS\n p(t1|start) .2\n ok .1 EOS\n .4\n start .5 yes .3 ok 1.0 EOS\n .1 .4\n EOS yes 1.0 EOS\n .3\n EOS\n\n t1 t2 t3\n\n V = {yes, ok, <s>}, showing the probability of generating each token from that state. Greedy\n search chooses yes followed by yes, instead of the globally most probable sequence ok ok.\n\n For some problems, like part-of-speech tagging or parsing as we will see in\n Chapter 17 or Chapter 18, we can use dynamic programming search (the Viterbi\n 12.4 ‚Ä¢ D ECODING IN MT: B EAM S EARCH 13\n\n algorithm) to address this problem. Unfortunately, dynamic programming is not applicable to generation problems with long-distance dependencies between the output\n decisions. The only method guaranteed to find the best solution is exhaustive search:\n computing the probability of every one of the V T possible sentences (for some length\n value T ) which is obviously too slow.\n\n The solution: beam search\n beam search Instead, MT systems generally decode using beam search, a heuristic search method\n first proposed by Lowerre (1976). In beam search, instead of choosing the best token\n to generate at each timestep, we keep k possible tokens at each step. This fixed-size\n beam width memory footprint k is called the beam width, on the metaphor of a flashlight beam\n that can be parameterized to be wider or narrower.\n Thus at the first step of decoding, we compute a softmax over the entire vocabulary, assigning a probability to each word. We then select the k-best options from\n this softmax output. These initial k outputs are the search frontier and these k initial\n words are called hypotheses. A hypothesis is an output sequence, a translation-sofar, together with its probability.\n\n arrived y2\n\n the green y3\n\n hd1 hd2 y2 y3\n y1\n a a\n y1 hd 1 hd2 hd 2\n BOS arrived ‚Ä¶ ‚Ä¶\n aardvark BOS the green mage\n a .. ..\n hd1\n ‚Ä¶ the the\n aardvark .. ..\n witch witch\n BOS .. ‚Ä¶ ‚Ä¶\n start arrived zebra zebra\n ..\n the\n y2 y3\n ‚Ä¶\n zebra a arrived\n ‚Ä¶ ‚Ä¶\n aardvark aardvark\n the y2 .. ..\n green green\n .. ..\n witch who\n hd1 hd2 ‚Ä¶ y3 ‚Ä¶\n the witch\n zebra zebra\n BOS the\n hd1 hd2 hd2\n\n t1 t2 BOS the witch t3\n\nhypotheses, form the V possible extensions of each, score those k √ó V hypotheses and choose the best k = 2\nto continue. At time 1, the frontier has the best 2 options from the initial decoder state: arrived and the. We\nextend each, compute the probability of all the hypotheses so far (arrived the, arrived aardvark, the green, the\nwitch) and again chose the best 2 (the green and the witch) to be the search frontier. The images on the arcs\nschematically represent the decoders that must be run at each step to score the next words (for simplicity not\ndepicting cross-attention).\n\n At subsequent steps, each of the k best hypotheses is extended incrementally\n14 C HAPTER 12 ‚Ä¢ M ACHINE T RANSLATION\n\n by being passed to distinct decoders, which each generate a softmax over the entire\n vocabulary to extend the hypothesis to every possible next token. Each of these k √óV\n hypotheses is scored by P(yi |x, y<i ): the product of the probability of the current\n word choice multiplied by the probability of the path that led to it. We then prune\n the k √óV hypotheses down to the k best hypotheses, so there are never more than k\n hypotheses at the frontier of the search, and never more than k decoders. Fig. 12.8\n illustrates this with a beam width of 2 for the beginning of The green witch arrived.\n This process continues until an EOS is generated indicating that a complete candidate output has been found. At this point, the completed hypothesis is removed\n from the frontier and the size of the beam is reduced by one. The search continues\n until the beam has been reduced to 0. The result will be k hypotheses.\n To score each node by its log probability, we use the chain rule of probability to\n break down p(y|x) into the product of the probability of each word given its prior\n context, which we can turn into a sum of logs (for an output string of length t):\n\n score(y) = log P(y|x)\n = log (P(y1 |x)P(y2 |y1 , x)P(y3 |y1 , y2 , x)...P(yt |y1 , ..., yt‚àí1 , x))\n X t\n = log P(yi |y1 , ..., yi‚àí1 , x) (12.15)\n i=1\n\n Thus at each step, to compute the probability of a partial sentence, we simply add the\n log probability of the prefix sentence so far to the log probability of generating the\n next token. Fig. 12.9 shows the scoring for the example sentence shown in Fig. 12.8,\n using some simple made-up probabilities. Log probabilities are negative or 0, and\n the max of two log probabilities is the one that is greater (closer to 0).\n\n log P (arrived the|x) log P (‚Äúthe green witch arrived‚Äù|x)\n = -2.3 = log P (the|x) + log P(green|the,x)\n + log P(witch | the, green,x)\n the +logP(arrived|the,green,witch,x)\n +log P(EOS|the,green,witch,arrived,x)\n log P(arrived|x) -2.7\n -.69 log P(arrived witch|x) -3.2\n =-1.6 = -3.9 mage -2.5 EOS\n arrived -2.3 witch -2.1 -.22\n arrived\n -2.3 -4.8\n -1.6 -1.6\n log P(the green|x) -.36 -3.7 at\n BOS = -1.6 came\n log P(the|x)\n -.51 witch -1.6\n -.92 =-.92 green\n -.69\n the -2.7\n log P(the witch|x)\n -2.2 EOS\n -1.2 = -2.1 -.51\n witch -.11 arrived\n -1.61 -3.8\n -2.3\n -4.4 by\n who\n log P(y1|x) log P(y2|y1,x) log P(y3|y2,y1,x) log P(y4|y3,y2,y1,x) log P(y5|y4,y3,y2,y1,x)\n y1 y2 y3 y4 y5\n\nof each hypothesis in the beam by incrementally adding the logprob of generating each next token. Only the top\nk paths are extended to the next step.\n\n Fig. 12.10 gives the algorithm. One problem with this version of the algorithm is\n that the completed hypotheses may have different lengths. Because language mod-\n12.4 ‚Ä¢ D ECODING IN MT: B EAM S EARCH 15\n\n function B EAM D ECODE(c, beam width) returns best paths\n\n y0 , h0 ‚Üê 0\n path ‚Üê ()\n complete paths ‚Üê ()\n state ‚Üê (c, y0 , h0 , path) ;initial state\n frontier ‚Üê hstatei ;initial frontier\n\n while frontier contains incomplete paths and beamwidth > 0\n extended frontier ‚Üê hi\n for each state ‚àà frontier do\n y ‚Üê D ECODE(state)\n for each word i ‚àà Vocabulary do\n successor ‚Üê N EW S TATE(state, i, yi )\n extended frontier ‚Üê A DD T O B EAM(successor, extended frontier,\n beam width)\n\n for each state in extended frontier do\n if state is complete do\n complete paths ‚Üê A PPEND(complete paths, state)\n extended frontier ‚Üê R EMOVE(extended frontier, state)\n beam width ‚Üê beam width - 1\n frontier ‚Üê extended frontier\n\n return completed paths\n\n function N EW S TATE(state, word, word prob) returns new state\n\n function A DD T O B EAM(state, frontier, width) returns updated frontier\n\n if L ENGTH(frontier) < width then\n frontier ‚Üê I NSERT(state, frontier)\n else if S CORE(state) > S CORE(W ORST O F(frontier))\n frontier ‚Üê R EMOVE(W ORST O F(frontier))\n frontier ‚Üê I NSERT(state, frontier)\n return frontier\n\n els generally assign lower probabilities to longer strings, a naive algorithm would\n choose shorter strings for y. (This is not an issue during the earlier steps of decoding; since beam search is breadth-first, all the hypotheses being compared had the\n same length.) For this reason we often apply length normalization methods, like\n dividing the logprob by the number of words:\n t\n 1 1X\n score(y) = log P(y|x) = log P(yi |y1 , ..., yi‚àí1 , x) (12.16)\n t t\n i=1\n\n For MT we generally use beam widths k between 5 and 10, giving us k hypotheses at\n the end. We can pass all k to the downstream application with their respective scores,\n or if we just need a single translation we can pass the most probable hypothesis.\n\n 12.4.1 Minimum Bayes Risk Decoding\nminimum\nBayes risk Minimum Bayes risk or MBR decoding is an alternative decoding algorithm that\n MBR\n16 C HAPTER 12 ‚Ä¢ M ACHINE T RANSLATION\n\n can work even better than beam search and also tends to be better than the other\n decoding algorithms like temperature sampling introduced in Section ??.\n The intuition of minimum Bayes risk is that instead of trying to choose the translation which is most probable, we choose the one that is likely to have the least error.\n For example, we might want our decoding algorithm to find the translation which\n has the highest score on some evaluation metric. For example in Section 12.6 we will\n introduce metrics like chrF or BERTScore that measure the goodness-of-fit between\n a candidate translation and a set of reference human translations. A translation that\n maximizes this score, especially with a hypothetically huge set of perfect human\n translations is likely to be a good one (have minimum risk) even if it is not the most\n probable translation by our particular probability estimator.\n In practice, we don‚Äôt know the perfect set of translations for a given sentence. So\n the standard simplification used in MBR decoding algorithms is to instead choose\n the candidate translation which is most similar (by some measure of goodness-of-fit)\n with some set of candidate translations. We‚Äôre essentially approximating the enormous space of all possible translations U with a smaller set of possible candidate\n translations Y .\n Given this set of possible candidate translations Y , and some similarity or alignment function util, we choose the best translation yÃÇ as the translation which is most\n similar to all the other candidate translations:\n X\n yÃÇ = argmax util(y, c) (12.17)\n y‚ààY c‚ààY\n\n Various util functions can be used, like chrF or BERTscore or BLEU. We can get the\n set of candidate translations by sampling using one of the basic sampling algorithms\n of Section ?? like temperature sampling; good results can be obtained with as few\n as 32 or 64 candidates.\n Minimum Bayes risk decoding can also be used for other NLP tasks; indeed\n it was widely applied to speech recognition (Stolcke et al., 1997; Goel and Byrne,\n 2000) before being applied to machine translation (Kumar and Byrne, 2004), and\n has been shown to work well across many other generation tasks as well (e.g., summarization, dialogue, and image captioning (Suzgun et al., 2023)).\n\n For some languages, and especially for English, online resources are widely available. There are many large parallel corpora that contain translations between English and many languages. But the vast majority of the world‚Äôs languages do not\n have large parallel training texts available. An important ongoing research question\n is how to get good translation with lesser resourced languages. The resource problem can even be true for high resource languages when we need to translate into low\n resource domains (for example in a particular genre that happens to have very little\n bitext).\n Here we briefly introduce two commonly used approaches for dealing with this\n data sparsity: backtranslation, which is a special case of the general statistical\n technique called data augmentation, and multilingual models, and also discuss\n some socio-technical issues.\n 12.5 ‚Ä¢ T RANSLATING IN LOW- RESOURCE SITUATIONS 17\n\n 12.5.1 Data Augmentation\n Data augmentation is a statistical technique for dealing with insufficient training\n data, by adding new synthetic data that is generated from the current natural data.\n The most common data augmentation technique for machine translation is called\nbacktranslation backtranslation. Backtranslation relies on the intuition that while parallel corpora\n may be limited for particular languages or domains, we can often find a large (or\n at least larger) monolingual corpus, to add to the smaller parallel corpora that are\n available. The algorithm makes use of monolingual corpora in the target language\n by creating synthetic bitexts.\n In backtranslation, our goal is to improve source-to-target MT, given a small\n parallel text (a bitext) in the source/target languages, and some monolingual data in\n the target language. We first use the bitext to train a MT system in the reverse direction: a target-to-source MT system . We then use it to translate the monolingual\n target data to the source language. Now we can add this synthetic bitext (natural\n target sentences, aligned with MT-produced source sentences) to our training data,\n and retrain our source-to-target MT model. For example suppose we want to translate from Navajo to English but only have a small Navajo-English bitext, although of\n course we can find lots of monolingual English data. We use the small bitext to build\n an MT engine going the other way (from English to Navajo). Once we translate the\n monolingual English text to Navajo, we can add this synthetic Navajo/English bitext\n to our training data.\n Backtranslation has various parameters. One is how we generate the backtranslated data; we can run the decoder in greedy inference, or use beam search. Or\n we can do sampling, like the temperature sampling algorithm we saw in Chapter 8.\n Another parameter is the ratio of backtranslated data to natural bitext data; we can\n choose to upsample the bitext data (include multiple copies of each sentence). In\n general backtranslation works surprisingly well; one estimate suggests that a system\n trained on backtranslated text gets about 2/3 of the gain as would training on the\n same amount of natural bitext (Edunov et al., 2018).\n\n 12.5.2 Multilingual models\n The models we‚Äôve described so far are for bilingual translation: one source language,\n one target language. It‚Äôs also possible to build a multilingual translator.\n In a multilingual translator, we train the system by giving it parallel sentences\n in many different pairs of languages. That means we need to tell the system which\n language to translate from and to! We tell the system which language is which\n by adding a special token ls to the encoder specifying the source language we‚Äôre\n translating from, and a special token lt to the decoder telling it the target language\n we‚Äôd like to translate into.\n Thus we slightly update Eq. 12.9 above to add these tokens in Eq. 12.19:\n\n h = encoder(x, ls ) (12.18)\n yi+1 = decoder(h, lt , y1 , . . . , yi ) ‚àÄi ‚àà [1, . . . , m] (12.19)\n\n One advantage of a multilingual model is that they can improve the translation\n of lower-resourced languages by drawing on information from a similar language\n in the training data that happens to have more resources. Perhaps we don‚Äôt know\n the meaning of a word in Galician, but the word appears in the similar and higherresourced language Spanish.\n18 C HAPTER 12 ‚Ä¢ M ACHINE T RANSLATION\n\n 12.5.3 Sociotechnical issues\n Many issues in dealing with low-resource languages go beyond the purely technical. One problem is that for low-resource languages, especially from low-income\n countries, native speakers are often not involved as the curators for content selection, as the language technologists, or as the evaluators who measure performance\n (‚àÄ et al., 2020). Indeed, one well-known study that manually audited a large set of\n parallel corpora and other major multilingual datasets found that for many of the\n corpora, less than 50% of the sentences were of acceptable quality, with a lot of\n data consisting of repeated sentences with web boilerplate or incorrect translations,\n suggesting that native speakers may not have been sufficiently involved in the data\n process (Kreutzer et al., 2022).\n Other issues, like the tendency of many MT approaches to focus on the case\n where one of the languages is English (Anastasopoulos and Neubig, 2020), have to\n do with allocation of resources. Where most large multilingual systems were trained\n on bitexts in which English was one of the two languages, recent huge corporate\n systems like those of Fan et al. (2021) and Costa-jussaÃÄ et al. (2022) and datasets\n like Schwenk et al. (2021) attempt to handle large numbers of languages (up to 200\n languages) and create bitexts between many more pairs of languages and not just\n through English.\n At the smaller end, ‚àÄ et al. (2020) propose a participatory design process to\n encourage content creators, curators, and language technologists who speak these\n low-resourced languages to participate in developing MT algorithms. They provide\n online groups, mentoring, and infrastructure, and report on a case study on developing MT algorithms for low-resource African languages. Among their conclusions\n was to perform MT evaluation by post-editing rather than direct evaluation, since\n having labelers edit an MT system and then measure the distance between the MT\n output and its post-edited version both was simpler to train evaluators and makes it\n easier to measure true errors in the MT output and not differences due to linguistic\n variation (Bentivogli et al., 2018).\n\n Translations are evaluated along two dimensions:\n adequacy 1. adequacy: how well the translation captures the exact meaning of the source\n sentence. Sometimes called faithfulness or fidelity.\n fluency 2. fluency: how fluent the translation is in the target language (is it grammatical,\n clear, readable, natural).\n Using humans to evaluate is most accurate, but automatic metrics are also used for\n convenience.\n\n 12.6.1 Using Human Raters to Evaluate MT\n The most accurate evaluations use human raters, such as online crowdworkers, to\n evaluate each translation along the two dimensions. For example, along the dimension of fluency, we can ask how intelligible, how clear, how readable, or how natural\n the MT output (the target text) is. We can give the raters a scale, for example, from\n 1 (totally unintelligible) to 5 (totally intelligible), or 1 to 100, and ask them to rate\n each sentence or paragraph of the MT output.\n 12.6 ‚Ä¢ MT E VALUATION 19\n\n We can do the same thing to judge the second dimension, adequacy, using raters\n to assign scores on a scale. If we have bilingual raters, we can give them the source\n sentence and a proposed target sentence, and rate, on a 5-point or 100-point scale,\n how much of the information in the source was preserved in the target. If we only\n have monolingual raters but we have a good human translation of the source text, we\n can give the monolingual raters the human reference translation and a target machine\n translation and again rate how much information is preserved. An alternative is to\nranking do ranking: give the raters a pair of candidate translations, and ask them which one\n they prefer.\n Training of human raters (who are often online crowdworkers) is essential; raters\n without translation expertise find it difficult to separate fluency and adequacy, and\n so training includes examples carefully distinguishing these. Raters often disagree\n (source sentences may be ambiguous, raters will have different world knowledge,\n raters may apply scales differently). It is therefore common to remove outlier raters,\n and (if we use a fine-grained enough scale) normalizing raters by subtracting the\n mean from their scores and dividing by the variance.\n As discussed above, an alternative way of using human raters is to have them\n post-edit translations, taking the MT output and changing it minimally until they\n feel it represents a correct translation. The difference between their post-edited\n translations and the original MT output can then be used as a measure of quality.\n\n 12.6.2 Automatic Evaluation\n While humans produce the best evaluations of machine translation output, running a\n human evaluation can be time consuming and expensive. For this reason automatic\n metrics are often used as temporary proxies. Automatic metrics are less accurate\n than human evaluation, but can help test potential system improvements, and even\n be used as an automatic loss function for training. In this section we introduce two\n families of such metrics, those based on character- or word-overlap and those based\n on embedding similarity.\n\n Automatic Evaluation by Character Overlap: chrF\n chrF The simplest and most robust metric for MT evaluation is called chrF, which stands\n for character F-score (PopovicÃÅ, 2015). chrF (along with many other earlier related\n metrics like BLEU, METEOR, TER, and others) is based on a simple intuition derived from the pioneering work of Miller and Beebe-Center (1956): a good machine\n translation will tend to contain characters and words that occur in a human translation of the same sentence. Consider a test set from a parallel corpus, in which\n each source sentence has both a gold human target translation and a candidate MT\n translation we‚Äôd like to evaluate. The chrF metric ranks each MT target sentence by\n a function of the number of character n-gram overlaps with the human translation.\n Given the hypothesis and the reference, chrF is given a parameter k indicating\n the length of character n-grams to be considered, and computes the average of the\n k precisions (unigram precision, bigram, and so on) and the average of the k recalls\n (unigram recall, bigram recall, etc.):\n chrP percentage of character 1-grams, 2-grams, ..., k-grams in the hypothesis that\n occur in the reference, averaged.\n chrR percentage of character 1-grams, 2-grams,..., k-grams in the reference that\n occur in the hypothesis, averaged.\n The metric then computes an F-score by combining chrP and chrR using a weighting\n20 C HAPTER 12 ‚Ä¢ M ACHINE T RANSLATION\n\n parameter Œ≤ . It is common to set Œ≤ = 2, thus weighing recall twice as much as\n precision:\n chrP ¬∑ chrR\n chrFŒ≤ = (1 + Œ≤ 2 ) (12.20)\n Œ≤ 2 ¬∑ chrP + chrR\n For Œ≤ = 2, that would be:\n 5 ¬∑ chrP ¬∑ chrR\n chrF2 =\n 4 ¬∑ chrP + chrR\n For example, consider two hypotheses that we‚Äôd like to score against the reference translation witness for the past. Here are the hypotheses along with chrF values\n computed using parameters k = Œ≤ = 2 (in real examples, k would be a higher number\n like 6):\n REF: witness for the past,\n HYP1: witness of the past, chrF2,2 = .86\n HYP2: past witness chrF2,2 = .62\n Let‚Äôs see how we computed that chrF value for HYP1 (we‚Äôll leave the computation of the chrF value for HYP2 as an exercise for the reader). First, chrF ignores\n spaces, so we‚Äôll remove them from both the reference and hypothesis:\n REF: witnessforthepast, (18 unigrams, 17 bigrams)\n HYP1: witnessofthepast, (17 unigrams, 16 bigrams)\n Next let‚Äôs see how many unigrams and bigrams match between the reference and\n hypothesis:\n unigrams that match: w i t n e s s f o t h e p a s t , (17 unigrams)\n bigrams that match: wi it tn ne es ss th he ep pa as st t, (13 bigrams)\n\n We use that to compute the unigram and bigram precisions and recalls:\n unigram P: 17/17 = 1 unigram R: 17/18 = .944\n bigram P: 13/16 = .813 bigram R: 13/17 = .765\n Finally we average to get chrP and chrR, and compute the F-score:\n\n chrP = (17/17 + 13/16)/2 = .906\n chrR = (17/18 + 13/17)/2 = .855\n chrP ‚àó chrR\n chrF2,2 = 5 = .86\n 4chrP + chrR\n chrF is simple, robust, and correlates very well with human judgments in many\n languages (Kocmi et al., 2021).\n\n Alternative overlap metric: BLEU\n There are various alternative overlap metrics. For example, before the development\n of chrF, it was common to use a word-based overlap metric called BLEU (for BiLingual Evaluation Understudy), that is purely precision-based rather than combining\n precision and recall (Papineni et al., 2002). The BLEU score for a corpus of candidate translation sentences is a function of the n-gram word precision over all the\n sentences combined with a brevity penalty computed over the corpus as a whole.\n What do we mean by n-gram precision? Consider a corpus composed of a single\n sentence. The unigram precision for this corpus is the percentage of unigram tokens\n 12.6 ‚Ä¢ MT E VALUATION 21\n\nin the candidate translation that also occur in the reference translation, and ditto for\nbigrams and so on, up to 4-grams. BLEU extends this unigram metric to the whole\ncorpus by computing the numerator as the sum over all sentences of the counts of all\nthe unigram types that also occur in the reference translation, and the denominator\nis the total of the counts of all unigrams in all candidate sentences. We compute\nthis n-gram precision for unigrams, bigrams, trigrams, and 4-grams and take the\ngeometric mean. BLEU has many further complications, including a brevity penalty\nfor penalizing candidate translations that are too short, and it also requires the ngram counts be clipped in a particular way.\n Because BLEU is a word-based metric, it is very sensitive to word tokenization,\nmaking it impossible to compare different systems if they rely on different tokenization standards, and doesn‚Äôt work as well in languages with complex morphology.\nNonetheless, you will sometimes still see systems evaluated by BLEU, particularly\nfor translation into English. In such cases it‚Äôs important to use packages that enforce\nstandardization for tokenization like S ACRE BLEU (Post, 2018).\n\nStatistical Significance Testing for MT evals\nCharacter or word overlap-based metrics like chrF (or BLEU, or etc.) are mainly\nused to compare two systems, with the goal of answering questions like: did the\nnew algorithm we just invented improve our MT system? To know if the difference\nbetween the chrF scores of two MT systems is a significant difference, we use the\npaired bootstrap test, or the similar randomization test.\n To get a confidence interval on a single chrF score using the bootstrap test, recall\nfrom Section ?? that we take our test set (or devset) and create thousands of pseudotestsets by repeatedly sampling with replacement from the original test set. We now\ncompute the chrF score of each of the pseudo-testsets. If we drop the top 2.5% and\nbottom 2.5% of the scores, the remaining scores will give us the 95% confidence\ninterval for the chrF score of our system.\n To compare two MT systems A and B, we draw the same set of pseudo-testsets,\nand compute the chrF scores for each of them. We then compute the percentage of\npseudo-test-sets in which A has a higher chrF score than B.\n\nchrF: Limitations\nWhile automatic character and word-overlap metrics like chrF or BLEU are useful,\nthey have important limitations. chrF is very local: a large phrase that is moved\naround might barely change the chrF score at all, and chrF can‚Äôt evaluate crosssentence properties of a document like its discourse coherence (Chapter 24). chrF\nand similar automatic metrics also do poorly at comparing very different kinds of\nsystems, such as comparing human-aided translation against machine translation, or\ndifferent machine translation architectures against each other (Callison-Burch et al.,\n2006). Instead, automatic overlap metrics like chrF are most appropriate when evaluating changes to a single system.\n\n12.6.3 Automatic Evaluation: Embedding-Based Methods\nThe chrF metric is based on measuring the exact character n-grams a human reference and candidate machine translation have in common. However, this criterion\nis overly strict, since a good translation may use alternate words or paraphrases. A\nsolution first pioneered in early metrics like METEOR (Banerjee and Lavie, 2005)\nwas to allow synonyms to match between the reference x and candidate xÃÉ. More\n22 C HAPTER 12 ‚Ä¢ M ACHINE T RANSLATION\n\n recent metrics use BERT or other embeddings to implement this intuition.\n For example, in some situations we might have datasets that have human assessments of translation quality. Such datasets consists of tuples (x, xÃÉ, r), where\n x = (x1 , . . . , xn ) is a reference translation, xÃÉ = (xÃÉ1 , . . . , xÃÉm ) is a candidate machine\n translation, and r ‚àà R is a human rating that expresses the quality of xÃÉ with respect\n to x. Given such data, algorithms like COMET (Rei et al., 2020) BLEURT (Sellam\n et al., 2020) train a predictor on the human-labeled datasets, for example by passing\n x and xÃÉ through a version of BERT (trained with extra pretraining, and then finetuned\n on the human-labeled sentences), followed by a linear layer that is trained to predict\n r. The output of such models correlates highly with human labels.\n In other cases, however, we don‚Äôt have such human-labeled datasets. In that\n case we can measure the similarity of x and xÃÉ by the similarity of their embeddings.\n The BERTS CORE algorithm (Zhang et al., 2020) shown in Fig. 12.11, for example,\n passes the reference x and the candidate xÃÉ through BERT, computing a BERT embedding for each token xi and xÃÉ j . Each pair of tokens (xi , xÃÉ j ) is scored by its cosine\n xi ¬∑xÃÉ j\n |xi ||xÃÉ j | . Each token in x is matched to a token in xÃÉ to compute recall, and each token in\n xÃÉ is matched to a token in x to compute precision (with each token greedily matched\n to the most similar token in the corresponding sentence). BERTS CORE provides\n precision and recall (and hence F1 ):\n Published as a conference paper at ICLR 1 X 2020 1 X\n RBERT = max xi ¬∑ xÃÉ j PBERT = max xi ¬∑ xÃÉ j (12.21)\n |x| x ‚ààx xÃÉ j ‚ààxÃÉ |xÃÉ| xÃÉ ‚ààxÃÉ xi ‚ààx\n i j\n\n Contextual Pairwise Cosine Maximum Similarity Importance Weighting\n Embedding Similarity (Optional)\n Reference x\n 1.27\n <latexit sha1_base64=\"f2yzimwbR/Dgjzp6tZ360fHRqNI=\">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cW7Ae0oWy2k3btZhN2N2IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3H1FpHst7M0nQj+hQ8pAzaqzUeOqXK27VnYOsEi8nFchR75e/eoOYpRFKwwTVuuu5ifEzqgxnAqelXqoxoWxMh9i1VNIItZ/ND52SM6sMSBgrW9KQufp7IqOR1pMosJ0RNSO97M3E/7xuasJrP+MySQ1KtlgUpoKYmMy+JgOukBkxsYQyxe2thI2ooszYbEo2BG/55VXSuqh6btVrXFZqN3kcRTiBUzgHD66gBndQhyYwQHiGV3hzHpwX5935WLQWnHzmGP7A+fwB5jmM/A==</latexit>\n\n 7.94\n the weather is\n Reference\n\n 1.82\n cold today\n 7.90\n RBERT = (0.713 1.27)+(0.515 7.94)+...\n 1.27+7.94+1.82+7.90+8.88\n <latexit sha1_base64=\"OJyoKlmBAgUA0KDtUcsH/di5BlI=\">AAACSHicbZDLattAFIaPnLRJ3JvTLrsZYgoJAqFxGqwsCqal0FVJQ5wELCNG41EyZHRh5ijECL1EnqAv002X2eUZsumipXRR6Mj2Ipf+MPDznXM4Z/64UNKg7187raXlR49XVtfaT54+e/6is/7y0OSl5mLIc5Xr45gZoWQmhihRieNCC5bGShzFZx+a+tG50Ebm2QFOCzFO2UkmE8kZWhR1ov2oClFcYPX+4/5BXZN3JEw049Wm7/XpdogyFYZQr9ffci3aoTsL1Pd23265oZrkaOqqaXAb5FIv6DXOdwMvCOqo0/U9fyby0NCF6Q52/15+BYC9qHMVTnJepiJDrpgxI+oXOK6YRsmVqNthaUTB+Bk7ESNrM2aPGVezIGryxpIJSXJtX4ZkRm9PVCw1ZprGtjNleGru1xr4v9qoxCQYVzIrShQZny9KSkUwJ02qZCK14Kim1jCupb2V8FNmc0SbfduGQO9/+aE57HnU9+gX2h18hrlW4TVswCZQ6MMAPsEeDIHDN7iBn/DL+e78cH47f+atLWcx8wruqNX6B8dUrVw=</latexit>\n sha1_base64=\"RInTcZkWiVBnf/ncBstCvatCtG4=\">AAACSHicbZDPShxBEMZ7Nproxugaj14al4AyMEyvyoyHwGIQPImKq8LOMvT09mhjzx+6a0KWYV4iL5EnySXH3HwGLx4U8SDYs7sHo/mg4eNXVVT1F+VSaHDda6vxbmb2/Ye5+ebHhU+LS63lz6c6KxTjPZbJTJ1HVHMpUt4DAZKf54rTJJL8LLr6VtfPvnOlRZaewCjng4RepCIWjIJBYSs8DssA+A8od/eOT6oKf8VBrCgr113HI5sBiIRrTJyOt2EbtE22p8hzdrY27EAOM9BVWTfYNbKJ43dq59q+4/tV2Gq7jjsWfmvI1LS7O08/f3nLi4dh628wzFiR8BSYpFr3iZvDoKQKBJO8agaF5jllV/SC941NqTlmUI6DqPAXQ4Y4zpR5KeAxfTlR0kTrURKZzoTCpX5dq+H/av0CYn9QijQvgKdssiguJIYM16nioVCcgRwZQ5kS5lbMLqnJEUz2TRMCef3lt+a04xDXIUek3T1AE82hVbSG1hFBHuqifXSIeoih3+gG3aF76491az1Yj5PWhjWdWUH/qNF4BkPYrbk=</latexit>\n sha1_base64=\"fGWl4NCvlvtMu17rjLtk25oWpdc=\">AAACSHicbZBLS+RAFIUrPT7bVzsu3RQ2ghIIqVbpuBgQRZiVqNgqdJpQqa5oYeVB1Y1ME/Lz3Lic3fwGNy6UwZ2VNgtfBwoO372Xe+uEmRQaXPef1fgxMTk1PTPbnJtfWFxqLf8812muGO+xVKbqMqSaS5HwHgiQ/DJTnMah5BfhzUFVv7jlSos0OYNRxgcxvUpEJBgFg4JWcBoUPvA/UOwfnp6VJf6F/UhRVmy4Tpds+SBirjFxOt1N26AdslOjrrO7vWn7cpiCLouqwa6QTRyvUznX9hzPK4NW23XcsfBXQ2rTRrWOg9Zff5iyPOYJMEm17hM3g0FBFQgmedn0c80zym7oFe8bm1BzzKAYB1HidUOGOEqVeQngMX0/UdBY61Ecms6YwrX+XKvgd7V+DpE3KESS5cAT9rYoyiWGFFep4qFQnIEcGUOZEuZWzK6pyRFM9k0TAvn85a/mvOMQ1yEnpL13VMcxg1bRGtpABHXRHvqNjlEPMXSHHtATerburUfrv/Xy1tqw6pkV9EGNxisxMKq0</latexit>\n\n Candidate xÃÇ <latexit sha1_base64=\"5QTnVRVSrnyzznVU7d5bF5u03Iw=\">AAAB7nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDbbTbt0swm7E7GE/ggvHhTx6u/x5r9x0+agrQ8GHu/NMDMvSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHJtRKwecJpwP6IjJULBKFqp0x9TzJ5mg2rNrbtzkFXiFaQGBZqD6ld/GLM04gqZpMb0PDdBP6MaBZN8VumnhieUTeiI9yxVNOLGz+bnzsiZVYYkjLUthWSu/p7IaGTMNApsZ0RxbJa9XPzP66UYXvuZUEmKXLHFojCVBGOS/06GQnOGcmoJZVrYWwkbU00Z2oQqNgRv+eVV0r6oe27du7+sNW6KOMpwAqdwDh5cQQPuoAktYDCBZ3iFNydxXpx352PRWnKKmWP4A+fzB7A8j8k=</latexit>\n 8.88\n\n it is freezing today idf\n weights\n\n Candidate\n\n Figure12.11\nFigure 1: Illustration of the computation\n The computation of the\n of BERTS CORE recall\n recall metric\n from RBERT\n reference . Given\n x and the xÃÇ,\n candidate reference x and\n from Figure 1 in\n candidate xÃÇ, we compute BERT embeddings and pairwise cosine similarity. We highlight the greedy\nZhang et al. (2020). This version shows an extended version of the metric in which tokens are also weighted by\n matching\ntheir in red, and include the optional idf importance weighting.\n idf values.\n\n We experiment with different models (Section 4), using the tokenizer provided with each model.\n Given a tokenized Ethical\n reference sentence xIssues\n = hx , . . . , x i, the embedding model generates a se- 1 k\n quence of vectors hx1 , . . . , xk i. Similarly, the tokenized candidate xÃÇ = hxÃÇ1 , . . . , xÃÇm i is mapped\n to hxÃÇ1 , . . . , xÃÇl i. The main model we use is BERT, which tokenizes the input text into a sequence\n of word pieces Machine (Wu et al.,translation raises\n 2016), where many ofwords\n unknown the same ethical\n are split intoissues\n severalthat we‚Äôve discussed\n commonly observed in\n sequences of characters. The representation\n earlier chapters. For example, for consider\n each wordMT piece is computed\n systems with from\n translating a Transformer\n Hungarian\n encoder (Vaswani et al.,has\n (which 2017) by repeatedly\n the gender neutralapplying\n pronounself-attention\n oÃã) or Spanishand nonlinear\n (which oftentransformations\n drops pronouns)\n in an alternatinginto fashion.\n English BERT embeddings\n (in which pronounshaveare\n been shown toand\n obligatory, benefit\n theyvarious NLP tasks (Devlin\n have grammatical gender).\n et al., 2019; Liu, 2019; Huang et al., 2019; Yang et al., 2019a).\n When translating a reference to a person described without specified gender, MT\n Similarity Measure systemsThe often default\n vector to male gender\n representation allows (Schiebinger 2014, Prates\n for a soft measure et al. 2019).\n of similarity instead And\n of\n exact-string (Papineni MT systemset al.,often\n 2002)assign\n or heuristic\n gender (Banerjee\n according & to Lavie,\n culture2005) matching.\n stereotypes of theThe\n sortcosine\n we saw\n ??. Fig. x>\n similarity of a in Sectiontoken\n reference 12.12\n xi and shows examples\n a candidate token xÃÇfrom Prates\n j is kx\n i xÃÇj et al. (2019), in which Huni kkxÃÇj k\n . We use pre-normalized\n garian gender-neutral oÃã is a nurse is translated> with she, but gender-neutral oÃã is a\n vectors, which reduces this calculation to the inner product xi xÃÇj . While this measure considers\n tokens in isolation, CEOthe is contextual\n translated with he. Prates\n embeddings et al.information\n contain (2019) find from\n that these stereotypes\n the rest can‚Äôt comof the sentence.\n pletely be accounted for by gender bias in US labor statistics, because the biases are\n BERTS CORE The complete score matches each token in x to a token in xÃÇ to compute recall,\n and each token in xÃÇ to a token in x to compute precision. We use greedy matching to maximize\n the matching similarity score,2 where each token is matched to the most similar token in the other\n sentence. We combine precision and recall to compute an F1 measure. For a reference x and\n candidate xÃÇ, the recall, precision, and F1 scores are:\n 1 X > 1 X > PBERT ¬∑ RBERT\n 12.8 ‚Ä¢ S UMMARY 23\n\n amplified by MT systems, with pronouns being mapped to male or female gender\n with a probability higher than if the mapping was based on actual labor employment\n statistics.\n\n Hungarian (gender neutral) source English MT output\n oÃã egy aÃÅpoloÃÅ she is a nurse\n oÃã egy tudoÃÅs he is a scientist\n oÃã egy meÃÅrnoÃàk he is an engineer\n oÃã egy peÃÅk he is a baker\n oÃã egy tanaÃÅr she is a teacher\n oÃã egy eskuÃàvoÃãszervezoÃã she is a wedding organizer\n oÃã egy vezeÃÅrigazgatoÃÅ he is a CEO\n current MT systems interpret people from traditionally male-dominated occupations as male,\n and traditionally female-dominated occupations as female (Prates et al., 2019).\n\n Similarly, a recent challenge set, the WinoMT dataset (Stanovsky et al., 2019)\n shows that MT systems perform worse when they are asked to translate sentences\n that describe people with non-stereotypical gender roles, like ‚ÄúThe doctor asked the\n nurse to help her in the operation‚Äù.\n Many ethical questions in MT require further research. One open problem is\n developing metrics for knowing what our systems don‚Äôt know. This is because MT\n systems can be used in urgent situations where human translators may be unavailable\n or delayed: in medical domains, to help translate when patients and doctors don‚Äôt\n speak the same language, or in legal domains, to help judges or lawyers communicate with witnesses or defendants. In order to ‚Äòdo no harm‚Äô, systems need ways to\n confidence assign confidence values to candidate translations, so they can abstain from giving\n incorrect translations that may cause harm.\n\n Machine translation is one of the most widely used applications of NLP, and the\n encoder-decoder model, first developed for MT is a key tool that has applications\n throughout NLP.\n ‚Ä¢ Languages have divergences, both structural and lexical, that make translation\n difficult.\n ‚Ä¢ The linguistic field of typology investigates some of these differences; languages can be classified by their position along typological dimensions like\n whether verbs precede their objects.\n ‚Ä¢ Encoder-decoder networks (for transformers just as we saw in Chapter 13\n for RNNs) are composed of an encoder network that takes an input sequence\n and creates a contextualized representation of it, the context. This context\n representation is then passed to a decoder which generates a task-specific\n output sequence.\n ‚Ä¢ Cross-attention allows the transformer decoder to view information from all\n the hidden states of the encoder.\n ‚Ä¢ Machine translation models are trained on a parallel corpus, sometimes called\n a bitext, a text that appears in two (or more) languages.\n24 C HAPTER 12 ‚Ä¢ M ACHINE T RANSLATION\n\n ‚Ä¢ Backtranslation is a way of making use of monolingual corpora in the target\n language by running a pilot MT engine backwards to create synthetic bitexts.\n ‚Ä¢ MT is evaluated by measuring a translation‚Äôs adequacy (how well it captures\n the meaning of the source sentence) and fluency (how fluent or natural it is\n in the target language). Human evaluation is the gold standard, but automatic\n evaluation metrics like chrF, which measure character n-gram overlap with\n human translations, or more recent metrics based on embedding similarity,\n are also commonly used.\n\nHistorical Notes\n MT was proposed seriously by the late 1940s, soon after the birth of the computer\n (Weaver, 1949/1955). In 1954, the first public demonstration of an MT system prototype (Dostert, 1955) led to great excitement in the press (Hutchins, 1997). The\n next decade saw a great flowering of ideas, prefiguring most subsequent developments. But this work was ahead of its time‚Äîimplementations were limited by, for\n example, the fact that pending the development of disks there was no good way to\n store dictionary information.\n As high-quality MT proved elusive (Bar-Hillel, 1960), there grew a consensus\n on the need for better evaluation and more basic research in the new fields of formal and computational linguistics. This consensus culminated in the famously critical ALPAC (Automatic Language Processing Advisory Committee) report of 1966\n (Pierce et al., 1966) that led in the mid 1960s to a dramatic cut in funding for MT\n in the US. As MT research lost academic respectability, the Association for Machine Translation and Computational Linguistics dropped MT from its name. Some\n MT developers, however, persevered, and there were early MT systems like MeÃÅteÃÅo,\n which translated weather forecasts from English to French (Chandioux, 1976), and\n industrial systems like Systran.\n In the early years, the space of MT architectures spanned three general models. In direct translation, the system proceeds word-by-word through the sourcelanguage text, translating each word incrementally. Direct translation uses a large\n bilingual dictionary, each of whose entries is a small program with the job of translating one word. In transfer approaches, we first parse the input text and then apply rules to transform the source-language parse into a target language parse. We\n then generate the target language sentence from the parse tree. In interlingua approaches, we analyze the source language text into some abstract meaning representation, called an interlingua. We then generate into the target language from\n this interlingual representation. A common way to visualize these three early ap-\nVauquois\n triangle proaches was the Vauquois triangle shown in Fig. 12.13. The triangle shows the\n increasing depth of analysis required (on both the analysis and generation end) as\n we move from the direct approach through transfer approaches to interlingual approaches. In addition, it shows the decreasing amount of transfer knowledge needed\n as we move up the triangle, from huge amounts of transfer at the direct level (almost all knowledge is transfer knowledge for each word) through transfer (transfer\n rules only for parse trees or thematic roles) through interlingua (no specific transfer\n knowledge). We can view the encoder-decoder network as an interlingual approach,\n with attention acting as an integration of direct and transfer, allowing words or their\n representations to be directly accessed by the decoder.\n H ISTORICAL N OTES 25\n\n Interlingua\n\n sis age\n\n ta gen\n aly gu\n\n rg\n an la n\n\n et era\n Source Text:\n Target Text:\n\n lan tion\n ce\n Semantic/Syntactic\n Transfer Semantic/Syntactic\n\n ur\n\n gu\n Structure\n\n so\n\n ag\n Structure\n\n e\n source Direct Translation target\n text text\n\n Statistical methods began to be applied around 1990, enabled first by the development of large bilingual corpora like the Hansard corpus of the proceedings of the\n Canadian Parliament, which are kept in both French and English, and then by the\n growth of the web. Early on, a number of researchers showed that it was possible\n to extract pairs of aligned sentences from bilingual corpora, using words or simple\n cues like sentence length (Kay and RoÃàscheisen 1988, Gale and Church 1991, Gale\n and Church 1993, Kay and RoÃàscheisen 1993).\n At the same time, the IBM group, drawing directly on the noisy channel model\nstatistical MT for speech recognition, proposed two related paradigms for statistical MT. These\n IBM Models include the generative algorithms that became known as IBM Models 1 through\n Candide 5, implemented in the Candide system. The algorithms (except for the decoder)\n were published in full detail‚Äî encouraged by the US government who had partially funded the work‚Äî which gave them a huge impact on the research community\n (Brown et al. 1990, Brown et al. 1993).\n The group also developed a discriminative approach, called MaxEnt (for maximum entropy, an alternative formulation of logistic regression), which allowed many\n features to be combined discriminatively rather than generatively (Berger et al.,\n 1996), which was further developed by Och and Ney (2002).\n By the turn of the century, most academic research on machine translation used\n statistical MT, either in the generative or discriminative mode. An extended version\nphrase-based of the generative approach, called phrase-based translation was developed, based\n translation\n on inducing translations for phrase-pairs (Och 1998, Marcu and Wong 2002, Koehn\n et al. (2003), Och and Ney 2004, Deng and Byrne 2005, inter alia).\n Once automatic metrics like BLEU were developed (Papineni et al., 2002), the\n discriminative log linear formulation (Och and Ney, 2004), drawing from the IBM\n MaxEnt work (Berger et al., 1996), was used to directly optimize evaluation metrics\n MERT like BLEU in a method known as Minimum Error Rate Training, or MERT (Och,\n 2003), also drawing from speech recognition models (Chou et al., 1993). Toolkits\n Moses like GIZA (Och and Ney, 2003) and Moses (Koehn et al. 2006, Zens and Ney 2007)\n were widely used.\n There were also approaches around the turn of the century that were based on\n transduction\n grammars syntactic structure (Chapter 18). Models based on transduction grammars (also\n called synchronous grammars) assign a parallel syntactic tree structure to a pair\n of sentences in different languages, with the goal of translating the sentences by\n applying reordering operations on the trees. From a generative perspective, we can\n view a transduction grammar as generating pairs of aligned sentences in two laninversion\n guages. Some of the most widely used models included the inversion transduction\n transduction\n grammar\n grammar (Wu, 1996) and synchronous context-free grammars (Chiang, 2005),\n26 C HAPTER 12 ‚Ä¢ M ACHINE T RANSLATION\n\n Neural networks had been applied at various times to various aspects of machine\n translation; for example Schwenk et al. (2006) showed how to use neural language\n models to replace n-gram language models in a Spanish-English system based on\n IBM Model 4. The modern neural encoder-decoder approach was pioneered by\n Kalchbrenner and Blunsom (2013), who used a CNN encoder and an RNN decoder,\n and was first applied to MT by Bahdanau et al. (2015). The transformer encoderdecoder was proposed by Vaswani et al. (2017) (see the History section of Chapter 8).\n Research on evaluation of machine translation began quite early. Miller and\n Beebe-Center (1956) proposed a number of methods drawing on work in psycholinguistics. These included the use of cloze and Shannon tasks to measure intelligibility\n as well as a metric of edit distance from a human translation, the intuition that underlies all modern overlap-based automatic evaluation metrics. The ALPAC report\n included an early evaluation study conducted by John Carroll that was extremely influential (Pierce et al., 1966, Appendix 10). Carroll proposed distinct measures for\n fidelity and intelligibility, and had raters score them subjectively on 9-point scales.\n Much early evaluation work focuses on automatic word-overlap metrics like BLEU\n (Papineni et al., 2002), NIST (Doddington, 2002), TER (Translation Error Rate)\n (Snover et al., 2006), Precision and Recall (Turian et al., 2003), and METEOR\n (Banerjee and Lavie, 2005); character n-gram overlap methods like chrF (PopovicÃÅ,\n 2015) came later. More recent evaluation work, echoing the ALPAC report, has\n emphasized the importance of careful statistical methodology and the use of human\n evaluation (Kocmi et al., 2021; Marie et al., 2021).\n The early history of MT is surveyed in Hutchins 1986 and 1997; Nirenburg et al.\n (2002) collects early readings. See Croft (1990) or Comrie (1989) for introductions\n to linguistic typology.\n\nExercises\n round to .62).\n Exercises 27\n\nAnastasopoulos, A. and G. Neubig. 2020. Should all cross- Costa-jussaÃÄ, M. R., J. Cross, O. CÃßelebi, M. Elbayad,\n lingual embeddings speak English? ACL. K. Heafield, K. Heffernan, E. Kalbassi, J. Lam, D. Licht,\nArtetxe, M. and H. Schwenk. 2019. Massively multilingual J. Maillard, A. Sun, S. Wang, G. Wenzek, A. Youngblood,\n sentence embeddings for zero-shot cross-lingual transfer B. Akula, L. Barrault, G. M. Gonzalez, P. Hansanti,\n and beyond. TACL, 7:597‚Äì610. J. Hoffman, S. Jarrett, K. R. Sadagopan, D. Rowe,\n S. Spruit, C. Tran, P. Andrews, N. F. Ayan, S. Bhosale,\nBahdanau, D., K. H. Cho, and Y. Bengio. 2015. Neural ma- S. Edunov, A. Fan, C. Gao, V. Goswami, F. GuzmaÃÅn,\n chine translation by jointly learning to align and translate. P. Koehn, A. Mourachko, C. Ropers, S. Saleem,\n ICLR 2015. H. Schwenk, J. Wang, and NLLB Team. 2022. No\nBanerjee, S. and A. Lavie. 2005. METEOR: An automatic language left behind: Scaling human-centered machine\n metric for MT evaluation with improved correlation with translation. ArXiv.\n human judgments. Proceedings of ACL Workshop on In- Croft, W. 1990. Typology and Universals. Cambridge Unitrinsic and Extrinsic Evaluation Measures for MT and/or versity Press.\n Summarization.\n Deng, Y. and W. Byrne. 2005. HMM word and phrase align-\nBanÃÉoÃÅn, M., P. Chen, B. Haddow, K. Heafield, H. Hoang, ment for statistical machine translation. HLT-EMNLP.\n M. EsplaÃÄ-Gomis, M. L. Forcada, A. Kamran, F. Kirefu,\n Doddington, G. 2002. Automatic evaluation of machine\n P. Koehn, S. Ortiz Rojas, L. Pla Sempere, G. Ramƒ±ÃÅreztranslation quality using n-gram co-occurrence statistics.\n SaÃÅnchez, E. Sarrƒ±ÃÅas, M. Strelec, B. Thompson, W. Waites,\n HLT.\n D. Wiggins, and J. Zaragoza. 2020. ParaCrawl: Webscale acquisition of parallel corpora. ACL. Dorr, B. 1994. Machine translation divergences: A formal\n description and proposed solution. Computational Lin-\nBar-Hillel, Y. 1960. The present status of automatic transla- guistics, 20(4):597‚Äì633.\n tion of languages. In F. Alt, ed., Advances in Computers\n 1, 91‚Äì163. Academic Press. Dostert, L. 1955. The Georgetown-I.B.M. experiment. In\n Machine Translation of Languages: Fourteen Essays,\nBentivogli, L., M. Cettolo, M. Federico, and C. Federmann. 124‚Äì135. MIT Press.\n 2018. Machine translation human evaluation: an investigation of evaluation based on post-editing and its relation Dryer, M. S. and M. Haspelmath, eds. 2013. The World Atlas\n with direct assessment. ICSLT. of Language Structures Online. Max Planck Institute for\n Evolutionary Anthropology, Leipzig. Available online at\nBerger, A., S. A. Della Pietra, and V. J. Della Pietra. 1996. A http://wals.info.\n maximum entropy approach to natural language process-\nEdunov, S., M. Ott, M. Auli, and D. Grangier. 2018. Undering. Computational Linguistics, 22(1):39‚Äì71.\n standing back-translation at scale. EMNLP.\nBickel, B. 2003. Referential density in discourse and syntac-\nFan, A., S. Bhosale, H. Schwenk, Z. Ma, A. El-Kishky,\n tic typology. Language, 79(2):708‚Äì736.\n S. Goyal, M. Baines, O. Celebi, G. Wenzek, V. Chaud-\nBostrom, K. and G. Durrett. 2020. Byte pair encoding is hary, N. Goyal, T. Birch, V. Liptchinsky, S. Edunov,\n suboptimal for language model pretraining. EMNLP. M. Auli, and A. Joulin. 2021. Beyond english-centric\nBrown, P. F., J. Cocke, S. A. Della Pietra, V. J. Della Pietra, multilingual machine translation. JMLR, 22(107):1‚Äì48.\n F. Jelinek, J. D. Lafferty, R. L. Mercer, and P. S. Roossin. ‚àÄ, W. Nekoto, V. Marivate, T. Matsila, T. Fasubaa,\n 1990. A statistical approach to machine translation. Com- T. Kolawole, T. Fagbohungbe, S. O. Akinola, S. H.\n putational Linguistics, 16(2):79‚Äì85. Muhammad, S. Kabongo, S. Osei, S. Freshia, R. A.\nBrown, P. F., S. A. Della Pietra, V. J. Della Pietra, and R. L. Niyongabo, R. M. P. Ogayo, O. Ahia, M. Meressa,\n Mercer. 1993. The mathematics of statistical machine M. Adeyemi, M. Mokgesi-Selinga, L. Okegbemi, L. J.\n translation: Parameter estimation. Computational Lin- Martinus, K. Tajudeen, K. Degila, K. Ogueji, K. Siminyu,\n guistics, 19(2):263‚Äì311. J. Kreutzer, J. Webster, J. T. Ali, J. A. I. Orife,\n I. Ezeani, I. A. Dangana, H. Kamper, H. Elsahar, G. Duru,\nCallison-Burch, C., M. Osborne, and P. Koehn. 2006. Re-\nG. Kioko, E. Murhabazi, E. van Biljon, D. Whitenack,\n evaluating the role of BLEU in machine translation re-\nC. Onyefuluchi, C. Emezue, B. Dossou, B. Sibanda, B. I.\n search. EACL.\n Bassey, A. Olabiyi, A. Ramkilowan, A. OÃàktem, A. Ak-\nChandioux, J. 1976. M EÃÅT EÃÅO: un systeÃÄme opeÃÅrationnel pour infaderin, and A. Bashir. 2020. Participatory research\n la traduction automatique des bulletins meÃÅteÃÅorologiques for low-resourced machine translation: A case study in\n destineÃÅs au grand public. Meta, 21:127‚Äì133. African languages. Findings of EMNLP. The authors use\nChiang, D. 2005. A hierarchical phrase-based model for sta- the forall symbol to represent the whole Masakhane comtistical machine translation. ACL. munity.\nChou, W., C.-H. Lee, and B. H. Juang. 1993. Minimum error Gale, W. A. and K. W. Church. 1991. A program for aligning\n rate training based on n-best string models. ICASSP. sentences in bilingual corpora. ACL.\nComrie, B. 1989. Language Universals and Linguistic Ty- Gale, W. A. and K. W. Church. 1993. A program for aligning\n pology, 2nd edition. Blackwell. sentences in bilingual corpora. Computational Linguistics, 19:75‚Äì102.\n Goel, V. and W. Byrne. 2000. Minimum bayes-risk automatic speech recognition. Computer Speech & Language,\n 14(2):115‚Äì135.\n Hutchins, W. J. 1986. Machine Translation: Past, Present,\n Future. Ellis Horwood, Chichester, England.\n28 Chapter 12 ‚Ä¢ Machine Translation\n\nHutchins, W. J. 1997. From first conception to first demon- McLuhan, M. 1964. Understanding Media: The Extensions\n stration: The nascent years of machine translation, 1947‚Äì of Man. New American Library.\n 1954. A chronology. Machine Translation, 12:192‚Äì252. Miller, G. A. and J. G. Beebe-Center. 1956. Some psycho-\nHutchins, W. J. and H. L. Somers. 1992. An Introduction to logical methods for evaluating the quality of translations.\n Machine Translation. Academic Press. Mechanical Translation, 3:73‚Äì80.\nKalchbrenner, N. and P. Blunsom. 2013. Recurrent continu- Nirenburg, S., H. L. Somers, and Y. Wilks, eds. 2002. Readous translation models. EMNLP. ings in Machine Translation. MIT Press.\nKay, M. and M. RoÃàscheisen. 1988. Text-translation align- Och, F. J. 1998. Ein beispielsbasierter und statisment. Technical Report P90-00143, Xerox Palo Alto Re- tischer Ansatz zum maschinellen Lernen von\n search Center, Palo Alto, CA. natuÃàrlichsprachlicher UÃàbersetzung. Ph.D. thesis, Uni-\nKay, M. and M. RoÃàscheisen. 1993. Text-translation align- versitaÃàt Erlangen-NuÃàrnberg, Germany. Diplomarbeit\n ment. Computational Linguistics, 19:121‚Äì142. (diploma thesis).\nKocmi, T., C. Federmann, R. Grundkiewicz, M. Junczys- Och, F. J. 2003. Minimum error rate training in statistical\n Dowmunt, H. Matsushita, and A. Menezes. 2021. To ship machine translation. ACL.\n or not to ship: An extensive evaluation of automatic met- Och, F. J. and H. Ney. 2002. Discriminative training and\n rics for machine translation. ArXiv. maximum entropy models for statistical machine transla-\nKoehn, P. 2005. Europarl: A parallel corpus for statistical tion. ACL.\n machine translation. MT summit, vol. 5. Och, F. J. and H. Ney. 2003. A systematic comparison of\nKoehn, P., H. Hoang, A. Birch, C. Callison-Burch, M. Fed- various statistical alignment models. Computational Linerico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, guistics, 29(1):19‚Äì51.\n R. Zens, C. Dyer, O. Bojar, A. Constantin, and E. Herbst. Och, F. J. and H. Ney. 2004. The alignment template ap-\n2006. Moses: Open source toolkit for statistical machine proach to statistical machine translation. Computational\n translation. ACL. Linguistics, 30(4):417‚Äì449.\nKoehn, P., F. J. Och, and D. Marcu. 2003. Statistical phrase- Papineni, K., S. Roukos, T. Ward, and W.-J. Zhu. 2002. Bleu:\n based translation. HLT-NAACL. A method for automatic evaluation of machine transla-\nKreutzer, J., I. Caswell, L. Wang, A. Wahab, D. van Esch, tion. ACL.\n N. Ulzii-Orshikh, A. Tapo, N. Subramani, A. Sokolov, Pierce, J. R., J. B. Carroll, E. P. Hamp, D. G. Hays, C. F.\n C. Sikasote, M. Setyawan, S. Sarin, S. Samb, B. Sagot, Hockett, A. G. Oettinger, and A. J. Perlis. 1966. Lan-\nC. Rivera, A. Rios, I. Papadimitriou, S. Osei, P. O. guage and Machines: Computers in Translation and Lin-\nSuarez, I. Orife, K. Ogueji, A. N. Rubungo, T. Q. Nguyen, guistics. ALPAC report. National Academy of Sciences,\n M. MuÃàller, A. MuÃàller, S. H. Muhammad, N. Muhammad, National Research Council, Washington, DC.\n A. Mnyakeni, J. Mirzakhalov, T. Matangira, C. Leong,\n PopovicÃÅ, M. 2015. chrF: character n-gram F-score for auto-\nN. Lawson, S. Kudugunta, Y. Jernite, M. Jenny, O. Firat,\n matic MT evaluation. Proceedings of the Tenth Workshop\n B. F. P. Dossou, S. Dlamini, N. de Silva, S. CÃßabuk Ballƒ±,\n on Statistical Machine Translation.\n S. Biderman, A. Battisti, A. Baruwa, A. Bapna, P. Baljekar, I. A. Azime, A. Awokoya, D. Ataman, O. Ahia, Post, M. 2018. A call for clarity in reporting BLEU scores.\n O. Ahia, S. Agrawal, and M. Adeyemi. 2022. Quality at WMT 2018.\n a glance: An audit of web-crawled multilingual datasets. Prates, M. O. R., P. H. Avelar, and L. C. Lamb. 2019. Assess-\nTACL, 10:50‚Äì72. ing gender bias in machine translation: a case study with\nKudo, T. 2018. Subword regularization: Improving neural Google Translate. Neural Computing and Applications,\n network translation models with multiple subword candi- 32:6363‚Äì6381.\n dates. ACL. Raffel, C., N. Shazeer, A. Roberts, K. Lee, S. Narang,\nKudo, T. and J. Richardson. 2018. SentencePiece: A simple M. Matena, Y. Zhou, W. Li, and P. J. Liu. 2020. Exploring\n and language independent subword tokenizer and detok- the limits of transfer learning with a unified text-to-text\n enizer for neural text processing. EMNLP. transformer. JMLR, 21(140):1‚Äì67.\nKumar, S. and W. Byrne. 2004. Minimum Bayes-risk decod- Rei, R., C. Stewart, A. C. Farinha, and A. Lavie.\n ing for statistical machine translation. HLT-NAACL. 2020. COMET: A neural framework for MT evaluation.\n EMNLP.\nLan, Z., M. Chen, S. Goodman, K. Gimpel, P. Sharma,\n and R. Soricut. 2020. ALBERT: A lite BERT for self- Schiebinger, L. 2014. Scientific research must take gender\n supervised learning of language representations. ICLR. into account. Nature, 507(7490):9.\nLison, P. and J. Tiedemann. 2016. Opensubtitles2016: Ex- Schwenk, H. 2018. Filtering and mining parallel data in a\n tracting large parallel corpora from movie and tv subti- joint multilingual space. ACL.\n tles. LREC. Schwenk, H., D. Dechelotte, and J.-L. Gauvain. 2006. Con-\nLowerre, B. T. 1976. The Harpy Speech Recognition System. tinuous space language models for statistical machine\n Ph.D. thesis, Carnegie Mellon University, Pittsburgh, PA. translation. COLING/ACL.\nMarcu, D. and W. Wong. 2002. A phrase-based, joint proba- Schwenk, H., G. Wenzek, S. Edunov, E. Grave, A. Joulin,\n bility model for statistical machine translation. EMNLP. and A. Fan. 2021. CCMatrix: Mining billions of high-\nMarie, B., A. Fujita, and R. Rubino. 2021. Scientific credi- quality parallel sentences on the web. ACL.\n bility of machine translation research: A meta-evaluation Sellam, T., D. Das, and A. Parikh. 2020. BLEURT: Learning\n of 769 papers. ACL. robust metrics for text generation. ACL.\n Exercises 29\n\nSlobin, D. I. 1996. Two ways to travel. In M. Shibatani and\n S. A. Thompson, eds, Grammatical Constructions: Their\n Form and Meaning, 195‚Äì220. Clarendon Press.\nSnover, M., B. Dorr, R. Schwartz, L. Micciulla, and\n J. Makhoul. 2006. A study of translation edit rate with\n targeted human annotation. AMTA-2006.\nStanovsky, G., N. A. Smith, and L. Zettlemoyer. 2019. Evaluating gender bias in machine translation. ACL.\nStolcke, A., Y. Konig, and M. Weintraub. 1997. Explicit\n word error minimization in N-best list rescoring. EU-\nROSPEECH, volume 1.\nSuzgun, M., L. Melas-Kyriazi, and D. Jurafsky. 2023. Follow the wisdom of the crowd: Effective text generation\n via minimum Bayes risk decoding. Findings of ACL\n 2023.\nTalmy, L. 1985. Lexicalization patterns: Semantic structure\n in lexical forms. In T. Shopen, ed., Language Typology\n and Syntactic Description, Volume 3. Cambridge University Press. Originally appeared as UC Berkeley Cognitive\n Science Program Report No. 30, 1980.\nTalmy, L. 1991. Path to realization: A typology of event\n conflation. BLS-91.\nThompson, B. and P. Koehn. 2019. Vecalign: Improved sentence alignment in linear time and space. EMNLP.\nTurian, J. P., L. Shen, and I. D. Melamed. 2003. Evaluation\n of machine translation and its evaluation. Proceedings of\n MT Summit IX.\nVaswani, A., N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\n A. N. Gomez, ≈Å. Kaiser, and I. Polosukhin. 2017. Attention is all you need. NeurIPS.\nVauquois, B. 1968. A survey of formal grammars and algorithms for recognition and transformation in machine\n translation. IFIP Congress 1968.\nWeaver, W. 1949/1955. Translation. In W. N. Locke and\n A. D. Boothe, eds, Machine Translation of Languages,\n 15‚Äì23. MIT Press. Reprinted from a memorandum written by Weaver in 1949.\nWu, D. 1996. A polynomial-time algorithm for statistical\n machine translation. ACL.\nWu, Y., M. Schuster, Z. Chen, Q. V. Le, M. Norouzi,\n W. Macherey, M. Krikun, Y. Cao, Q. Gao, K. Macherey,\n J. Klingner, A. Shah, M. Johnson, X. Liu, ≈Å. Kaiser,\n S. Gouws, Y. Kato, T. Kudo, H. Kazawa, K. Stevens,\n G. Kurian, N. Patil, W. Wang, C. Young, J. Smith,\n J. Riesa, A. Rudnick, O. Vinyals, G. S. Corrado,\n M. Hughes, and J. Dean. 2016. Google‚Äôs neural machine\n translation system: Bridging the gap between human and\n machine translation. ArXiv preprint arXiv:1609.08144.\nZens, R. and H. Ney. 2007. Efficient phrase-table representation for machine translation with applications to online\n MT and speech translation. NAACL-HLT.\nZhang, T., V. Kishore, F. Wu, K. Q. Weinberger, and Y. Artzi.\n 2020. BERTscore: Evaluating text generation with\n BERT. ICLR 2020.\nZiemski, M., M. Junczys-Dowmunt, and B. Pouliquen. 2016.\n The United Nations parallel corpus v1.0. LREC.\n",
    "file_path": "/Users/colinsidberry/nlp-textbook-rag/data/extracted_text/12.Machine Translation.txt",
    "file_size_kb": 92.89
  },
  {
    "id": "6e20c368dfc54e7b",
    "source": "nlp_textbook",
    "chapter": "RNNs and LSTMs 13 Time will explain. Jane Austen, Persuasion",
    "filename": "13.RNNs and LSTMs.txt",
    "content": "rights reserved. Draft of August 24, 2025.\n\nCHAPTER\n\n RNNs and LSTMs\n13 Time will explain.\n Jane Austen, Persuasion\n\n Language is an inherently temporal phenomenon. Spoken language is a sequence of\n acoustic events over time, and we comprehend and produce both spoken and written\n language as a sequential input stream. The temporal nature of language is reflected\n in the metaphors we use; we talk of the flow of conversations, news feeds, and twitter\n streams, all of which emphasize that language is a sequence that unfolds in time.\n This chapter introduces a deep learning architecture, the recurrent neural network (RNN), and RNN variants like LSTMs, that offer a different way of representing time than feedforward and transformer networks. RNNs have a mechanism that\n deals directly with the sequential nature of language, allowing them to handle the\n temporal nature of language without the use of arbitrary fixed-sized windows. The\n recurrent network offers a new way to represent the prior context, in its recurrent\n connections, allowing the model‚Äôs decision to depend on information from hundreds\n of words in the past. We‚Äôll see how to apply the model to the task of language modeling, to text classification tasks like sentiment analysis, and to sequence modeling\n tasks like part-of-speech tagging.\n\n A recurrent neural network (RNN) is any network that contains a cycle within its\n network connections, meaning that the value of some unit is directly, or indirectly,\n dependent on its own earlier outputs as an input. While powerful, such networks\n are difficult to reason about and to train. However, within the general class of recurrent networks there are constrained architectures that have proven to be extremely\n effective when applied to language. In this section, we consider a class of recurrent\n Elman networks referred to as Elman Networks (Elman, 1990) or simple recurrent net-\nNetworks\n works. These networks are useful in their own right and serve as the basis for more\n complex approaches like the Long Short-Term Memory (LSTM) networks discussed\n later in this chapter. In this chapter when we use the term RNN we‚Äôll be referring to\n these simpler more constrained networks (although you will often see the term RNN\n to mean any net with recurrent properties including LSTMs).\n Fig. 13.1 illustrates the structure of an RNN. As with ordinary feedforward networks, an input vector representing the current input, xt , is multiplied by a weight\n matrix and then passed through a non-linear activation function to compute the values for a layer of hidden units. This hidden layer is then used to calculate a corresponding output, yt . In a departure from our earlier window-based approach, sequences are processed by presenting one item at a time to the network. We‚Äôll use\n2 C HAPTER 13 ‚Ä¢ RNN S AND LSTM S\n\n xt ht yt\n\n layer depends on the current input as well as the activation value of the hidden layer from the\n previous time step.\n\n subscripts to represent time, thus xt will mean the input vector x at time t. The key\n difference from a feedforward network lies in the recurrent link shown in the figure\n with the dashed line. This link augments the input to the computation at the hidden\n layer with the value of the hidden layer from the preceding point in time.\n The hidden layer from the previous time step provides a form of memory, or\n context, that encodes earlier processing and informs the decisions to be made at\n later points in time. Critically, this approach does not impose a fixed-length limit\n on this prior context; the context embodied in the previous hidden layer can include\n information extending back to the beginning of the sequence.\n Adding this temporal dimension makes RNNs appear to be more complex than\n non-recurrent architectures. But in reality, they‚Äôre not all that different. Given an\n input vector and the values for the hidden layer from the previous time step, we‚Äôre\n still performing the standard feedforward calculation introduced in Chapter 6. To\n see this, consider Fig. 13.2 which clarifies the nature of the recurrence and how it\n factors into the computation at the hidden layer. The most significant change lies in\n the new set of weights, U, that connect the hidden layer from the previous time step\n to the current hidden layer. These weights determine how the network makes use of\n past context in calculating the output for the current input. As with the other weights\n in the network, these connections are trained via backpropagation.\n\n yt\n\n V\n\n ht\n\n U W\n\n ht-1 xt\n\n hidden layer ht‚àí1 from the prior time step is multiplied by weight matrix U and then added\n to the feedforward component from the current time step.\n\n 13.1.1 Inference in RNNs\n Forward inference (mapping a sequence of inputs to a sequence of outputs) in an\n RNN is nearly identical to what we‚Äôve already seen with feedforward networks. To\n compute an output yt for an input xt , we need the activation value for the hidden\n layer ht . To calculate this, we multiply the input xt with the weight matrix W, and\n 13.1 ‚Ä¢ R ECURRENT N EURAL N ETWORKS 3\n\nthe hidden layer from the previous time step ht‚àí1 with the weight matrix U. We\nadd these values together and pass them through a suitable activation function, g,\nto arrive at the activation value for the current hidden layer, ht . Once we have the\nvalues for the hidden layer, we proceed with the usual computation to generate the\noutput vector.\n\n ht = g(Uht‚àí1 + Wxt ) (13.1)\n yt = f (Vht ) (13.2)\n\nLet‚Äôs refer to the input, hidden and output layer dimensions as din , dh , and dout\nrespectively. Given this, our three parameter matrices are: W ‚àà Rdh √ódin , U ‚àà Rdh √ódh ,\nand V ‚àà Rdout √ódh .\n We compute yt via a softmax computation that gives a probability distribution\nover the possible output classes.\n\n yt = softmax(Vht ) (13.3)\n\nThe fact that the computation at time t requires the value of the hidden layer from\ntime t ‚àí 1 mandates an incremental inference algorithm that proceeds from the start\nof the sequence to the end as illustrated in Fig. 13.3. The sequential nature of simple\nrecurrent networks can also be seen by unrolling the network in time as is shown in\nFig. 13.4. In this figure, the various layers of units are copied for each time step to\nillustrate that they will have differing values over time. However, the various weight\nmatrices are shared across time.\n\n function F ORWARD RNN(x, network) returns output sequence y\n\n h0 ‚Üê 0\n for i ‚Üê 1 to L ENGTH(x) do\n hi ‚Üê g(Uhi‚àí1 + Wxi )\n yi ‚Üê f (Vhi )\n return y\n\nare shared across time, while new values for h and y are calculated with each time step.\n\n13.1.2 Training\nAs with feedforward networks, we‚Äôll use a training set, a loss function, and backpropagation to obtain the gradients needed to adjust the weights in these recurrent\nnetworks. As shown in Fig. 13.2, we now have 3 sets of weights to update: W, the\nweights from the input layer to the hidden layer, U, the weights from the previous\nhidden layer to the current hidden layer, and finally V, the weights from the hidden\nlayer to the output layer.\n Fig. 13.4 highlights two considerations that we didn‚Äôt have to worry about with\nbackpropagation in feedforward networks. First, to compute the loss function for\nthe output at time t we need the hidden layer from time t ‚àí 1. Second, the hidden\nlayer at time t influences both the output at time t and the hidden layer at time t + 1\n(and hence the output and loss at t + 1). It follows from this that to assess the error\naccruing to ht , we‚Äôll need to know its influence on both the current output as well as\nthe ones that follow.\n4 C HAPTER 13 ‚Ä¢ RNN S AND LSTM S\n\n y3\n\n V\n\n y2 h3\n\n V U W\n\n h2\n y1 x3\n\n U W\n V\n\n h1 x2\n\n U W\n\n h0 x1\n\neach time step, while the weights U, V and W are shared across all time steps.\n\n Tailoring the backpropagation algorithm to this situation leads to a two-pass algorithm for training the weights in RNNs. In the first pass, we perform forward\n inference, computing ht , yt , accumulating the loss at each step in time, saving the\n value of the hidden layer at each step for use at the next time step. In the second\n pass, we process the sequence in reverse, computing the required gradients as we go,\n computing and saving the error term for use in the hidden layer for each step backward in time. This general approach is commonly referred to as backpropagation\n backpropagation through through time (Werbos 1974, Rumelhart et al. 1986, Werbos 1990).\n time\n Fortunately, with modern computational frameworks and adequate computing\n resources, there is no need for a specialized approach to training RNNs. As illustrated in Fig. 13.4, explicitly unrolling a recurrent network into a feedforward computational graph eliminates any explicit recurrences, allowing the network weights\n to be trained directly. In such an approach, we provide a template that specifies the\n basic structure of the network, including all the necessary parameters for the input,\n output, and hidden layers, the weight matrices, as well as the activation and output\n functions to be used. Then, when presented with a specific input sequence, we can\n generate an unrolled feedforward network specific to that input, and use that graph\n to perform forward inference or training via ordinary backpropagation.\n For applications that involve much longer input sequences, such as speech recognition, character-level processing, or streaming continuous inputs, unrolling an entire input sequence may not be feasible. In these cases, we can unroll the input into\n manageable fixed-length segments and treat each segment as a distinct training item.\n 13.2 ‚Ä¢ RNN S AS L ANGUAGE M ODELS 5\n\n Let‚Äôs see how to apply RNNs to the language modeling task. Recall from Chapter 3\n that language models predict the next word in a sequence given some preceding\n context. For example, if the preceding context is ‚ÄúThanks for all the‚Äù and we want\n to know how likely the next word is ‚Äúfish‚Äù we would compute:\n P(fish|Thanks for all the)\n Language models give us the ability to assign such a conditional probability to every\n possible next word, giving us a distribution over the entire vocabulary. We can also\n assign probabilities to entire sequences by combining these conditional probabilities\n with the chain rule:\n n\n Y\n P(w1:n ) = P(wi |w<i )\n i=1\n\n The n-gram language models of Chapter 3 compute the probability of a word given\n counts of its occurrence with the n ‚àí 1 prior words. The context is thus of size n ‚àí 1.\n For the feedforward language models of Chapter 6, the context is the window size.\n RNN language models (Mikolov et al., 2010) process the input sequence one\n word at a time, attempting to predict the next word from the current word and the\n previous hidden state. RNNs thus don‚Äôt have the limited context problem that n-gram\n models have, or the fixed context that feedforward language models have, since the\n hidden state can in principle represent information about all of the preceding words\n all the way back to the beginning of the sequence. Fig. 13.5 sketches this difference\n between a FFN language model and an RNN language model, showing that the\n RNN language model uses ht‚àí1 , the hidden state from the previous time step, as a\n representation of the past context.\n\n ^\n yt\n a) b)\n ^\n yt\n U\n\n V\n ht\n\n ht-2 U ht-1 U ht\n W\n W W W\n\n et-2 et-1 et et-2 et-1 et\n\n schematic context of three tokens: (a) a feedforward neural language model which has a fixed\n context input to the weight matrix W, (b) an RNN language model, in which the hidden state\n ht‚àí1 summarizes the prior context.\n\n 13.2.1 Forward Inference in an RNN language model\n Forward inference in a recurrent language model proceeds exactly as described in\n Section 13.1.1. The input sequence X = [x1 ; ...; xt ; ...; xN ] consists of a series of\n6 C HAPTER 13 ‚Ä¢ RNN S AND LSTM S\n\n words each represented as a one-hot vector of size |V | √ó 1, and the output prediction, yÃÇ, is a vector representing a probability distribution over the vocabulary. At\n each step, the model uses the word embedding matrix E to retrieve the embedding\n for the current word, multiples it by the weight matrix W, and then adds it to the hidden layer from the previous step (weighted by weight matrix U) to compute a new\n hidden layer. This hidden layer is then used to generate an output layer which is\n passed through a softmax layer to generate a probability distribution over the entire\n vocabulary. That is, at time t:\n\n et = Ext (13.4)\n ht = g(Uht‚àí1 + Wet ) (13.5)\n yÃÇt = softmax(Vht ) (13.6)\n\n When we do language modeling with RNNs (and we‚Äôll see this again in Chapter 8\n with transformers), it‚Äôs convenient to make the assumption that the embedding dimension de and the hidden dimension dh are the same. So we‚Äôll just call both of\n these the model dimension d. So the embedding matrix E is of shape [d √ó |V |], and\n xt is a one-hot vector of shape [|V | √ó 1]. The product et is thus of shape [d √ó 1]. W\n and U are of shape [d √ó d], so ht is also of shape [d √ó 1]. V is of shape [|V | √ó d],\n so the result of Vh is a vector of shape [|V | √ó 1]. This vector can be thought of as\n a set of scores over the vocabulary given the evidence provided in h. Passing these\n scores through the softmax normalizes the scores into a probability distribution. The\n probability that a particular word k in the vocabulary is the next word is represented\n by yÃÇt [k], the kth component of yÃÇt :\n\n P(wt+1 = k|w1 , . . . , wt ) = yÃÇt [k] (13.7)\n\n The probability of an entire sequence is just the product of the probabilities of each\n item in the sequence, where we‚Äôll use yÃÇi [wi ] to mean the probability of the true word\n wi at time step i.\n n\n Y\n P(w1:n ) = P(wi |w1:i‚àí1 ) (13.8)\n i=1\n Yn\n = yÃÇi [wi ] (13.9)\n i=1\n\n 13.2.2 Training an RNN language model\nself-supervision To train an RNN as a language model, we use the same self-supervision (or selftraining) algorithm we saw in Section ??: we take a corpus of text as training\n material and at each time step t ask the model to predict the next word. We call\n such a model self-supervised because we don‚Äôt have to add any special gold labels\n to the data; the natural sequence of words is its own supervision! We simply train\n the model to minimize the error in predicting the true next word in the training\n sequence, using cross-entropy as the loss function. Recall that the cross-entropy\n loss measures the difference between a predicted probability distribution and the\n correct distribution.\n X\n LCE = ‚àí yt [w] log yÃÇt [w] (13.10)\n w‚ààV\n 13.2 ‚Ä¢ RNN S AS L ANGUAGE M ODELS 7\n\n Next word long and thanks for all\n\n Loss log yÃÇlong log yÃÇand log yÃÇfor log yÃÇall ‚Ä¶\n <latexit sha1_base64=\"9tru+5ysH1zS9iUXRg/IsnxmpMA=\">AAAB/XicbVDLSsNAFL3xWesr6lKQwSK4sSQi1WXRjcsK9gFNCZPpJB06yYSZiRBCcOOvuBFxo+Av+Av+jUnbTVsPDBzOOcO993gxZ0pb1q+xsrq2vrFZ2apu7+zu7ZsHhx0lEklomwguZM/DinIW0bZmmtNeLCkOPU673viu9LtPVComokedxnQQ4iBiPiNYF5Jrnlw4XATIGWGdpbmbOSHWIxlmXERBnldds2bVrQnQMrFnpAYztFzzxxkKkoQ00oRjpfq2FetBhqVmhNO86iSKxpiMcUCzyfY5OiukIfKFLF6k0USdy+FQqTT0imS5nFr0SvE/r59o/2aQsShONI3IdJCfcKQFKqtAQyYp0TwtCCaSFRsiMsISE10UVp5uLx66TDqXdbtRbzxc1Zq3sxIqcAyncA42XEMT7qEFbSDwAm/wCV/Gs/FqvBsf0+iKMftzBHMwvv8ADJKVcA==</latexit>\n <latexit sha1_base64=\"tuzkS/BeX/Xmg79qpWZlpeYDhtE=\">AAAB/HicbVDLSsNAFL3xWesr6lKEwSK4sSQi1WXRjcsK9gFNCZPJpB06mYSZiRBC3PgrbkTcKPgN/oJ/Y9J209YDA4dzznDvPV7MmdKW9WusrK6tb2xWtqrbO7t7++bBYUdFiSS0TSIeyZ6HFeVM0LZmmtNeLCkOPU673viu9LtPVCoWiUedxnQQ4qFgASNYF5Jrnlw4PBoiZ4R1luZu5oRYj2SYYeHnedU1a1bdmgAtE3tGajBDyzV/HD8iSUiFJhwr1betWA8yLDUjnOZVJ1E0xmSMhzSbLJ+js0LyURDJ4gmNJupcDodKpaFXJMvd1KJXiv95/UQHN4OMiTjRVJDpoCDhSEeobAL5TFKieVoQTCQrNkRkhCUmuuirPN1ePHSZdC7rdqPeeLiqNW9nJVTgGE7hHGy4hibcQwvaQOAF3uATvoxn49V4Nz6m0RVj9ucI5mB8/wEiupTp</latexit> <latexit sha1_base64=\"D3c31Jvxp3QWPr2h4tzQWmeenDs=\">AAAB/HicbVDLSsNAFL3xWesr6lKEwSK4sSQi1WXRjcsK9gFNCZPppB06yYSZiRBC3PgrbkTcKPgN/oJ/Y9Jm09YDA4dzznDvPV7EmdKW9WusrK6tb2xWtqrbO7t7++bBYUeJWBLaJoIL2fOwopyFtK2Z5rQXSYoDj9OuN7kr/O4TlYqJ8FEnER0EeBQynxGsc8k1Ty4cLkbIGWOdJpmbOgHWYxmkvpBZVnXNmlW3pkDLxC5JDUq0XPPHGQoSBzTUhGOl+rYV6UGKpWaE06zqxIpGmEzwiKbT5TN0lktDlM/LX6jRVJ3L4UCpJPDyZLGbWvQK8T+vH2v/ZpCyMIo1DclskB9zpAUqmkBDJinRPMkJJpLlGyIyxhITnfdVnG4vHrpMOpd1u1FvPFzVmrdlCRU4hlM4BxuuoQn30II2EHiBN/iEL+PZeDXejY9ZdMUo/xzBHIzvP0CJlP0=</latexit>\n\n <latexit sha1_base64=\"PI3y1fb9LhumoVCQRh2+Y84dRkc=\">AAAB/HicbVDLSsNAFL3xWesr6lKEwSK4sSQi1WXRjcsK9gFNCZPppB06yYSZiRBC3PgrbkTcKPgN/oJ/Y9Jm09YDA4dzznDvPV7EmdKW9WusrK6tb2xWtqrbO7t7++bBYUeJWBLaJoIL2fOwopyFtK2Z5rQXSYoDj9OuN7kr/O4TlYqJ8FEnER0EeBQynxGsc8k1Ty4cLkbIGWOdJpmbOgHWYxmkmPMsq7pmzapbU6BlYpekBiVarvnjDAWJAxpqwrFSfduK9CDFUjPCaVZ1YkUjTCZ4RNPp8hk6y6Uh8oXMX6jRVJ3L4UCpJPDyZLGbWvQK8T+vH2v/ZpCyMIo1DclskB9zpAUqmkBDJinRPMkJJpLlGyIyxhITnfdVnG4vHrpMOpd1u1FvPFzVmrdlCRU4hlM4BxuuoQn30II2EHiBN/iEL+PZeDXejY9ZdMUo/xzBHIzvPyumlO8=</latexit>\n\n log yÃÇthanks\n <latexit sha1_base64=\"0zdsmbBovZ+hafWZN7Hvufo85tU=\">AAAB/3icbVDLSsNAFJ3UV62vqEs3g0VwY0lEqsuiG5cV7AOaEibTSTN0kgkzN0IIWbjxV9yIuFHwD/wF/8ak7aatBwYO55zh3nu8WHANlvVrVNbWNza3qtu1nd29/QPz8KirZaIo61AppOp7RDPBI9YBDoL1Y8VI6AnW8yZ3pd97YkpzGT1CGrNhSMYR9zklUEiuiS8cIcfYCQhkae5mTkggUGEGAYkmOs9rrlm3GtYUeJXYc1JHc7Rd88cZSZqELAIqiNYD24phmBEFnAqW15xEs5jQCRmzbLp/js8KaYR9qYoXAZ6qCzkSap2GXpEs19PLXin+5w0S8G+GGY/iBFhEZ4P8RGCQuCwDj7hiFERaEEIVLzbENCCKUCgqK0+3lw9dJd3Lht1sNB+u6q3beQlVdIJO0Tmy0TVqoXvURh1E0Qt6Q5/oy3g2Xo1342MWrRjzP8doAcb3H7Aall0=</latexit>\n\n y\n\n Softmax over\n Vocabulary\n Vh\n h\n RNN ‚Ä¶\n\n Input\n e ‚Ä¶\n Embeddings\n\n So long and thanks for\n\n In the case of language modeling, the correct distribution yt comes from knowing the\n next word. This is represented as a one-hot vector corresponding to the vocabulary\n where the entry for the actual next word is 1, and all the other entries are 0. Thus,\n the cross-entropy loss for language modeling is determined by the probability the\n model assigns to the correct next word. So at time t the CE loss is the negative log\n probability the model assigns to the next word in the training sequence.\n\n LCE (yÃÇt , yt ) = ‚àí log yÃÇt [wt+1 ] (13.11)\n\n Thus at each word position t of the input, the model takes as input the correct word wt\n together with ht‚àí1 , encoding information from the preceding w1:t‚àí1 , and uses them\n to compute a probability distribution over possible next words so as to compute the\n model‚Äôs loss for the next token wt+1 . Then we move to the next word, we ignore\n what the model predicted for the next word and instead use the correct word wt+1\n along with the prior history encoded to estimate the probability of token wt+2 . This\n idea that we always give the model the correct history sequence to predict the next\n word (rather than feeding the model its best case from the previous time step) is\nteacher forcing called teacher forcing.\n The weights in the network are adjusted to minimize the average CE loss over\n the training sequence via gradient descent. Fig. 13.6 illustrates this training regimen.\n\n 13.2.3 Weight Tying\n Careful readers may have noticed that the input embedding matrix E and the final\n layer matrix V, which feeds the output softmax, are quite similar.\n The columns of E represent the word embeddings for each word in the vocabulary learned during the training process with the goal that words that have similar\n meaning and function will have similar embeddings. And, since when we use RNNs\n for language modeling we make the assumption that the embedding dimension and\n the hidden dimension are the same (= the model dimension d), the embedding matrix E has shape [d √ó |V |]. And the final layer matrix V provides a way to score\n the likelihood of each word in the vocabulary given the evidence present in the final\n hidden layer of the network through the calculation of Vh. V is of shape [|V | √ó d].\n That is, is, the rows of V are shaped like a transpose of E, meaning that V provides\n8 C HAPTER 13 ‚Ä¢ RNN S AND LSTM S\n\n a second set of learned word embeddings.\n Instead of having two sets of embedding matrices, language models use a single\n embedding matrix, which appears at both the input and softmax layers. That is,\n we dispense with V and use E at the start of the computation and E| (because the\n shape of V is the transpose of E at the end. Using the same matrix (transposed) in\n weight tying two places is called weight tying.1 The weight-tied equations for an RNN language\n model then become:\n\n et = Ext (13.12)\n ht = g(Uht‚àí1 + Wet ) (13.13)\n yÃÇt = softmax(E ht ) (13.14)\n\n In addition to providing improved model perplexity, this approach significantly reduces the number of parameters required for the model.\n\n Now that we‚Äôve seen the basic RNN architecture, let‚Äôs consider how to apply it to\n three types of NLP tasks: sequence classification tasks like sentiment analysis and\n topic classification, sequence labeling tasks like part-of-speech tagging, and text\n generation tasks, including with a new architecture called the encoder-decoder.\n\n 13.3.1 Sequence Labeling\n In sequence labeling, the network‚Äôs task is to assign a label chosen from a small\n fixed set of labels to each element of a sequence. One classic sequence labeling\n tasks is part-of-speech (POS) tagging (assigning grammatical tags like NOUN and\n VERB to each word in a sentence). We‚Äôll discuss part-of-speech tagging in detail\n in Chapter 17, but let‚Äôs give a motivating example here. In an RNN approach to\n sequence labeling, inputs are word embeddings and the outputs are tag probabilities\n generated by a softmax layer over the given tagset, as illustrated in Fig. 13.7.\n In this figure, the inputs at each time step are pretrained word embeddings corresponding to the input tokens. The RNN block is an abstraction that represents\n an unrolled simple recurrent network consisting of an input layer, hidden layer, and\n output layer at each time step, as well as the shared U, V and W weight matrices\n that comprise the network. The outputs of the network at each time step represent\n the distribution over the POS tagset generated by a softmax layer.\n To generate a sequence of tags for a given input, we run forward inference over\n the input sequence and select the most likely tag from the softmax at each step. Since\n we‚Äôre using a softmax layer to generate the probability distribution over the output\n tagset at each time step, we will again employ the cross-entropy loss during training.\n\n 13.3.2 RNNs for Sequence Classification\n Another use of RNNs is to classify entire sequences rather than the tokens within\n them. This is the set of tasks commonly called text classification, like sentiment\n analysis or spam detection, in which we classify a text into two or three classes\n (like positive or negative), as well as classification tasks with a large number of\n 1 We also do this for transformers (Chapter 8) where it‚Äôs common to call E| the unembedding matrix.\n 13.3 ‚Ä¢ RNN S FOR OTHER NLP TASKS 9\n\n Argmax NNP MD VB DT NN\n y\n Softmax over\n tags\n\n Vh\n RNN h\n Layer(s)\n\n Embeddings e\n\n Words Janet will back the bill\npart-of-speech (POS) tagging is to assign a grammatical label to each word in a sentence,\ndrawn from a predefined set of tags. (The tags for this sentence include NNP (proper noun),\nMD (modal verb) and others; we‚Äôll give a complete description of the task of part-of-speech\ntagging in Chapter 17.) Pre-trained word embeddings serve as inputs and a softmax layer\nprovides a probability distribution over the part-of-speech tags as output at each time step.\n\ncategories, like document-level topic classification, or message routing for customer\nservice applications.\n To apply RNNs in this setting, we pass the text to be classified through the RNN\na word at a time generating a new hidden layer representation at each time step.\nWe can then take the hidden layer for the last token of the text, hn , to constitute a\ncompressed representation of the entire sequence. We can pass this representation\nhn to a feedforward network that chooses a class via a softmax over the possible\nclasses. Fig. 13.8 illustrates this approach.\n\n Softmax\n\n FFN\n hn\n\n RNN\n\n x1 x2 x3 xn\n\nperforms the classification.\n\n Note that in this approach we don‚Äôt need intermediate outputs for the words in\nthe sequence preceding the last element. Therefore, there are no loss terms associated with those elements. Instead, the loss function used to train the weights in the\nnetwork is based entirely on the final text classification task. The output from the\nsoftmax output from the feedforward classifier together with a cross-entropy loss\n10 C HAPTER 13 ‚Ä¢ RNN S AND LSTM S\n\n drives the training. The error signal from the classification is backpropagated all the\n way through the weights in the feedforward classifier through, to its input, and then\n through to the three sets of weights in the RNN as described earlier in Section 13.1.2.\n The training regimen that uses the loss from a downstream application to adjust the\n end-to-end\n training weights all the way through the network is referred to as end-to-end training.\n Another option, instead of using just the hidden state of the last token hn to\n pooling represent the whole sequence, is to use some sort of pooling function of all the\n hidden states hi for each word i in the sequence. For example, we can create a\n representation that pools all the n hidden states by taking their element-wise mean:\n n\n 1X\n hmean = hi (13.15)\n n\n i=1\n\n Or we can take the element-wise max; the element-wise max of a set of n vectors is\n a new vector whose kth element is the max of the kth elements of all the n vectors.\n The long contexts of RNNs makes it quite difficult to successfully backpropagate\n error all the way through the entire input; we‚Äôll talk about this problem, and some\n standard solutions, in Section 13.5.\n\n 13.3.3 Generation with RNN-Based Language Models\n RNN-based language models can also be used to generate text, Text generation,\n along with image generation and code generation, constitute a new area of AI that\n is often called generative AI. Those of you who have already read Chapter 7 and\n Chapter 8 will have already seen this, but we reintroduce it here for those who are\n reading in a different order.\n Recall back in Chapter 3 we saw how to generate text from an n-gram language\n model by adapting a sampling technique suggested at about the same time by Claude\n Shannon (Shannon, 1951) and the psychologists George Miller and Jennifer Selfridge (Miller and Selfridge, 1950). We first randomly sample a word to begin a\n sequence based on its suitability as the start of a sequence. We then continue to\n sample words conditioned on our previous choices until we reach a pre-determined\n length, or an end of sequence token is generated.\n Today, this approach of using a language model to incrementally generate words\n by repeatedly sampling the next word conditioned on our previous choices is called\n autoregressive\n generation autoregressive generation or causal LM generation. The procedure is basically\n the same as that described on page ??, but adapted to a neural context:\n ‚Ä¢ Sample a word in the output from the softmax distribution that results from\n using the beginning of sentence marker, <s>, as the first input.\n ‚Ä¢ Use the word embedding for that first word as the input to the network at the\n next time step, and then sample the next word in the same fashion.\n ‚Ä¢ Continue generating until the end of sentence marker, </s>, is sampled or a\n fixed length limit is reached.\n Technically an autoregressive model is a model that predicts a value at time t based\n on a linear function of the previous values at times t ‚àí 1, t ‚àí 2, and so on. Although\n language models are not linear (since they have many layers of non-linearities), we\n loosely refer to this generation technique as autoregressive generation since the\n word generated at each time step is conditioned on the word selected by the network\n from the previous step. Fig. 13.9 illustrates this approach. In this figure, the details\n of the RNN‚Äôs hidden layers and recurrent connections are hidden within the blue\n block.\n 13.4 ‚Ä¢ S TACKED AND B IDIRECTIONAL RNN ARCHITECTURES 11\n\n This simple architecture underlies state-of-the-art approaches to applications\n such as machine translation, summarization, and question answering. The key to\n these approaches is to prime the generation component with an appropriate context.\n That is, instead of simply using <s> to get things started we can provide a richer\n task-appropriate context; for translation the context is the sentence in the source\n language; for summarization it‚Äôs the long text we want to summarize.\n\n Sampled Word So long and ?\n\n Softmax\n\n RNN\n\n Embedding\n\n Input Word <s> So long and\n\n Recurrent networks are quite flexible. By combining the feedforward nature of unrolled computational graphs with vectors as common inputs and outputs, complex\n networks can be treated as modules that can be combined in creative ways. This\n section introduces two of the more common network architectures used in language\n processing with RNNs.\n\n 13.4.1 Stacked RNNs\n In our examples thus far, the inputs to our RNNs have consisted of sequences of\n word or character embeddings (vectors) and the outputs have been vectors useful for\n predicting words, tags or sequence labels. However, nothing prevents us from using\n the entire sequence of outputs from one RNN as an input sequence to another one.\nStacked RNNs Stacked RNNs consist of multiple networks where the output of one layer serves as\n the input to a subsequent layer, as shown in Fig. 13.10.\n Stacked RNNs generally outperform single-layer networks. One reason for this\n success seems to be that the network induces representations at differing levels of\n abstraction across layers. Just as the early stages of the human visual system detect\n edges that are then used for finding larger regions and shapes, the initial layers of\n stacked networks can induce representations that serve as useful abstractions for\n further layers‚Äîrepresentations that might prove difficult to induce in a single RNN.\n The optimal number of stacked RNNs is specific to each application and to each\n training set. However, as the number of stacks is increased the training costs rise\n12 C HAPTER 13 ‚Ä¢ RNN S AND LSTM S\n\n y1 y2 y3 yn\n\n RNN 3\n\n RNN 2\n\n RNN 1\n x1 x2 x3 xn\n\n to higher levels with the output of the last network serving as the final output.\n\n quickly.\n\n 13.4.2 Bidirectional RNNs\n The RNN uses information from the left (prior) context to make its predictions at\n time t. But in many applications we have access to the entire input sequence; in\n those cases we would like to use words from the context to the right of t. One way\n to do this is to run two separate RNNs, one left-to-right, and one right-to-left, and\n concatenate their representations.\n In the left-to-right RNNs we‚Äôve discussed so far, the hidden state at a given time\n t represents everything the network knows about the sequence up to that point. The\n state is a function of the inputs x1 , ..., xt and represents the context of the network to\n the left of the current time.\n\n h ft = RNNforward (x1 , . . . , xt ) (13.16)\n\n This new notation h ft simply corresponds to the normal hidden state at time t, representing everything the network has gleaned from the sequence so far.\n To take advantage of context to the right of the current input, we can train an\n RNN on a reversed input sequence. With this approach, the hidden state at time t\n represents information about the sequence to the right of the current input:\n\n hbt = RNNbackward (xt , . . . xn ) (13.17)\n\n Here, the hidden state hbt represents all the information we have discerned about the\n sequence from t to the end of the sequence.\n bidirectional A bidirectional RNN (Schuster and Paliwal, 1997) combines two independent\n RNN\n RNNs, one where the input is processed from the start to the end, and the other from\n the end to the start. We then concatenate the two representations computed by the\n networks into a single vector that captures both the left and right contexts of an input\n at each point in time. Here we use either the semicolon ‚Äù;‚Äù or the equivalent symbol\n ‚äï to mean vector concatenation:\n\n ht = [h ft ; hbt ]\n = h ft ‚äï hbt (13.18)\n 13.5 ‚Ä¢ T HE LSTM 13\n\n Fig. 13.11 illustrates such a bidirectional network that concatenates the outputs of\n the forward and backward pass. Other simple ways to combine the forward and\n backward contexts include element-wise addition or multiplication. The output at\n each step in time thus captures information to the left and to the right of the current\n input. In sequence labeling applications, these concatenated outputs can serve as the\n basis for a local labeling decision.\n\n y1 y2 y3 yn\n\n concatenated\n outputs\n\n RNN 2\n\n RNN 1\n\n x1 x2 x3 xn\n\n the bidirectional state at that time point.\n\n Bidirectional RNNs have also proven to be quite effective for sequence classification. Recall from Fig. 13.8 that for sequence classification we used the final\n hidden state of the RNN as the input to a subsequent feedforward classifier. A difficulty with this approach is that the final state naturally reflects more information\n about the end of the sentence than its beginning. Bidirectional RNNs provide a simple solution to this problem; as shown in Fig. 13.12, we simply combine the final\n hidden states from the forward and backward passes (for example by concatenation)\n and use that as input for follow-on processing.\n\n In practice, it is quite difficult to train RNNs for tasks that require a network to make\n use of information distant from the current point of processing. Despite having access to the entire preceding sequence, the information encoded in hidden states tends\n to be fairly local, more relevant to the most recent parts of the input sequence and\n recent decisions. Yet distant information is critical to many language applications.\n Consider the following example in the context of language modeling.\n (13.19) The flights the airline was canceling were full.\n Assigning a high probability to was following airline is straightforward since airline\n provides a strong local context for the singular agreement. However, assigning an\n appropriate probability to were is quite difficult, not only because the plural flights\n is quite distant, but also because the singular noun airline is closer in the intervening\n14 C HAPTER 13 ‚Ä¢ RNN S AND LSTM S\n\n Softmax\n\n FFN\n\n ‚Üê ‚Üí\n h1 hn\n\n ‚Üê\n h1 RNN 2\n\n ‚Üí\n RNN 1 hn\n\n x1 x2 x3 xn\n\n the forward and backward passes are combined to represent the entire sequence. This combined representation serves as input to the subsequent classifier.\n\n context. Ideally, a network should be able to retain the distant information about\n plural flights until it is needed, while still processing the intermediate parts of the\n sequence correctly.\n One reason for the inability of RNNs to carry forward critical information is that\n the hidden layers, and, by extension, the weights that determine the values in the hidden layer, are being asked to perform two tasks simultaneously: provide information\n useful for the current decision, and updating and carrying forward information required for future decisions.\n A second difficulty with training RNNs arises from the need to backpropagate\n the error signal back through time. Recall from Section 13.1.2 that the hidden layer\n at time t contributes to the loss at the next time step since it takes part in that calculation. As a result, during the backward pass of training, the hidden layers are subject\n to repeated multiplications, as determined by the length of the sequence. A frequent\n result of this process is that the gradients are eventually driven to zero, a situation\n vanishing\n gradients called the vanishing gradients problem.\n To address these issues, more complex network architectures have been designed\n to explicitly manage the task of maintaining relevant context over time, by enabling\n the network to learn to forget information that is no longer needed and to remember\n information required for decisions still to come.\n The most commonly used such extension to RNNs is the long short-term memlong short-term\n memory ory (LSTM) network (Hochreiter and Schmidhuber, 1997). LSTMs divide the context management problem into two subproblems: removing information no longer\n needed from the context, and adding information likely to be needed for later decision making. The key to solving both problems is to learn how to manage this\n context rather than hard-coding a strategy into the architecture. LSTMs accomplish\n this by first adding an explicit context layer to the architecture (in addition to the\n usual recurrent hidden layer), and through the use of specialized neural units that\n make use of gates to control the flow of information into and out of the units that\n 13.5 ‚Ä¢ T HE LSTM 15\n\n comprise the network layers. These gates are implemented through the use of additional weights that operate sequentially on the input, and previous hidden layer, and\n previous context layers.\n The gates in an LSTM share a common design pattern; each consists of a feedforward layer, followed by a sigmoid activation function, followed by a pointwise\n multiplication with the layer being gated. The choice of the sigmoid as the activation\n function arises from its tendency to push its outputs to either 0 or 1. Combining this\n with a pointwise multiplication has an effect similar to that of a binary mask. Values\n in the layer being gated that align with values near 1 in the mask are passed through\n nearly unchanged; values corresponding to lower values are essentially erased.\nforget gate The first gate we‚Äôll consider is the forget gate. The purpose of this gate is\n to delete information from the context that is no longer needed. The forget gate\n computes a weighted sum of the previous state‚Äôs hidden layer and the current input and passes that through a sigmoid. This mask is then multiplied element-wise\n by the context vector to remove the information from context that is no longer required. Element-wise multiplication of two vectors (represented by the operator ,\n and sometimes called the Hadamard product) is the vector of the same dimension\n as the two input vectors, where each element i is the product of element i in the two\n input vectors:\n\n ft = œÉ (U f ht‚àí1 + W f xt ) (13.20)\n kt = ct‚àí1 ft (13.21)\n\n The next task is to compute the actual information we need to extract from the previous hidden state and current inputs‚Äîthe same basic computation we‚Äôve been using\n for all our recurrent networks.\n\n gt = tanh(Ug ht‚àí1 + Wg xt ) (13.22)\n\n add gate Next, we generate the mask for the add gate to select the information to add to the\n current context.\n\n it = œÉ (Ui ht‚àí1 + Wi xt ) (13.23)\n jt = gt it (13.24)\n\n Next, we add this to the modified context vector to get our new context vector.\n\n ct = jt + kt (13.25)\n\noutput gate The final gate we‚Äôll use is the output gate which is used to decide what information is required for the current hidden state (as opposed to what information needs\n to be preserved for future decisions).\n\n ot = œÉ (Uo ht‚àí1 + Wo xt ) (13.26)\n ht = ot tanh(ct ) (13.27)\n\n Fig. 13.13 illustrates the complete computation for a single LSTM unit. Given the\n appropriate weights for the various gates, an LSTM accepts as input the context\n layer, and hidden layer from the previous time step, along with the current input\n vector. It then generates updated context and hidden vectors as output.\n It is the hidden state, ht , that provides the output for the LSTM at each time step.\n This output can be used as the input to subsequent layers in a stacked RNN, or at the\n final layer of a network ht can be used to provide the final output of the LSTM.\n16 C HAPTER 13 ‚Ä¢ RNN S AND LSTM S\n\n ct-1 ct-1\n\n ‚¶ø\n f\n œÉ ct\n\n ct\n\n ht-1 ht-1\n tanh\n tanh\n\n g\n ht ht\n\n ‚¶ø\n ‚¶ø\n i\n œÉ\n\n xt xt\n\n œÉ\n o\n LSTM\n\ncurrent input, x, the previous hidden state, ht‚àí1 , and the previous context, ct‚àí1 . The outputs are a new hidden\nstate, ht and an updated context, ct .\n\n h ht ct ht\n\n a a\n\n g g\n LSTM\n z z\n Unit\n ‚åÉ ‚åÉ\n\n x ht-1 xt ct-1 ht-1 xt\n\n (a) (b) (c)\n\n and long short-term memory (LSTM).\n\n 13.5.1 Gated Units, Layers and Networks\n The neural units used in LSTMs are obviously much more complex than those used\n in basic feedforward networks. Fortunately, this complexity is encapsulated within\n the basic processing units, allowing us to maintain modularity and to easily experiment with different architectures. To see this, consider Fig. 13.14 which illustrates\n the inputs and outputs associated with each kind of unit.\n At the far left, (a) is the basic feedforward unit where a single set of weights and\n a single activation function determine its output, and when arranged in a layer there\n are no connections among the units in the layer. Next, (b) represents the unit in a\n simple recurrent network. Now there are two inputs and an additional set of weights\n to go with it. However, there is still a single activation function and output.\n The increased complexity of the LSTM units is encapsulated within the unit\n itself. The only additional external complexity for the LSTM over the basic recurrent\n unit (b) is the presence of the additional context vector as an input and output.\n This modularity is key to the power and widespread applicability of LSTM units.\n LSTM units (or other varieties, like GRUs) can be substituted into any of the network\n architectures described in Section 13.4. And, as with simple RNNs, multi-layered\n 13.6 ‚Ä¢ S UMMARY: C OMMON RNN NLP A RCHITECTURES 17\n\n networks making use of gated units can be unrolled into deep feedforward networks\n and trained in the usual fashion with backpropagation. In practice, therefore, LSTMs\n rather than RNNs have become the standard unit for any modern system that makes\n use of recurrent networks.\n\n We‚Äôve now introduced the RNN, seen advanced components like stacking multiple\n layers and using the LSTM version, and seen how the RNN can be applied to various\n tasks. Let‚Äôs take a moment to summarize the architectures for these applications.\n Fig. 13.15 shows the three architectures we‚Äôve discussed so far: sequence labeling, sequence classification, and language modeling. In sequence labeling (for\n example for part of speech tagging), we train a model to produce a label for each\n input word or token. In sequence classification, for example for sentiment analysis,\n we ignore the output for each token, and only take the value from the end of the\n sequence (and similarly the model‚Äôs training signal comes from backpropagation\n from that last token). In language modeling, we train the model to predict the next\n word at each token step. In the next section we‚Äôll introduce a fourth architecture, the\n encoder-decoder.\n\n y\n y1 y2 yn\n ‚Ä¶\n\n RNN RNN\n\n x1 x2 ‚Ä¶ xn x1 x2 ‚Ä¶ xn\n\n a) sequence labeling b) sequence classification\n\n y1 y2 ‚Ä¶ ym\n\n x2 x3 ‚Ä¶ xt Decoder RNN\n\n Context\n RNN\n Encoder RNN\n\n x1 x2 ‚Ä¶ xt-1\n x1 x2 ‚Ä¶ xn\n\n c) language modeling d) encoder-decoder\neach input token xi to an output token yi . In sequence classification we map the entire input sequence to a single\nclass. In language modeling we output the next token conditioned on previous tokens. In the encoder model we\nhave two separate RNN models, one of which maps from an input sequence x to an intermediate representation\nwe call the context, and a second of which maps from the context to an output sequence y.\n18 C HAPTER 13 ‚Ä¢ RNN S AND LSTM S\n\n In this section we introduce the encoder-decoder model, which is used when we are\n taking an input sequence and translating it to an output sequence that is of a different\n length than the input, and doesn‚Äôt align with it in a word-to-word way.\n Those of you who already read Chapter 12 will have already seen this model in\n the transformer architecture, and its application to machine translation, but we introduce this architecture again here for those who come to the concepts in a different\n order and are reading about RNNs before transformers.\n Recall that in the sequence labeling task, we have two sequences, but they are\n the same length (for example in part-of-speech tagging each token gets an associated\n tag), each input is associated with a specific output, and the labeling for that output\n takes mostly local information. Thus deciding whether a word is a verb or a noun,\n we look mostly at the word and the neighboring words.\n By contrast, encoder-decoder models are used especially for tasks like machine\n translation, where the input sequence and output sequence can have different lengths\n and the mapping between a token in the input and a token in the output can be very\n indirect (in some languages the verb appears at the beginning of the sentence; in\n other languages at the end). We introduced machine translation in Chapter 12, but\n for now we‚Äôll just point out that the mapping for a sentence in English to a sentence\n in Tagalog or Yoruba can have very different numbers of words, and the words can\n be in a very different order.\n encoder- Encoder-decoder networks, sometimes called sequence-to-sequence networks,\n decoder\n are models capable of generating contextually appropriate, arbitrary length, output\n sequences given an input sequence. Encoder-decoder networks have been applied\n to a very wide range of applications including summarization, question answering,\n and dialogue, but they are particularly popular for machine translation.\n The key idea underlying these networks is the use of an encoder network that\n takes an input sequence and creates a contextualized representation of it, often called\n the context. This representation is then passed to a decoder which generates a taskspecific output sequence. Fig. 13.16 illustrates the architecture.\n\n y1 y2 ‚Ä¶ ym\n\n Decoder\n\n Context\n\n Encoder\n\n x1 x2 ‚Ä¶ xn\n\n representations of the input, and may be used by the decoder in a variety of ways.\n\n Encoder-decoder networks consist of three conceptual components:\n 1. An encoder that accepts an input sequence, x1:n , and generates a corresponding sequence of contextualized representations, h1:n . LSTMs, convolutional\n networks, and transformers can all be employed as encoders.\n 2. A context vector, c, which is a function of h1:n , and conveys the essence of\n the input to the decoder.\n 13.7 ‚Ä¢ T HE E NCODER -D ECODER M ODEL WITH RNN S 19\n\n 3. A decoder, which accepts c as input and generates an arbitrary length sequence of hidden states h1:m , from which a corresponding sequence of output\n states y1:m , can be obtained. Just as with encoders, decoders can be realized\n by any kind of sequence architecture.\n In this section we‚Äôll describe an encoder-decoder network based on a pair of\n RNNs, but we‚Äôll see in Chapter 12 how to apply them to transformers as well. We‚Äôll\n build up the equations for encoder-decoder models by starting with the conditional\n RNN language model p(y), the probability of a sequence y.\n Recall that in any language model, we can break down the probability as follows:\n\n p(y) = p(y1 )p(y2 |y1 )p(y3 |y1 , y2 ) . . . p(ym |y1 , ..., ym‚àí1 ) (13.28)\n\n In RNN language modeling, at a particular time t, we pass the prefix of t ‚àí 1\n tokens through the language model, using forward inference to produce a sequence\n of hidden states, ending with the hidden state corresponding to the last word of\n the prefix. We then use the final hidden state of the prefix as our starting point to\n generate the next token.\n More formally, if g is an activation function like tanh or ReLU, a function of\n the input at time t and the hidden state at time t ‚àí 1, and the softmax is over the\n set of possible vocabulary items, then at time t the output yt and hidden state ht are\n computed as:\n\n ht = g(ht‚àí1 , xt ) (13.29)\n yÃÇt = softmax(ht ) (13.30)\n\n We only have to make one slight change to turn this language model with autoregressive generation into an encoder-decoder model that is a translation model\n that can translate from a source text in one language to a target text in a second:\n sentence\nseparation add a sentence separation marker at the end of the source text, and then simply\n concatenate the target text.\n Let‚Äôs use <s> for our sentence separator token, and let‚Äôs think about translating\n an English source text (‚Äúthe green witch arrived‚Äù), to a Spanish sentence (‚ÄúllegoÃÅ\n la bruja verde‚Äù (which can be glossed word-by-word as ‚Äòarrived the witch green‚Äô).\n We could also illustrate encoder-decoder models with a question-answer pair, or a\n text-summarization pair.\n Let‚Äôs use x to refer to the source text (in this case in English) plus the separator\n token <s>, and y to refer to the target text y (in this case in Spanish). Then an\n encoder-decoder model computes the probability p(y|x) as follows:\n\n p(y|x) = p(y1 |x)p(y2 |y1 , x)p(y3 |y1 , y2 , x) . . . p(ym |y1 , ..., ym‚àí1 , x) (13.31)\n\n Fig. 13.17 shows the setup for a simplified version of the encoder-decoder model\n (we‚Äôll see the full model, which requires the new concept of attention, in the next\n section).\n Fig. 13.17 shows an English source text (‚Äúthe green witch arrived‚Äù), a sentence\n separator token (<s>, and a Spanish target text (‚ÄúllegoÃÅ la bruja verde‚Äù). To translate a source text, we run it through the network performing forward inference to\n generate hidden states until we get to the end of the source. Then we begin autoregressive generation, asking for a word in the context of the hidden layer from the\n end of the source input as well as the end-of-sentence marker. Subsequent words\n are conditioned on the previous hidden state and the embedding for the last word\n generated.\n20 C HAPTER 13 ‚Ä¢ RNN S AND LSTM S\n\n Target Text\n\n lleg√≥ la bruja verde </s>\n\n softmax (output of source is ignored)\n\n hidden hn\n layer(s)\n\n embedding\n layer\n\n the green witch arrived <s> lleg√≥ la bruja verde\n\n Separator\n Source Text\n\nand the decoder uses context information from the encoder‚Äôs last hidden state.\n\n Let‚Äôs formalize and generalize this model a bit in Fig. 13.18. (To help keep\n things straight, we‚Äôll use the superscripts e and d where needed to distinguish the\n hidden states of the encoder and the decoder.) The elements of the network on the\n left process the input sequence x and comprise the encoder. While our simplified\n figure shows only a single network layer for the encoder, stacked architectures are\n the norm, where the output states from the top layer of the stack are taken as the\n final representation, and the encoder consists of stacked biLSTMs where the hidden\n states from top layers from the forward and backward passes are concatenated to\n provide the contextualized representations for each time step.\n\n Decoder\n\n y1 y2 y3 y4 </s>\n (output is ignored during encoding)\n softmax\n\n he1 he2 he3 hd hd hd hd hd\n hhn n = c = h 0\n e d\n hidden 1 2 3 4 m\n layer(s)\n\n embedding\n layer\n\n x1 x2 x3 ‚Ä¶ xn <s> y1 y2 y3 ‚Ä¶ ym-1\n\n Encoder\n\nencoder-decoder architecture. The final hidden state of the encoder RNN, hen , serves as the context for the\ndecoder in its role as hd0 in the decoder RNN, and is also made available to each decoder hidden state.\n\n The entire purpose of the encoder is to generate a contextualized representation\n of the input. This representation is embodied in the final hidden state of the encoder,\n hen . This representation, also called c for context, is then passed to the decoder.\n The simplest version of the decoder network would take this state and use it just\n to initialize the first hidden state of the decoder; the first decoder RNN cell would\n 13.7 ‚Ä¢ T HE E NCODER -D ECODER M ODEL WITH RNN S 21\n\n use c as its prior hidden state hd0 . The decoder would then autoregressively generate\n a sequence of outputs, an element at a time, until an end-of-sequence marker is\n generated. Each hidden state is conditioned on the previous hidden state and the\n output generated in the previous state.\n As Fig. 13.18 shows, we do something more complex: we make the context\n vector c available to more than just the first decoder hidden state, to ensure that the\n influence of the context vector, c, doesn‚Äôt wane as the output sequence is generated.\n We do this by adding c as a parameter to the computation of the current hidden state.\n using the following equation:\n\n htd = g(yÃÇt‚àí1 , ht‚àí1\n d\n , c) (13.32)\n\n Now we‚Äôre ready to see the full equations for this version of the decoder in the basic\n encoder-decoder model, with context available at each decoding timestep. Recall\n that g is a stand-in for some flavor of RNN and yÃÇt‚àí1 is the embedding for the output\n sampled from the softmax at the previous step:\n\n c = hen\n hd0 = c\n htd = g(yÃÇt‚àí1 , ht‚àí1\n d\n , c)\n yÃÇt = softmax(htd ) (13.33)\n\n Thus yÃÇt is a vector of probabilities over the vocabulary, representing the probability\n of each word occurring at time t. To generate text, we sample from this distribution\n yÃÇt . For example, the greedy choice is simply to choose the most probable word to\n generate at each timestep. We discussed other sampling methods in Section ??.\n\n 13.7.1 Training the Encoder-Decoder Model\n Encoder-decoder architectures are trained end-to-end. Each training example is a\n tuple of paired strings, a source and a target. Concatenated with a separator token,\n these source-target pairs can now serve as training data.\n For MT, the training data typically consists of sets of sentences and their translations. These can be drawn from standard datasets of aligned sentence pairs, as we‚Äôll\n discuss in Section ??. Once we have a training set, the training itself proceeds as\n with any RNN-based language model. The network is given the source text and then\n starting with the separator token is trained autoregressively to predict the next word,\n as shown in Fig. 13.19.\n Note the differences between training (Fig. 13.19) and inference (Fig. 13.17)\n with respect to the outputs at each time step. The decoder during inference uses its\n own estimated output yÀÜt as the input for the next time step xt+1 . Thus the decoder will\n tend to deviate more and more from the gold target sentence as it keeps generating\nteacher forcing more tokens. In training, therefore, it is more common to use teacher forcing in the\n decoder. Teacher forcing means that we force the system to use the gold target token\n from training as the next input xt+1 , rather than allowing it to rely on the (possibly\n erroneous) decoder output yÀÜt . This speeds up training.\n22 C HAPTER 13 ‚Ä¢ RNN S AND LSTM S\n\n Decoder\n\n gold\n lleg√≥ la bruja verde </s> answers\n y1 y2 y3 y4 y5\n\n Total loss is the average L1 = L2 = L3 = L4 = L5 =\n cross-entropy loss per -log P(y1) -log P(y2) -log P(y3) -log P(y4) -log P(y5) per-word\n target word: loss\n\n softmax\n\n hidden\n layer(s)\n\n embedding\n layer\n x1 x2 x3 x4\n the green witch arrived <s> lleg√≥ la bruja verde\n\n Encoder\n\ndecoder we usually don‚Äôt propagate the model‚Äôs softmax outputs yÃÇt , but use teacher forcing to force each input\nto the correct gold value for training. We compute the softmax output distribution over yÃÇ in the decoder in order\nto compute the loss at each token, which can then be averaged to compute a loss for the sentence. This loss is\nthen propagated through the decoder parameters and the encoder parameters.\n\n The simplicity of the encoder-decoder model is its clean separation of the encoder‚Äî\n which builds a representation of the source text‚Äîfrom the decoder, which uses this\n context to generate a target text. In the model as we‚Äôve described it so far, this\n context vector is hn , the hidden state of the last (nth ) time step of the source text.\n This final hidden state is thus acting as a bottleneck: it must represent absolutely\n everything about the meaning of the source text, since the only thing the decoder\n knows about the source text is what‚Äôs in this context vector (Fig. 13.20). Information\n at the beginning of the sentence, especially for long sentences, may not be equally\n well represented in the context vector.\n\n Encoder bottleneck Decoder\n bottleneck\n\n the information from the entire source sentence to pass through this representational bottleneck.\n\n attention The attention mechanism is a solution to the bottleneck problem, a way of\n mechanism\n allowing the decoder to get information from all the hidden states of the encoder,\n not just the last hidden state.\n In the attention mechanism, as in the vanilla encoder-decoder model, the context\n vector c is a single vector that is a function of the hidden states of the encoder. But\n instead of being taken from the last hidden state, it‚Äôs a weighted average of all the\n 13.8 ‚Ä¢ ATTENTION 23\n\n hidden states of the encoder. And this weighted average is also informed by part of\n the decoder state as well, the state of the decoder right before the current token i.\n That is, ci = f (he1 . . . hen , hdi‚àí1 ). The weights focus on (‚Äòattend to‚Äô) a particular part of\n the source text that is relevant for the token i that the decoder is currently producing.\n Attention thus replaces the static context vector with one that is dynamically derived\n from the encoder hidden states, but also informed by and hence different for each\n token in decoding.\n This context vector, ci , is generated anew with each decoding step i and takes\n all of the encoder hidden states into account in its derivation. We then make this\n context available during decoding by conditioning the computation of the current\n decoder hidden state on it (along with the prior hidden state and the previous output\n generated by the decoder), as we see in this equation (and Fig. 13.21):\n\n hdi = g(yÃÇi‚àí1 , hdi‚àí1 , ci ) (13.34)\n\n y1 y2 yi\n\n hd1 hd2 hdi ‚Ä¶\n ‚Ä¶\n\n c1 c2 ci\n\n different, dynamic, context, which is a function of all the encoder hidden states.\n\n The first step in computing ci is to compute how much to focus on each encoder\n state, how relevant each encoder state is to the decoder state captured in hdi‚àí1 . We\n capture relevance by computing‚Äî at each state i during decoding‚Äîa score(hdi‚àí1 , hej )\n for each encoder state j.\ndot-product The simplest such score, called dot-product attention, implements relevance as\n attention\n similarity: measuring how similar the decoder hidden state is to an encoder hidden\n state, by computing the dot product between them:\n\n score(hdi‚àí1 , hej ) = hdi‚àí1 ¬∑ hej (13.35)\n\n The score that results from this dot product is a scalar that reflects the degree of\n similarity between the two vectors. The vector of these scores across all the encoder\n hidden states gives us the relevance of each encoder state to the current step of the\n decoder.\n To make use of these scores, we‚Äôll normalize them with a softmax to create a\n vector of weights, Œ±i j , that tells us the proportional relevance of each encoder hidden\n state j to the prior hidden decoder state, hdi‚àí1 .\n\n Œ±i j = softmax(score(hdi‚àí1 , hej ))\n exp(score(hdi‚àí1 , hej )\n = P d e\n (13.36)\n k exp(score(hi‚àí1 , hk ))\n\n Finally, given the distribution in Œ±, we can compute a fixed-length context vector for\n the current decoder state by taking a weighted average over all the encoder hidden\n states.\n X\n ci = Œ±i j hej (13.37)\n j\n24 C HAPTER 13 ‚Ä¢ RNN S AND LSTM S\n\n With this, we finally have a fixed-length context vector that takes into account\n information from the entire encoder state that is dynamically updated to reflect the\n needs of the decoder at each step of decoding. Fig. 13.22 illustrates an encoderdecoder network with attention, focusing on the computation of one context vector\n ci .\n\n Decoder\n X\n ci\n <latexit sha1_base64=\"TNdNmv/RIlrhPa6LgQyjjQLqyBA=\">AAACAnicdVDLSsNAFJ3UV62vqCtxM1gEVyHpI9Vd0Y3LCvYBTQyT6bSddvJgZiKUUNz4K25cKOLWr3Dn3zhpK6jogQuHc+7l3nv8mFEhTfNDyy0tr6yu5dcLG5tb2zv67l5LRAnHpIkjFvGOjwRhNCRNSSUjnZgTFPiMtP3xRea3bwkXNAqv5SQmboAGIe1TjKSSPP3AEUngjVIHsXiIvJSOpnB4Q7zR1NOLpmGaVbtqQdOwLbtk24qY5Yp9VoOWsjIUwQINT393ehFOAhJKzJAQXcuMpZsiLilmZFpwEkFihMdoQLqKhiggwk1nL0zhsVJ6sB9xVaGEM/X7RIoCISaBrzoDJIfit5eJf3ndRPZP3ZSGcSJJiOeL+gmDMoJZHrBHOcGSTRRBmFN1K8RDxBGWKrWCCuHrU/g/aZUMyzbKV5Vi/XwRRx4cgiNwAixQA3VwCRqgCTC4Aw/gCTxr99qj9qK9zltz2mJmH/yA9vYJSymYCA==</latexit>\n\n ‚Üµij hej\n j yi-1 yi\n attention\n .4 .3 .1 .2\n weights\n ‚Üµij\n hdi 1 ¬∑ hej\n <latexit sha1_base64=\"y8s4mGdpwrGrBnuSR+p1gJJXYdo=\">AAAB/nicdVDJSgNBEO2JW4zbqHjy0hgEL4YeJyQBL0EvHiOYBbIMPT09mTY9C909QhgC/ooXD4p49Tu8+Td2FkFFHxQ83quiqp6bcCYVQh9Gbml5ZXUtv17Y2Nza3jF391oyTgWhTRLzWHRcLClnEW0qpjjtJILi0OW07Y4up377jgrJ4uhGjRPaD/EwYj4jWGnJMQ+Cgedk7NSa9IgXq955MKDOrWMWUQnNAFGpYtfsakUTZNtWGUFrYRXBAg3HfO95MUlDGinCsZRdCyWqn2GhGOF0UuilkiaYjPCQdjWNcEhlP5udP4HHWvGgHwtdkYIz9ftEhkMpx6GrO0OsAvnbm4p/ed1U+bV+xqIkVTQi80V+yqGK4TQL6DFBieJjTTARTN8KSYAFJkonVtAhfH0K/yets5JVKdnX5WL9YhFHHhyCI3ACLFAFdXAFGqAJCMjAA3gCz8a98Wi8GK/z1pyxmNkHP2C8fQICDpWK</latexit>\n\n hidden he1 he2 he3 hen ‚Ä¶ ci-1\n hdi-1 hdi ‚Ä¶\n layer(s)\n\n ci\n x1 x2 x3 ‚Ä¶ xn\n yi-2 yi-1\n Encoder\nThe context value ci is one of the inputs to the computation of hdi . It is computed by taking the weighted sum\nof all the encoder hidden states, each weighted by their dot product with the prior decoder hidden state hdi‚àí1 .\n\n It‚Äôs also possible to create more sophisticated scoring functions for attention\n models. Instead of simple dot product attention, we can get a more powerful function\n that computes the relevance of each encoder hidden state to the decoder hidden state\n by parameterizing the score with its own set of weights, Ws .\n score(hdi‚àí1 , hej ) = hdi‚àí1 Ws hej (13.38)\n The weights Ws , which are then trained during normal end-to-end training, give the\n network the ability to learn which aspects of similarity between the decoder and\n encoder states are important to the current application. This bilinear model also\n allows the encoder and decoder to use different dimensional vectors, whereas the\n simple dot-product attention requires that the encoder and decoder hidden states\n have the same dimensionality.\n We‚Äôll return to the concept of attention when we define the transformer architecture in Chapter 8, which is based on a slight modification of attention called\n self-attention.\n\n This chapter has introduced the concepts of recurrent neural networks and how they\n can be applied to language problems. Here‚Äôs a summary of the main points that we\n covered:\n ‚Ä¢ In simple Recurrent Neural Networks sequences are processed one element at\n a time, with the output of each neural unit at time t based both on the current\n input at t and the hidden layer from time t ‚àí 1.\n H ISTORICAL N OTES 25\n\n ‚Ä¢ RNNs can be trained with a straightforward extension of the backpropagation\n algorithm, known as backpropagation through time (BPTT).\n ‚Ä¢ Simple recurrent networks fail on long inputs because of problems like vanishing gradients; instead modern systems use more complex gated architectures such as LSTMs that explicitly decide what to remember and forget in\n their hidden and context layers.\n ‚Ä¢ Common language-based applications for RNNs include:\n ‚Äì Probabilistic language modeling: assigning a probability to a sequence,\n or to the next element of a sequence given the preceding words.\n ‚Äì Auto-regressive generation using a trained language model.\n ‚Äì Sequence labeling like part-of-speech tagging, where each element of a\n sequence is assigned a label.\n ‚Äì Sequence classification, where an entire text is assigned to a category, as\n in spam detection, sentiment analysis or topic classification.\n ‚Äì Encoder-decoder architectures, where an input is mapped to an output\n of different length and alignment.\n\nHistorical Notes\n Influential investigations of RNNs were conducted in the context of the Parallel Distributed Processing (PDP) group at UC San Diego in the 1980‚Äôs. Much of this work\n was directed at human cognitive modeling rather than practical NLP applications\n (Rumelhart and McClelland 1986, McClelland and Rumelhart 1986). Models using\n recurrence at the hidden layer in a feedforward network (Elman networks) were introduced by Elman (1990). Similar architectures were investigated by Jordan (1986)\n with a recurrence from the output layer, and Mathis and Mozer (1995) with the\n addition of a recurrent context layer prior to the hidden layer. The possibility of\n unrolling a recurrent network into an equivalent feedforward network is discussed\n in (Rumelhart and McClelland, 1986).\n In parallel with work in cognitive modeling, RNNs were investigated extensively\n in the continuous domain in the signal processing and speech communities (Giles\n et al. 1994, Robinson et al. 1996). Schuster and Paliwal (1997) introduced bidirectional RNNs and described results on the TIMIT phoneme transcription task.\n While theoretically interesting, the difficulty with training RNNs and managing context over long sequences impeded progress on practical applications. This\n situation changed with the introduction of LSTMs in Hochreiter and Schmidhuber\n (1997) and Gers et al. (2000). Impressive performance gains were demonstrated\n on tasks at the boundary of signal processing and language processing including\n phoneme recognition (Graves and Schmidhuber, 2005), handwriting recognition\n (Graves et al., 2007) and most significantly speech recognition (Graves et al., 2013).\n Interest in applying neural networks to practical NLP problems surged with the\n work of Collobert and Weston (2008) and Collobert et al. (2011). These efforts made\n use of learned word embeddings, convolutional networks, and end-to-end training.\n They demonstrated near state-of-the-art performance on a number of standard shared\n tasks including part-of-speech tagging, chunking, named entity recognition and semantic role labeling without the use of hand-engineered features.\n Approaches that married LSTMs with pretrained collections of word-embeddings\n based on word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014)\n26 C HAPTER 13 ‚Ä¢ RNN S AND LSTM S\n\n quickly came to dominate many common tasks: part-of-speech tagging (Ling et al.,\n 2015), syntactic chunking (S√∏gaard and Goldberg, 2016), named entity recognition\n (Chiu and Nichols, 2016; Ma and Hovy, 2016), opinion mining (Irsoy and Cardie,\n 2014), semantic role labeling (Zhou and Xu, 2015) and AMR parsing (Foland and\n Martin, 2016). As with the earlier surge of progress involving statistical machine\n learning, these advances were made possible by the availability of training data provided by CONLL, SemEval, and other shared tasks, as well as shared resources such\n as Ontonotes (Pradhan et al., 2007), and PropBank (Palmer et al., 2005).\n The modern neural encoder-decoder approach was pioneered by Kalchbrenner\n and Blunsom (2013), who used a CNN encoder and an RNN decoder. Cho et al.\n (2014) (who coined the name ‚Äúencoder-decoder‚Äù) and Sutskever et al. (2014) then\n showed how to use extended RNNs for both encoder and decoder. The idea that a\n generative decoder should take as input a soft weighting of the inputs, the central\n idea of attention, was first developed by Graves (2013) in the context of handwriting\n recognition. Bahdanau et al. (2015) extended the idea, named it ‚Äúattention‚Äù and\n applied it to MT.\n Historical Notes 27\n\nBahdanau, D., K. H. Cho, and Y. Bengio. 2015. Neural ma- Mikolov, T., K. Chen, G. S. Corrado, and J. Dean. 2013. Efchine translation by jointly learning to align and translate. ficient estimation of word representations in vector space.\n ICLR 2015. ICLR 2013.\nChiu, J. P. C. and E. Nichols. 2016. Named entity recognition Mikolov, T., M. KarafiaÃÅt, L. Burget, J. CÃåernockyÃÄ, and\n with bidirectional LSTM-CNNs. TACL, 4:357‚Äì370. S. Khudanpur. 2010. Recurrent neural network based lan-\nCho, K., B. van MerrieÃànboer, C. Gulcehre, D. Bahdanau, guage model. INTERSPEECH.\n F. Bougares, H. Schwenk, and Y. Bengio. 2014. Learn- Miller, G. A. and J. A. Selfridge. 1950. Verbal context and\n ing phrase representations using RNN encoder‚Äìdecoder the recall of meaningful material. American Journal of\n for statistical machine translation. EMNLP. Psychology, 63:176‚Äì185.\nCollobert, R. and J. Weston. 2008. A unified architecture for Palmer, M., P. Kingsbury, and D. Gildea. 2005. The proposinatural language processing: Deep neural networks with tion bank: An annotated corpus of semantic roles. Commultitask learning. ICML. putational Linguistics, 31(1):71‚Äì106.\nCollobert, R., J. Weston, L. Bottou, M. Karlen, Pennington, J., R. Socher, and C. D. Manning. 2014. GloVe:\n K. Kavukcuoglu, and P. Kuksa. 2011. Natural language Global vectors for word representation. EMNLP.\n processing (almost) from scratch. JMLR, 12:2493‚Äì2537.\n Pradhan, S., E. H. Hovy, M. P. Marcus, M. Palmer, L. A.\nElman, J. L. 1990. Finding structure in time. Cognitive sci- Ramshaw, and R. M. Weischedel. 2007. Ontonotes: a\n ence, 14(2):179‚Äì211. unified relational semantic representation. Int. J. Seman-\nFoland, W. and J. H. Martin. 2016. CU-NLP at SemEval- tic Computing, 1(4):405‚Äì419.\n 2016 task 8: AMR parsing using LSTM-based recurrent Robinson, T., M. Hochberg, and S. Renals. 1996. The use\n neural networks. SemEval-2016. of recurrent neural networks in continuous speech recog-\nGers, F. A., J. Schmidhuber, and F. Cummins. 2000. Learn- nition. In C.-H. Lee, F. K. Soong, and K. K. Paliwal,\n ing to forget: Continual prediction with lstm. Neural eds, Automatic speech and speaker recognition, 233‚Äì258.\n computation, 12(10):2451‚Äì2471. Springer.\nGiles, C. L., G. M. Kuhn, and R. J. Williams. 1994. Dynamic Rumelhart, D. E., G. E. Hinton, and R. J. Williams. 1986.\n recurrent neural networks: Theory and applications. Learning internal representations by error propagation. In\n IEEE Trans. Neural Netw. Learning Syst., 5(2):153‚Äì156. D. E. Rumelhart and J. L. McClelland, eds, Parallel Dis-\nGraves, A. 2013. Generating sequences with recurrent neural tributed Processing, volume 2, 318‚Äì362. MIT Press.\n networks. ArXiv. Rumelhart, D. E. and J. L. McClelland, eds. 1986. Parallel\nGraves, A., S. FernaÃÅndez, M. Liwicki, H. Bunke, and Distributed Processing: Explorations in the Microstruc-\nJ. Schmidhuber. 2007. Unconstrained on-line handwrit- ture of Cognition, volume 1: Foundations. MIT Press.\n ing recognition with recurrent neural networks. NeurIPS. Schuster, M. and K. K. Paliwal. 1997. Bidirectional recurrent\nGraves, A., A.-r. Mohamed, and G. Hinton. 2013. neural networks. IEEE Transactions on Signal Process-\nSpeech recognition with deep recurrent neural networks. ing, 45:2673‚Äì2681.\n ICASSP. Shannon, C. E. 1951. Prediction and entropy of printed En-\nGraves, A. and J. Schmidhuber. 2005. Framewise phoneme glish. Bell System Technical Journal, 30:50‚Äì64.\n classification with bidirectional LSTM and other neural S√∏gaard, A. and Y. Goldberg. 2016. Deep multi-task learning\n network architectures. Neural Networks, 18(5-6):602‚Äì with low level tasks supervised at lower layers. ACL.\n 610.\n Sutskever, I., O. Vinyals, and Q. V. Le. 2014. Sequence to\nHochreiter, S. and J. Schmidhuber. 1997. Long short-term sequence learning with neural networks. NeurIPS.\n memory. Neural Computation, 9(8):1735‚Äì1780.\n Werbos, P. 1974. Beyond regression: new tools for predic-\nIrsoy, O. and C. Cardie. 2014. Opinion mining with deep tion and analysis in the behavioral sciences. Ph.D. thesis,\n recurrent neural networks. EMNLP. Harvard University.\nJordan, M. 1986. Serial order: A parallel distributed process-\nWerbos, P. J. 1990. Backpropagation through time: what\n ing approach. Technical Report ICS Report 8604, Univerit does and how to do it. Proceedings of the IEEE,\n sity of California, San Diego.\n 78(10):1550‚Äì1560.\nKalchbrenner, N. and P. Blunsom. 2013. Recurrent continu-\nZhou, J. and W. Xu. 2015. End-to-end learning of semantic\n ous translation models. EMNLP.\n role labeling using recurrent neural networks. ACL.\nLing, W., C. Dyer, A. W. Black, I. Trancoso, R. Fermandez,\n S. Amir, L. Marujo, and T. Luƒ±ÃÅs. 2015. Finding function\n in form: Compositional character models for open vocabulary word representation. EMNLP.\nMa, X. and E. H. Hovy. 2016. End-to-end sequence labeling\n via bi-directional LSTM-CNNs-CRF. ACL.\nMathis, D. A. and M. C. Mozer. 1995. On the computational\n utility of consciousness. NeurIPS. MIT Press.\nMcClelland, J. L. and D. E. Rumelhart, eds. 1986. Parallel\n Distributed Processing: Explorations in the Microstructure of Cognition, volume 2: Psychological and Biological Models. MIT Press.\n",
    "file_path": "/Users/colinsidberry/nlp-textbook-rag/data/extracted_text/13.RNNs and LSTMs.txt",
    "file_size_kb": 70.74
  },
  {
    "id": "57d780368aae45f3",
    "source": "nlp_textbook",
    "chapter": "Phonetics and Speech Feature 14 Extraction",
    "filename": "14.Phonetics and Speech Feature Extraction.txt",
    "content": "rights reserved. Draft of August 24, 2025.\n\nCHAPTER\n\n Phonetics and Speech Feature\n14 Extraction\n The characters that make up the texts we‚Äôve been discussing in this book aren‚Äôt just\n random symbols. They are also an amazing scientific invention: a theoretical model\n of the elements that make up human speech.\n The earliest writing systems we know of (Sumerian, Chinese, Mayan) were\n mainly logographic: one symbol representing a whole word. But from the earliest stages we can find, some symbols were also used to represent the sounds\n that made up words. The cuneiform sign to the right pronounced ba and meaning ‚Äúration‚Äù in Sumerian could also\n function purely as the sound /ba/. The earliest Chinese characters we have, carved into bones for divination, similarly\n contain phonetic elements. Purely sound-based writing systems, whether syllabic\n (like Japanese hiragana), alphabetic (like the Roman alphabet), or consonantal (like\n Semitic writing systems), trace back to these early logo-syllabic systems, often as\n two cultures came together. Thus, the Arabic, Aramaic, Hebrew, Greek, and Roman\n systems all derive from a West Semitic script that is presumed to have been modified\n by Western Semitic mercenaries from a cursive form of Egyptian hieroglyphs. The\n Japanese syllabaries were modified from a cursive form of Chinese phonetic characters, which themselves were used in Chinese to phonetically represent the Sanskrit\n in the Buddhist scriptures that came to China in the Tang dynasty.\n This implicit idea that the spoken word is composed of smaller units of speech\n underlies algorithms for both speech recognition (transcribing waveforms into text)\n and text-to-speech (converting text into waveforms). In this chapter we give a comphonetics putational perspective on phonetics, the study of the speech sounds used in the\n languages of the world, how they are produced in the human vocal tract, how they\n are realized acoustically, and how they can be digitized and processed.\n\n A letter like ‚Äòp‚Äô or ‚Äòa‚Äô is already a useful model of the sounds of human speech,\n and indeed we‚Äôll see in Chapter 15 how to map between letters and waveforms.\n Nonetheless, it is helpful to represent sounds slightly more abstractly. We‚Äôll reprephone sent the pronunciation of a word as a string of phones, which are speech sounds,\n each represented with symbols adapted from the Roman alphabet.\n The standard phonetic representation for transcribing the world‚Äôs languages is\n IPA the International Phonetic Alphabet (IPA), an evolving standard first developed in\n 1888, But in this chapter we‚Äôll instead represent phones with the ARPAbet (Shoup,\n 1980), a simple phonetic alphabet (Fig. 14.1) that conveniently uses ASCII symbols\n to represent an American-English subset of the IPA.\n Many of the IPA and ARPAbet symbols are equivalent to familiar Roman letters. So, for example, the ARPAbet phone [p] represents the consonant sound at the\n2 C HAPTER 14 ‚Ä¢ P HONETICS AND S PEECH F EATURE E XTRACTION\n\nARPAbet IPA ARPAbet ARPAbet IPA ARPAbet\nSymbol Symbol Word Transcription Symbol Symbol Word Transcription\n[p] [p] parsley [p aa r s l iy] [iy] [i] lily [l ih l iy]\n[t] [t] tea [t iy] [ih] [I] lily [l ih l iy]\n[k] [k] cook [k uh k] [ey] [eI] daisy [d ey z iy]\n[b] [b] bay [b ey] [eh] [E] pen [p eh n]\n[d] [d] dill [d ih l] [ae] [√¶] aster [ae s t axr]\n[g] [g] garlic [g aa r l ix k] [aa] [A] poppy [p aa p iy]\n[m] [m] mint [m ih n t] [ao] [O] orchid [ao r k ix d]\n[n] [n] nutmeg [n ah t m eh g] [uh] [U] wood [w uh d]\n[ng] [N] baking [b ey k ix ng] [ow] [oU] lotus [l ow dx ax s]\n[f] [f] flour [f l aw axr] [uw] [u] tulip [t uw l ix p]\n[v] [v] clove [k l ow v] [ah] [2] butter [b ah dx axr]\n[th] [T] thick [th ih k] [er] [√á] bird [b er d]\n[dh] [D] those [dh ow z] [ay] [aI] iris [ay r ix s]\n[s] [s] soup [s uw p] [aw] [aU] flower [f l aw axr]\n[z] [z] eggs [eh g z] [oy] [oI] soil [s oy l]\n[sh] [S] squash [s k w aa sh] [ax] [@] pita [p iy t ax]\n[zh] [Z] ambrosia [ae m b r ow zh ax]\n[ch] [tS] cherry [ch eh r iy]\n[jh] [dZ] jar [jh aa r]\n[l] [l] licorice [l ih k axr ix sh]\n[w] [w] kiwi [k iy w iy]\n[r] [r] rice [r ay s]\n[y] [j] yellow [y eh l ow]\n[h] [h] honey [h ah n iy]\n\n beginning of platypus, puma, and plantain, the middle of leopard, or the end of antelope. In general, however, the mapping between the letters of English orthography\n and phones is relatively opaque; a single letter can represent very different sounds\n in different contexts. The English letter c corresponds to phone [k] in cougar [k uw\n g axr], but phone [s] in cell [s eh l]. Besides appearing as c and k, the phone [k] can\n appear as part of x (fox [f aa k s]), as ck (jackal [jh ae k el]) and as cc (raccoon [r ae\n k uw n]). Many other languages, for example, Spanish, are much more transparent\n in their sound-orthography mapping than English.\n There are a wide variety of phonetic resources for phonetic transcription. Onpronunciation\n dictionary line pronunciation dictionaries give phonetic transcriptions for words. The LDC\n distributes pronunciation lexicons for Egyptian Arabic, Dutch, English, German,\n Japanese, Korean, Mandarin, and Spanish. For English, the CELEX dictionary\n (Baayen et al., 1995) has pronunciations for 160,595 wordforms, with syllabification, stress, and morphological and part-of-speech information. The open-source\n CMU Pronouncing Dictionary (CMU, 1993) has pronunciations for about 134,000\n wordforms, while the fine-grained 110,000 word UNISYN dictionary (Fitt, 2002),\n freely available for research purposes, gives syllabifications, stress, and also pronunciations for dozens of dialects of English.\n Another useful resource is a phonetically annotated corpus, in which a collection of waveforms is hand-labeled with the corresponding string of phones. The\n TIMIT corpus (NIST, 1990), originally a joint project between Texas Instruments\n (TI), MIT, and SRI, is a corpus of 6300 read sentences, with 10 sentences each from\n 14.2 ‚Ä¢ A RTICULATORY P HONETICS 3\n\n 630 speakers. The 6300 sentences were drawn from a set of 2342 sentences, some\n selected to have particular dialect shibboleths, others to maximize phonetic diphone\n coverage. Each sentence in the corpus was phonetically hand-labeled, the sequence\n of phones was automatically aligned with the sentence wavefile, and then the automatic phone boundaries were manually hand-corrected (Seneff and Zue, 1988).\n time-aligned\n transcription The result is a time-aligned transcription: a transcription in which each phone is\n associated with a start and end time in the waveform, like the example in Fig. 14.2.\n\n she had your dark suit in greasy wash water all year\n sh iy hv ae dcl jh axr dcl d aa r kcl s ux q en gcl g r iy s ix w aa sh q w aa dx axr q aa l y ix axr\nto [q], and flap of [t] in water. The TIMIT corpus also includes time-alignments (not shown).\n\n The Switchboard Transcription Project phonetically annotated corpus consists\n of 3.5 hours of sentences extracted from the Switchboard corpus (Greenberg et al.,\n 1996), together with transcriptions time-aligned at the syllable level. Figure 14.3\n shows an example .\n\n dh er k aa n ax v ih m b ix t w iy n r ay n aw\n right now. Note vowel reduction in they‚Äôre and of, coda deletion in kind and right, and resyllabification (the [v] of of attaches as the onset of in). Time is given in number of seconds\n from the beginning of sentence to the start of each syllable.\n\n The Buckeye corpus (Pitt et al. 2007, Pitt et al. 2005) is a phonetically transcribed corpus of spontaneous American speech, containing about 300,000 words\n from 40 talkers. Phonetically transcribed corpora are also available for other languages, including the Kiel corpus of German and Mandarin corpora transcribed by\n the Chinese Academy of Social Sciences (Li et al., 2000).\n\n articulatory\n phonetics Articulatory phonetics is the study of how these phones are produced as the various\n organs in the mouth, throat, and nose modify the airflow from the lungs.\n\n The Vocal Organs\n of air. Humans produce most sounds in spoken languages by expelling air from the\n lungs through the windpipe (technically, the trachea) and then out the mouth or\n nose. As it passes through the trachea, the air passes through the larynx, commonly\n known as the Adam‚Äôs apple or voice box. The larynx contains two small folds of\n muscle, the vocal folds (often referred to non-technically as the vocal cords), which\n can be moved together or apart. The space between these two folds is called the\n glottis glottis. If the folds are close together (but not tightly closed), they will vibrate as air\n passes through them; if they are far apart, they won‚Äôt vibrate. Sounds made with the\n voiced sound vocal folds together and vibrating are called voiced; sounds made without this vocal\nunvoiced sound cord vibration are called unvoiced or voiceless. Voiced sounds include [b], [d], [g],\n4 C HAPTER 14 ‚Ä¢ P HONETICS AND S PEECH F EATURE E XTRACTION\n\n Physics, CC BY 4.0)\n\n [v], [z], and all the English vowels, among others. Unvoiced sounds include [p], [t],\n [k], [f], [s], and others.\n The area above the trachea is called the vocal tract; it consists of the oral tract\n and the nasal tract. After the air leaves the trachea, it can exit the body through the\n mouth or the nose. Most sounds are made by air passing through the mouth. Sounds\n nasal made by air passing through the nose are called nasal sounds; nasal sounds (like\n English [m], [n], and [ng]) use both the oral and nasal tracts as resonating cavities.\n consonant Phones are divided into two main classes: consonants and vowels. Both kinds\n vowel of sounds are formed by the motion of air through the mouth, throat or nose. Consonants are made by restriction or blocking of the airflow in some way, and can be\n voiced or unvoiced. Vowels have less obstruction, are usually voiced, and are generally louder and longer-lasting than consonants. The technical use of these terms is\n much like the common usage; [p], [b], [t], [d], [k], [g], [f], [v], [s], [z], [r], [l], etc.,\n are consonants; [aa], [ae], [ao], [ih], [aw], [ow], [uw], etc., are vowels. Semivowels (such as [y] and [w]) have some of the properties of both; they are voiced like\n vowels, but they are short and less syllabic like consonants.\n\n Consonants: Place of Articulation\n Because consonants are made by restricting airflow, we can group them into classes\n place of by their point of maximum restriction, their place of articulation (Fig. 14.5).\n articulation\n\n labial Labial: Consonants whose main restriction is formed by the two lips coming together have a bilabial place of articulation. In English these include [p] as\n in possum, [b] as in bear, and [m] as in marmot. The English labiodental\n consonants [v] and [f] are made by pressing the bottom lip against the upper\n row of teeth and letting the air flow through the space in the upper teeth.\n 14.2 ‚Ä¢ A RTICULATORY P HONETICS 5\n\n (nasal tract)\n\n alveolar\n dental\n palatal\n velar\n bilabial\n glottal\n\n dental Dental: Sounds that are made by placing the tongue against the teeth are dentals.\n The main dentals in English are the [th] of thing and the [dh] of though, which\n are made by placing the tongue behind the teeth with the tip slightly between\n the teeth.\n alveolar Alveolar: The alveolar ridge is the portion of the roof of the mouth just behind the\n upper teeth. Most speakers of American English make the phones [s], [z], [t],\n and [d] by placing the tip of the tongue against the alveolar ridge. The word\n coronal is often used to refer to both dental and alveolar.\n palatal Palatal: The roof of the mouth (the palate) rises sharply from the back of the\n palate alveolar ridge. The palato-alveolar sounds [sh] (shrimp), [ch] (china), [zh]\n (Asian), and [jh] (jar) are made with the blade of the tongue against the rising\n back of the alveolar ridge. The palatal sound [y] of yak is made by placing the\n front of the tongue up close to the palate.\n velar Velar: The velum, or soft palate, is a movable muscular flap at the very back of the\n roof of the mouth. The sounds [k] (cuckoo), [g] (goose), and [N] (kingfisher)\n are made by pressing the back of the tongue up against the velum.\n glottal Glottal: The glottal stop [q] is made by closing the glottis (by bringing the vocal\n folds together).\n\n Consonants: Manner of Articulation\n Consonants are also distinguished by how the restriction in airflow is made, for example, by a complete stoppage of air or by a partial blockage. This feature is called\n manner of the manner of articulation of a consonant. The combination of place and manner\narticulation\n of articulation is usually sufficient to uniquely identify a consonant. Following are\n the major manners of articulation for English consonants:\n stop A stop is a consonant in which airflow is completely blocked for a short time.\n This blockage is followed by an explosive sound as the air is released. The period\n of blockage is called the closure, and the explosion is called the release. English\n has voiced stops like [b], [d], and [g] as well as unvoiced stops like [p], [t], and [k].\n Stops are also called plosives.\n nasal The nasal sounds [n], [m], and [ng] are made by lowering the velum and allowing air to pass into the nasal cavity.\n fricatives In fricatives, airflow is constricted but not cut off completely. The turbulent\n airflow that results from the constriction produces a characteristic ‚Äúhissing‚Äù sound.\n The English labiodental fricatives [f] and [v] are produced by pressing the lower\n lip against the upper teeth, allowing a restricted airflow between the upper teeth.\n The dental fricatives [th] and [dh] allow air to flow around the tongue between the\n teeth. The alveolar fricatives [s] and [z] are produced with the tongue against the\n6 C HAPTER 14 ‚Ä¢ P HONETICS AND S PEECH F EATURE E XTRACTION\n\n alveolar ridge, forcing air over the edge of the teeth. In the palato-alveolar fricatives\n [sh] and [zh], the tongue is at the back of the alveolar ridge, forcing air through a\n groove formed in the tongue. The higher-pitched fricatives (in English [s], [z], [sh]\n sibilants and [zh]) are called sibilants. Stops that are followed immediately by fricatives are\n called affricates; these include English [ch] (chicken) and [jh] (giraffe).\n approximant In approximants, the two articulators are close together but not close enough to\n cause turbulent airflow. In English [y] (yellow), the tongue moves close to the roof\n of the mouth but not close enough to cause the turbulence that would characterize a\n fricative. In English [w] (wood), the back of the tongue comes close to the velum.\n American [r] can be formed in at least two ways; with just the tip of the tongue\n extended and close to the palate or with the whole tongue bunched up near the palate.\n [l] is formed with the tip of the tongue up against the alveolar ridge or the teeth, with\n one or both sides of the tongue lowered to allow air to flow over it. [l] is called a\n lateral sound because of the drop in the sides of the tongue.\n tap A tap or flap [dx] is a quick motion of the tongue against the alveolar ridge. The\n consonant in the middle of the word lotus ([l ow dx ax s]) is a tap in most dialects of\n American English; speakers of many U.K. dialects would use a [t] instead.\n\n Vowels\n Like consonants, vowels can be characterized by the position of the articulators as\n they are made. The three most relevant parameters for vowels are what is called\n vowel height, which correlates roughly with the height of the highest part of the\n tongue, vowel frontness or backness, indicating whether this high point is toward\n the front or back of the oral tract and whether the shape of the lips is rounded or\n not. Figure 14.6 shows the position of the tongue for different vowels.\n\n tongue palate\n closed\n velum\n\n beet [iy] bat [ae] boot [uw]\n\n In the vowel [iy], for example, the highest point of the tongue is toward the\n front of the mouth. In the vowel [uw], by contrast, the high-point of the tongue is\n located toward the back of the mouth. Vowels in which the tongue is raised toward\n Front vowel the front are called front vowels; those in which the tongue is raised toward the\n back vowel back are called back vowels. Note that while both [ih] and [eh] are front vowels,\n the tongue is higher for [ih] than for [eh]. Vowels in which the highest point of the\n high vowel tongue is comparatively high are called high vowels; vowels with mid or low values\n of maximum tongue height are called mid vowels or low vowels, respectively.\n It is schematic because the abstract property height correlates only roughly with actual tongue positions; it is, in fact, a more accurate reflection of acoustic facts. Note\n that the chart has two kinds of vowels: those in which tongue height is represented\n as a point and those in which it is represented as a path. A vowel in which the tongue\n diphthong position changes markedly during the production of the vowel is a diphthong. En-\n14.2 ‚Ä¢ A RTICULATORY P HONETICS 7\n\n high\n\n iy y uw\n uw\n\n ih uh\n\n ow\n ey oy ax\n front back\n eh\n\n aw\n ao\n ay ah\n\n ae aa\n\n low\n\n glish is particularly rich in diphthongs.\n The second important articulatory dimension for vowels is the shape of the lips.\n Certain vowels are pronounced with the lips rounded (the same lip shape used for\nrounded vowel whistling). These rounded vowels include [uw], [ao], and [ow].\n\n Syllables\n syllable Consonants and vowels combine to make a syllable. A syllable is a vowel-like (or\n sonorant) sound together with some of the surrounding consonants that are most\n closely associated with it. The word dog has one syllable, [d aa g] (in our dialect);\n the word catnip has two syllables, [k ae t] and [n ih p]. We call the vowel at the\n nucleus core of a syllable the nucleus. Initial consonants, if any, are called the onset. Onsets\n onset with more than one consonant (as in strike [s t r ay k]), are called complex onsets.\n coda The coda is the optional consonant or sequence of consonants following the nucleus.\n rime Thus [d] is the onset of dog, and [g] is the coda. The rime, or rhyme, is the nucleus\n plus coda. Figure 14.8 shows some sample syllable structures.\n\n œÉ œÉ œÉ\n\n Onset Rime Onset Rime Rime\n\n h Nucleus Coda g r Nucleus Coda Nucleus Coda\n\n ae m iy n eh g z\n\n The task of automatically breaking up a word into syllables is called syllabificasyllabification tion. Syllable structure is also closely related to the phonotactics of a language. The\n phonotactics term phonotactics means the constraints on which phones can follow each other in\n a language. For example, English has strong constraints on what kinds of consonants can appear together in an onset; the sequence [zdr], for example, cannot be a\n8 C HAPTER 14 ‚Ä¢ P HONETICS AND S PEECH F EATURE E XTRACTION\n\n legal English syllable onset. Phonotactics can be represented by a language model\n or finite-state model of phone sequences.\n\n prosody Prosody is the study of the intonational and rhythmic aspects of language, and in\n particular the use of F0, energy, and duration to convey pragmatic, affective, or\n conversation-interactional meanings.1 We‚Äôll introduce these acoustic quantities in\n detail in the next section when we turn to acoustic phonetics, but briefly we can\n think of energy as the acoustic quality that we perceive as loudness, and F0 as the\n frequency of the sound that is produced, the acoustic quality that we hear as the\n pitch of an utterance. Prosody can be used to mark discourse structure, like the\n difference between statements and questions, or the way that a conversation is structured. Prosody is used to mark the saliency of a particular word or phrase. Prosody\n is heavily used for paralinguistic functions like conveying affective meanings like\n happiness, surprise, or anger. And prosody plays an important role in managing\n turn-taking in conversation.\n\n 14.3.1 Prosodic Prominence: Accent, Stress and Schwa\n prominence In a natural utterance of American English, some words sound more prominent than\n others, and certain syllables in these words are also more prominent than others.\n What we mean by prominence is that these words or syllables are perceptually more\n salient to the listener. Speakers make a word or syllable more salient in English\n by saying it louder, saying it slower (so it has a longer duration), or by varying F0\n during the word, making it higher or more variable.\n pitch accent Accent We represent prominence via a linguistic marker called pitch accent. Words\n or syllables that are prominent are said to bear (be associated with) a pitch accent.\n Thus this utterance might be pronounced by accenting the underlined words:\n (14.1) I‚Äôm a little surprised to hear it characterized as happy.\n Lexical Stress The syllables that bear pitch accent are called accented syllables.\n Not every syllable of a word can be accented: pitch accent has to be realized on the\n lexical stress syllable that has lexical stress. Lexical stress is a property of the word‚Äôs pronunciation in dictionaries; the syllable that has lexical stress is the one that will be louder\n or longer if the word is accented. For example, the word surprised is stressed on its\n second syllable, not its first. (Try stressing the other syllable by saying SURprised;\n hopefully that sounds wrong to you). Thus, if the word surprised receives a pitch\n accent in a sentence, it is the second syllable that will be stronger. The following example shows underlined accented words with the stressed syllable bearing the accent\n (the louder, longer syllable) in boldface:\n (14.2) I‚Äôm a little surprised to hear it characterized as happy.\n Stress is marked in dictionaries. The CMU dictionary (CMU, 1993), for example, marks vowels with 0 (unstressed) or 1 (stressed) as in entries for counter:\n [K AW1 N T ER0], or table: [T EY1 B AH0 L]. Difference in lexical stress can\n affect word meaning; the noun content is pronounced [K AA1 N T EH0 N T], while\n the adjective is pronounced [K AA0 N T EH1 N T].\n 1 The word is used in a different but related way in poetry, to mean the study of verse metrical structure.\n 14.3 ‚Ä¢ P ROSODY 9\n\n Reduced Vowels and Schwa Unstressed vowels can be weakened even further to\nreduced vowel reduced vowels, the most common of which is schwa ([ax]), as in the second vowel\n schwa of parakeet: [p ae r ax k iy t]. In a reduced vowel the articulatory gesture isn‚Äôt as\n complete as for a full vowel. Not all unstressed vowels are reduced; any vowel, and\n diphthongs in particular, can retain its full quality even in unstressed position. For\n example, the vowel [iy] can appear in stressed position as in the word eat [iy t] or in\n unstressed position as in the word carry [k ae r iy].\n prominence In summary, there is a continuum of prosodic prominence, for which it is often\n useful to represent levels like accented, stressed, full vowel, and reduced vowel.\n\n 14.3.2 Prosodic Structure\n Spoken sentences have prosodic structure: some words seem to group naturally together, while some words seem to have a noticeable break or disjuncture between\n prosodic\n phrasing them. Prosodic structure is often described in terms of prosodic phrasing, meaning that an utterance has a prosodic phrase structure in a similar way to it having\n a syntactic phrase structure. For example, the sentence I wanted to go to London,\n intonation\n phrase but could only get tickets for France seems to have two main intonation phrases,\n their boundary occurring at the comma. Furthermore, in the first phrase, there seems\n to be another set of lesser prosodic phrase boundaries (often called intermediate\n intermediate\n phrase phrases) that split up the words as I wanted | to go | to London. These kinds of\n intonation phrases are often correlated with syntactic structure constituents (Price\n et al. 1991, Bennett and Elfner 2019).\n Automatically predicting prosodic boundaries can be important for tasks like\n TTS. Modern approaches use sequence models that take either raw text or text annotated with features like parse trees as input, and make a break/no-break decision\n at each word boundary. They can be trained on data labeled for prosodic structure\n like the Boston University Radio News Corpus (Ostendorf et al., 1995).\n\n 14.3.3 Tune\n Two utterances with the same prominence and phrasing patterns can still differ\n tune prosodically by having different tunes. The tune of an utterance is the rise and\n fall of its F0 over time. A very obvious example of tune is the difference between\n statements and yes-no questions in English. The same words can be said with a final\n question rise F0 rise to indicate a yes-no question (called a question rise):\n\n You know what I mean ?\n\n final fall or a final drop in F0 (called a final fall) to indicate a declarative intonation:\n\n You know what I mean .\n\n Languages make wide use of tune to express meaning (Xu, 2005). In English,\n for example, besides this well-known rise for yes-no questions, a phrase containing\n a list of nouns separated by commas often has a short rise called a continuation\n continuation rise after each noun. Other examples include the characteristic English contours for\n rise\n expressing contradiction and expressing surprise.\n10 C HAPTER 14 ‚Ä¢ P HONETICS AND S PEECH F EATURE E XTRACTION\n\n Linking Prominence and Tune\n Pitch accents come in different varieties that are related to tune; high pitched accents,\n for example, have different functions than low pitched accents. There are many\n typologies of accent classes in different languages. One such typology is part of the\n ToBI ToBI (Tone and Break Indices) theory of intonation (Silverman et al. 1992). Each\n word in ToBI can be associated with one of five types of pitch accents shown in\n in Fig. 14.9. Each utterance in ToBI consists of a sequence of intonational phrases,\n boundary tone each of which ends in one of four boundary tones shown in Fig. 14.9, representing\n the utterance final aspects of tune. There are version of ToBI for many languages.\n\n Pitch Accents Boundary Tones\n H* peak accent L-L% ‚Äúfinal fall‚Äù: ‚Äúdeclarative contour‚Äù of American\n English\n L* low accent L-H% continuation rise\n L*+H scooped accent H-H% ‚Äúquestion rise‚Äù: cantonical yes-no question\n contour\n L+H* rising peak accent H-L% final level plateau\n H+!H* step down\n American English intonation (Beckman and Ayers 1997, Beckman and Hirschberg 1994).\n\n We begin with a very brief introduction to the acoustic waveform and its digitization\n and frequency analysis; the interested reader is encouraged to consult the references\n at the end of the chapter.\n\n 14.4.1 Waves\n Acoustic analysis is based on the sine and cosine functions. Figure 14.10 shows a\n plot of a sine wave, in particular the function\n y = A ‚àó sin(2œÄ f t) (14.3)\n where we have set the amplitude A to 1 and the frequency f to 10 cycles per second.\n\n 1.0\n\n ‚Äì1.0\n 0 0.1 0.2 0.3 0.4 0.5\n Time (s)\n\n Recall from basic mathematics that two important characteristics of a wave are\n frequency its frequency and amplitude. The frequency is the number of times a second that\n amplitude\n 14.4 ‚Ä¢ ACOUSTIC P HONETICS AND S IGNALS 11\n\n a wave repeats itself, that is, the number of cycles. We usually measure frequency\n in cycles per second. The signal in Fig. 14.10 repeats itself 5 times in .5 seconds,\n Hertz hence 10 cycles per second. Cycles per second are usually called hertz (shortened\n to Hz), so the frequency in Fig. 14.10 would be described as 10 Hz. The amplitude\n period A of a sine wave is the maximum value on the Y axis. The period T of the wave is\n the time it takes for one cycle to complete, defined as\n\n T= (14.4)\n f\n\n Each cycle in Fig. 14.10 lasts a tenth of a second; hence T = .1 seconds.\n\n 14.4.2 Speech Sound Waves\n Let‚Äôs turn from hypothetical waves to sound waves. The input to a speech recognizer, like the input to the human ear, is a complex series of changes in air pressure.\n These changes in air pressure obviously originate with the speaker and are caused\n by the specific way that air passes through the glottis and out the oral or nasal cavities. We represent sound waves by plotting the change in air pressure over time.\n One metaphor which sometimes helps in understanding these graphs is that of a vertical plate blocking the air pressure waves (perhaps in a microphone in front of a\n speaker‚Äôs mouth, or the eardrum in a hearer‚Äôs ear). The graph measures the amount\n of compression or rarefaction (uncompression) of the air molecules at this plate.\n corpus of telephone speech of the vowel [iy] from someone saying ‚Äúshe just had a\n baby‚Äù.\n\n 0.02283\n\n ‚Äì0.01697\n 0 0.03875\n Time (s)\n\ny-axis shows the level of air pressure above and below normal atmospheric pressure. The x-axis shows time.\nNotice that the wave repeats regularly.\n\n The first step in digitizing a sound wave like Fig. 14.11 is to convert the analog\n representations (first air pressure and then analog electric signals in a microphone)\n sampling into a digital signal. This analog-to-digital conversion has two steps: sampling and\n quantization. To sample a signal, we measure its amplitude at a particular time; the\n sampling rate is the number of samples taken per second. To accurately measure a\n wave, we must have at least two samples in each cycle: one measuring the positive\n part of the wave and one measuring the negative part. More than two samples per\n cycle increases the amplitude accuracy, but fewer than two samples causes the frequency of the wave to be completely missed. Thus, the maximum frequency wave\n that can be measured is one whose frequency is half the sample rate (since every\n cycle needs two samples). This maximum frequency for a given sampling rate is\n Nyquist\n frequency called the Nyquist frequency. Most information in human speech is in frequencies\n below 10,000 Hz; thus, a 20,000 Hz sampling rate would be necessary for complete accuracy. But telephone speech is filtered by the switching network, and only\n12 C HAPTER 14 ‚Ä¢ P HONETICS AND S PEECH F EATURE E XTRACTION\n\n frequencies less than 4,000 Hz are transmitted by telephones. Thus, an 8,000 Hz\n sampling rate is sufficient for telephone-bandwidth speech like the Switchboard\n corpus, while 16,000 Hz sampling is often used for microphone speech.\n Even an 8,000 Hz sampling rate requires 8000 amplitude measurements for each\n second of speech, so it is important to store amplitude measurements efficiently.\n They are usually stored as integers, either 8 bit (values from -128‚Äì127) or 16 bit\n (values from -32768‚Äì32767). This process of representing real-valued numbers as\n quantization integers is called quantization because the difference between two integers acts as\n a minimum granularity (a quantum size) and all values that are closer together than\n this quantum size are represented identically.\n Once data is quantized, it is stored in various formats. One parameter of these\n formats is the sample rate and sample size discussed above; telephone speech is\n often sampled at 8 kHz and stored as 8-bit samples, and microphone data is often\n sampled at 16 kHz and stored as 16-bit samples. Another parameter is the number of\n channel channels. For stereo data or for two-party conversations, we can store both channels\n in the same file or we can store them in separate files. A final parameter is individual\n sample storage‚Äîlinearly or compressed. One common compression format used for\n telephone speech is ¬µ-law (often written u-law but still pronounced mu-law). The\n intuition of log compression algorithms like ¬µ-law is that human hearing is more\n sensitive at small intensities than large ones; the log represents small values with\n more faithfulness at the expense of more error on large values. The linear (unlogged)\n PCM values are generally referred to as linear PCM values (PCM stands for pulse code\n modulation, but never mind that). Here‚Äôs the equation for compressing a linear PCM\n sample value x to 8-bit ¬µ-law, (where ¬µ=255 for 8 bits):\n\n sgn(x) log(1 + ¬µ|x|)\n F(x) = ‚àí1 ‚â§ x ‚â§ 1 (14.5)\n log(1 + ¬µ)\n\n There are a number of standard file formats for storing the resulting digitized wavefile, such as Microsoft‚Äôs .wav and Apple‚Äôs AIFF all of which have special headers;\n simple headerless ‚Äúraw‚Äù files are also used. For example, the .wav format is a subset\n of Microsoft‚Äôs RIFF format for multimedia files; RIFF is a general format that can\n represent a series of nested chunks of data and control information. Figure 14.12\n shows a simple .wav file with a single data chunk together with its format chunk.\n\n 14.4.3 Frequency and Amplitude; Pitch and Loudness\n Sound waves, like all waves, can be described in terms of frequency, amplitude, and\n the other characteristics that we introduced earlier for pure sine waves. In sound\n waves, these are not quite as simple to measure as they were for sine waves. Let‚Äôs\n consider frequency. Note in Fig. 14.11 that although not exactly a sine, the wave is\n nonetheless periodic, repeating 10 times in the 38.75 milliseconds (.03875 seconds)\n 14.4 ‚Ä¢ ACOUSTIC P HONETICS AND S IGNALS 13\n\n captured in the figure. Thus, the frequency of this segment of the wave is 10/.03875\n or 258 Hz.\n Where does this periodic 258 Hz wave come from? It comes from the speed\n of vibration of the vocal folds; since the waveform in Fig. 14.11 is from the vowel\n [iy], it is voiced. Recall that voicing is caused by regular openings and closing of\n the vocal folds. When the vocal folds are open, air is pushing up through the lungs,\n creating a region of high pressure. When the folds are closed, there is no pressure\n from the lungs. Thus, when the vocal folds are vibrating, we expect to see regular\n peaks in amplitude of the kind we see in Fig. 14.11, each major peak corresponding\n to an opening of the vocal folds. The frequency of the vocal fold vibration, or the\nfundamental\n frequency frequency of the complex wave, is called the fundamental frequency of the wave-\nF0 form, often abbreviated F0. We can plot F0 over time in a pitch track. Figure 14.13\n pitch track shows the pitch track of a short question, ‚ÄúThree o‚Äôclock?‚Äù represented below the\n waveform. Note the rise in F0 at the end of the question.\n\n 500 Hz\n\n 0 Hz\n\n three o‚Äôclock\n\n 0 0.544375\n Time (s)\n\n the rise in F0 at the end of the question. Note the lack of pitch trace during the very quiet part\n (the ‚Äúo‚Äô‚Äù of ‚Äúo‚Äôclock‚Äù; automatic pitch tracking is based on counting the pulses in the voiced\n regions, and doesn‚Äôt work if there is no voicing (or insufficient sound).\n\n The vertical axis in Fig. 14.11 measures the amount of air pressure variation;\n pressure is force per unit area, measured in Pascals (Pa). A high value on the vertical\n axis (a high amplitude) indicates that there is more air pressure at that point in time,\n a zero value means there is normal (atmospheric) air pressure, and a negative value\n means there is lower than normal air pressure (rarefaction).\n In addition to this value of the amplitude at any point in time, we also often\n need to know the average amplitude over some time range, to give us some idea\n of how great the average displacement of air pressure is. But we can‚Äôt just take\n the average of the amplitude values over a range; the positive and negative values\n would (mostly) cancel out, leaving us with a number close to zero. Instead, we\n generally use the RMS (root-mean-square) amplitude, which squares each number\n before averaging (making it positive), and then takes the square root at the end.\n v\n u\n u1 X N\n N\n RMS amplitudei=1 = t xi2 (14.6)\n N\n i=1\n\n power The power of the signal is related to the square of the amplitude. If the number\n14 C HAPTER 14 ‚Ä¢ P HONETICS AND S PEECH F EATURE E XTRACTION\n\n of samples of a sound is N, the power is\n N\n 1X 2\n Power = xi (14.7)\n N\n i=1\n intensity Rather than power, we more often refer to the intensity of the sound, which\n normalizes the power to the human auditory threshold and is measured in dB. If P0\n is the auditory threshold pressure = 2 √ó 10‚àí5 Pa, then intensity is defined as follows:\n N\n 1 X 2\n Intensity = 10 log10 xi (14.8)\n NP0\n i=1\n the CallHome corpus, again shown below the waveform plot.\n\n is it a long movie?\n\n 0 1.1675\n Time (s)\n\n at each vowel and the especially high peak for the word long.\n\n Two important perceptual properties, pitch and loudness, are related to frepitch quency and intensity. The pitch of a sound is the mental sensation, or perceptual\n correlate, of fundamental frequency; in general, if a sound has a higher fundamental frequency we perceive it as having a higher pitch. We say ‚Äúin general‚Äù because\n the relationship is not linear, since human hearing has different acuities for different\n frequencies. Roughly speaking, human pitch perception is most accurate between\n 100 Hz and 1000 Hz and in this range pitch correlates linearly with frequency. Human hearing represents frequencies above 1000 Hz less accurately, and above this\n range, pitch correlates logarithmically with frequency. Logarithmic representation\n means that the differences between high frequencies are compressed and hence not\n as accurately perceived. There are various psychoacoustic models of pitch percep-\nMel tion scales. One common model is the mel scale (Stevens et al. 1937, Stevens and\n Volkmann 1940). A mel is a unit of pitch defined such that pairs of sounds which\n are perceptually equidistant in pitch are separated by an equal number of mels. The\n mel frequency m can be computed from the raw acoustic frequency as follows:\n f\n m = 1127 ln(1 + ) (14.9)\n As we‚Äôll see in Chapter 15, the mel scale plays an important role in speech\n recognition.\n 14.4 ‚Ä¢ ACOUSTIC P HONETICS AND S IGNALS 15\n\n The loudness of a sound is the perceptual correlate of the power. So sounds with\n higher amplitudes are perceived as louder, but again the relationship is not linear.\n First of all, as we mentioned above when we defined ¬µ-law compression, humans\n have greater resolution in the low-power range; the ear is more sensitive to small\n power differences. Second, it turns out that there is a complex relationship between\n power, frequency, and perceived loudness; sounds in certain frequency ranges are\n perceived as being louder than those in other frequency ranges.\n Various algorithms exist for automatically extracting F0. In a slight abuse of terpitch extraction minology, these are called pitch extraction algorithms. The autocorrelation method\n of pitch extraction, for example, correlates the signal with itself at various offsets.\n The offset that gives the highest correlation gives the period of the signal. There\n are various publicly available pitch extraction toolkits; for example, an augmented\n autocorrelation pitch tracker is provided with Praat (Boersma and Weenink, 2005).\n\n 14.4.4 Interpretation of Phones from a Waveform\n Much can be learned from a visual inspection of a waveform. For example, vowels\n are pretty easy to spot. Recall that vowels are voiced; another property of vowels\n is that they tend to be long and are relatively loud (as we can see in the intensity\n plot in Fig. 14.14). Length in time manifests itself directly on the x-axis, and loudness is related to (the square of) amplitude on the y-axis. We saw in the previous\n section that voicing is realized by regular peaks in amplitude of the kind we saw in\n Fig. 14.11, each major peak corresponding to an opening of the vocal folds. Figure 14.15 shows the waveform of the short sentence ‚Äúshe just had a baby‚Äù. We have\n labeled this waveform with word and phone labels. Notice that each of the six vowels in Fig. 14.15, [iy], [ax], [ae], [ax], [ey], [iy], all have regular amplitude peaks\n indicating voicing.\n\n she just had a baby\n\n sh iy j ax s h ae dx ax b ey b iy\n\n 0 1.059\n Time (s)\n\n4325). The speaker is female, was 20 years old in 1991, which is approximately when the recording was made,\nand speaks the South Midlands dialect of American English.\n\n For a stop consonant, which consists of a closure followed by a release, we can\n often see a period of silence or near silence followed by a slight burst of amplitude.\n We can see this for both of the [b]‚Äôs in baby in Fig. 14.15.\n Another phone that is often quite recognizable in a waveform is a fricative. Recall that fricatives, especially very strident fricatives like [sh], are made when a\n narrow channel for airflow causes noisy, turbulent air. The resulting hissy sounds\n have a noisy, irregular waveform. This can be seen somewhat in Fig. 14.15; it‚Äôs even\n clearer in Fig. 14.16, where we‚Äôve magnified just the first word she.\n16 C HAPTER 14 ‚Ä¢ P HONETICS AND S PEECH F EATURE E XTRACTION\n\n she\n\n sh iy\n\n 0 0.257\n Time (s)\n\nthe difference between the random noise of the fricative [sh] and the regular voicing of the vowel [iy].\n\n 14.4.5 Spectra and the Frequency Domain\n While some broad phonetic features (such as energy, pitch, and the presence of voicing, stop closures, or fricatives) can be interpreted directly from the waveform, most\n computational applications such as speech recognition (as well as human auditory\n processing) are based on a different representation of the sound in terms of its component frequencies. The insight of Fourier analysis is that every complex wave can\n be represented as a sum of many sine waves of different frequencies. Consider the\n waveform in Fig. 14.17. This waveform was created (in Praat) by summing two sine\n waveforms, one of frequency 10 Hz and one of frequency 100 Hz.\n\n ‚Äì1\n 0 0.5\n Time (s)\n\n Hz (note five repetitions in the half-second window) and one of frequency 100 Hz, both of\n amplitude 1.\n\n spectrum We can represent these two component frequencies with a spectrum. The spectrum of a signal is a representation of each of its frequency components and their\n amplitudes. Figure 14.18 shows the spectrum of Fig. 14.17. Frequency in Hz is\n on the x-axis and amplitude on the y-axis. Note the two spikes in the figure, one\n at 10 Hz and one at 100 Hz. Thus, the spectrum is an alternative representation of\n the original waveform, and we use the spectrum as a tool to study the component\n frequencies of a sound wave at a particular time point.\n Let‚Äôs look now at the frequency components of a speech waveform. Figure 14.19\n shows part of the waveform for the vowel [ae] of the word had, cut out from the\n sentence shown in Fig. 14.15.\n Note that there is a complex wave that repeats about ten times in the figure; but\n there is also a smaller repeated wave that repeats four times for every larger pattern\n (notice the four small peaks inside each repeated wave). The complex wave has a\n frequency of about 234 Hz (we can figure this out since it repeats roughly 10 times\n 14.4 ‚Ä¢ ACOUSTIC P HONETICS AND S IGNALS 17\n\n Sound pressure level (dB/Hz)\n\n 1 2 5 10 20 50 100 200\n Frequency (Hz)\n\n 0.04968\n\n ‚Äì0.05554\n 0 0.04275\n Time (s)\n\nwaveform shown in Fig. 14.15.\n\nin .0427 seconds, and 10 cycles/.0427 seconds = 234 Hz).\n The smaller wave then should have a frequency of roughly four times the frequency of the larger wave, or roughly 936 Hz. Then, if you look carefully, you can\nsee two little waves on the peak of many of the 936 Hz waves. The frequency of this\ntiniest wave must be roughly twice that of the 936 Hz wave, hence 1872 Hz.\n\n Sound pressure level (dB/Hz)\n\n ‚Äì20\n\n 0 1000 2000 3000 4000\n Frequency (Hz)\n\nhad a baby in Fig. 14.15.\n\n The x-axis of a spectrum shows frequency, and the y-axis shows some measure of the magnitude of each frequency component (in decibels (dB), a logarithmic\nmeasure of amplitude that we saw earlier). Thus, Fig. 14.20 shows significant frequency components at around 930 Hz, 1860 Hz, and 3020 Hz, along with many\nother lower-magnitude frequency components. These first two components are just\nwhat we noticed in the time domain by looking at the wave in Fig. 14.19!\n Why is a spectrum useful? It turns out that these spectral peaks that are easily\nvisible in a spectrum are characteristic of different phones; phones have characteris-\n18 C HAPTER 14 ‚Ä¢ P HONETICS AND S PEECH F EATURE E XTRACTION\n\n tic spectral ‚Äúsignatures‚Äù. Just as chemical elements give off different wavelengths of\n light when they burn, allowing us to detect elements in stars by looking at the spectrum of the light, we can detect the characteristic signature of the different phones\n by looking at the spectrum of a waveform. This use of spectral information is essential to both human and machine speech recognition. In human audition, the function\n cochlea of the cochlea, or inner ear, is to compute a spectrum of the incoming waveform.\n Similarly, the acoustic features used in speech recognition are spectral representations.\n Let‚Äôs look at the spectrum of different vowels. Since some vowels change over\n time, we‚Äôll use a different kind of plot called a spectrogram. While a spectrum\n spectrogram shows the frequency components of a wave at one point in time, a spectrogram is a\n way of envisioning how the different frequencies that make up a waveform change\n over time. The x-axis shows time, as it did for the waveform, but the y-axis now\n shows frequencies in hertz. The darkness of a point on a spectrogram corresponds\n to the amplitude of the frequency component. Very dark points have high amplitude,\n light points have low amplitude. Thus, the spectrogram is a useful way of visualizing\n the three dimensions (time x frequency x amplitude).\n and [ah]. Note that each vowel has a set of dark bars at various frequency bands,\n slightly different bands for each vowel. Each of these represents the same kind of\n spectral peak that we saw in Fig. 14.19.\n\n Frequency (Hz)\n\n 0 2.81397\n Time (s)\n\n formant Each dark bar (or spectral peak) is called a formant. As we discuss below, a\n formant is a frequency band that is particularly amplified by the vocal tract. Since\n different vowels are produced with the vocal tract in different positions, they will\n produce different kinds of amplifications or resonances. Let‚Äôs look at the first two\n formants, called F1 and F2. Note that F1, the dark bar closest to the bottom, is in a\n different position for the three vowels; it‚Äôs low for [ih] (centered at about 470 Hz)\n and somewhat higher for [ae] and [ah] (somewhere around 800 Hz). By contrast,\n F2, the second dark bar from the bottom, is highest for [ih], in the middle for [ae],\n and lowest for [ah].\n We can see the same formants in running speech, although the reduction and\n coarticulation processes make them somewhat harder to see. Figure 14.22 shows\n the spectrogram of ‚Äúshe just had a baby‚Äù, whose waveform was shown in Fig. 14.15.\n F1 and F2 (and also F3) are pretty clear for the [ax] of just, the [ae] of had, and the\n [ey] of baby.\n What specific clues can spectral representations give for phone identification?\n First, since different vowels have their formants at characteristic places, the spectrum\n can distinguish vowels from each other. We‚Äôve seen that [ae] in the sample waveform\n had formants at 930 Hz, 1860 Hz, and 3020 Hz. Consider the vowel [iy] at the\n 14.4 ‚Ä¢ ACOUSTIC P HONETICS AND S IGNALS 19\n\n she just had a baby\n\n sh iy j ax s h ae dx ax b ey b iy\n\n 0 1.059\n Time (s)\n\nWe can think of a spectrogram as a collection of spectra (time slices), like Fig. 14.20 placed end to end.\n\n beginning of the utterance in Fig. 14.15. The spectrum for this vowel is shown in\n Fig. 14.23. The first formant of [iy] is 540 Hz, much lower than the first formant for\n [ae], and the second formant (2581 Hz) is much higher than the second formant for\n [ae]. If you look carefully, you can see these formants as dark bars in Fig. 14.22 just\n around 0.5 seconds.\n\n ‚àí10\n 0 1000 2000 3000\n\n baby. Note that the first formant (540 Hz) is much lower than the first formant for [ae] shown\n in Fig. 14.20, and the second formant (2581 Hz) is much higher than the second formant for\n [ae].\n\n The location of the first two formants (called F1 and F2) plays a large role in determining vowel identity, although the formants still differ from speaker to speaker.\n Higher formants tend to be caused more by general characteristics of a speaker‚Äôs\n vocal tract rather than by individual vowels. Formants also can be used to identify\n the nasal phones [n], [m], and [ng] and the liquids [l] and [r].\n\n 14.4.6 The Source-Filter Model\n Why do different vowels have different spectral signatures? As we briefly mentioned\n above, the formants are caused by the resonant cavities of the mouth. The sourcesource-filter filter model is a way of explaining the acoustics of a sound by modeling how the\n model\n pulses produced by the glottis (the source) are shaped by the vocal tract (the filter).\n Let‚Äôs see how this works. Whenever we have a wave such as the vibration in air\n harmonic caused by the glottal pulse, the wave also has harmonics. A harmonic is another\n wave whose frequency is a multiple of the fundamental wave. Thus, for example, a\n20 C HAPTER 14 ‚Ä¢ P HONETICS AND S PEECH F EATURE E XTRACTION\n\n 115 Hz glottal fold vibration leads to harmonics (other waves) of 230 Hz, 345 Hz,\n 460 Hz, and so on on. In general, each of these waves will be weaker, that is, will\n have much less amplitude than the wave at the fundamental frequency.\n It turns out, however, that the vocal tract acts as a kind of filter or amplifier;\n indeed any cavity, such as a tube, causes waves of certain frequencies to be amplified\n and others to be damped. This amplification process is caused by the shape of the\n cavity; a given shape will cause sounds of a certain frequency to resonate and hence\n be amplified. Thus, by changing the shape of the cavity, we can cause different\n frequencies to be amplified.\n When we produce particular vowels, we are essentially changing the shape of\n the vocal tract cavity by placing the tongue and the other articulators in particular\n positions. The result is that different vowels cause different harmonics to be amplified. So a wave of the same fundamental frequency passed through different vocal\n tract positions will result in different harmonics being amplified.\n We can see the result of this amplification by looking at the relationship between\n the shape of the vocal tract and the corresponding spectrum. Figure 14.24 shows\n the vocal tract position for three vowels and a typical resulting spectrum. The formants are places in the spectrum where the vocal tract happens to amplify particular\n harmonic frequencies.\n\n Sound pressure level (dB/Hz)\n\n Sound pressure level (dB/Hz)\n\n Sound pressure level (dB/Hz)\n\n [iy] (tea) 0 [uw] (moo)\n F1\n [ae] (cat)\n F2\n F1 F2\n 0 ‚Äì20 F1 F2\n\n 0 268 2416 4000 0 903 1695 4000 0 295 817 4000\n Frequency (Hz) Frequency (Hz) Frequency (Hz)\n\n [iy] (tea) [ae] (cat) [uw] (moo)\nthe resulting smoothed spectra showing F1 and F2.\n\n Mel Spectrum\n The same tools that we used to analyze the acoustic phonetics of the waveforms are\n also often used as inputs to speech processing algorithms. In this section we introduce a signal processing pipeline that is often used as part of tasks like automatic\n speech recognition (ASR), as we we will see in Chapter 15. The first step in speech\n processing is often to transform the input waveform into a sequence of acoustic fea-\n14.5 ‚Ä¢ F EATURE E XTRACTION FOR S PEECH R ECOGNITION : L OG M EL S PECTRUM 21\n\n feature vector ture vectors, each vector representing the information in a small time window of\n the signal. Sometimes speech recognition or processing algorithms will start with\n the waveform, in which case that processing is done by the convolutional networks\n (convnets) that we will introduce in Chapter 15.\n Other systems begin instead at a higher level, with the log mel spectrum. So in\n this section we introduce this commonly used feature vector: sequences of log mel\n spectrum vectors. In the following section we‚Äôll introduce an alternative vector, the\n MFCC representation. We‚Äôll introduce these concepts at a relatively high level; a\n speech signal processing course is recommended for more details.\n We begin by repeating from Section 14.4.2 the process of digitizing and quantizing an analog speech waveform.\n\n 14.5.1 Sampling and Quantization\n The input to a speech recognizer is a complex series of changes in air pressure.\n These changes in air pressure obviously originate with the speaker and are caused\n by the specific way that air passes through the glottis and out the oral or nasal cavities. We represent sound waves by plotting the change in air pressure over time.\n One metaphor which sometimes helps in understanding these graphs is that of a vertical plate blocking the air pressure waves (perhaps in a microphone in front of a\n speaker‚Äôs mouth, or the eardrum in a hearer‚Äôs ear). The graph measures the amount\n of compression or rarefaction (uncompression) of the air molecules at this plate.\n someone saying ‚Äúshe just had a baby‚Äù.\n\n 0.02283\n\n ‚Äì0.01697\n 0 0.03875\n Time (s)\n\nshows the level of air pressure above and below normal atmospheric pressure. The x-axis shows time. Notice\nthat the wave repeats regularly. Repeated from Fig. 14.11.\n\n The first step in digitizing a sound wave like Fig. 14.15 is to convert the analog\n representations (first air pressure and then analog electric signals in a microphone)\n sampling into a digital signal. This analog-to-digital conversion has two steps: sampling and\n quantization. To sample a signal, we measure its amplitude at a particular time; the\n sampling rate is the number of samples taken per second. To accurately measure a\n wave, we must have at least two samples in each cycle: one measuring the positive\n part of the wave and one measuring the negative part. More than two samples per\n cycle increases the amplitude accuracy, but fewer than two samples causes the frequency of the wave to be completely missed. Thus, the maximum frequency wave\n that can be measured is one whose frequency is half the sample rate (since every\n cycle needs two samples). This maximum frequency for a given sampling rate is\n Nyquist\n frequency called the Nyquist frequency. Most information in human speech is in frequencies\n below 10,000 Hz; thus, a 20,000 Hz sampling rate would be necessary for complete accuracy. But telephone speech is filtered by the switching network, and only\n frequencies less than 4,000 Hz are transmitted by telephones. Thus, an 8,000 Hz\n22 C HAPTER 14 ‚Ä¢ P HONETICS AND S PEECH F EATURE E XTRACTION\n\n sampling rate is sufficient for telephone-bandwidth speech like the Switchboard\n corpus, while 16,000 Hz sampling is often used for microphone speech.\n Although using higher sampling rates produces higher ASR accuracy, we can‚Äôt\n combine different sampling rates for training and testing ASR systems. Thus if\n we are testing on a telephone corpus like Switchboard (8 KHz sampling), we must\n downsample our training corpus to 8 KHz. Similarly, if we are training on multiple corpora and one of them includes telephone speech, we downsample all the\n wideband corpora to 8KHz.\n Amplitude measurements are stored as integers, either 8 bit (values from -128‚Äì\n 127) or 16 bit (values from -32768‚Äì32767). This process of representing real-valued\n quantization numbers as integers is called quantization; all values that are closer together than\n the minimum granularity (the quantum size) are represented identically. We refer to\n each sample at time index n in the digitized, quantized waveform as x[n].\n Once data is quantized, it is stored in various formats. One parameter of these\n formats is the sample rate and sample size discussed above; telephone speech is\n often sampled at 8 kHz and stored as 8-bit samples, and microphone data is often\n sampled at 16 kHz and stored as 16-bit samples. Another parameter is the number of\n channel channels. For stereo data or for two-party conversations, we can store both channels\n in the same file or we can store them in separate files. A final parameter is individual\n sample storage‚Äîlinearly or compressed. One common compression format used for\n telephone speech is ¬µ-law (often written u-law but still pronounced mu-law). The\n intuition of log compression algorithms like ¬µ-law is that human hearing is more\n sensitive at small intensities than large ones; the log represents small values with\n more faithfulness at the expense of more error on large values. The linear (unlogged)\n PCM values are generally referred to as linear PCM values (PCM stands for pulse code\n modulation, but never mind that). Here‚Äôs the equation for compressing a linear PCM\n sample value x to 8-bit ¬µ-law, (where ¬µ=255 for 8 bits):\n\n sgn(x) log(1 + ¬µ|x|)\n F(x) = ‚àí1 ‚â§ x ‚â§ 1 (14.10)\n log(1 + ¬µ)\n\n 14.5.2 Windowing\n From the digitized, quantized representation of the waveform, we need to extract\n spectral features from a small window of speech that characterizes part of a particular phoneme. Inside this small window, we can roughly think of the signal as\n stationary stationary (that is, its statistical properties are constant within this region). (By\n non-stationary contrast, in general, speech is a non-stationary signal, meaning that its statistical\n properties are not constant over time). We extract this roughly stationary portion of\n speech by using a window which is non-zero inside a region and zero elsewhere, running this window across the speech signal and multiplying it by the input waveform\n to produce a windowed waveform.\n frame The speech extracted from each window is called a frame. The windowing is\n characterized by three parameters: the window size or frame size of the window\n stride (its width in milliseconds), the frame stride, (also called shift or offset) between\n successive windows, and the shape of the window.\n To extract the signal we multiply the value of the signal at time n, s[n] by the\n value of the window at time n, w[n]:\n\n y[n] = w[n]s[n] (14.11)\n\n rectangular The window shape sketched in Fig. 14.26 is rectangular; you can see the ex-\n14.5 ‚Ä¢ F EATURE E XTRACTION FOR S PEECH R ECOGNITION : L OG M EL S PECTRUM 23\n\n Window\n 25 ms\n Shift\n 10 Window\n ms 25 ms\n Shift\n 10 Window\n ms 25 ms\n\n tracted windowed signal looks just like the original signal. The rectangular window,\n however, abruptly cuts off the signal at its boundaries, which creates problems when\n we do Fourier analysis. For this reason, for acoustic feature creation we more com-\nHamming monly use the Hamming window, which shrinks the values of the signal toward\n zero at the window boundaries, avoiding discontinuities. Figure 14.27 shows both;\n the equations are as follows (assuming a window that is L frames long):\n \u001a\n 1 0 ‚â§ n ‚â§ L‚àí1\n rectangular w[n] = (14.12)\n 0 otherwise\n 0.54 ‚àí 0.46 cos( 2œÄn\n \u001a\n L ) 0 ‚â§ n ‚â§ L‚àí1\n Hamming w[n] = (14.13)\n 0 otherwise\n\n 0.4999\n\n ‚Äì0.5\n 0 0.0475896\n Time (s)\n\n Rectangular window Hamming window\n\n 0 0\n\n ‚Äì0.5 ‚Äì0.4826\n Time (s) Time (s)\n\n24 C HAPTER 14 ‚Ä¢ P HONETICS AND S PEECH F EATURE E XTRACTION\n\n 14.5.3 Discrete Fourier Transform\n The next step is to extract spectral information for our windowed signal; we need to\n know how much energy the signal contains at different frequency bands. The tool\n for extracting spectral information for discrete frequency bands for a discrete-time\n Discrete\n Fourier (sampled) signal is the discrete Fourier transform or DFT.\n transform\n DFT The input to the DFT is a windowed signal x[n]...x[m], and the output, for each of\n N discrete frequency bands, is a complex number X[k] representing the magnitude\n and phase of that frequency component in the original signal. If we plot the magnitude against the frequency, we can visualize the spectrum (see Chapter 14 for more\n on spectra). For example, Fig. 14.28 shows a 25 ms Hamming-windowed portion of\n a signal and its spectrum as computed by a DFT (with some additional smoothing).\n\n 0.04414\n\n Sound pressure level (dB/Hz)\n\n 0 0\n\n ‚Äì20\n\n ‚Äì0.04121\n Time (s) Frequency (Hz)\n\n (a) (b)\n and (b) its spectrum computed by a DFT.\n\n We do not introduce the mathematical details of the DFT here, except to note\nEuler‚Äôs formula that Fourier analysis relies on Euler‚Äôs formula, with j as the imaginary unit:\n\n e jŒ∏ = cos Œ∏ + j sin Œ∏ (14.14)\n\n As a brief reminder for those students who have already studied signal processing,\n the DFT is defined as follows:\n N‚àí1\n X 2œÄ\n X[k] = x[n]e‚àí j N kn (14.15)\n n=0\n fast Fourier A commonly used algorithm for computing the DFT is the fast Fourier transform\n transform\n FFT or FFT. This implementation of the DFT is very efficient but only works for values\n of N that are powers of 2.\n\n 14.5.4 Mel Filter Bank and Log\n The results of the FFT tell us the energy at each frequency band. Human hearing,\n however, is not equally sensitive at all frequency bands; it is less sensitive at higher\n frequencies. This bias toward low frequencies helps human recognition, since information in low frequencies (like formants) is crucial for distinguishing vowels or\n nasals, while information in high frequencies (like stop bursts or fricative noise) is\n less crucial for successful recognition. Modeling this human perceptual property\n improves speech recognition performance in the same way.\n We implement this intuition by collecting energies, not equally at each frequency\n mel band, but according to the mel scale, an auditory frequency scale. A mel (Stevens\n 14.6 ‚Ä¢ MFCC: M EL F REQUENCY C EPSTRAL C OEFFICIENTS 25\n\n et al. 1937, Stevens and Volkmann 1940) is a unit of pitch. Pairs of sounds that are\n perceptually equidistant in pitch are separated by an equal number of mels. The mel\n frequency m can be computed from the raw acoustic frequency by a log transformation:\n f\n mel( f ) = 1127 ln(1 + ) (14.16)\n We implement this intuition by creating a bank of filters that collect energy from\n each frequency band, spread logarithmically so that we have very fine resolution\n at low frequencies, and less resolution at high frequencies. Figure 14.29 shows a\n sample bank of triangular filters that implement this idea, that can be multiplied by\n the spectrum to get a mel spectrum.\n\n Amplitude\n\n 0.5\n\n 0 7700\n 8K\n Frequency (Hz)\n\n mel spectrum m1 m2 ... mM\n\n spaced logarithmically along the mel scale, collects energy from a given frequency range.\n\n Finally, we take the log of each of the mel spectrum values. The human response\n to signal level is logarithmic (like the human response to frequency). Humans are\n less sensitive to slight differences in amplitude at high amplitudes than at low amplitudes. In addition, using a log makes the feature estimates less sensitive to variations\n in input such as power variations due to the speaker‚Äôs mouth moving closer or further\n from the microphone.\n channel We call each scalar output from a particular filter a channel, and so the output\n for each input frame from the filterbank is a vector of, say 80 or 128 channels, each\n of which represents the log energy of a particular (mel-spaced) frequency band.\n Before we send this log mel channel vector to the downstream neural network\n layers, it‚Äôs common for speech systems to rescale them so they have comparable\n ranges. A common type of normalization for speech is to scale the input to be between -1 and 1 with zero mean across the entire pretraining dataset (see Section ??\n in Chapter 4).\n\n MFCC The MFCC, mel frequency cepstral coefficients, is a useful representation of the\n waveform that emphasizes aspects of the signal that are relevant for detection of\n phonetic units. The MFCC is a 39-dimensional feature vector consisting of:\n 12 cepstral coefficients 1 energy coefficient\n 12 delta cepstral coefficients 1 delta energy coefficient\n 12 double delta cepstral coefficients 1 double delta energy coefficient\n Below we sketch how these features are computed; students interested in more\n detail are encouraged to follow up with a signal processing course.\n26 C HAPTER 14 ‚Ä¢ P HONETICS AND S PEECH F EATURE E XTRACTION\n\n The Cepstrum: Inverse Discrete Fourier Transform\n cepstrum MFCC coefficients are based on the cepstrum. One way to think about the cepstrum\n is as a useful way of separating the source and filter. Recall from Section 14.4.6\n that the speech waveform is created when a glottal source waveform of a particular\n fundamental frequency is passed through the vocal tract, which because of its shape\n has a particular filtering characteristic. But many characteristics of the glottal source\n (its fundamental frequency, the details of the glottal pulse, etc.) are not important\n for distinguishing different phones. Instead, the most useful information for phone\n detection is the filter, that is, the exact position of the vocal tract. If we knew the\n shape of the vocal tract, we would know which phone was being produced. This\n suggests that useful features for phone detection would find a way to deconvolve\n (separate) the source and filter and show us only the vocal tract filter. It turns out\n that the cepstrum is one way to do this.\n\n 14 120 700\n 12 100\n 10 400\n amplitude\n\n amplitude\n\n amplitude\n 8 300\n 60 200\n 6 100\n 4 0\n -100\n 2 20\n -200\n 0 0 -300\n 0 1000 2000 3000 4000 5000 6000 7000 8000 0 1000 2000 3000 4000 5000 6000 7000 8000 0 50 100 150 200 250\n normalise frequency normalise frequency samples\n\n (a) (b) (c)\nby permission. The two spectra have a smoothed spectral envelope laid on top to help visualize the spectrum.\n\n For simplicity, let‚Äôs consider as input the log magnitude spectrum and ignore\n the mel scaling. The cepstrum can be thought of as the spectrum of the log of the\n spectrum. This may sound confusing. But let‚Äôs begin with the easy part: the log\n of the spectrum. That is, the cepstrum begins with a standard magnitude spectrum,\n such as the one for a vowel shown in Fig. 14.30(a) from Taylor (2009). We then take\n the log, that is, replace each amplitude value in the magnitude spectrum with its log,\n as shown in Fig. 14.30(b).\n The next step is to visualize the log spectrum as if itself were a waveform. In\n other words, consider the log spectrum in Fig. 14.30(b). Let‚Äôs imagine removing the\n axis labels that tell us that this is a spectrum (frequency on the x-axis) and imagine\n that we are dealing with just a normal speech signal with time on the x-axis. What\n can we now say about the spectrum of this ‚Äúpseudo-signal‚Äù? Notice that there is a\n high frequency repetitive component in this wave: small waves that repeat about 8\n times in each 1000 along the x-axis, for a frequency of about 120 Hz. This high\n frequency component is caused by the fundamental frequency of the signal and represents the little peaks in the spectrum at each harmonic of the signal. In addition,\n there are some lower frequency components in this ‚Äúpseudo-signal‚Äù; for example,\n the envelope or formant structure has about four large peaks in the window, for a\n much lower frequency.\n of the log spectrum. This cepstrum (the word cepstrum is formed by reversing\n the first four letters of spectrum) is shown with samples along the x-axis. This\n is because by taking the spectrum of the log spectrum, we have left the frequency\n domain of the spectrum, and gone back to the time domain. It turns out that the\n 14.7 ‚Ä¢ S UMMARY 27\n\n correct unit of a cepstrum is the sample.\n Examining this cepstrum, we see that there is indeed a large peak around 120,\n corresponding to the F0 and representing the glottal pulse. There are other various\n components at lower values on the x-axis. These represent the vocal tract filter\n (the position of the tongue and the other articulators). Thus, if we are interested\n in detecting phones, we can make use of just the lower cepstral values. If we are\n interested in detecting pitch, we can use the higher cepstral values.\n For the purposes of MFCC extraction, we generally just take the first 12 cepstral\n values. These 12 coefficients will represent information solely about the vocal tract\n filter, cleanly separated from information about the glottal source.\n It turns out that cepstral coefficients have the extremely useful property that the\n variance of the different coefficients tends to be uncorrelated. This is not true for the\n spectrum, where spectral coefficients at different frequency bands are correlated.\n For those who have had signal processing, the cepstrum is more formally defined\n as the inverse DFT of the log magnitude of the DFT of a signal; hence, for a\n windowed frame of speech x[n],\n\n N‚àí1 N‚àí1\n !\n ‚àí j 2œÄ 2œÄ\n X X\n c[n] = log x[n]e N kn e j N kn (14.17)\n n=0 n=0\n\n Energy To the 12 cepstral coefficients from the prior section we add a 13th feature:\n the energy from the frame. Energy is a useful cue for phone detection (for example\n energy vowels and sibilants have more energy than stops). The energy in a frame is the sum\n over time of the power of the samples in the frame; thus, for a signal x in a window\n from time sample t1 to time sample t2 , the energy is\n t2\n X\n Energy = x2 [t] (14.18)\n t=t1\n\n Delta features We also add features related to the change in cepstral features over\n time. Changes in the speech signal, like the slope of a formant at its transitions,\n or the change from a stop closure to stop burst, can again provide a useful cue for\n delta feature phone identity. To each of the 13 features (12 cepstral features plus energy) a delta\n double delta or velocity feature and a double delta or acceleration feature. Each of the 13 delta\n features represents the change between frames in the corresponding cepstral/energy\n feature, and each of the 13 double delta features represents the change between\n frames in the corresponding delta features. These deltas can be simply computed by\n just subtracting the value at a frame from the prior value, but in practice it‚Äôs common\n to fit a polynomial and take its first and second derivative.\n\n This chapter has introduced many of the important concepts of phonetics and computational phonetics.\n ‚Ä¢ We can represent the pronunciation of words in terms of units called phones.\n The standard system for representing phones is the International Phonetic\n Alphabet or IPA. The most common computational system for transcription\n of English is the ARPAbet, which conveniently uses ASCII symbols.\n28 C HAPTER 14 ‚Ä¢ P HONETICS AND S PEECH F EATURE E XTRACTION\n\n ‚Ä¢ Phones can be described by how they are produced articulatorily by the vocal\n organs; consonants are defined in terms of their place and manner of articulation and voicing; vowels by their height, backness, and roundness.\n ‚Ä¢ Speech sounds can also be described acoustically. Sound waves can be described in terms of frequency, amplitude, or their perceptual correlates, pitch\n and loudness.\n ‚Ä¢ The spectrum of a sound describes its different frequency components. While\n some phonetic properties are recognizable from the waveform, both humans\n and machines rely on spectral analysis for phone detection.\n ‚Ä¢ A spectrogram is a plot of a spectrum over time. Vowels are described by\n characteristic harmonics called formants.\n\nHistorical Notes\n The major insights of articulatory phonetics date to the linguists of 800‚Äì150 B . C .\n India. They invented the concepts of place and manner of articulation, worked out\n the glottal mechanism of voicing, and understood the concept of assimilation. European science did not catch up with the Indian phoneticians until over 2000 years\n later, in the late 19th century. The Greeks did have some rudimentary phonetic\n knowledge; by the time of Plato‚Äôs Theaetetus and Cratylus, for example, they distinguished vowels from consonants, and stop consonants from continuants. The Stoics\n developed the idea of the syllable and were aware of phonotactic constraints on possible words. An unknown Icelandic scholar of the 12th century exploited the concept\n of the phoneme and proposed a phonemic writing system for Icelandic, including\n diacritics for length and nasality. But his text remained unpublished until 1818 and\n even then was largely unknown outside Scandinavia (Robins, 1967). The modern\n era of phonetics is usually said to have begun with Sweet, who proposed what is\n essentially the phoneme in his Handbook of Phonetics 1877. He also devised an alphabet for transcription and distinguished between broad and narrow transcription,\n proposing many ideas that were eventually incorporated into the IPA. Sweet was\n considered the best practicing phonetician of his time; he made the first scientific\n recordings of languages for phonetic purposes and advanced the state of the art of\n articulatory description. He was also infamously difficult to get along with, a trait\n that is well captured in Henry Higgins, the stage character that George Bernard Shaw\n modeled after him. The phoneme was first named by the Polish scholar Baudouin\n de Courtenay, who published his theories in 1894.\n Introductory phonetics textbooks include Ladefoged (1993) and Clark and Yallop (1995). Wells (1982) is the definitive three-volume source on dialects of English.\n Many of the classic insights in acoustic phonetics had been developed by the\n late 1950s or early 1960s; just a few highlights include techniques like the sound\n spectrograph (Koenig et al., 1946), theoretical insights like the working out of the\n source-filter theory and other issues in the mapping between articulation and acoustics ((Fant, 1960), Stevens et al. 1953, Stevens and House 1955, Heinz and Stevens\n 1961, Stevens and House 1961) the F1xF2 space of vowel formants (Peterson and\n Barney, 1952), the understanding of the phonetic nature of stress and the use of\n duration and intensity as cues (Fry, 1955), and a basic understanding of issues in\n phone perception (Miller and Nicely 1955,Liberman et al. 1952). Lehiste (1967) is\n a collection of classic papers on acoustic phonetics. Many of the seminal papers of\n Gunnar Fant have been collected in Fant (2004).\n E XERCISES 29\n\n Speech feature-extraction algorithms were developed in the 1960s and early\n 1970s, including the efficient fast Fourier transform (FFT) (Cooley and Tukey, 1965),\n the application of cepstral processing to speech (Oppenheim et al., 1968), and the\n development of LPC for speech coding (Atal and Hanauer, 1971).\n Excellent textbooks on acoustic phonetics include Johnson (2003) and Ladefoged (1996). Coleman (2005) includes an introduction to computational processing of acoustics and speech from a linguistic perspective. Stevens (1998) lays out\n an influential theory of speech sound production. There are a number of software\n packages for acoustic phonetic analysis. Many of the figures in this book were gen-\nPraat erated by the Praat package (Boersma and Weenink, 2005), which includes pitch,\n spectral, and formant analysis, as well as a scripting language.\n\nExercises\n a. ‚Äúthree‚Äù [dh r i] d. ‚Äústudy‚Äù [s t uh d i] g. ‚Äúslight‚Äù [s l iy t]\n b. ‚Äúsing‚Äù [s ih n g] e. ‚Äúthough‚Äù [th ow]\n c. ‚Äúeyes‚Äù [ay s] f. ‚Äúplanning‚Äù [p pl aa n ih ng]\n nunciations (each) of the words ‚Äútomato‚Äù, ‚Äúpotato‚Äù, and ‚Äúeither‚Äù. Transcribe\n into the ARPAbet both pronunciations of each of these three words.\n 1. dark\n 2. suit\n 3. greasy\n 4. wash\n 5. water\n Download the Praat software, and use it to transcribe the wavefiles at the word\n level and into ARPAbet phones, using Praat to help you play pieces of each\n wavefile and to look at the wavefile and the spectrogram.\n Find F1 and F2 for each of your vowels.\n30 Chapter 14 ‚Ä¢ Phonetics and Speech Feature Extraction\n\nAtal, B. S. and S. Hanauer. 1971. Speech analysis and syn- Liberman, A. M., P. C. Delattre, and F. S. Cooper. 1952. The\n thesis by prediction of the speech wave. JASA, 50:637‚Äì role of selected stimulus variables in the perception of the\n 655. unvoiced stop consonants. American Journal of Psychol-\nBaayen, R. H., R. Piepenbrock, and L. Gulikers. 1995. ogy, 65:497‚Äì516.\n The CELEX Lexical Database (Release 2) [CD-ROM]. Miller, G. A. and P. E. Nicely. 1955. An analysis of percep-\nLinguistic Data Consortium, University of Pennsylvania tual confusions among some English consonants. JASA,\n [Distributor]. 27:338‚Äì352.\nBeckman, M. E. and G. M. Ayers. 1997. Guidelines NIST. 1990. TIMIT Acoustic-Phonetic Continuous Speech\n for ToBI labelling. Unpublished manuscript, Ohio Corpus. National Institute of Standards and Technology\n State University, http://www.ling.ohio-state. Speech Disc 1-1.1. NIST Order No. PB91-505065.\n edu/research/phonetics/E_ToBI/. Oppenheim, A. V., R. W. Schafer, and T. G. J. Stockham.\nBeckman, M. E. and J. Hirschberg. 1994. The ToBI annota- 1968. Nonlinear filtering of multiplied and convolved sigtion conventions. Manuscript, Ohio State University. nals. Proceedings of the IEEE, 56(8):1264‚Äì1291.\nBennett, R. and E. Elfner. 2019. The syntax‚Äìprosody inter- Ostendorf, M., P. Price, and S. Shattuck-Hufnagel. 1995. The\n face. Annual Review of Linguistics, 5:151‚Äì171. Boston University Radio News Corpus. Technical Report\nBoersma, P. and D. Weenink. 2005. Praat: doing phonetics ECS-95-001, Boston University.\n by computer (version 4.3.14). [Computer program]. Re- Peterson, G. E. and H. L. Barney. 1952. Control methods\n trieved May 26, 2005, from http://www.praat.org/. used in a study of the vowels. JASA, 24:175‚Äì184.\nClark, J. and C. Yallop. 1995. An Introduction to Phonetics Pitt, M. A., L. Dilley, K. Johnson, S. Kiesling, W. D. Rayand Phonology, 2nd edition. Blackwell. mond, E. Hume, and E. Fosler-Lussier. 2007. Buckeye\nCMU. 1993. The Carnegie Mellon Pronouncing Dictionary corpus of conversational speech (2nd release). Departv0.1. Carnegie Mellon University. ment of Psychology, Ohio State University (Distributor).\nColeman, J. 2005. Introducing Speech and Language Pro- Pitt, M. A., K. Johnson, E. Hume, S. Kiesling, and W. D.\n cessing. Cambridge University Press. Raymond. 2005. The buckeye corpus of conversational\n speech: Labeling conventions and a test of transcriber re-\nCooley, J. W. and J. W. Tukey. 1965. An algorithm for the liability. Speech Communication, 45:90‚Äì95.\n machine calculation of complex Fourier series. Mathematics of Computation, 19(90):297‚Äì301. Price, P. J., M. Ostendorf, S. Shattuck-Hufnagel, and\n C. Fong. 1991. The use of prosody in syntactic disam-\nDavis, S. and P. Mermelstein. 1980. Comparison of para- biguation. JASA, 90(6).\n metric representations for monosyllabic word recognition\n in continuously spoken sentences. IEEE Transactions on Robins, R. H. 1967. A Short History of Linguistics. Indiana\n ASSP, 28(4):357‚Äì366. University Press, Bloomington.\nFant, G. M. 1960. Acoustic Theory of Speech Production. Seneff, S. and V. W. Zue. 1988. Transcription and align-\nMouton. ment of the TIMIT database. Proceedings of the Second\n Symposium on Advanced Man-Machine Interface through\nFant, G. M. 2004. Speech Acoustics and Phonetics. Kluwer. Spoken Language.\nFitt, S. 2002. Unisyn lexicon. http://www.cstr.ed.ac. Shoup, J. E. 1980. Phonological aspects of speech recogniuk/projects/unisyn/. tion. In W. A. Lea, ed., Trends in Speech Recognition,\nFry, D. B. 1955. Duration and intensity as physical correlates 125‚Äì138. Prentice Hall.\n of linguistic stress. JASA, 27:765‚Äì768. Silverman, K., M. E. Beckman, J. F. Pitrelli, M. Ostendorf,\nGreenberg, S., D. Ellis, and J. Hollenback. 1996. Insights C. W. Wightman, P. J. Price, J. B. Pierrehumbert, and\n into spoken language gleaned from phonetic transcription J. Hirschberg. 1992. ToBI: A standard for labelling Enof the Switchboard corpus. ICSLP. glish prosody. ICSLP.\nHeinz, J. M. and K. N. Stevens. 1961. On the properties of Stevens, K. N. 1998. Acoustic Phonetics. MIT Press.\n voiceless fricative consonants. JASA, 33:589‚Äì596. Stevens, K. N. and A. S. House. 1955. Development of\nJohnson, K. 2003. Acoustic and Auditory Phonetics, 2nd a quantitative description of vowel articulation. JASA,\n edition. Blackwell. 27:484‚Äì493.\nKoenig, W., H. K. Dunn, and L. Y. Lacy. 1946. The sound Stevens, K. N. and A. S. House. 1961. An acoustical theory\n spectrograph. JASA, 18:19‚Äì49. of vowel production and some of its implications. Journal\nLadefoged, P. 1993. A Course in Phonetics. Harcourt Brace of Speech and Hearing Research, 4:303‚Äì320.\n Jovanovich. (3rd ed.). Stevens, K. N., S. Kasowski, and G. M. Fant. 1953. An elec-\nLadefoged, P. 1996. Elements of Acoustic Phonetics, 2nd trical analog of the vocal tract. JASA, 25(4):734‚Äì742.\n edition. University of Chicago. Stevens, S. S. and J. Volkmann. 1940. The relation of pitch\nLehiste, I., ed. 1967. Readings in Acoustic Phonetics. MIT to frequency: A revised scale. The American Journal of\n Press. Psychology, 53(3):329‚Äì353.\nLi, A., F. Zheng, W. Byrne, P. Fung, T. Kamm, L. Yi, Stevens, S. S., J. Volkmann, and E. B. Newman. 1937. A\n Z. Song, U. Ruhi, V. Venkataramani, and X. Chen. 2000. scale for the measurement of the psychological magni-\nCASS: A phonetically transcribed corpus of Mandarin tude pitch. JASA, 8:185‚Äì190.\n spontaneous speech. ICSLP. Sweet, H. 1877. A Handbook of Phonetics. Clarendon Press.\n Exercises 31\n\nTaylor, P. 2009. Text-to-Speech Synthesis. Cambridge University Press.\nWells, J. C. 1982. Accents of English. Cambridge University\n Press.\nXu, Y. 2005. Speech melody as articulatorily implemented\n communicative functions. Speech communication, 46(3-\n4):220‚Äì251.\n",
    "file_path": "/Users/colinsidberry/nlp-textbook-rag/data/extracted_text/14.Phonetics and Speech Feature Extraction.txt",
    "file_size_kb": 79.32
  },
  {
    "id": "d2d192af03311f77",
    "source": "nlp_textbook",
    "chapter": "15 Automatic Speech Recognition",
    "filename": "15.Automatic Speech Recognition.txt",
    "content": "rights reserved. Draft of August 24, 2025.\n\nCHAPTER\n\n15 Automatic Speech Recognition\n\n I KNOW not whether\n I see your meaning: if I do, it lies\n Upon the wordy wavelets of your voice,\n Dim as an evening shadow in a brook,\n Thomas Lovell Beddoes, 1851\n\n Understanding spoken language, or at least transcribing the words into writing, is\n one of the earliest goals of computer language processing. In fact, speech processing\n predates the computer by many decades!\n The first machine that recognized speech\n was a toy from the 1920s. ‚ÄúRadio Rex‚Äù,\n shown to the right, was a celluloid dog\n that moved (by means of a spring) when\n the spring was released by 500 Hz acoustic energy. Since 500 Hz is roughly the\n first formant of the vowel [eh] in ‚ÄúRex‚Äù,\n Rex seemed to come when he was called\n (David, Jr. and Selfridge, 1962).\n In modern times, we expect more of our automatic systems. The task of auto-\nASR matic speech recognition (ASR) is to map any waveform like this:\n\n to the appropriate string of words:\n It‚Äôs time for lunch!\n Automatic transcription of speech by any speaker in any environment is still far from\n solved, but ASR technology has matured to the point where it is now viable for many\n practical tasks. Speech is a natural interface for communicating with appliances, or\n with digital assistants or chatbots, especially on cellphones, where keyboards are\n less convenient. ASR is also useful for general transcription, for example for automatically generating captions for audio or video text (transcribing movies or videos\n or live discussions). Transcription is important in fields like law where dictation\n plays an important role. Finally, ASR is important as part of augmentative communication (interaction between computers and humans with some disability resulting\n in difficulties or inabilities in typing or audition). The blind Milton famously dictated Paradise Lost to his daughters, and Henry James dictated his later novels after\n a repetitive stress injury.\n In the next sections we‚Äôll introduce the various goals of the ASR task, describe\n how acoustic features are extracted, and introduce the convolutional neural net\n architecture which is commonly used as an initial layer in speech recognition tasks.\n2 C HAPTER 15 ‚Ä¢ AUTOMATIC S PEECH R ECOGNITION\n\n We‚Äôll then introduce two families of methods for ASR. The first is the encoderdecoder paradigm, and we‚Äôll introduce the baseline attention-based encoder decoder\n algorithm, sometimes called Listen Attend and Spell after an early implementation.\n We‚Äôll also introduce a more advanced encoder-decoder system, OpenAI‚Äôs Whisper\n system (Radford et al., 2023) as well an open system based on the same architecture,\n OWSM (the Open Whisper-style Speech Model) (Peng et al., 2023). (These models have additional capabilities including translation, as we‚Äôll discuss later). The\n second is the use of self-supervised speech models (sometimes called SSL for selfsupervised learning) like Wav2Vec2.0 or HuBERT, which are encoders that learn\n abstract representations of speech that can be used for ASR by pairing them with the\n CTC loss function for decoding.\n We‚Äôll conclude with the standard word error rate metric used to evaluate ASR.\n\n Before describing algorithms for ASR, let‚Äôs talk about how the ASR task itself\n varies. One dimension of variation is vocabulary size. Some ASR tasks have long\n been solved with extremely high accuracy, like those with a 2-word vocabulary (yes\n digit\n recognition versus no) or an 11 word vocabulary like digit recognition (recognizing sequences\n of digits including zero to nine plus oh). Open-ended tasks like accurately transcribing videos or human conversations, with large vocabularies of 60,000 or more\n words, are much harder.\n A second dimension of variation is who the speaker is talking to. Humans speaking to machines (either dictating or talking to a dialogue system) are easier to recogread speech nize than humans speaking to humans. Read speech, in which humans are reading\n out loud, for example in audio books, is also relatively easy to recognize. Recogconversational\n speech nizing the speech of two humans talking to each other in conversational speech,\n for example, for transcribing a business meeting, is the hardest. It seems that when\n humans talk to machines, or read without an audience present, they simplify their\n speech quite a bit, talking more slowly and more clearly.\n A third dimension of variation is channel and noise. Speech is easier to recognize\n if it‚Äôs recorded in a quiet room with head-mounted microphones than if it‚Äôs recorded\n by a distant microphone on a noisy city street, or in a car with the window open.\n A final dimension of variation is accent or speaker-class characteristics. Speech\n is easier to recognize if the speaker is speaking the same dialect or variety that the\n system was trained on. Speech by speakers of regional or ethnic dialects, or speech\n by children can be quite difficult to recognize if the system is only trained on speakers of standard dialects, or only adult speakers.\n A number of publicly available corpora with human-created transcripts are used\n to create ASR test and training sets to explore this variation; we mention a few of\n LibriSpeech them here since you will encounter them in the literature. LibriSpeech is a large\n open-source read-speech 16 kHz dataset with over 1000 hours of audio books from\n the LibriVox project, which has volunteers read and record copyright-free books\n (Panayotov et al., 2015). It has transcripts aligned at the sentence level. It is divided\n into an easier (‚Äúclean‚Äù) and a more difficult portion (‚Äúother‚Äù) with the clean portion\n of higher recording quality and with accents closer to US English; The division was\n done when the corpus was first released by running a speech recognizer (trained\n on read speech from the Wall Street Journal) on all the audio, computing the WER\n for each speaker based on the gold transcripts, and dividing the speakers roughly in\n 15.1 ‚Ä¢ T HE AUTOMATIC S PEECH R ECOGNITION TASK 3\n\n half, with recordings from lower-WER speakers called ‚Äúclean‚Äù and recordings from\n higher-WER speakers ‚Äúother‚Äù.\nSwitchboard The Switchboard corpus of prompted telephone conversations between strangers\n was collected in the early 1990s; it contains 2430 conversations averaging 6 minutes each, totaling 240 hours of 8 kHz speech and about 3 million words (Godfrey\n et al., 1992). Switchboard has the singular advantage of an enormous amount of\n auxiliary hand-done linguistic labeling, including parses, dialogue act tags, phonetic\nCALLHOME and prosodic labeling, and discourse and information structure. The CALLHOME\n corpus was collected in the late 1990s and consists of 120 unscripted 30-minute\n telephone conversations between native speakers of English who were usually close\n friends or family (Canavan et al., 1997).\n CHiME A variety of corpora try to include input that is more natural. The CHiME\n Challenge is a series of difficult shared tasks with corpora that deal with robustness\n in ASR. The CHiME 6 task, for example, is ASR of conversational speech in real\n home environments (specifically dinner parties). The corpus contains recordings of\n twenty different dinner parties in real homes, each with four participants, and in three\n locations (kitchen, dining area, living room), recorded with distant microphones.\n AMI The AMI Meeting Corpus contains 100 hours of recorded group meetings (some\n natural meetings, some specially organized), with manual transcriptions and some\n CORAAL additional hand-labels (Renals et al., 2007). CORAAL is a collection of over 150\n sociolinguistic interviews with African American speakers, with the goal of studying\n African American English (AAE), the many variations of language used in African\n American communities and others (Kendall and Farrington, 2020). The interviews\n are anonymized with transcripts aligned at the utterance level.\n There are a wide variety of corpora available in other languages. In Chinese,\n HKUST for example, the HKUST Mandarin Telephone Speech corpus has 1206 transcribed\n ten-minute telephone conversations between speakers of Mandarin across China including conversations between friends and between strangers (Liu et al., 2006). The\nAISHELL-1 AISHELL-1 corpus contains 170 hours of Mandarin read speech of sentences taken\n from various domains, read by different speakers mainly from northern China (Bu\n et al., 2017).\n Finally, there are many multilingual corpora. Common Voice (Ardila et al.,\n 2020) is a freely available crowd-sourced corpus of transcribed read speech, stored\n in MPEG-3 format and designed for ASR. Crowd-working volunteers record themselves reading scripted speech, with scripts often extracted from from Wikipedia\n articles. The recordings are then verified by other contributors. As of the writing of\n this chapter, Common Voice includes 33,150 hours of speech from 133 languages.\n FLEURS (Conneau et al., 2023) is a parallel speech dataset, built on the MT benchmark FLoRes-101 (Goyal et al., 2022), which has 3001 sentences extracted from\n English Wikipedia and translated into 101 other languages by human translators.\n For a subset of 2009 of the sentences in each of the 102 languages, FLEURS has\n recordings of 3 different native speakers reading the sentence, in total about 12 hours\n of speech per language.\n or WER, defined on page 22) from roughly state-of-the-art systems as of the time of\n this writing on some of these tasks. Note that the error rate on English read speech\n (like the LibriSpeech clean audiobook corpus) is around 2% ; transcription of speech\n read in English is highly accurate. By contrast, the error rate for transcribing conversations between humans is higher; 5.8 to 11% for the Switchboard and CALLHOME\n corpora or AMI meetings. The error rate is higher yet again for speakers of varieties\n4 C HAPTER 15 ‚Ä¢ AUTOMATIC S PEECH R ECOGNITION\n\n like African American English, and yet again for difficult conversational tasks like\n transcription of 4-speaker dinner party speech, which can have error rates as high as\n 25.5%. Character error rates (CER) are also higher for Mandarin natural conversation than for Mandarin read speech. Error rates are even higher for lower resource\n languages; we‚Äôve shown a handful of examples.\n\n English Tasks WER%\n LibriSpeech audiobooks 960hour clean 1.4\n LibriSpeech audiobooks 960hour other 2.6\n Switchboard telephone conversations between strangers 5.8\n CALLHOME telephone conversations between family 11\n AMI meetings 11\n Sociolinguistic interviews, CORAAL (AAE) 16.2\n CHiME6 dinner parties with distant microphones 25.5\n Sample tasks in other languages WER%\n Common Voice 15 Vietnamese 39.8\n Common Voice 15 Swahili 51.2\n FLEURS Bengali 50\n Chinese (Mandarin) Tasks CER%\n AISHELL-1 Mandarin read speech corpus 3.9\n HKUST Mandarin Chinese telephone conversations 18.5\n 2023-4 for ASR on various American English and other language recognition tasks, and character error rates (CER) for two Chinese recognition tasks.\n\n CNN The convolutional neural network, or CNN (and sometimes shortened as convnet),\n is a network architecture that is particularly useful for extracting features in speech\n and vision applications. A convolutional layer for speech takes as input a representation of the audio input (either as the raw audio or as Mel spectra) and produces\n as output a sequence of latent representations of the input speech. In ASR systems\n like Whisper, wav2vec2.0, or HuBERT, convolutional layers are stacked as an initial\n set of layers producing speech representations that are then passed to transformer\n layers.\n A standard feedforward layer is fully connected; every input is connected to every output. By contrast, a convolutional network makes use of the idea of a kernel, a\n kind of smaller network that we pass over the input. For example in image classification tasks, we pass the kernel horizontally and vertically over the image to recognize\n visual features, and so we describe a visual as a 2d (for 2 dimensional) convolutional\n network. For speech, we will slide our kernel over the signal in the time dimension\n to extract speech features, so CNNs for speech are 1d convolutional networks.\n Let‚Äôs flesh out this intuition a bit more. We‚Äôll start with a very schematic version\n of a convolutional layer that takes as input a single sequence of vectors x1 . . . xt\n and produces as output a single sequence of vectors z1 . . . zt , of the same length t.\n Afterwards we‚Äôll see how to deal with more complex inputs and outputs.\n kernel A CNN uses a kernel, a small vector of weights w1 . . . wk , to extract features. It\n convolving does this by convolving this kernel with the input. The convolution of a kernel with\n 15.2 ‚Ä¢ C ONVOLUTIONAL N EURAL N ETWORKS 5\n\n a signal has 3 steps\n 1. Flip the kernel left-to-right\n 2. Pass the kernel frame by frame (temporally) across the input\n ‚Ä¢ At each frame computing the dot product of the kernel with the local\n input values\n 3. The output is the resulting sequence of dot products\n We can think of the convolution process as finding regions in the signal that are\n similar to the kernel, since the dot product is high when two vectors are similar. The\n convolution operation is represented by the * operator (an unfortunate overloading\n of this symbol that also refers to simple multiplication). Let‚Äôs see how to compute\n x ‚àó w, the convolution of a single vector x with a kernel vector w. Let‚Äôs first think\n about the simple case of a kernel width of 1. We compute each output element z j as\n the product of the kernel with x j :\n\n convolution with width-1 kernel: z j = x j w1 ‚àÄ j : 1 ‚â§ j ‚â§ t (15.1)\n\n Fig. 15.2 shows an intuition of this computation.\n\n input x[n]\n x1 xt\n\n kernel w[k]\n w1\n\n ‚Ä¶\n output z[n]\n z1 z2 z3 ‚Ä¶ zt\n\n The kernel is walked across the input, and the output at each frame zi is the dot product of the\n kernel with the input frame. With a kernel of length 1 we don‚Äôt have to worry about flipping\n the kernel, and the dot product is just the scalar product. The figure shows the computation of\n z3 as x3 √ó w1 .\n\n Let‚Äôs now turn to longer kernels. Although we‚Äôve described the first step of the\n convolution as flipping the kernel, in fact in ASR systems (or in component libraries\n like pytorch) we skip this step. Technically this means that the algorithm we are\n crosscorrelation using is not in fact convolution, it‚Äôs instead cross-correlation, which is the name for\n an algorithm of walking a kernel across a signal, computing its dot product frame by\n frame, without flipping it first. The difference doesn‚Äôt matter, since the parameters\n of the kernel will be learned during training, and so the model could easily learn a\n kernel with the parameters in either order. Still, for historical reasons we still call\n this process a 1d convolution rather than cross-correlation.\n Let‚Äôs see a more general equation for these longer kernels. To avoid the conpad volution being undefined at the left and right edges of the signal, we can pad the\n input by adding a small number p of zeros at the beginning and end of the signal,\n so that we can start the center of the kernel at the first element x1 , and there will be\n a defined value to the left of x1 . This also turns out to make it simple to have the\n6 C HAPTER 15 ‚Ä¢ AUTOMATIC S PEECH R ECOGNITION\n\n output length as the same as the input length. To do this, it‚Äôs convenient to define the\n kernel vector as having an odd number of elements of length k = 2p + 1, thus with\n the center element having p elements on either side. Each element z j of the output\n vector z is then computed as the following dot product:\n\n p\n X\n zj = x j+i wi+p (15.2)\n i=‚àíp\n\n Fig. 15.3 shows the computation of the convolution x ‚àó w with a kernel whose width\n is 3, and with padding of 1 frame at the beginning and end of x with a value of zero.\n\n padding padding\n\n input x[n]\n x1 xt\n\n kernel w[k]\n w1 wk\n\n ‚Ä¶\n output z[n]\n z1 z2 z3 ‚Ä¶\n\n padding of 1, showing a zero value added at the start and end of the signal. The (already\n flipped) kernel is walked across the input, and the output at each frame zi is the dot product\n of the kernel with the input in the window. The figure shows the computation of z3 .\n\n Note that the size k (the receptive field) of the kernel is designed to be small\n compared to the signal. For example for the convolutional layers in Whisper, the\n kernel width is 3 frames, meaning the kernel is a vector of length 3 (we say that\n receptive field the kernel has a receptive field of 3). That means that the kernel is being compared\n to 3 frames of speech. In Whisper there is a frame every 10 ms and each frame\n represent a window of 25ms of speech information. That means each kernel is extracting information from about 40 ms of speech (10 + 10 + 12.5 + 12.5). That‚Äôs long\n enough to extract various phonetic features like formant transitions or stop closures\n or aspiration.\n We‚Äôve now described a simplified view of convolution in which the input is a\n single vector x and the output is a single vector z, both corresponding to a signal\n over time. In practice, the input to a convolutional layer is commonly the output\n from a log mel spectrum, which means it has many (say 128) channels, one for each\n log mel filters output. The kernel will have separate vectors for each of these input\n depth channels. We say that the kernel has a depth of 128, meaning that the kernel is of\n shape [128,3].\n To get the output of the kernel, we sum over all the input channels. That is, we\n get a single output zc for each of the input channels xc by convolving the kernel w\n 15.2 ‚Ä¢ C ONVOLUTIONAL N EURAL N ETWORKS 7\n\n with it, and then we sum up all the resulting outputs:\n Ci\n X\n z= xc ‚àó w (15.3)\n c=1\n\n The output at frame j, z j , thus integrates information from all of the input channels.\n Finally, the output from a convolution layer is also more complex than just a vector consisting of a single scalar value to represent each frame. Instead, the output of\n the convolution layer for a given input frame needs to be an embedding, a latent representation of that frame. As with all neural models, latent representations should\n have the model dimensionality, whatever that is. For example the model dimensionality of Whisper is 1280, and so the convolutional layer needs have one output\n channel for each of these 1280 dimensions of the model. In order to do this, we‚Äôll\n actually learn one separate kernel for each of the model dimensions. That is, we‚Äôll\n learn 1280 separate kernels, each kernel having the depth of the number of input\n channels (for example 128), and a filter-width (say of 3). That way, the embedding\n representation of each frame will have 1280 independently computed features of the\n input signal. We show a schematic in Fig. 15.4\n\n dim 1024\n output ‚Ä¶\n dimension\n dim 35\n channels\n ‚Ä¶\n dim 1\n zi time\n ‚àë\n kernel 35 128\n 128 ‚Ä¶\n Depth ‚Ä¶ 2\n 2 1\n 1 dot product\n Width 3 with\n kernel 35\n\n 128 ‚Ä¶\n log mel\n input 2\n channels\n xi-1 xi xi+1 time\n\n output channels. We see how at time point i one of the 1024 kernels (‚Äúkernel 35‚Äù, each of\n depth 128 and width 3) is dot-product-ed with (each of) the 128 log mel spectrum input vectors, and then summed to produce a single value for one dimension of the output embedding\n at time i.\n\nstride A 1d convolution layer can also have a stride. Stride is the amount that we move\n the kernel over the input between each step. The figures above show a stride of 1,\n meaning that we first position the kernel over x1 , then x2 , then x3 , and so on. For a\n stride of 2, we would first position the kernel over x1 , then x3 , then x5 , and so on.\n A longer stride means a shorter output sequence; a stride of two means the output\n8 C HAPTER 15 ‚Ä¢ AUTOMATIC S PEECH R ECOGNITION\n\n sequence z will be half the length of the input sequence x. Convolutional layers with\n strides greater than 1 are commonly used to shorten an input sequence. This is useful\n partly because a shorter signal takes less memory and computational bandwidth, but\n also, as we‚Äôll see in the next section, because it helps address the mismatch between\n the length of acoustic frame embeddings (10 ms) and letters or words, which cover\n much more of the signal.\n Finally, in practice a convolutional layer can be followed by an output nonlinearity, like a ReLU layer.\n\n The first ASR architecture we introduce is the encoder-decoder architecture, the\n same architecture introduced for MT in Chapter 12. Fig. 15.5 sketches this architec-\nAED ture, called attention-based encoder decoder or AED, or listen attend and spell\n listen attend\n and spell (LAS) after the two papers which first applied it to speech (Chorowski et al. 2014,\n Chan et al. 2016).\n The input to the architecture x is a sequence of t acoustic feature vectors X =\n x1 , x2 , ..., xt , one vector per 10 ms frame. We often start from the log mel spectral\n features described in the previous section, although it‚Äôs also possible to start from a\n raw wavfile. The output sequence Y can be either letters or tokens (BPE or sentencepiece); we‚Äôll assume letters just to simplify the explanation here.. Thus the output\n sequence Y = (hSOSi, y1 , ..., ym hEOSi), assuming special start of sequence and end\n of sequence tokens hsosi and heosi and each yi is a character; for English we might\n choose the set:\n yi ‚àà {a, b, c, ..., z, 0, ..., 9, hspacei, hcommai, hperiodi, hapostrophei, hunki}\n\n y1 y2 y3 y4 y5 y6 y7 y8 y9 ym\n\n i t ‚Äò s t i m e ‚Ä¶\n\n H\n DECODER\n\n ENCODER\n <s> i t ‚Äò s t i m ‚Ä¶\n Shorter sequence X x1 ‚Ä¶ xn\n\n Subsampling\n 80-dimensional\n log Mel spectrum f1 ‚Ä¶ ft\n per frame\n Feature Computation\n\n This architecture is also used in the Whisper model from OpenAI (Radford et al.,\n 2023). Fig. 15.6 shows a subpart of the Whisper architecture (Whisper also does\n other speech tasks like speech translation and voice activity detection, which we‚Äôll\n discuss in the next chapter). Whisper models and inference code are publicly released, but the training code and training data are not. However, there are opensource projects that use a Whisper-style architecture, like the Open Whisper-style\n 15.3\nSpeech Recognition via Large-Scale Weak Supervision ‚Ä¢ T HE E NCODER -D ECODER A RCHITECTURE FOR ASR 4 9\n\n Sequence-to-sequence learning\n task training data (680k hours) EN\n TRANS-\nCRIBE 0.0 The quick brown ‚ãØ\n next-token\n h transcription prediction\n\n ‚ÄúAsk not what your country can do for ‚ãØ‚Äù MLP\n\n MLP cross attention\n Ask not what your country can do for ‚ãØ\n self attention self attention\n\no-English speech translation\n\n cross attention\n ‚ãÆ ‚ãÆ ‚ãÆ\n ‚ÄúEl r√°pido zorro marr√≥n salta sobre ‚ãØ‚Äù Transformer\n Encoder Blocks MLP MLP Transformer\n The quick brown fox jumps over ‚ãØ Decoder Blocks\n self attention cross attention\n\n self attention\nEnglish transcription MLP\n\n self attention MLP\n Ïñ∏Îçï ÏúÑÏóê Ïò¨Îùº ÎÇ¥Î†§Îã§Î≥¥Î©¥ ÎÑàÎ¨¥ÎÇò ÎÑìÍ≥† ÎÑìÏùÄ ‚ãØ‚Äù\n\n ~\n cross attention\n Sinusoidal\n Ïñ∏Îçï ÏúÑÏóê Ïò¨Îùº ÎÇ¥Î†§Îã§Î≥¥Î©¥ ÎÑàÎ¨¥ÎÇò ÎÑìÍ≥† ÎÑìÏùÄ ‚ãØ Positional self attention\n Encoding\n eech Learned\n 2 √ó Conv1D + GELU Positional\n Encoding\n (background music playing)\n TRANS-\n ‚àÖ SOT EN CRIBE 0.0 The quick ‚ãØ\n Log-Mel Spectrogram Tokens in Multitask Training Format\n\n Whisper is a multitask system that also does translation and diarization (we‚Äôll discuss these\n non-ASR tasks in the following chapter), Whisper‚Äôs transcription format has a Start of Tranask training format Language\n script (SOT)X‚ÜíX\n token, a language tag, and then an instruction\n Time-aligned token for whether to transcribe or\n transcription\n identification Transcription\n translate.\n LANGUAGE begin end begin end\n time ‚ãØ\n TRANSCRIBE text tokens text tokens\n previous START OF\n TAG Speech Model (OWSM), time which reproduces reproduces\n time Whisper-style\n time training but\nREV EOT\n text tokens TRANSCRIPT offers a fully open-source toolkit and publicly available data (Peng et al., 2023).\n NO NO\n TRANSLATE text tokens\n SPEECH TIMESTAMPS\n Custom vocabulary /\n prompting\n 15.3.1X ‚Üí Input\n Voice activity English and Convolutional Layers\n Text-only transcription\n detection Translation\n l text timestamp (allows dataset-specific fine-tuning)\ns tokens tokens The encoder-decoder architecture is particularly appropriate when input and output\n (VAD)\n\n sequences have stark length differences, as they do for speech, with long acoustic\n feature sequences mapping to much shorter sequences of letters or words. For example English,\n Overview of our approach. A sequence-to-sequence words aremodel\n Transformer on average 5 letters\n is trained on manyor 1.3 BPE tokens\n different speechlong (Bostrom\n processing and\n tasks,\n Durrett, 2020)\ng multilingual speech recognition, speech translation, and,language\n spoken in natural conversation,\n identification, andthe average\n voice word\n activity lasts about\n detection. All of 250 milthese\n jointly represented as a sequence of tokensliseconds (Yuanby\n to be predicted et the\n al.,decoder,\n 2006), or 25 frames\n allowing for aofsingle\n 10ms. So the\n model speechmany\n to replace signaldifferent\n in 10ms\n a traditional speech processing pipeline. The frames is about\n multitask 5 (25/5)\n training to uses\n format 19 (25/1.3)\n a set of times\n specialonger\n tokensthan\n that the\n servetext\n as signal in wordsoror\n task specifiers\n tion targets, as further explained in Sectiontokens.\n 2.3.\n Because this length difference is so extreme for speech, encoder-decoder architectures for speech usually have a compression stage that shortens the acoustic\n ining Details feature sequence beforelarge the encoder\n dataset stage. (We can\n to encourage additionally make\n generalization use of a loss\n and robustness.\n function that is designed to deal well with compression, like the CTC loss function\n Please see Appendix F for full training hyperparameters.3\n a suite of models of various sizes in order we‚Äôll to study later.)\n introduce\n ng properties of Whisper. Please see TableThe anof theDuring\n goal\n 1 for early is\n subsampling development\n to produce aand evaluation\n shorter sequence weXobserved\n = x1 , ..., xthat\n n that\nw. We train with data parallelism acrosswill be the input toWhisper\n accelerators the transformer\n modelsencoder. A very simple\n had a tendency baselineplausible\n to transcribe algorithmbut is a\nP16 with dynamic loss scalinglow frame methodcheckrateactivation\n and called low\n sometimesalmost frame\n always rate (Pundak\n incorrect guesses andfor\n Sainath, 2016):ofor\n the names time i we\n speakers.\ng (Griewank & Walther, 2000; Chen stack et al.,(concatenate)\n 2016). the acoustic\n This happensfeature\n because vector\n many fi with the priorintwo\n transcripts thevectors fi‚àí1 and\n pre-training\n were trained with AdamW (Loshchilov & Hutter, f i‚àí2 to make a new vector three times longer. Then we simply\n dataset include the name of the person who is speaking, delete f i‚àí1 and fi‚àí2 .\n Thus instead of (say) a 40-dimensional acoustic feature vector every 10 ms, we have\n nd gradient norm clipping (Pascanu et al., 2013) encouraging the model to try to predict them, but this infora longer vector (say 120-dimensional) every 30 ms, with a shorter sequence length\nnear learning rate decay to zero after a warmup over mation is only rarely inferable from only the most recent 30\n 2048 updates. A batch size of 256 segments was 3\n After the original release of Whisper, we trained an additional\n d the models are trained for 220 updates which is Large model (denoted V2) for 2.5X more epochs while adding\n two and three passes over the dataset. Due to only SpecAugment (Park et al., 2019), Stochastic Depth (Huang et al.,\nfor a few epochs, over-fitting is not a large concern, 2016), and BPE Dropout (Provilkov et al., 2019) for regularization.\ndo not use any data augmentation or regularization Reported results have been updated to this improved model unless\n10 C HAPTER 15 ‚Ä¢ AUTOMATIC S PEECH R ECOGNITION\n\n n = 3t .\n But the most common way of creating a shorter input sequence is to use the\n convolutional layers we introduced in the previous section. When a convolutional\n layer has a stride greater than 1, the output sequence becomes shorter than the input\n sequence. Let‚Äôs see this in two commonly used ASR systems.\n The Whisper system (Radford et al., 2023) has an audio context window of 30\n seconds. It extracts 128 channel log mel features for each frame, with a 25ms window and a stride of 10ms. These are then normalized to 0 mean and a range of -1\n to 1. A stride of 10 ms (100 frames per second) means there are 3000 input frames\n in a 30 second context window. These 3000 frames are passed to two convolutional\n layers, each one followed by a nonlinearity (Whisper uses GELU (Gaussian Error\n Linear Unit), which is a smoother version of ReLU). The first convolutional layer\n has 128 input channels and uses a stride of 1, with number of output channels being\n the model dimensionality, and the window length is 3000. For the second convolutional layer the number of input and output channels is the model dimensionality,\n and there is a stride of 2. The stride of 2 in the second convolutional layer makes the\n output sequence half the length of the input sequence, bringing the output window\n length down to 1500 and producing an audio token every 20 ms. Sinusoidal position\n embeddings are added to these audio encodings before the output of this front end\n is passed to the transformer encoder.\n HuBERT (Hsu et al., 2021) uses an alternative front end architecture, in which\n convolutional layers are used to completely replace the computation of the spectrum.\n So the input is raw 16kHz sampled audio, and it is passed through seven 512-channel\n layers with strides [5,2,2,2,2,2,2] and kernel widths [10,3,3,3,3,2,2] which learn both\n to extract spectral information, and to shorten the input sequence by 320x, from\n 16kHz (= one representation per .0625 ms) down to a 20 ms framerate. Positional\n encodings are added to the input, and then a GELU and layer norm are applied\n before the output is passed to the transformer encoder.\n\n 15.3.2 Inference\n After the convolutional stage, encoder-decoders for speech use the same architecture\n (transformer with cross-attention) as for MT.\n Let‚Äôs remind ourselves of the encoder-decoder architecture that we introduced\n in Chapter 12. It uses two transformers: an encoder, which is the same as the basic\n transformer from Chapter 8, and a decoder, which has one addition: a new layer\n called the cross-attention layer. The encoder takes the acoustic input X = x1 , ..., xn\n and maps them to an output representation Henc = h1 , ..., hn ; via a stack of encoder\n blocks.\n The decoder is essentially a conditional language model that attends to the encoder representation and generates the target text (letters or tokens) one by one, at\n each timestep conditioning on the audio representations from the encoder and the\n previously generated text to generate a new letter or token.\n The transformer blocks in the decoder have an extra layer with a special kind\ncross-attention of attention, cross-attention. Cross-attention has the same form as the multi-head\n attention in a normal transformer block, except that while the queries as usual come\n from the previous layer of the decoder, the keys and values come from the output of\n the encoder.\n That is, where in standard multi-head attention the input to each attention layer is\n X, in cross attention the input is the the final output of the encoder Henc = h1 , ..., hn .\n Henc is of shape [n √ó d], each row representing one acoustic input token. To link the\n 15.3 ‚Ä¢ T HE E NCODER -D ECODER A RCHITECTURE FOR ASR 11\n\n y1 y2 yi+1 ym\n ‚Ä¶ Language\n Modeling\n Henc h1 h2 hi hn Head\n ‚Ä¶ ‚Ä¶\n Unembedding Matrix\n Block K Block L\n\n ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶\n Block 2\n Block 2\n\n + +\n\n Feedforward Feedforward\n Encoder\n Block 1 Layer Normalize\n Layer Normalize\n Cross-Attention\n Multi-Head Attention Decoder\n Layer Normalize Block 1\n Layer Normalize +\n\n Causal (Left-to-Right)\n x1 x2 ‚Ä¶ xi ‚Ä¶ xn\n Multi-Head Attention\n\n Encoder Layer Normalize\n\n <> y1 ‚Ä¶ yi ‚Ä¶ ym\n\n ‚Ä¶\n Decoder\n\nfinal output of the encoder Henc = h1 , ..., hn is the context used in the decoder. The decoder is a standard\ntransformer except with one extra layer, the cross-attention layer, which takes that encoder output Henc and\nuses it to form its K and V inputs.\n\n keys and values from the encoder with the query from the prior layer of the decoder,\n we multiply the encoder output Henc by the cross-attention layer‚Äôs key weights WK\n and value weights WV . The query comes from the output from the prior decoder\n layer Hdec[`‚àí1] , which is multiplied by the cross-attention layer‚Äôs query weights WQ :\n\n Q = Hdec[`‚àí1] WQ ; K = Henc WK ; V = Henc WV (15.4)\n\n QK|\n \u0012 \u0013\n CrossAttention(Q, K, V) = softmax ‚àö V (15.5)\n dk\n\n The cross attention thus allows the decoder to attend to the acoustic input as projected into the entire encoder final output representations. The other attention layer\n in each decoder block, the multi-head attention layer, is the same causal (left-toright) attention that we saw in Chapter 8. But the multi-head attention in the encoder, however, is allowed to look ahead at the entire source language text, so it is\n not masked.\n For inference, the probability of the output string y is decomposed as:\n n\n Y\n p(y1 , . . . , yn ) = p(yi |y1 , . . . , yi‚àí1 , X) (15.6)\n i=1\n\n We can produce each letter of the output via greedy decoding:\n\n yÃÇi = argmaxchar‚àà Alphabet P(char|y1 ...yi‚àí1 , X) (15.7)\n12 C HAPTER 15 ‚Ä¢ AUTOMATIC S PEECH R ECOGNITION\n\n Alternatively encoder-decoders like Whisper or OWSM also use beam search as\n described in the next section. This is particularly relevant when we are adding a\n language model.\n Adding a language model Since an encoder-decoder model is essentially a conditional language model, encoder-decoders implicitly learn a language model for the\n output domain of letters from their training data. However, the training data (speech\n paired with text transcriptions) may not include sufficient text to train a good language model. After all, it‚Äôs easier to find enormous amounts of pure text training\n data than it is to find text paired with speech. Thus we can can usually improve a\n model at least slightly by incorporating a very large language model.\n The simplest way to do this is to use beam search to get a final beam of hyn-best list pothesized sentences; this beam is sometimes called an n-best list. We then use a\n rescore language model to rescore each hypothesis on the beam. The scoring is done by interpolating the score assigned by the language model with the encoder-decoder score\n used to create the beam, with a weight Œª tuned on a held-out set. Also, since most\n models prefer shorter sentences, ASR systems normally have some way of adding a\n length factor. One way to do this is to normalize the probability by the number of\n characters in the hypothesis |Y |c . The following is the scoring function for Listen,\n Attend, and Spell (Chan et al., 2016):\n score(Y |X) = log P(Y |X) + Œª log PLM (Y ) (15.8)\n |Y |c\n\n 15.3.3 Learning\n Encoder-decoders for speech are trained with the normal cross-entropy loss generally used for conditional language models. At timestep i of decoding, the loss is the\n log probability of the correct token (letter) yi :\n\n LCE = ‚àí log p(yi |y1 , . . . , yi‚àí1 , X) (15.9)\n\n The loss for the entire sentence is the sum of these losses:\n m\n X\n LCE = ‚àí log p(yi |y1 , . . . , yi‚àí1 , X) (15.10)\n i=1\n\n This loss is then backpropagated through the entire end-to-end model to train the\n entire encoder-decoder.\n As we described in Chapter 12, we normally use teacher forcing, in which the\n decoder history is forced to be the correct gold yi rather than the predicted yÃÇi . It‚Äôs\n also possible to use a mixture of the gold and decoder output, for example using\n the gold output 90% of the time, but with probability .1 taking the decoder output\n instead:\n\n LCE = ‚àí log p(yi |y1 , . . . , yÃÇi‚àí1 , X) (15.11)\n\n Modern data sizes are quite large. For example Whisper-v2 is trained on a corpus\n of 680,000 hours of speech, mostly from English, but also including 118,000 hours\n from 96 other languages. Data quality is important, so systems that scrape web\n data for training implement methods to remove ASR-generated transcripts from their\n training corpora, such as filtering data that is all uppercase or all lowercase. The\n open OWSM system is trained on 180k hours, mainly hand-transcribed publicly\n 15.4 ‚Ä¢ S ELF - SUPERVISED MODELS : H U BERT 13\n\n available data, including such datasets as LibriSpeech and Multilingual LibriSpeech,\n Common Voice, FLEURS, Switchboard, AMI, and others; see (Peng et al., 2023) for\n details.\n\nself-supervised An alternative to the encoder-decoder architecture are the class of self-supervised\n speech models. These models don‚Äôt directly learn to map an acoustic input to a\n string of letters and tokens. Instead, they first bootstrap a set of discrete phonetic\n units from the acoustic input, learning to map from waveforms to these induced\n units. This pretraining phase doesn‚Äôt require transcripts; just unlabeled speech files..\n After they are pretrained, these models can then be finetuned to do ASR on a smaller\n set of labeled data, audio paired with transcripts. These models have the advantage\n that they can take advantage of large amounts of untranscribed audio for most of\n their training.\n HuBERT Here‚Äôs we‚Äôll introduce one self-supervised model called HuBERT (Hsu et al.,\n wav2vec 2.0 2021). HuBERT and similar models like wav2vec 2.0 (Baevski et al., 2020) use the\n same intuition as the masked language models like BERT introduced in Chapter 10.\n We mask out some part of the input, and train the model to guess what was hidden\n by the mask.\n\n y1 y2 y3 y4 y5 ‚Ä¶ yn\n\n softmax\n\n cosines w/each class cosine cosine cosine cosine cosine cosine\n\n projection layer A A A A A A\n\n h1 h2 h3 h4 h5 ‚Ä¶ hn\n\n Transformer\n Stack\n x1 x2\n MSK x3\n MSK x4\n MSK x5 ‚Ä¶ xn\n\n 7 CNN Layers\n\n wavfile is passed through a series of convolutional layers, some frames are replaced with a\n MASK token, and then the sequence is passed though a transformer stack, and then a linear\n layer that projects the transformer output to an output embedding. This embedding is compared via cosine with the embeddings for each of the 100/500 phonetic classes to produce a\n logit which is passed through softmax to get a probability distribution over the classes at each\n frame.\n14 C HAPTER 15 ‚Ä¢ AUTOMATIC S PEECH R ECOGNITION\n\n 15.4.1 HuBERT forward pass\n Let‚Äôs first show just the forward pass for HuBERT used during training, and then\n we‚Äôll see this in its full training context with the backwards pass. As discussed\n earlier, the input to the HuBERT forward pass is raw 16kHz wavefiles as input,\n and the output at each 20ms time frame will be a probability distribution over a\n set of induced phonetic classes C (100 classes or 500 classes, depending on the\n stage). Fig. 15.8 shows a sketch of the components. The wavefile is passed through\n 7 512-channel convolutional layers which learn both to extract spectral information,\n and to shorten the input sequence down to a 20 ms frame, after which positional\n encodings are added, and then GELU and layer norm. Selected tokens are then\n replaced with a mask token, a trained embedding that is shared by all masked frames.\n The whole sequence is passed through a transformer stack, and the output is passed\n through a linear projection layer A. The output embedding at each 20ms frame is\n then compared via cosine with each of the embeddings for the 100 (/500) phonetic\n classes, resulting in a set of 100 logits representing the similarity of the current\n 20ms audio timestep to each class. These are then passed through a softmax to get a\n probability distribution over the classes.\n\n 15.4.2 Learning for HuBERT\n Let‚Äôs first discuss how we induce the 100 or 500 phonetic classes that are the target\n of training. To bootstrap these units, HuBERT starts with mel frequency cepstral\n coefficients, or MFCC vectors, a 39-dimensional feature vector that emphasizes aspects of the signal that are relevant for detection of phonetic units. These vectors\n can be extracted from the acoustic signal as summarized in Section ??. We extract\n MFCC vectors for the entire acoustic training dataset (the original HuBERT implementation used 960 hours of LibriSpeech data resulting in 172 million vectors). Next\n we cluster the MFCC vectors using the k-means clustering algorithm described below in Section 15.4.3. Clustering means to group the vectors into k classes. The\n output of clustering is a codebook of k vectors, called codewords or templates or\n prototypes, each representing a cluster. Each of these k clusters is an acoustic unit\n that we can use as the gold targets for training.\n Now let‚Äôs consider the entire training process. After the acoustic input is run\n through the CNN layers, a span of tokens in the context window is chosen to be\n masked, and for those tokens the CNN output is replaced by a MASK embedding.\n The entire context window is passed through the transformer layers, and the transformer output htL at each timestep t is multiplied by the projection layer matrix A to\n project it into the class embedding space. The resulting representation is then compared to the embedding for each of the classes in C (using cosine), and a softmax\n (with temperature parameter œÑ=0.1) is used to turn the similarity into a probability:\n\n exp(sim(Aht , ec )/œÑ)\n p(c|X,t) = PC (15.12)\n c0 =1 exp(sim(Aht , ec0 )/œÑ)\n\n As Fig. 15.9 shows, in parallel with this forward pass, the input waveform is\n passed through an MFCC to create a 39-dimensional vector which is then mapped\n to one of the 100 classes by choosing the most similar centroid in the codebook. The\n loss function is then the sum, over the set of masked tokens M, of the probability\n that the model assigns to these correct units:\n 15.4 ‚Ä¢ S ELF - SUPERVISED MODELS : H U BERT 15\n\n X\n L= log p(zt |X) (15.13)\n t‚ààM\n\n Thus, as in masked language modeling, the model is being trained to predict the\n units associated with the masked frames. This loss is then backpropagated through\n the model\n\n -logP(z2) -log P(z3) -log P(z4)\n ‚Ä¶\n softmax Network is trained\n to produce these\n cosines with each class cosine cosine cosine cosine cosine\n acoustic units(class)\n linear projection layer A A A A A A\n\n h1 h2 h3 h4 h5 ‚Ä¶ hn\n z1 z2 z3 z4 z5 zn\n ‚Ä¶\n Transformer map to class in\n Stack\n codebook ‚Ä¶\n x1 MSK MSK MSK x5 ‚Ä¶ xn\n f1 f2 f3 f4 f5 fn\n ‚Ä¶\n\n 7 CNN Layers MFCC\n Training SIgnal\n\n computes the probability of that class, and uses the logprob as the loss.\n\n Once the model has been initially trained to map to MFCC vector centroids, a\n second stage of training occurs, where we take the representations produced by the\n model, cluster them into 500 clusters, and use those instead as the target for training.\n The intuition is that the initial MFCC clusters will bias the model toward phonetic\n representations, but after enough training the model will learn more accurate and\n fine-grained representations. Fig. 15.10 shows the intuition.\n After HuBERT has been pretrained, the projection and cosine layers are removed\n and a randomly initialized linear + softmax layer is added, mapping into 29 classes\n (corresponding to the 26 English letters and a few extra characters) for the ASR task.\n The CNNs are frozen and the rest of the model is finetuned for ASR using the CTC\n loss function to be described in Section 15.5.\n\n 15.4.3 K-means clustering\nk-means In this section we give the k-means clustering algorithm more formally. K-means\n is a family of algorithms for grouping a set of vector data into k clusters. Clustering\n is useful whenever we want to treat a group of elements in the same way. In speech\n processing it is very commonly used whenever we need to convert a set of vectors\n over real values into a set of discrete symbols. Besides its use here in HuBERT,\n we‚Äôll return to it in Chapter 16 as an algorithm for creating discrete acoustic tokens\n for TTS.\n We generally use the name k-means to mean a simple version of the family:\n a two-step iterative algorithm that is given a set of N vectors v(1) ..v(N) each of d\n dimensions, i.e. ‚àÄi, v(i) ‚àà Rd , and a constant k, where usually N >> k.\n16 C HAPTER 15 ‚Ä¢ AUTOMATIC S PEECH R ECOGNITION\n\n Stage 1 class induction Stage 2 class induction\n 39-d 768-d\n 1 1\n 2 2\n 100 units 500 units\n 100 500\n\n codebook codebook\n\n k-means clustering k-means clustering\n\n MFCC\n Hubert forward pass\n layer 6 output\n Entire training data\n 10% of training data\n\n 100 acoustic units are created by computing 39-dimensional MFCC vectors for the entire\n training data and then clustering them with k-means. In the second stage, 500 units are created\n by passing a subsample of the training data through the HuBERT model after the first stage\n training, taking the output of an intermediate transformer layer (layer 6) and clustering them\n with k-means.\n\n i i i t t ‚Ä¶ h\n target letters\n\n softmax\n\n projection A A A A A A\n\n h1 h2 h3 h4 h5 ‚Ä¶ hn\n\n Transformer\n Stack\n x1 x2 x3 x4 x5 ‚Ä¶ xn\n\n 7 CNN Layers (Frozen)\n\n CNN layers are frozen, and the rest of the model is finetuned on a dataset of audio with transcripts, trained with the CTC loss (Section 15.5) to produce letters as output. The parameters\n that are updated in finetuning are shown in red (the projection layer and the transformer stack).\n\n The two-step algorithm is based on iteratively updating a set of k centroid veccentroid tors. A centroid is the geometric center of a set of a points in n-dimensional space..\n The algorithm has two steps. In the assignment step, given a set of k current centroids and a dataset of vectors, it assigns each vector to the cluster whose codeword\n is the closest (by squared Euclidean distance). In the re-estimation step, it recomputes the codeword for each cluster by recomputing the mean vector. Note that the\n resulting mean vector need not be an actual point from the dataset. We iterate back\n 15.5 ‚Ä¢ CTC 17\n\n and forth between these two steps.\n Here‚Äôs the algorithm:\n Initialization: For each cluster k choose a random vector ¬µk ‚àà Rd to be the\n codeword codeword (also called template or prototype) for the cluster. The result is a\n template codebook that has one codeword for each of the k clusters.\n prototype Then repeat iteratively until convergence:\n codebook\n 1. Assignment: For each vector v(i) in the dataset assign it to one of the k clusters\n by choosing the one with the nearest codeword ¬µ. Most simply we can define\n ‚Äònearest‚Äô as the cluster whose codeword has the smallest squared Euclidean\n distance to v(i) .\n\n cluster(i) = argmin ||v(i) ‚àí ¬µ j ||2 (15.14)\n 1< j<k\n\n where ||v|| is the L2 norm of the vector dj=1 v2j\n P\n\n 2. Re-estimation: Re-estimate the codeword for each cluster by recomputing\n the mean (centroid) of all the vectors in the cluster. If Si is the set of vectors\n in cluster i, then\n\n ‚àÄi :\n 1 X\n ¬µi = v (15.15)\n |Si |\n v‚ààSi\n\n We pointed out in the previous section that speech recognition has two particular\n properties that make it very appropriate for the encoder-decoder architecture, where\n the encoder produces an encoding of the input that the decoder uses attention to\n explore. First, in speech we have a very long acoustic input sequence X mapping to\n a much shorter sequence of letters Y , and second, it‚Äôs hard to know exactly which\n part of X maps to which part of Y .\n In this section we briefly introduce an alternative to encoder-decoder: an algo-\nCTC rithm and loss function called CTC, short for Connectionist Temporal Classification (Graves et al., 2006), that deals with these problems in a very different way. The\n intuition of CTC is to output a single character for every frame of the input, so that\n the output is the same length as the input, and then to apply a collapsing function\n that combines sequences of identical letters, resulting in a shorter sequence.\n Let‚Äôs imagine inference on someone saying the word dinner, and let‚Äôs suppose\n we had a function that chooses the most probable letter for each input spectral frame\n representation xi . We‚Äôll call the sequence of letters corresponding to each input\n alignment frame an alignment, because it tells us where in the acoustic signal each letter aligns\n to. Fig. 15.12 shows one such alignment, and what happens if we use a collapsing\n function that just removes consecutive duplicate letters.\n Well, that doesn‚Äôt work; our naive algorithm has transcribed the speech as diner,\n not dinner! Collapsing doesn‚Äôt handle double letters. There‚Äôs also another problem\n with our naive function; it doesn‚Äôt tell us what symbol to align with silence in the\n input. We don‚Äôt want to be transcribing silence as random letters!\n The CTC algorithm solves both problems by adding to the transcription alphabet\n blank a special symbol for a blank, which we‚Äôll represent as . The blank can be used in\n18 C HAPTER 15 ‚Ä¢ AUTOMATIC S PEECH R ECOGNITION\n\n Y (output) d i n e r\n\n A (alignment) d i i n n n n e r r r r r r\n\n X (input) x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 x11 x12 x13 x14\n\n wavefile\n\n the alignment whenever we don‚Äôt want to transcribe a letter. Blank can also be used\n between letters; since our collapsing function collapses only consecutive duplicate\n letters, it won‚Äôt collapse across . More formally, let‚Äôs define the mapping B : a ‚Üí y\n between an alignment a and an output y, which collapses all repeated letters and\n then removes all blanks. Fig. 15.13 sketches this collapsing function B.\n\n Y (output) d i n n e r\n remove blanks d i n n e r\n merge duplicates d i ‚ê£ n ‚ê£ n e r ‚ê£\n A (alignment) d i ‚ê£ n n ‚ê£ n e r r r r ‚ê£ ‚ê£\n\n X (input) x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 x11 x12 x13 x14\n\n The CTC collapsing function is many-to-one; lots of different alignments map\n to the same output string. For example, the alignment shown in Fig. 15.13 is not\n the only alignment that results in the string dinner. Fig. 15.14 shows some other\n alignments that would produce the same output.\n\n d i ‚ê£ n n e e e r r r ‚ê£\n i n\n d d i n n ‚ê£ n e r r ‚ê£ ‚ê£ ‚ê£ ‚ê£\n d d d i n ‚ê£ n n ‚ê£ ‚ê£ ‚ê£ e r r\n\n It‚Äôs useful to think of the set of all alignments that might produce the same output\n Y . We‚Äôll use the inverse of our B function, called B‚àí1 , and represent that set as\n B‚àí1 (Y ).\n\n 15.5.1 CTC Inference\n Before we see how to compute PCTC (Y |X) let‚Äôs first see how CTC assigns a probability to one particular alignment AÃÇ = {aÃÇ1 , . . . , aÃÇn }. CTC makes a strong conditional\n independence assumption: it assumes that, given the input X, the CTC model output\n 15.5 ‚Ä¢ CTC 19\n\nat at time t is independent of the output labels at any other time ai . Thus:\n\n T\n Y\n PCTC (A|X) = p(at |X) (15.16)\n t=1\n\nThus to find the best alignment AÃÇ = {aÃÇ1 , . . . , aÃÇT } we can greedily choose the character with the max probability at each time step t:\n\n aÃÇt = argmax pt (c|X) (15.17)\n c‚ààC\n\nWe then pass the resulting sequence A to the CTC collapsing function B to get the\noutput sequence Y .\n Let‚Äôs talk about how this simple inference algorithm for finding the best alignment A would be implemented. Because we are making a decision at each time\npoint, we can treat CTC as a sequence-modeling task, where we output one letter\nyÃÇt at time t corresponding to each input token xt , eliminating the need for a full decoder. Fig. 15.15 sketches this architecture, where we take an encoder, produce a\nhidden state ht at each timestep, and decode by taking a softmax over the character\nvocabulary at each time step.\n\n output letter y1 y2 y3 y4 y5 ‚Ä¶ yn\n sequence Y\n i i i t t ‚Ä¶\n\n Classifier ‚Ä¶\n +softmax\n\n ENCODER\n Shorter input\n sequence X x1 ‚Ä¶ xn\n\n Subsampling\n\n log Mel spectrum f1 ‚Ä¶ ft\n\n Feature Computation\n\nsimple softmaxes over the hidden state ht at each output step.\n\n Alas, there is a potential flaw with the inference algorithm sketched in (Eq. 15.17)\nand Fig. 15.14. The problem is that we chose the most likely alignment A, but the\nmost likely alignment may not correspond to the most likely final collapsed output\nstring Y . That‚Äôs because there are many possible alignments that lead to the same\noutput string, and hence the most likely output string might not correspond to the\nmost probable alignment. For example, imagine the most probable alignment A for\nan input X = [x1 x2 x3 ] is the string [a b \u000f] but the next two most probable alignments\nare [b \u000f b] and [\u000f b b]. The output Y =[b b], summing over those two alignments,\nmight be more probable than Y =[a b].\n For this reason, the most probable output sequence Y is the one that has, not\nthe single best CTC alignment, but the highest sum over the probability of all its\n20 C HAPTER 15 ‚Ä¢ AUTOMATIC S PEECH R ECOGNITION\n\n possible alignments:\n X\n PCTC (Y |X) = P(A|X)\n A‚ààB‚àí1 (Y )\n\n X T\n Y\n = p(at |ht )\n A‚ààB‚àí1 (Y ) t=1\n\n YÃÇ = argmax PCTC (Y |X) (15.18)\n Y\n\n Alas, summing over all alignments is very expensive (there are a lot of alignments),\n so we approximate this sum by using a version of Viterbi beam search that cleverly\n keeps in the beam the high-probability alignments that map to the same output string,\n and sums those as an approximation of (Eq. 15.18). See Hannun (2017) for a clear\n explanation of this extension of beam search for CTC.\n Because of the strong conditional independence assumption mentioned earlier\n (that the output at time t is independent of the output at time t ‚àí 1, given the input),\n CTC does not implicitly learn a language model over the data (unlike the attentionbased encoder-decoder architectures). It is therefore essential when using CTC to\n interpolate a language model (and some sort of length factor L(Y )) using interpolation weights that are trained on a devset:\n\n scoreCTC (Y |X) = log PCTC (Y |X) + Œª1 log PLM (Y )Œª2 L(Y ) (15.19)\n\n 15.5.2 CTC Training\n To train a CTC-based ASR system, we use negative log-likelihood loss with a special\n CTC loss function. Thus the loss for an entire dataset D is the sum of the negative\n log-likelihoods of the correct output Y for each input X:\n X\n LCTC = ‚àí log PCTC (Y |X) (15.20)\n (X,Y )‚ààD\n\n To compute CTC loss function for a single input pair (X,Y ), we need the probability\n of the output Y given the input X. As we saw in Eq. 15.18, to compute the probability\n of a given output Y we need to sum over all the possible alignments that would\n collapse to Y . In other words:\n\n X T\n Y\n PCTC (Y |X) = p(at |ht ) (15.21)\n A‚ààB‚àí1 (Y ) t=1\n\n Naively summing over all possible alignments is not feasible (there are too many\n alignments). However, we can efficiently compute the sum by using dynamic programming to merge alignments, with a version of the forward-backward algorithm also used to train HMMs (Appendix A) and CRFs. The original dynamic programming algorithms for both training and inference are laid out in (Graves et al.,\n 2006); see (Hannun, 2017) for a detailed explanation of both.\n\n 15.5.3 Combining CTC and Encoder-Decoder\n It‚Äôs also possible to combine the two architectures/loss functions we‚Äôve described,\n the cross-entropy loss from the encoder-decoder architecture, and the CTC loss.\n 15.5 ‚Ä¢ CTC 21\n\n Fig. 15.16 shows a sketch. For training, we can simply weight the two losses with a\n Œª tuned on a devset:\n L = ‚àíŒª log Pencdec (Y |X) ‚àí (1 ‚àí Œª ) log Pctc (Y |X) (15.22)\n\n For inference, we can combine the two with the language model (or the length\n penalty), again with learned weights:\n YÃÇ = argmax [Œª log Pencdec (Y |X) ‚àí (1 ‚àí Œª ) log PCTC (Y |X) + Œ≥ log PLM (Y )] (15.23)\n Y\n\n i t ‚Äô s t i m e ‚Ä¶\n\n CTC Loss Encoder-Decoder Loss\n ‚Ä¶\n DECODER\n ‚Ä¶ H\n\n ENCODER\n <s> i t ‚Äò s t i m ‚Ä¶\n\n ‚Ä¶\n x1 xn\n\n 15.5.4 Streaming Models: RNN-T for improving CTC\n Because of the strong independence assumption in CTC (assuming that the output\n at time t is independent of the output at time t ‚àí 1), recognizers based on CTC\n don‚Äôt achieve as high an accuracy as the attention-based encoder-decoder recognizers. CTC recognizers have the advantage, however, that they can be used for\nstreaming streaming. Streaming means recognizing words on-line rather than waiting until\n the end of the sentence to recognize them. Streaming is crucial for many applications, from commands to dictation, where we want to start recognition while the\n user is still talking. Algorithms that use attention need to compute the hidden state\n sequence over the entire input first in order to provide the attention distribution context, before the decoder can start decoding. By contrast, a CTC algorithm can input\n letters from left to right immediately.\n If we want to do streaming, we need a way to improve CTC recognition to remove the conditional independent assumption, enabling it to know about output his-\nRNN-T tory. The RNN-Transducer (RNN-T), shown in Fig. 15.17, is just such a model\n (Graves 2012, Graves et al. 2013). The RNN-T has two main components: a CTC\n acoustic model, and a separate language model component called the predictor that\n conditions on the output token history. At each time step t, the CTC encoder outputs\n a hidden state htenc given the input x1 ...xt . The language model predictor takes as inpred\n put the previous output token (not counting blanks), outputting a hidden state hu .\n The two are passed through another network whose output is then passed through a\n softmax to predict the next character.\n X\n PRNN‚àíT (Y |X) = P(A|X)\n A‚ààB‚àí1 (Y )\n\n X T\n Y\n = p(at |ht , y<ut )\n A‚ààB‚àí1 (Y ) t=1\n22 C HAPTER 15 ‚Ä¢ AUTOMATIC S PEECH R ECOGNITION\n\n P ( yt,u | x[1..t] , y[1..u-1] )\n\n SOFTMAX\n zt,u\n PREDICTION\n NETWORK hpred\n JOINT NETWORK DECODER\n u\n henct\n yu-1\n ENCODER\n\n xt\n\n word error The standard evaluation metric for speech recognition systems is the word error\n rate. The word error rate is based on how much the word string returned by the\n recognizer (the hypothesized word string) differs from a reference transcription.\n The first step in computing word error is to compute the minimum edit distance in\n words between the hypothesized and correct strings, giving us the minimum number of word substitutions, word insertions, and word deletions necessary to map\n between the correct and hypothesized strings. The word error rate (WER) is then\n defined as follows (note that because the equation includes insertions, the error rate\n can be greater than 100%):\n Insertions + Substitutions + Deletions\n Word Error Rate = 100 √ó\n Total Words in Correct Transcript\n alignment Here is a sample alignment between a reference and a hypothesis utterance from\n the CallHome corpus, showing the counts used to compute the error rate:\nREF: i *** ** UM the PHONE IS i LEFT THE portable **** PHONE UPSTAIRS last night\nHYP: i GOT IT TO the ***** FULLEST i LOVE TO portable FORM OF STORES last night\nEval: I I S D S S S I S S\n\n This utterance has six substitutions, three insertions, and one deletion:\n 6+3+1\n Word Error Rate = 100 = 76.9%\n The standard method for computing word error rates is a free script called sclite,\n available from the National Institute of Standards and Technologies (NIST) (NIST,\n 2005). Sclite is given a series of reference (hand-transcribed, gold-standard) sentences and a matching set of hypothesis sentences. Besides performing alignments,\n and computing word error rate, sclite performs a number of other useful tasks. For\n example, for error analysis it gives useful information such as confusion matrices\n showing which words are often misrecognized for others, and summarizes statistics\n of words that are often inserted or deleted. sclite also gives error rates by speaker\n (if sentences are labeled for speaker ID), as well as useful statistics like the sentence\n Sentence error error rate, the percentage of sentences with at least one word error.\n rate\n 15.6 ‚Ä¢ ASR E VALUATION : W ORD E RROR R ATE 23\n\nText normalization before evaluation\nIt‚Äôs normal for systems to normalize text before computing word error rate. There\nare a variety of packages for implementing normalization rules. For example some\nstandard English normalization rules include:\n 1. Removing metalanguage [non-language, notes, transcription comments] that\n occur between matching brackets ([, ])\n 2. Remove or standardize interjections or filled pauses (‚Äúuh‚Äù, ‚Äúum‚Äù, ‚Äúerr‚Äù)\n 3. Standardize contracted and non-contracted forms of English (‚ÄúI‚Äôm‚Äù/‚ÄúI am‚Äù)\n 4. Normalize non-standard-words (number, quantities, dates, times) [e.g., ‚Äú$100\n ‚Üí ‚ÄúOne hundred dollars‚Äù]\n 5. Unify US and UK spelling conventions\n\nStatistical significance for ASR: MAPSSWE or MacNemar\nAs with other language processing algorithms, we need to know whether a particular\nimprovement in word error rate is significant or not.\n The standard statistical tests for determining if two word error rates are different\nis the Matched-Pair Sentence Segment Word Error (MAPSSWE) test, introduced in\nGillick and Cox (1989).\n The MAPSSWE test is a parametric test that looks at the difference between\nthe number of word errors the two systems produce, averaged across a number of\nsegments. The segments may be quite short or as long as an entire utterance; in\ngeneral, we want to have the largest number of (short) segments in order to justify\nthe normality assumption and to maximize power. The test requires that the errors\nin one segment be statistically independent of the errors in another segment. Since\nASR systems tend to use trigram LMs, we can approximate this requirement by\ndefining a segment as a region bounded on both sides by words that both recognizers\nget correct (or by turn/utterance boundaries). Here‚Äôs an example from NIST (2007)\nwith four regions:\n\n I II III IV\nREF: |it was|the best|of|times it|was the worst|of times| |it was\n | | | | | | | |\nSYS A:|ITS |the best|of|times it|IS the worst |of times|OR|it was\n | | | | | | | |\nSYS B:|it was|the best| |times it|WON the TEST |of times| |it was\n\n In region I, system A has two errors (a deletion and an insertion) and system B\nhas zero; in region III, system A has one error (a substitution) and system B has two.\nLet‚Äôs define a sequence of variables Z representing the difference between the errors\nin the two systems as follows:\n NAi the number of errors made on segment i by system A\n NBi the number of errors made on segment i by system B\n Z NAi ‚àí NBi , i = 1, 2, ¬∑ ¬∑ ¬∑ , n where n is the number of segments\n In the example above, the sequence of Z values is {2, ‚àí1, ‚àí1, 1}. Intuitively, if\nthe two systems are identical, we would expect the average difference, that is, the\naverage of the Z values, to be zero. If we call the true average of the differences\nmuz , we would thus like to know whether muz = 0. Following closely the original\nproposal and notation of Gillick P\n and Cox (1989), we can estimate the true average\nfrom our limited sample as ¬µÃÇz = ni=1 Zi /n. The estimate of the variance of the Zi ‚Äôs\n24 C HAPTER 15 ‚Ä¢ AUTOMATIC S PEECH R ECOGNITION\n\n is\n n\n 1 X\n œÉz2 = (Zi ‚àí ¬µz )2 (15.24)\n n‚àí1\n i=1\n\n Let\n ¬µÃÇz\n W= ‚àö (15.25)\n œÉz / n\n\n For a large enough n (> 50), W will approximately have a normal distribution with\n unit variance. The null hypothesis is H0 : ¬µz = 0, and it can thus be rejected if\n 2 ‚àó P(Z ‚â• |w|) ‚â§ 0.05 (two-tailed) or P(Z ‚â• |w|) ‚â§ 0.05 (one-tailed), where Z is\n standard normal and w is the realized value W ; these probabilities can be looked up\n in the standard tables of the normal distribution.\nMcNemar‚Äôs test Earlier work sometimes used McNemar‚Äôs test for significance, but McNemar‚Äôs\n is only applicable when the errors made by the system are independent, which is not\n true in continuous speech recognition, where errors made on a word are extremely\n dependent on errors made on neighboring words.\n Could we improve on word error rate as a metric? It would be nice, for example, to have something that didn‚Äôt give equal weight to every word, perhaps valuing\n content words like Tuesday more than function words like a or of. While researchers\n generally agree that this would be a good idea, it has proved difficult to agree on a\n metric that works in every application of ASR.\n\n This chapter introduced the fundamental algorithms of automatic speech recognition\n (ASR).\n ‚Ä¢ The task of speech recognition (or speech-to-text) is to map acoustic waveforms to sequences of graphemes.\n ‚Ä¢ The input to a speech recognizer is a series of acoustic waves. that are sampled, quantized, and converted to a spectral representation like the log mel\n spectrum.\n ‚Ä¢ Two common paradigms for speech recognition are the encoder-decoder with\n attention model, and models based on the CTC loss function. Attentionbased models have higher accuracies, but models based on CTC more easily\n adapt to streaming: outputting graphemes online instead of waiting until the\n acoustic input is complete.\n ‚Ä¢ ASR is evaluated using the Word Error Rate; the edit distance between the\n hypothesis and the gold transcription.\n\nHistorical Notes\n A number of speech recognition systems were developed by the late 1940s and early\n 1950s. An early Bell Labs system could recognize any of the 10 digits from a single\n speaker (Davis et al., 1952). This system had 10 speaker-dependent stored patterns,\n one for each digit, each of which roughly represented the first two vowel formants\n H ISTORICAL N OTES 25\n\n in the digit. They achieved 97%‚Äì99% accuracy by choosing the pattern that had\n the highest relative correlation coefficient with the input. Fry (1959) and Denes\n (1959) built a phoneme recognizer at University College, London, that recognized\n four vowels and nine consonants based on a similar pattern-recognition principle.\n Fry and Denes‚Äôs system was the first to use phoneme transition probabilities to constrain the recognizer.\n The late 1960s and early 1970s produced a number of important paradigm shifts.\n First were a number of feature-extraction algorithms, including the efficient fast\n Fourier transform (FFT) (Cooley and Tukey, 1965), the application of cepstral processing to speech (Oppenheim et al., 1968), and the development of LPC for speech\n coding (Atal and Hanauer, 1971). Second were a number of ways of handling warpwarping ing; stretching or shrinking the input signal to handle differences in speaking rate\n and segment length when matching against stored patterns. The natural algorithm for\n solving this problem was dynamic programming, and, as we saw in Appendix A, the\n algorithm was reinvented multiple times to address this problem. The first application to speech processing was by Vintsyuk (1968), although his result was not picked\n up by other researchers, and was reinvented by Velichko and Zagoruyko (1970) and\n Sakoe and Chiba (1971) (and 1984). Soon afterward, Itakura (1975) combined this\n dynamic programming idea with the LPC coefficients that had previously been used\n only for speech coding. The resulting system extracted LPC features from incoming\n words and used dynamic programming to match them against stored LPC templates.\n The non-probabilistic use of dynamic programming to match a template against indynamic time\n warping coming speech is called dynamic time warping.\n The third innovation of this period was the rise of the HMM. Hidden Markov\n models seem to have been applied to speech independently at two laboratories around\n 1972. One application arose from the work of statisticians, in particular Baum and\n colleagues at the Institute for Defense Analyses in Princeton who applied HMMs\n to various prediction problems (Baum and Petrie 1966, Baum and Eagon 1967).\n James Baker learned of this work and applied the algorithm to speech processing\n (Baker, 1975) during his graduate work at CMU. Independently, Frederick Jelinek\n and collaborators (drawing from their research in information-theoretical models\n influenced by the work of Shannon (1948)) applied HMMs to speech at the IBM\n Thomas J. Watson Research Center (Jelinek et al., 1975). One early difference was\n the decoding algorithm; Baker‚Äôs DRAGON system used Viterbi (dynamic programming) decoding, while the IBM system applied Jelinek‚Äôs stack decoding algorithm\n (Jelinek, 1969). Baker then joined the IBM group for a brief time before founding\n the speech-recognition company Dragon Systems.\n The use of the HMM, with Gaussian Mixture Models (GMMs) as the phonetic\n component, slowly spread through the speech community, becoming the dominant\n paradigm by the 1990s. One cause was encouragement by ARPA, the Advanced\n Research Projects Agency of the U.S. Department of Defense. ARPA started a\n five-year program in 1971 to build 1000-word, constrained grammar, few speaker\n speech understanding (Klatt, 1977), and funded four competing systems of which\n Carnegie-Mellon University‚Äôs Harpy system (Lowerre, 1976), which used a simplified version of Baker‚Äôs HMM-based DRAGON system was the best of the tested systems. ARPA (and then DARPA) funded a number of new speech research programs,\n beginning with 1000-word speaker-independent read-speech tasks like ‚ÄúResource\n Management‚Äù (Price et al., 1988), recognition of sentences read from the Wall Street\n Journal (WSJ), Broadcast News domain (LDC 1998, Graff 1997) (transcription of\n actual news broadcasts, including quite difficult passages such as on-the-street inter-\n26 C HAPTER 15 ‚Ä¢ AUTOMATIC S PEECH R ECOGNITION\n\n views) and the Switchboard, CallHome, CallFriend, and Fisher domains (Godfrey\n et al. 1992, Cieri et al. 2004) (natural telephone conversations between friends or\n bakeoff strangers). Each of the ARPA tasks involved an approximately annual bakeoff at\n which systems were evaluated against each other. The ARPA competitions resulted\n in wide-scale borrowing of techniques among labs since it was easy to see which\n ideas reduced errors the previous year, and the competitions were probably an important factor in the eventual spread of the HMM paradigm.\n By around 1990 neural alternatives to the HMM/GMM architecture for ASR\n arose, based on a number of earlier experiments with neural networks for phoneme\n recognition and other speech tasks. Architectures included the time-delay neural\n network (TDNN)‚Äîthe first use of convolutional networks for speech‚Äî (Waibel\n hybrid et al. 1989, Lang et al. 1990), RNNs (Robinson and Fallside, 1991), and the hybrid\n HMM/MLP architecture in which a feedforward neural network is trained as a phonetic classifier whose outputs are used as probability estimates for an HMM-based\n architecture (Morgan and Bourlard 1990, Bourlard and Morgan 1994, Morgan and\n Bourlard 1995).\n While the hybrid systems showed performance close to the standard HMM/GMM\n models, the problem was speed: large hybrid models were too slow to train on the\n CPUs of that era. For example, the largest hybrid system, a feedforward network,\n was limited to a hidden layer of 4000 units, producing probabilities over only a few\n dozen monophones. Yet training this model still required the research group to design special hardware boards to do vector processing (Morgan and Bourlard, 1995).\n A later analytic study showed the performance of such simple feedforward MLPs\n for ASR increases sharply with more than 1 hidden layer, even controlling for the\n total number of parameters (Maas et al., 2017). But the computational resources of\n the time were insufficient for more layers.\n Over the next two decades a combination of Moore‚Äôs law and the rise of GPUs\n allowed deep neural networks with many layers. Performance was getting close to\n traditional systems on smaller tasks like TIMIT phone recognition by 2009 (Mohamed et al., 2009), and by 2012, the performance of hybrid systems had surpassed\n traditional HMM/GMM systems (Jaitly et al. 2012, Dahl et al. 2012, inter alia).\n Originally it seemed that unsupervised pretraining of the networks using a technique like deep belief networks was important, but by 2013, it was clear that for\n hybrid HMM/GMM feedforward networks, all that mattered was to use a lot of data\n and enough layers, although a few other components did improve performance: using log mel features instead of MFCCs, using dropout, and using rectified linear\n units (Deng et al. 2013, Maas et al. 2013, Dahl et al. 2013).\n Meanwhile early work had proposed the CTC loss function by 2006 (Graves\n et al., 2006), and by 2012 the RNN-Transducer was defined and applied to phone\n recognition (Graves 2012, Graves et al. 2013), and then to end-to-end speech recognition rescoring (Graves and Jaitly, 2014), and then recognition (Maas et al., 2015),\n with advances such as specialized beam search (Hannun et al., 2014). (Our description of CTC in the chapter draws on Hannun (2017), which we encourage the\n interested reader to follow).\n The encoder-decoder architecture was applied to speech at about the same time\n by two different groups, in the Listen Attend and Spell system of Chan et al. (2016)\n and the attention-based encoder decoder architecture of Chorowski et al. (2014)\n and Bahdanau et al. (2016). By 2018 Transformers were included in this encoderdecoder architecture. Karita et al. (2019) is a nice comparison of RNNs vs Transformers in encoder-architectures for ASR, TTS, and speech-to-speech translation.\n E XERCISES 27\n\n Kaldi Popular toolkits for speech processing include Kaldi (Povey et al., 2011) and\n ESPnet ESPnet (Watanabe et al. 2018, Hayashi et al. 2020).\n\nExercises\n28 Chapter 15 ‚Ä¢ Automatic Speech Recognition\n\nArdila, R., M. Branson, K. Davis, M. Kohler, J. Meyer, Davis, K. H., R. Biddulph, and S. Balashek. 1952. Automatic\n M. Henretty, R. Morais, L. Saunders, F. Tyers, and G. We- recognition of spoken digits. JASA, 24(6):637‚Äì642.\n ber. 2020. Common voice: A massively-multilingual Denes, P. 1959. The design and operation of the mechanical\n speech corpus. LREC. speech recognizer at University College London. Journal\nAtal, B. S. and S. Hanauer. 1971. Speech analysis and syn- of the British Institution of Radio Engineers, 19(4):219‚Äì\n thesis by prediction of the speech wave. JASA, 50:637‚Äì 234. Appears together with companion paper (Fry 1959).\n 655. Deng, L., G. Hinton, and B. Kingsbury. 2013. New types of\nBaevski, A., Y. Zhou, A. Mohamed, and M. Auli. 2020. deep neural network learning for speech recognition and\n wav2vec 2.0: A framework for self-supervised learning related applications: An overview. ICASSP.\n of speech representations. NeurIPS, volume 33. Fry, D. B. 1959. Theoretical aspects of mechanical speech\nBahdanau, D., J. Chorowski, D. Serdyuk, P. Brakel, and recognition. Journal of the British Institution of Radio\n Y. Bengio. 2016. End-to-end attention-based large vo- Engineers, 19(4):211‚Äì218. Appears together with comcabulary speech recognition. ICASSP. panion paper (Denes 1959).\nBaker, J. K. 1975. The DRAGON system ‚Äì An overview. Gillick, L. and S. J. Cox. 1989. Some statistical issues in the\n IEEE Transactions on ASSP, ASSP-23(1):24‚Äì29. comparison of speech recognition algorithms. ICASSP.\nBaum, L. E. and J. A. Eagon. 1967. An inequality with appli- Godfrey, J., E. Holliman, and J. McDaniel. 1992. SWITCHcations to statistical estimation for probabilistic functions BOARD: Telephone speech corpus for research and deof Markov processes and to a model for ecology. Bulletin velopment. ICASSP.\n of the American Mathematical Society, 73(3):360‚Äì363. Goyal, N., C. Gao, V. Chaudhary, P.-J. Chen, G. Wenzek,\nBaum, L. E. and T. Petrie. 1966. Statistical inference for D. Ju, S. Krishnan, M. Ranzato, F. GuzmaÃÅn, and A. Fan.\n probabilistic functions of finite-state Markov chains. An- 2022. The flores-101 evaluation benchmark for lownals of Mathematical Statistics, 37(6):1554‚Äì1563. resource and multilingual machine translation. TACL,\nBostrom, K. and G. Durrett. 2020. Byte pair encoding is 10:522‚Äì538.\n suboptimal for language model pretraining. EMNLP. Graff, D. 1997. The 1996 Broadcast News speech and\nBourlard, H. and N. Morgan. 1994. Connectionist Speech language-model corpus. Proceedings DARPA Speech\n Recognition: A Hybrid Approach. Kluwer. Recognition Workshop.\nBu, H., J. Du, X. Na, B. Wu, and H. Zheng. 2017. AISHELL- Graves, A. 2012. Sequence transduction with recurrent neu-\n1: An open-source Mandarin speech corpus and a speech ral networks. ICASSP.\n recognition baseline. O-COCOSDA Proceedings. Graves, A., S. FernaÃÅndez, F. Gomez, and J. Schmidhuber.\nCanavan, A., D. Graff, and G. Zipperlen. 1997. CALL- 2006. Connectionist temporal classification: Labelling\n HOME American English speech LDC97S42. Linguistic unsegmented sequence data with recurrent neural net-\nData Consortium. works. ICML.\nChan, W., N. Jaitly, Q. Le, and O. Vinyals. 2016. Listen, Graves, A. and N. Jaitly. 2014. Towards end-to-end speech\n attend and spell: A neural network for large vocabulary recognition with recurrent neural networks. ICML.\n conversational speech recognition. ICASSP. Graves, A., A.-r. Mohamed, and G. Hinton. 2013.\nChorowski, J., D. Bahdanau, K. Cho, and Y. Bengio. Speech recognition with deep recurrent neural networks.\n 2014. End-to-end continuous speech recognition us- ICASSP.\n ing attention-based recurrent NN: First results. NeurIPS Hannun, A. 2017. Sequence modeling with CTC. Distill,\n Deep Learning and Representation Learning Workshop. 2(11).\nCieri, C., D. Miller, and K. Walker. 2004. The Fisher cor- Hannun, A. Y., A. L. Maas, D. Jurafsky, and A. Y. Ng. 2014.\n pus: A resource for the next generations of speech-to-text. First-pass large vocabulary continuous speech recogni-\nLREC. tion using bi-directional recurrent DNNs. ArXiv preprint\nConneau, A., M. Ma, S. Khanuja, Y. Zhang, V. Axelrod, arXiv:1408.2873.\n S. Dalmia, J. Riesa, C. Rivera, and A. Bapna. 2023. Hayashi, T., R. Yamamoto, K. Inoue, T. Yoshimura,\n Fleurs: Few-shot learning evaluation of universal repre- S. Watanabe, T. Toda, K. Takeda, Y. Zhang, and X. Tan.\n sentations of speech. IEEE SLT. 2020. ESPnet-TTS: Unified, reproducible, and inte-\nCooley, J. W. and J. W. Tukey. 1965. An algorithm for the gratable open source end-to-end text-to-speech toolkit.\n machine calculation of complex Fourier series. Mathe- ICASSP.\n matics of Computation, 19(90):297‚Äì301. Hsu, W.-N., B. Bolte, Y.-H. H. Tsai, K. Lakhotia,\nDahl, G. E., T. N. Sainath, and G. E. Hinton. 2013. Im- R. Salakhutdinov, and A. Mohamed. 2021. Hubert: Selfproving deep neural networks for LVCSR using rectified supervised speech representation learning by masked prelinear units and dropout. ICASSP. diction of hidden units. IEEE/ACM TASLP, 29:3451‚Äì\nDahl, G. E., D. Yu, L. Deng, and A. Acero. 2012. Context- 3460.\n dependent pre-trained deep neural networks for large- Itakura, F. 1975. Minimum prediction residual principle apvocabulary speech recognition. IEEE Transactions on au- plied to speech recognition. IEEE Transactions on ASSP,\n dio, speech, and language processing, 20(1):30‚Äì42. ASSP-32:67‚Äì72.\nDavid, Jr., E. E. and O. G. Selfridge. 1962. Eyes and ears Jaitly, N., P. Nguyen, A. Senior, and V. Vanhoucke. 2012.\n for computers. Proceedings of the IRE (Institute of Radio Application of pretrained deep neural networks to large\n Engineers), 50:1093‚Äì1101. vocabulary speech recognition. INTERSPEECH.\n Exercises 29\n\nJelinek, F. 1969. A fast sequential decoding algorithm us- Peng, Y., J. Tian, B. Yan, D. Berrebbi, X. Chang, X. Li,\n ing a stack. IBM Journal of Research and Development, J. Shi, S. Arora, W. Chen, R. Sharma, W. Zhang, Y. Sudo,\n 13:675‚Äì685. M. Shakee, J. weon Jung, S. Maiti, and S. Watanabe.\nJelinek, F., R. L. Mercer, and L. R. Bahl. 1975. Design of a 2023. Reproducing whisper-style training using an openlinguistic statistical decoder for the recognition of contin- source toolkit and publicly available data. ASRU.\n uous speech. IEEE Transactions on Information Theory, Povey, D., A. Ghoshal, G. Boulianne, L. Burget, O. Glem-\nIT-21(3):250‚Äì256. bek, N. Goel, M. Hannemann, P. Motlicek, Y. Qian,\nKarita, S., N. Chen, T. Hayashi, T. Hori, H. Inaguma, P. Schwarz, J. SilovskyÃÅ, G. Stemmer, and K. VeselyÃÅ.\n Z. Jiang, M. Someki, N. E. Y. Soplin, R. Yamamoto, 2011. The Kaldi speech recognition toolkit. ASRU.\n X. Wang, S. Watanabe, T. Yoshimura, and W. Zhang. Price, P. J., W. Fisher, J. Bernstein, and D. Pallet. 1988. The\n 2019. A comparative study on transformer vs RNN in DARPA 1000-word resource management database for\n speech applications. IEEE ASRU-19. continuous speech recognition. ICASSP.\nKendall, T. and C. Farrington. 2020. The Corpus of Regional Pundak, G. and T. N. Sainath. 2016. Lower frame rate neural\n African American Language. Version 2020.05. Eugene, network acoustic models. INTERSPEECH.\n OR: The Online Resources for African American Lan-\nRadford, A., J. W. Kim, T. Xu, G. Brockman, C. McLeavey,\n guage Project. http://oraal.uoregon.edu/coraal.\n and I. Sutskever. 2023. Robust speech recognition via\nKlatt, D. H. 1977. Review of the ARPA speech understand- large-scale weak supervision. ICML.\n ing project. JASA, 62(6):1345‚Äì1366.\n Renals, S., T. Hain, and H. Bourlard. 2007. Recognition\nLang, K. J., A. H. Waibel, and G. E. Hinton. 1990. A and understanding of meetings: The AMI and AMIDA\n time-delay neural network architecture for isolated word projects. ASRU.\n recognition. Neural networks, 3(1):23‚Äì43.\n Robinson, T. and F. Fallside. 1991. A recurrent error prop-\nLDC. 1998. LDC Catalog: Hub4 project. Univeragation network speech recognition system. Computer\n sity of Pennsylvania. www.ldc.upenn.edu/Catalog/\n Speech & Language, 5(3):259‚Äì274.\n LDC98S71.html.\n Sakoe, H. and S. Chiba. 1971. A dynamic programming\nLiu, Y., P. Fung, Y. Yang, C. Cieri, S. Huang, and D. Graff.\n approach to continuous speech recognition. Proceedings\n 2006. HKUST/MTS: A very large scale Mandarin teleof the Seventh International Congress on Acoustics, volphone speech corpus. International Conference on Chiume 3. AkadeÃÅmiai KiadoÃÅ.\n nese Spoken Language Processing.\n Sakoe, H. and S. Chiba. 1984. Dynamic programming al-\nLowerre, B. T. 1976. The Harpy Speech Recognition System.\n gorithm optimization for spoken word recognition. IEEE\n Ph.D. thesis, Carnegie Mellon University, Pittsburgh, PA.\n Transactions on ASSP, ASSP-26(1):43‚Äì49.\nMaas, A., Z. Xie, D. Jurafsky, and A. Y. Ng. 2015. Lexiconfree conversational speech recognition with neural net- Shannon, C. E. 1948. A mathematical theory of commuworks. NAACL HLT. nication. Bell System Technical Journal, 27(3):379‚Äì423.\n Continued in the following volume.\nMaas, A. L., A. Y. Hannun, and A. Y. Ng. 2013. Rectifier\n nonlinearities improve neural network acoustic models. Velichko, V. M. and N. G. Zagoruyko. 1970. Automatic\n ICML. recognition of 200 words. International Journal of Man-\nMachine Studies, 2:223‚Äì234.\nMaas, A. L., P. Qi, Z. Xie, A. Y. Hannun, C. T. Lengerich,\n D. Jurafsky, and A. Y. Ng. 2017. Building dnn acoustic Vintsyuk, T. K. 1968. Speech discrimination by dynamic\n models for large vocabulary speech recognition. Com- programming. Cybernetics, 4(1):52‚Äì57. Original Rusputer Speech & Language, 41:195‚Äì213. sian: Kibernetika 4(1):81-88. 1968.\nMohamed, A., G. E. Dahl, and G. E. Hinton. 2009. Deep Waibel, A., T. Hanazawa, G. Hinton, K. Shikano, and K. J.\n Belief Networks for phone recognition. NIPS Workshop Lang. 1989. Phoneme recognition using time-delay neuon Deep Learning for Speech Recognition and Related ral networks. IEEE Transactions on ASSP, 37(3):328‚Äì\n Applications. 339.\nMorgan, N. and H. Bourlard. 1990. Continuous speech Watanabe, S., T. Hori, S. Karita, T. Hayashi, J. Nishitoba,\n recognition using multilayer perceptrons with hidden Y. Unno, N. E. Y. Soplin, J. Heymann, M. Wiesner,\n markov models. ICASSP. N. Chen, A. Renduchintala, and T. Ochiai. 2018. ESPnet:\nMorgan, N. and H. A. Bourlard. 1995. Neural networks for End-to-end speech processing toolkit. INTERSPEECH.\n statistical recognition of continuous speech. Proceedings Yuan, J., M. Liberman, and C. Cieri. 2006. Towards an inof the IEEE, 83(5):742‚Äì772. tegrated understanding of speaking rate in conversation.\nNIST. 2005. Speech recognition scoring toolkit (sctk) ver- Interspeech.\n sion 2.1. http://www.nist.gov/speech/tools/.\nNIST. 2007. Matched Pairs Sentence-Segment Word Error\n (MAPSSWE) Test.\nOppenheim, A. V., R. W. Schafer, and T. G. J. Stockham.\n 1968. Nonlinear filtering of multiplied and convolved signals. Proceedings of the IEEE, 56(8):1264‚Äì1291.\nPanayotov, V., G. Chen, D. Povey, and S. Khudanpur. 2015.\n Librispeech: an ASR corpus based on public domain audio books. ICASSP.\n",
    "file_path": "/Users/colinsidberry/nlp-textbook-rag/data/extracted_text/15.Automatic Speech Recognition.txt",
    "file_size_kb": 82.79
  },
  {
    "id": "9a4ddff0866b71c1",
    "source": "nlp_textbook",
    "chapter": "16 Text-to-Speech",
    "filename": "16.txt",
    "content": "rights reserved. Draft of August 24, 2025.\n\nCHAPTER\n\n16 Text-to-Speech\n\n ‚ÄúWords mean more than what is set down on paper. It takes the human voice\n to infuse them with shades of deeper meaning.‚Äù\n Maya Angelou, I Know Why the Caged Bird Sings\n\n The task of mapping from text to speech is a task with an even longer history than\n speech to text. In Vienna in 1769, Wolfgang von Kempelen built for the Empress\n Maria Theresa the famous Mechanical Turk, a chess-playing automaton consisting\n of a wooden box filled with gears, behind which sat a robot mannequin who played\n chess by moving pieces with his mechanical arm. The Turk toured Europe and the\n Americas for decades, defeating Napoleon Bonaparte and even playing Charles Babbage. The Mechanical Turk might have been one of the early successes of artificial\n intelligence were it not for the fact that it was, alas, a hoax, powered by a human\n chess player hidden inside the box.\n What is less well known is that von Kempelen, an extraordinarily\n prolific inventor, also built between\n 1769 and 1790 what was definitely\n not a hoax: the first full-sentence\n speech synthesizer, shown partially to\n the right. His device consisted of a\n bellows to simulate the lungs, a rubber mouthpiece and a nose aperture, a\n reed to simulate the vocal folds, various whistles for the fricatives, and a\n small auxiliary bellows to provide the puff of air for plosives. By moving levers\n with both hands to open and close apertures, and adjusting the flexible leather ‚Äúvocal tract‚Äù, an operator could produce different consonants and vowels.\n More than two centuries later, we no longer build our synthesizers out of wood\n text-to-speech and leather, nor do we need human operators. The modern task of text-to-speech or\n TTS TTS, also called speech synthesis, is exactly the reverse of ASR; to map text:\n speech\n synthesis It‚Äôs time for lunch!\n to an acoustic waveform:\n\n TTS has a wide variety of applications. It is used in spoken language models\n that interact with people, for reading text out loud, for games, and to produce speech\n for sufferers of neurological disorders, like the late astrophysicist Steven Hawking\n after he lost the use of his voice because of ALS.\n In this chapter we introduce an algorithm for TTS that, like the ASR algorithms\n of the prior chapter, are trained on enormous amounts of speech datasets. We‚Äôll also\n briefly touch on other speech applications.\n 2 C HAPTER 16 ‚Ä¢ T EXT- TO -S PEECH\n\n The task of text-to-speech is to generate a speech waveform that corresponds to a\n desired text, using in a particular voice specified by the user.\n Historically TTS was done by collecting hundreds of hours of speech from a\n single talker in a lab and training a large system on it. The resulting TTS system only\n worked in one voice; if you wanted a second voice, you went back and collected data\n from a second talker.\n The modern method is instead to train a speaker-independent synthesizer on tens\n of thousands of hours of speech from thousands of talkers. To create speech in a new\n voice unseen in training, we use a very small amount of speech from the desired\n talker to guide the creation of the voice. So the input to a modern TTS system is a\n text prompt and perhaps 3 seconds of speech from the voice we‚Äôd like to generate\n LANGUAGE PROCESSING,\n zero-shotVOL.\n TTS 33, 2025\n the speech in. This TTS task is called zero-shot TTS because the desired voice may 705\n never have been seen in training.\n The way modern TTS systems address this task is to use language modeling, and\n\nLanguage Models are Zero-Shot Text\n in particular conditional generation. The intuition is to take an enormous dataset of\n speech, and use an audio tokenizer based on an audio codec to induce discrete audio tokens from that speech that represent the speech. Then we can train a language\n\no Speech Synthesizers\n model whose vocabulary includes both speech tokens and text tokens.\n We train this language model to take as input two sequences, a text transcript\n and a small sample of speech from the desired talker, to tokenize both the text and\n the speech into discrete tokens, and then to conditionally generate discrete samples\nYu Wu , Ziqiang Zhang of the , Long\n speechZhou, Shujie\n corresponding Liu\n to the , Member,\n text string, voice. Zhuo Chen,\n IEEE,\n in the desired\n Wang, Jinyu Li , Fellow, IEEE, Lei He , Sheng Zhao, and Furu Wei text string and\n At inference time we prompt this language model with a tokenized\n a sample of the desired voice (tokenized by the codec into discrete audio tokens) and\n conditionally generate to produce the desired audio tokens. Then these tokens can\n be converted into a waveform.\n\n odeling approach for text\n we train a neural codec\n screte codes derived from\ndel, and regard TTS as a\n er than continuous signal\n he pre-training stage, we\n rs of English speech which\nsystems. VALL-E emerges\n e used to synthesize highsecond enrolled recording\neriment results show that\n state-of-the-art zero-shot\n ss and speaker similarity.\n rve the speaker‚Äôs emotion Figure 16.1 VALL-E architecture for personalized TTS (figure from Chen et al. (2025)).\nmpt in synthesis. Fig. 1. The overview of VALL-E. Unlike the previous pipeline (e.g., text ‚Üí\n mel-spectrogram\n Fig. 16.1 from ‚Üí waveform),\n Chen the shows\n et al. (2025) pipeline\n theofintuition\n VALL-E forisone\n textsuch\n ‚Üí discrete code\n TTS system,\n peech synthesis, speech\n VALL-E\n ‚Üí waveform. VALL-E generates the discrete audio codec codes based\n called VALL-E. VALL-E is trained on 60K hours of English speech, from over 7000 on text\n odeling, pre-training, in- input and acoustic code prompt, corresponding\n unique talkers. Systems like VALL-E have 2 components: to the target content and the\n speaker‚Äôs voice.\n 1. The audio tokenizer, generally based on an audio codec, a system we‚Äôll de-\nON adaptation and speaker encoding methods, requiring additional\n matic breakthroughs in fine-tuning, complex pre-designed features, or heavy structure\nvelopment of neural net- engineering [6], [7], [8], [9].\n 16.2 ‚Ä¢ U SING A CODEC TO LEARN DISCRETE AUDIO TOKENS 3\n\n scribe in the next section. Codecs have three parts: an encoder (that turns\n speech into embedding vectors), a quantizer (that turns the embeddings into\n discrete tokens) and decoders (that turns the discrete tokens back into speech).\n 2. The 2-stage conditional language model that can generate audio tokens corresponding to the desired text. We‚Äôll sketch this in Section 16.3.\n\n Modern TTS systems are based around converting the waveform into a sequence of\n discrete audio tokens. This idea of manipulating discrete audio tokens is also useful\n for other speech-enabled systems like spoken language models, which take text\n or speech input and can generate text or speech output to solve tasks like speechto-speech translation, diarization, or spoken question answering. Having discrete\n tokens means that we can make use of language model technology, since language\n models are specialized for sequences of discrete tokens. Audio tokenizers are thus\n an important component of the modern speech toolkit.\n codec The standard way to learn audio tokens is from a neural audio codec (the word\n is formed from coder/decoder). Historically a codec was a hardware device that\n digitized analog symbols. More generally we use the word to mean a mechanism\n for encoding analog speech signals into a digitized compressed representation that\n can be efficiently stored and sent. Codecs are still used for compression, but for TTS\n and also for spoken language models, we employ them for converting speech into\n discrete tokens.\n Of course the digital representation of speech we described in Chapter 14 is already discrete. For example 16 kHz speech stored in 16-bit format could be thought\n of as a series of 216 = 65,536 symbols, with 16,000 of those symbols per second of\n speech. But a system that generates 16,000 symbols per second makes the speech\n signal too long to be feasibly processed by a language model, especially one based\n on transformers with their inefficient quadratic attention. Instead we want symbols\n that represent longer chunks of speech, perhaps something on the order of a few\n hundred tokens a second.\n\n En r\n cod de\n er co\n De\n Quantization\n x zt qt,1 qt,2 ‚Ä¶ qt,N zqt x^\n c\n\n adapted from Mousavi et al. (2025). An input waveform x is encoded (generally using a series\n of downsampling convolution networks) into a series of embeddings zt . Each embedding is\n then passed through a quantizer to produce a series of quantized tokens qt . To regenerate the\n speech signal, the quantized tokens are re-mapped back to a vector zq t and then encoded (usually using a series of upsampling convolution networks) back to a waveform. We‚Äôll discuss\n how the architecture is trained in Section 16.2.4.\n\n Fig. 16.2 adapted from Mousavi et al. (2025). shows the standard architecture\n of an audio tokenizer. Audio tokenizers take as input an audio waveform, and are\n4 C HAPTER 16 ‚Ä¢ T EXT- TO -S PEECH\n\n trained to recreate the same audio waveform out, via an intermediate representation\n consisting of discrete tokens created by vector quantization.\n Audio tokenizers have three stages:\n 1. an encoder maps the acoustic waveform, a series of T values x = x1 , x2 , ..., xT ,\n to a sequence of œÑ embeddings z = z1 , z2 , ..., zœÑ . œÑ is typically 100-1000 times\n smaller than T .\n 2. a vector quantizer that takes each embedding zt corresponding to part of the\n waveform, and represents it by a sequence of discrete tokens each taken from\n one of the Nc codebooks, qt = qt,1 , qt,2 , ..., qt,Nc . The vector quantizer also\n sums the vector codewords from each codebook to create a quantizer output\n vector zq t .\n 3. a decoder that generates a lossy reconstructed waveform span xÃÇ from the\n quantizer output vector zq t .\n Audio tokenizers are generally learned end-to-end, using loss functions that reward a tokenization that allows the system to reconstruct the input waveform.\n In the following subsections we‚Äôll go through the components of one particular\n tokenizer, the E N C ODEC tokenizer of DeÃÅfossez et al. (2023).\n\n 16.2.1 The Encoder and Decoder for the E N C ODEC model\n\n Decoder\n Encoder\n\n Embeddings @ 75Hz Embeddings @ 75Hz\n ‚Ä¶ D D ‚Ä¶\n\n Conv1D (k=7, n=D) Conv1DT (k=7, n=D)\n\n EncoderBlock (N, S) Residual Unit (N)\n L S T M L S T M\n DecoderBlock (N, S)\n EncoderBlock\n DecoderBlock\n\n (N=16C, S=8)\n Conv1D (N=16C, S=8)\n (K=2S, N=C, Stride=S)\n EncoderBlock Conv1D Conv1DT (K=2S, N=C)\n (N=8C, S=5) (K=3, N=C)\n DecoderBlock\n (N=8C, S=5)\n Residual Unit Residual Unit\n EncoderBlock Conv1D\n (K=3, N=C) DecoderBlock\n (N=4C, S=4) (N=4C, S=4)\n\n EncoderBlock DecoderBlock\n (N=2C, S=2) (N=2C, S=2)\n\n Conv1D (k=7, n=C) Conv1D (k=7, n=C)\n\n Waveform @ 24kHz Waveform @ 24kHz\n\nBecause the original signal was represented at 24kHz, this is a downsampling of 24000\n 75 = 320 times. Between\nthe encoder and decoder is a quantization step producing a lossy embedding zq t . The goal of the decoder is to\ntake the lossy embedding zq t and upsample it, converting it back to a waveform.\n\n The encoder and decoder of the E N C ODEC model (DeÃÅfossez et al., 2023) are\n sketched in Fig. 16.3. The goal of the encoder is to downsample a span of waveform\n at time t, which is at 24kHz‚Äîone second of speech has 24,000 real values‚Äîto an\n embedding representation zt at 75Hz‚Äîone second of audio is represented by 75\n 16.2 ‚Ä¢ U SING A CODEC TO LEARN DISCRETE AUDIO TOKENS 5\n\n vectors, each of dimensionality D. For the purposes of this explanation, we‚Äôll use\n D = 256.\n This downsampling is accomplished by having a series of encoder blocks that are\n made up of convolutional layers with strides larger than 1 that iteratively downsample the audio, as we discussed at the end of Section ??. The convolution blocks are\n sketched in Fig. 16.3, and include a long series of convolutions as well as residual\n units that add a convolution to the prior input.\n The output of the encoder is an embedding zt at time t, 75 of which are produced\n per second. This embedding is then quantized (as discussed in the next section),\n turning each embedding zt into a series of Nc discrete symbols qt = qt,1 , qt,2 , ..., qt,Nc ,\n and also turning the series of symbols into a new quantizer output vector zq t . Finally, the decoder takes the output embedding from the quantizer zq t and generates\n a waveform via a symmetric set of convnets that upsample the audio.\n In summary, a 24kHz waveform comes through, we encode/downsample it into\n a vector zt of dimensionality D = 256, quantize it into discrete symbols qt , turn it\n back into a vector zq t of dimensionality D = 256, and then decode/upsample that\n vector back into a waveform at 24kHz.\n\n 16.2.2 Vector Quantization\n vector\nquantization The goal of the vector quantization or VQ step is to turn a series of vectors into a\n VQ series of discrete symbols.\n Historically vector quantization (Gray, 1984) was used to compress a speech\n signal, to reduce the bit rate for transmission or storage. To compress a sequence\n of vector representations of speech, we turn each vector into an integer, an index\n representing a class or cluster. Then instead of transmitting a big vector of floating\n point numbers, we transmit that integer index. At the other end of the transmission,\n we reconstitute the vector from the index.\n For TTS and other modern speech applications we use vector quantization for a\n different reason: because VQ conveniently creates discrete tokens, and those fit well\n into the language modeling paradigm, since language models do well at predicting\n sequences of discrete tokens.\n In practice for the E N C ODEC model and other audio tokenizers, we use a powerful from of vector quantization called residual vector quantization that we‚Äôll define\n in the following section. But it will be helpful to first see the basic VQ algorithm\n before we extend it.\n Vector quantization has a training phase and an inference phase. We already\n introduced the core of the basic VQ training algorithm when we described k-means\n clustering of vectors in Section ??, since k-means clustering is the most common\n algorithm used to implement VQ. To review, in VQ training, we run a big set of\n speech wavefiles through an encoder to generate N vectors, each one corresponding\n to some frame of speech. Then we cluster all these N vectors into k clusters; k is set\n by the designer as a parameter to the algorithm as the number of discrete symbols\n we want, generally with k << N. In the simplest VQ algorithm, we use the iterative\n k-means algorithm to learn the clusters. Recall from Section ?? that k-means is\n a two-step algorithm based on iteratively updating a set of k centroid vectors. A\n centroid centroid is the geometric center of a set of a points in n-dimensional space.\n The k-means algorithm for clustering starts by assigning a random vector to each\n cluster k. Then there are two iterative steps. In the assignment step, given a set of\n k current centroids and the entire dataset of vectors, each vector is assigned to the\n cluster whose codeword is the closest (by squared Euclidean distance). In the re-\n6 C HAPTER 16 ‚Ä¢ T EXT- TO -S PEECH\n\n 256-d vector\n Output:\n to be quantized\n Vector Quantizer discrete symbol\n 0 1.2\n\n 0.9\n Encoder 0.1 Similarity 3\n ‚Ä¶\n\n -55\n\n 0.2\n\n 1 2 3 4 5 ‚Ä¶ 1024 Cluster #\n 255 -9\n\n ‚Ä¶ 256-d codewords\n (vectors)\n\n Codebook\n\n The input is a span of speech encoded by the encoder into a vector of dimensionality D =\n 256. This vector is compared with each codeword (cluster centroid) in the codebook. The\n codeword for cluster 3 is most similar, so the VQ outputs 3 as the discrete representation of\n this vector.\n\n estimation step, the codeword for each cluster is recomputed by recalculating a new\n mean vector. The result is that the clusters and their centroids slowly adjust to the\n training space. We iterate back and forth between these two steps until the algorithm\n converges.\n VQ can also be used as part of end-to-end training, as we will discuss below,\n in which case instead of iterative k-means, we instead recompute the means during\n minibatch training via online algorithms like exponential moving averages.\n At the end of clustering, the cluster index can be used as a discrete symbol. Each\n codeword cluster is also associated with a codeword, the vector which is the centroid of all\n the vectors in the cluster. We call the list of cluster ids (tokens) together with their\n codebook codeword the codebook, and we often call the cluster id the code.\n code In inference, when a new vector comes in, we compare it to each vector in the\n codebook. Whichever codeword is closest, we assign it to that codeword‚Äôs associated\n cluster. Fig. 16.4 shows an intuition of this inference step in the context of speech\n encoding:\n 1. an input speech waveform is encoded into a vector v,\n 2. this input vector v is compared to each of the 1024 possible codewords in the\n codebook,\n 3. v is found to be most similar to codeword 3,\n 4. and so the output of VQ is the discrete symbol 3 as a representation of v.\n As we will see below, for training the E N C ODEC model end-to-end we will need\n a way to turn this discrete symbol back into a waveform. For simple VQ we do\n that by directly using the codeword for that cluster, passing that codeword to the\n decoder for it to reconstruct the waveform. Of course the codeword vector won‚Äôt\n exactly match the original vector encoding of the input speech span, especially with\n only 1024 possible codewords, but the hope is that it‚Äôs at least close if our codebook\n is good, and the decoder will still produce reasonable speech. Nonetheless, more\n powerful methods are usually used, as we‚Äôll see in the next section.\n 16.2 ‚Ä¢ U SING A CODEC TO LEARN DISCRETE AUDIO TOKENS 7\n\n 16.2.3 Residual Vector Quantization\n In practice, simple VQ doesn‚Äôt produce good enough reconstructions, at least not\n with codebook sizes of 1024. 1024 codeword vectors just isn‚Äôt enough to represent\n the wide variety of embeddings we get from encoding all possible speech waveforms. So what the E N C ODEC model (and many other audio tokenization methods)\nresidual vector\n quantization use instead is a more sophisticated variant called residual vector quantization, or\n RVQ RVQ. In residual vector quantization, we use multiple codebooks arranged in a kind\n of hierarchy.\n\nFigure\n put 2: The neural\n embedding audio codec\n to produce modelsymbol\n a discrete revisit. Because\n and the RVQ is employed,\n corresponding the first quantizer\n codeword. We thenplays\n look\n the most important role in reconstruction, and the impact from others gradually decreases.\n at the residual, the difference between the encoder output embedding zt and the codeword\n chosen by VQ. We then take a second codebook and run VQ on this residual. We repeat the\n we can explicitly\n process until we control\n have 8 the content in speech synthesis. Another direction is to apply pre-training\n tokens.\n to the neural TTS. Chung et al. [2018] pre-trains speech decoder in TTS through autoregressive\n mel-spectrogram prediction. In Ao et al. [2022], the authors propose a unified-modal encoder-decoder\n The idea\n framework is verywhich\n SpeechT5, simple. We rununlabeled\n can leverage standardspeech\n VQ with a codebook\n and text just as\n data to pre-train all in Fig. 16.4\n components\n in the prior\n of TTS model.section.\n Tjandra etThen for an\n al. [2019] input unlabeled\n quantizes embedding zt we\n speech into take thetokens\n discrete codeword vector\n by a VQVAE\n model\n that is [van den Oord\n produced, it zq1and\n call2017],\n et al.,\n let‚Äôs fortrain\n the az model\n as with the token-to-speech\n quantified by codebook sequence.\n 1, and They\n take the\n demonstrate that the pre-trained modelt only requires t a small amount of real data for fine-tuning. Bai\n difference between the two:\n et al. [2022] proposes mask and reconstruction on mel spectrogram and showing better performance\n on speech editing and synthesis. Previous TTS pre-training work leverages less than 1K hours of\n (1)\n data, whereas VALL-E is pre-trainedresidual\n with 60K = zt ‚àíofzdata.\n hours q t Furthermore, VALL-E is the first (16.1)\n to\n use audio codec codes as intermediate representations, and emerge in-context learning capability in\n residual zero-shot\n This TTS. is the error in the VQ; the part of the original vector that the VQ\n residual\n didn‚Äôt capture. The residual is kind of a rounding error; it‚Äôs as if in VQ we ‚Äòround‚Äô\n 3 vector\n the Background: Speech\n to the nearest Quantization\n codeword, and that creates some error. So we then take that\n residual vector and pass it through another\n Since audio is typically stored as a sequence of 16-bit vector\n integerquantizer! That gives\n values, a generative modelusisarequired\n second\n codeword 16that represents the residual part of the vector. We then take the residual\n to output 2 = 65, 536 probabilities per timestep o synthesize the raw audio. In addition, the audio\n samplethe\n from ratesecond\n exceeding ten thousand\n codeword, leads\n and dotothis\n an extraordinarily\n again. The long totalsequence length,\n result is making it more\n 8 codewords (the\n intractable for raw audio synthesis. To this end, speech quantization is required to compress integer\n original codeword and the 7 residuals).\n values and sequence length. ¬µ-law transformation can quantize each timestep to 256 values and\n That means\n reconstruct for RVQ\n high-quality we represent\n raw audio. It is widelytheusedoriginal\n in speechspeech span\n generative by asuch\n models, sequence of 8\n as WaveNet\n [van den Oord\n discrete et al.,(instead\n symbols 2016], butofthe1 inference\n discrete speed\n symbolis still\n in slow\n basicsince\n VQ).the Fig.\n sequence\n 16.5length\n shows is not\n the\n reduced. Recently, vector quantization is widely applied in self-supervised speech models for feature\n intuition.\n extraction, such as vq-wav2vec [Baevski et al., 2020a] and HuBERT [Hsu et al., 2021]. The following\n workWhat do we\n [Lakhotia do2021,\n et al., when Duwe want\n et al., 2022]toshows\n reconstruct\n the codesthe speech?\n from The method\n self-supervised models canusedalsoin\n Ereconstruct\n N C ODEC content,\n RVQand the inference\n is again simple: speedweis faster than 8WaveNet.\n take the codewordsHowever,\n andthe speaker\n add themidentity has\n together!\n been discarded and the reconstruction quality is low [Borsos et al., 2022]. AudioLM [Borsos et al.,\n The resulting vector\n 2022] trains speech-to-speech z is then passed through the decoder to generate a\n q t language models on both k-means tokens from a self-supervised model waveform.\n and .acoustic tokens from a neural codec model, leading to high-quality speech-to-speech generation.\n In this paper, we follow AudioLM [Borsos et al., 2022] to leverage neural codec models to represent\n 16.2.4 Training\n speech in discrete tokens. the\n To compress audio formodel\n E N C ODEC network of audio tokens\n transmission, codec models are able to\n encode waveform into discrete acoustic codes and reconstruct high-quality waveform even if the\n speaker\n The E NisCunseen\n ODEC in training.\n model Compared\n (like similartoaudio\n traditional audio codec\n tokenizer approaches,\n models) the neural-based\n is trained end to end.\n codec is significantly better at low bitrates, and we believe the quantized tokens contain sufficient\n The input isabout\n information a waveform,\n the speakeraand\n span of speech\n recording of perhaps\n conditions. 1 or to\n Compared 10other\n seconds extracted\n quantization from\n methods,\n athelonger original\n audio codec waveform.\n shows The\n the following desired output\n advantages: is theabundant\n 1) It contains same waveform span, since\n speaker information and\n acoustic information, which could maintain speaker identity in reconstruction compared to HuBERT\n codes [Hsu et al., 2021]. 2) There is an off-the-shelf codec decoder to convert discrete tokens into a\n waveform, without the additional efforts on vocoder training like VQ-based methods that operated on\n spectrum [Du et al., 2022]. 3) It could reduce the length of time steps for efficiency to address the\n problem in ¬µ-law transformation [van den Oord et al., 2016].\n\n8 C HAPTER 16 ‚Ä¢ T EXT- TO -S PEECH\n\n the model is a kind of autoencoder that learns to map to itself. The model is trained\n to do this reconstruction on large speech datasets like Common Voice (Ardila et al.,\n 2020) (over 30,000 hours of speech in 133 languages) as well as other audio data\n like Audio Set (Gemmeke et al., 2017) (1.7 million 10 sec excerpts from YouTube\n videos labeled from a large ontology including natural, animal, and machine sounds,\n music, and so on).\n\n En\n cod ùìõ GAN de\n r\n er co\n De\n Quantization\n x zt qt,1 qt,2 ‚Ä¶ qt,N zqt x^\n\n ùìõ VQ\n c\n\n ùìõ reconstruction\n (2025). The audio tokenizer is trained with a weighted combination of various loss functions,\n summarized in the figure and described below.\n\n The E N C ODEC model, like most audio tokenizers, is trained with a number of\n reconstruction loss functions, as suggested in Fig. 16.6. The reconstruction loss Lreconstruction mealoss\n sures how similar the output waveform is to the input waveform, for example by the\n sum-squared difference between the original and reconstructed audio:\n\n T\n X\n Lreconstruction (x, xÃÇ) = ||xt ‚àí xÃÇt ||2 (16.2)\n t=1\n\n Similarity can additionally be measured in the frequency domain, by comparing the\n original and reconstructed mel-spectrogram, again using sum-squared (L2) distance\n or L1 distance or some combination.\nadversarial loss Another kind of loss is the adversarial loss LGAN . For this loss we train a\n generative adversarial network, a generator and a binary discriminator D, which\n is a classifier to distinguish between the true wavefile x and a generated one. We\n want to train the model to fool this discriminator, so the better the discriminator, the\n worse our reconstruction must be, and so we use the discriminator‚Äôs success as a loss\n function, We can also incorporate various features from the generator.\n Finally, we need a loss for the quantizer. This is because having a quantizer in\n the middle of end-to-end training causes problems in propagation of the gradient in\n the backward pass of training, because the quantization step is not differentiable.\n We deal with this problem in two ways. First, we ignore the quantization step in\n the backward pass. Instead we copy the gradients from the output of the quantizer\n (zq t ) back to the input of the quantizer (zt ), a method called the straight-through\n estimator (Van Den Oord et al., 2017).\n But then we need a method to make sure the code words in the vector quantizer\n step get updated during training. One method is to start these off using k-means\n clustering of the vectors zt to get an initial clustering. Then we can add to a loss\n component, LVQ , which will be a function of the difference between the encoder\n 16.3 ‚Ä¢ VALL-E: G ENERATING AUDIO WITH 2- STAGE LM 9\n\n output vector zt and the reconstructed vector after the quantization zq t , i.e. the codeword, summed over all the Nc codebooks and residuals.\n Nc\n T X\n X (c)\n LVQ (x, xÃÇ) = ||z(c) t ‚àí zq t || (16.3)\n t=1 c=1\n\n The total loss function can then just be a weighted sum of these losses:\n\n L(x, xÃÇ) = Œª1 Lreconstruction (x, xÃÇ) + Œª2 LGAN (x, xÃÇ) + Œª3 LVQ (x, xÃÇ) (16.4)\n\n As we summarized in the introduction, the structure of TTS systems like VALL-E\n is to take as input a text to be synthesized and a sample of the voice to be used, and\n tokenize both, using BPE for the text and an audio codec for the speech. We then\n use a language model to conditionally generate discrete audio tokens corresponding\n to the text prompt, in the voice of the speech sample.\n\n Output code sequence ‚Ä¶\n ‚Ä¶\n Non-Autoregressive\n CT‚Äô+1,1 CT‚Äô+2,1 CT,1\n x C CT‚Äô+1,2\n CT‚Äô+1,3\n CT‚Äô+2,2\n CT‚Äô+2,3\n ‚Ä¶ CT,2\n CT,3\n\n Non-Autoregressive\n CT‚Äô+1,1\n CT‚Äô+2,1 CT,1\n x C\n ‚Ä¶\n CT‚Äô+1,2 CT‚Äô+2,2 CT,2\n\n Non-Autoregressive\n x C CT‚Äô+1,1 CT‚Äô+2,1 ‚Ä¶ CT,1\n\n Autoregressive (AR) Transformer\n\n x C\n Text Audio Prompt\n\n stage for the autoregressive transformer and the first 3 of the 7 non-autoregressive transformers. The output sequence of discrete audio codes is generated in two stages. First the autoregressive LM generates all the codes for the first quantizer from left to right. Then the\n non-autoregressive model is called 7 times to generate the remaining codes conditioned on all\n the codes from the preceding quantizer, including conditioning on the codes to the right.\n\n Instead of doing this conditional generation with a single autoregressive language model, VALL-E does the conditional generation in a 2-stage process, using\n two distinct language models. This architectural choice is influenced by the hierarchical nature of the RVQ quantizer that generates the audio tokens. The output of the\n first RVQ quantizer is the most important token to the final speech, while the subsequent quantizers contribute less and less residual information to the final signal. So\n 10 C HAPTER 16 ‚Ä¢ T EXT- TO -S PEECH\n\n the language model generates the acoustic codes in two stages. First, an autoregressive LM generates the first-quantizer codes for the entire output sequence, given the\n input text and enrolled audio. Then given those codes, a non-autoregressive LM is\n run 7 times, each time taking as input the output of the initial autoregressive codes\n and the prior non-autoregressive quantizer and thus generating the codes from the\n remaining quantizers one by one. Fig. 16.7 shows the intuition for the inference\n step.\n Now let‚Äôs see the architecture in a bit more detail. For training, we are given\n an audio sample y and its tokenized text transcription x = [x0 , x1 , . . . , xL ]. We use a\n pretrained E N C ODEC to convert y into a code matrix C. Let T be the number of\n downsampled vectors output by E N C ODEC, with 8 codes per vector. Then we can\n represent the encoder output as\n CT √ó8 = E N C ODEC(y) (16.5)\n\n Here C is a two-dimensional acoustic code matrix that has T √ó 8 entries, where the\n columns represent time and the rows represent different quantizers. That is, the row\n vector ct,: of the matrix contains the 8 codes for the t-th frame, and the column vector\n c:, j contains the code sequence from the j-th vector quantizer where j ‚àà [1, ..., 8].\n Given the text x and audio C, we train the TTS as a conditional code language\n model to maximize the likelihood of C conditioned on x:\n L = ‚àí log p(C|x)\n T\n Y\n = ‚àí log p(c<t,: , x) (16.6)\nCHEN et al.: NEURAL CODEC LANGUAGE MODELS ARE ZERO-SHOT TEXT TO SPEECH\n t=0 SYNTHESIZERS 709\n\n Figure\nFig. 3. Training overview 16.8 We\n of VALL-E. Training\n regard TTSprocedure for VALL-E.\n as a conditional Given\n codec language the text\n modeling task. prompt, theVALL-E\n We structure autoregressive\n as two conditional codec language\nmodels in a hierarchicaltransformer\n structure. The is\n ARfirst trained\n model is usedtotogenerate eachcode\n generate each code of the\n of the first first-quantizer\n code sequence incode sequence, manner,\n an autoregressive autore-while the NAR model is\nused to generate each remaining code sequence based on the previous code sequences in a non-autoregressive manner.\n gressively The the non-autoregressive transformer generates the rest of the codes. Figure from\n Chen et al. (2025).\n\n Fig.AR\nB. Hierarchical Structure: 16.8 andshows the intuition. On the left,\n NAR Model we have an audio\n simultaneously, sample the\n thus reducing andtime\n its trancomplexity from O(T )\n scription, and both are tokenized. Then\n As introduced in Section III, the codec codes derived from the wetoappend\n O(1). an [ EOS ] and [ BOS ] token to x\n and anwith\nneural audio codec model [ EOS ] token\n RVQ to the\n exhibit twoend\n keyof C and train the autoregressive transformer to predict\n properties:\n(1) A single speechthe acoustic\n sample tokens,into\n is encoded starting c0,1 ,sewithcode\n multiple untilC.[ EOS\n Training:\n ], and Conditional Codec Language Modeling\n then the non-autoregressive\nquences with multiple transformers\n quantizers in to the\n fill audio\n in thecodec\n other model.\n tokens.(2) As depicted in Fig. 3, VALL-E is trained using the condiis presentinference,\nA hierarchical structure During where the code we are given afrom\n sequence text sequence to belanguage\n tional codec spoken as y0 , an enwell as method.\n modeling It is noteworthy that\n rolledmost\nthe first quantizer covers speech\n of thesample\n acoustic from some unseen\n information, whilespeaker, for which\n the training we have requires\n of VALL-E the transcription\n only simple utterance-wise\nsubsequent code sequences contain the residual acoustic infor- audio-transcription pair data, and no complex data such as\nmation from their predecessors, serving to refine and augment force-alignment information or additional audio clips of the\nthe acoustic details. same speaker for reference. This greatly simplifies the process of\n Inspired by these properties, we design VALL-E as two collecting and processing training data, facilitating scalability.\nconditional codec language models in a hierarchical structure: Specifically, for each audio and corresponding transcription\nan Autoregressive (AR) codec language model and a Non- in the training dataset, we initially utilize the audio codec\n 16.4 ‚Ä¢ TTS E VALUATION 11\n\n transcript(y0 ). We first run the codec to get an acoustic code matrix for y0 , which will\n be CP = C:T 0 ,: = [c0,: , c1,: , . . . cT 0 ,: :]. Next we concatenate the transcription of y0 to\n the text sequence to be spoken to create the total input text x, which we pass through\n a text tokenizer. At this stage we thus have a tokenized text x and a tokenized audio\n prompt CP .\n Then we generate CT = C>T 0 ,: = [cT 0 +1,: , . . . cT,: ] conditioned on the text sequence x and the prompt CP :\n CT = argmax p(CT |CP , x)\n CT\n T\n Y\n = argmax p(ct,: |c<t,: , x) (16.7)\n CT t=T 0 +1\n IEEE TRANSACTIONS ON AUDIO,\n Then the generated SPEECH\n tokens ANDbeLANGUAGE\n CT can converted byPROCESSING, VOL.\n the E N C ODEC 33, 2025\n decoder into\n a waveform. Fig. 16.9 shows the intuition.\n\n The AR model is fed with\nnding code sequence with\nd at the end using a code\nattention mask strategy, the\ny attend to the text sequence\n demonstrated in the lower\n\nptimized by minimizing the\n code sequence c:,1 condiŒ∏AR ) (10)\n\n |c<t,1 , x; Œ∏AR ). (11)\nFig. 4. Inference overview of VALL-E. We perform zero-shot TTS via promptscript for the 3 seconds of enrolled speech is first prepended to the text to be generated, and\nLanguage Modeling: Given ing\n both the conditional\n the codec\n speech and text languageNext\n are tokenized. model.\n the autoregressive transformer starts generating\nby the AR model, the NAR the first codes ct 0 +1,1 conditioned on the transcript and acoustic prompt.\n\n remaining code sequence See Chen et al. (2025) for more details on the transformer components and other\n ence x and the preceding Overall,\n details the NAR model is optimized by minimizing the\n of training.\n toregressive manner, where negative log likelihood of each j-th target code sequence c>T ‚Ä≤ ,j\n conditioned on the text sequence x, all the code sequences of\n e sequences of the prompt the acoustic condition C:T ‚Ä≤ ,: and the preceding j ‚àí 1 target code\ne speaker information of the sequences c>T ‚Ä≤ ,<j .\n TTS systems are evaluated by humans, by playing an utterance to listeners and askitly split the code matrix MOSC ingLthem\n NARto= give mean\n ‚àíalog p(Copinion score\n >T ‚Ä≤ ,>1 |x,(MOS),\n C<T ‚Ä≤a,:rating\n , c>Tof‚Ä≤ ,1how (15)\n good)the synthesized\n ; Œ∏NAR\n d target code matrix C>T ‚Ä≤ ,: utterances are, usually on a scale from 1‚Äì5. We can then compare systems by com-\nThe model is then optimized paring their MOS$ scores\n 8 on the same sentences (using, e.g., paired t-tests to test for\n significant=\n differences).\n ‚àí log p(c>T ‚Ä≤ ,j |x, C<T ‚Ä≤ ,: , C>T ‚Ä≤ ,<j ; Œ∏NAR ). (16)\n e c>T ‚Ä≤ ,j conditioned on the\n j=2\n es in the acoustic condition\nnces in the target code matrix In practice, to optimize computational efficiency during training,\nner. we do not calculate the training loss by iterating over all values of\nt of Fig. 3, we first obtain the j and aggregating the corresponding losses. Instead, during each\n12 C HAPTER 16 ‚Ä¢ T EXT- TO -S PEECH\n\n If we are comparing exactly two systems (perhaps to see if a particular change\n CMOS actually improved the system), we can also compare using CMOS (Comparative\n MOS). where users give their preference on which of the two utterances is better.\n CMOS scores range from -3 (the system is much worse than the reference) to 3 (the\n system is better than the reference) Here we play the same sentence synthesized by\n two different systems. The human listeners choose which of the two utterances they\n like better. We do this for say 50 sentences (presented in random order) and compare\n the number of sentences preferred for each system.\n Although speech synthesis systems are best evaluated by human listeners, some\n automatic metrics can be used to add more information. For example we can run the\n output through an ASR system and compute the word error rate (WER) to see how\n robust the synthesized output is. Or for measuring how well the voice output of the\n TTS system matches the enrolled voice, we can treat the task as if it were speaker\n verification, passing the two voices to a speaker verification system and using the\n resulting score as a similarity score.\n\n There are a wide variety of other speech-related tasks.\n speaker Speaker diarization is the task of determining ‚Äòwho spoke when‚Äô in a long\n diarization\n multi-speaker audio recording, marking the start and end of each speaker‚Äôs turns in\n the interaction. This can be useful for transcribing meetings, classroom speech, or\n medical interactions. Often diarization systems use voice activity detection (VAD) to\n find segments of continuous speech, extract speaker embedding vectors, and cluster\n the vectors to group together segments likely from the same speaker. More recent\n work is investigating end-to-end algorithms to map directly from input speech to a\n sequence of speaker labels for each frame.\n speaker\n recognition Speaker recognition, is the task of identifying a speaker. We generally distinspeaker guish the subtasks of speaker verification, where we make a binary decision (is\n verification\n this speaker X or not?), such as for security when accessing personal information\n over the telephone, and speaker identification, where we make a one of N decision\n trying to match a speaker‚Äôs voice against a database of many speakers.\n language In the task of language identification, we are given a wavefile and must identify\n identification\n which language is being spoken; this is an important part of building multilingual\n models, creating datasets, and even plays a role in online systems.\n wake word The task of wake word detection is to detect a word or short phrase, usually in\n order to wake up a voice-enable assistant like Alexa, Siri, or the Google Assistant.\n The goal with wake words is build the detection into small devices at the computing\n edge, to maintain privacy by transmitting the least amount of user speech to a cloudbased server. Thus wake word detectors need to be fast, small footprint software that\n can fit into embedded devices. Wake word detectors usually use the same frontend\n feature extraction we saw for ASR, often followed by a whole-word classifier.\n\n TBD\n 16.7 ‚Ä¢ S UMMARY 13\n\n This chapter introduced the fundamental algorithms of text-to-speech (TTS).\n ‚Ä¢ A common modern algorithm for TTS is to use conditional generation with a\n language model over audio tokens learned by a codec model.\n ‚Ä¢ A neural audio codec, short for coder/decoder, is a system that encodes analog speech signals into a digitized, discrete compressed representation for\n compression.\n ‚Ä¢ The discrete symbols that a codec produces as its compressed representation\n can be used as discrete codes for language modeling.\n ‚Ä¢ A codec includes an encoder that uses convnets to downsample speech into\n a downsampled embedding, a quantizer that converts the embedding into a\n series of discrete tokens, and a decoder that uses convnets to upsample the\n tokens/embedding back into a lossy reconstructed waveform.\n ‚Ä¢ Vector Quantization (VQ) is a method for turning a series of vectors into a\n series of discrete symbols. This can be done by using k-means clustering, and\n then creating a codebook in which each code is represented by a vector at the\n centroid of each cluster, called a codeword. Input vector can be assigned the\n nearest codeword cluster.\n ‚Ä¢ Residual Vector Quantization (RVQ) is a hierarchical version of vector\n quantization that produces multiple codes for an input vector by first quantizing a vector into a codebook, and then quantizing the residual (the difference\n between the codeword and the input vector) and then iterating.\n ‚Ä¢ TTS systems like VALL-E take a text to be synthesized and a sample of the\n voice to be used, tokenize with BPE (text) and an audio codec (speech) and\n then use an LM to conditionally generate discrete audio tokens corresponding\n to the text prompt, in the voice of the speech sample.\n ‚Ä¢ TTS is evaluated by playing a sentence to human listeners and having them\n give a mean opinion score (MOS).\n\nHistorical Notes\n As we noted at the beginning of the chapter, speech synthesis is one of the earliest\n fields of speech and language processing. The 18th century saw a number of physical\n models of the articulation process, including the von Kempelen model mentioned\n above, as well as the 1773 vowel model of Kratzenstein in Copenhagen using organ\n pipes.\n The early 1950s saw the development of three early paradigms of waveform\n synthesis: formant synthesis, articulatory synthesis, and concatenative synthesis.\n Formant synthesizers originally were inspired by attempts to mimic human\n speech by generating artificial spectrograms. The Haskins Laboratories Pattern\n Playback Machine generated a sound wave by painting spectrogram patterns on a\n moving transparent belt and using reflectance to filter the harmonics of a waveform (Cooper et al., 1951); other very early formant synthesizers include those of\n Lawrence (1953) and Fant (1951). Perhaps the most well-known of the formant\n synthesizers were the Klatt formant synthesizer and its successor systems, including the MITalk system (Allen et al., 1987) and the Klattalk software used in Digital\n Equipment Corporation‚Äôs DECtalk (Klatt, 1982). See Klatt (1975) for details.\n14 C HAPTER 16 ‚Ä¢ T EXT- TO -S PEECH\n\n A second early paradigm, concatenative synthesis, seems to have been first proposed by Harris (1953) at Bell Laboratories; he literally spliced together pieces of\n magnetic tape corresponding to phones. Soon afterwards, Peterson et al. (1958) proposed a theoretical model based on diphones, including a database with multiple\n copies of each diphone with differing prosody, each labeled with prosodic features\n including F0, stress, and duration, and the use of join costs based on F0 and formant\n distance between neighboring units. But such diphone synthesis models were not\n actually implemented until decades later (Dixon and Maxey 1968, Olive 1977). The\n 1980s and 1990s saw the invention of unit selection synthesis, based on larger units\n of non-uniform length and the use of a target cost, (Sagisaka 1988, Sagisaka et al.\n 1992, Hunt and Black 1996, Black and Taylor 1994, Syrdal et al. 2000).\n A third paradigm, articulatory synthesizers attempt to synthesize speech by\n modeling the physics of the vocal tract as an open tube. Representative models\n include Stevens et al. (1953), Flanagan et al. (1975), and Fant (1986). See Klatt\n (1975) and Flanagan (1972) for more details.\n Most early TTS systems used phonemes as input; development of the text analysis components of TTS came somewhat later, drawing on NLP. Indeed the first\n true text-to-speech system seems to have been the system of Umeda and Teranishi\n (Umeda et al. 1968, Teranishi and Umeda 1968, Umeda 1976), which included a\n parser that assigned prosodic boundaries, as well as accent and stress.\n History of codecs and modern history of neural TTS TBD.\n\nExercises\n Exercises 15\n\nAllen, J., M. S. Hunnicut, and D. H. Klatt. 1987. From Text Olive, J. P. 1977. Rule synthesis of speech from dyadic units.\n to Speech: The MITalk system. Cambridge University ICASSP77.\n Press. Peterson, G. E., W. S.-Y. Wang, and E. Sivertsen. 1958.\nArdila, R., M. Branson, K. Davis, M. Kohler, J. Meyer, Segmentation techniques in speech synthesis. JASA,\n M. Henretty, R. Morais, L. Saunders, F. Tyers, and G. We- 30(8):739‚Äì742.\n ber. 2020. Common voice: A massively-multilingual Sagisaka, Y. 1988. Speech synthesis by rule using an optimal\n speech corpus. LREC. selection of non-uniform synthesis units. ICASSP.\nBlack, A. W. and P. Taylor. 1994. CHATR: A generic speech Sagisaka, Y., N. Kaiki, N. Iwahashi, and K. Mimura. 1992.\n synthesis system. COLING. Atr ‚Äì ŒΩ-talk speech synthesis system. ICSLP.\nChen, S., C. Wang, Y. Wu, Z. Zhang, L. Zhou, S. Liu, Stevens, K. N., S. Kasowski, and G. M. Fant. 1953. An elec-\nZ. Chen, Y. Liu, H. Wang, J. Li, L. He, S. Zhao, and trical analog of the vocal tract. JASA, 25(4):734‚Äì742.\n F. Wei. 2025. Neural codec language models are zeroshot text to speech synthesizers. IEEE Trans. on TASLP, Syrdal, A. K., C. W. Wightman, A. Conkie, Y. Stylianou,\n 33:705‚Äì718. M. Beutnagel, J. Schroeter, V. Strom, and K.-S. Lee.\n 2000. Corpus-based techniques in the AT&T NEXTGEN\nCooper, F. S., A. M. Liberman, and J. M. Borst. 1951. The\n synthesis system. ICSLP.\n interconversion of audible and visible patterns as a basis\n for research in the perception of speech. Proceedings of Teranishi, R. and N. Umeda. 1968. Use of pronouncing dicthe National Academy of Sciences, 37(5):318‚Äì325. tionary in speech synthesis experiments. 6th International\n Congress on Acoustics.\nDeÃÅfossez, A., J. Copet, G. Synnaeve, and Y. Adi. 2023. High\n fidelity neural audio compression. TMLR. Umeda, N. 1976. Linguistic rules for text-to-speech synthesis. Proceedings of the IEEE, 64(4):443‚Äì451.\nDixon, N. and H. Maxey. 1968. Terminal analog synthesis of\n continuous speech using the diphone method of segment Umeda, N., E. Matui, T. Suzuki, and H. Omura. 1968. Synassembly. IEEE Transactions on Audio and Electroacous- thesis of fairy tale using an analog vocal tract. 6th Intertics, 16(1):40‚Äì50. national Congress on Acoustics.\nFant, G. M. 1951. Speech communication research. Ing. Van Den Oord, A., O. Vinyals, and K. Kavukcuoglu. 2017.\n Vetenskaps Akad. Stockholm, Sweden, 24:331‚Äì337. Neural discrete representation learning. NeurIPS.\nFant, G. M. 1986. Glottal flow: Models and interaction.\n Journal of Phonetics, 14:393‚Äì399.\nFlanagan, J. L. 1972. Speech Analysis, Synthesis, and Perception. Springer.\nFlanagan, J. L., K. Ishizaka, and K. L. Shipley. 1975. Synthesis of speech from a dynamic model of the vocal\n cords and vocal tract. The Bell System Technical Journal, 54(3):485‚Äì506.\nGemmeke, J. F., D. P. Ellis, D. Freedman, A. Jansen,\n W. Lawrence, R. C. Moore, M. Plakal, and M. Ritter.\n 2017. Audio Set: An ontology and human-labeled dataset\n for audio events. ICASSP.\nGray, R. M. 1984. Vector quantization. IEEE Transactions\n on ASSP, ASSP-1(2):4‚Äì29.\nHarris, C. M. 1953. A study of the building blocks in speech.\n JASA, 25(5):962‚Äì969.\nHunt, A. J. and A. W. Black. 1996. Unit selection in a concatenative speech synthesis system using a large speech\n database. ICASSP.\nKlatt, D. H. 1975. Voice onset time, friction, and aspiration\n in word-initial consonant clusters. Journal of Speech and\n Hearing Research, 18:686‚Äì706.\nKlatt, D. H. 1982. The Klattalk text-to-speech conversion\n system. ICASSP.\nLawrence, W. 1953. The synthesis of speech from signals\n which have a low information rate. In W. Jackson, ed.,\n Communication Theory, 460‚Äì469. Butterworth.\nMousavi, P., G. Maimon, A. Moumen, D. Petermann,\n J. Shi, H. Wu, H. Yang, A. Kuznetsova, A. Ploujnikov,\n R. Marxer, B. Ramabhadran, B. Elizalde, L. Lugosch,\n J. Li, C. Subakan, P. Woodland, M. Kim, H. yi Lee,\n S. Watanabe, Y. Adi, and M. Ravanelli. 2025. Discrete\n audio tokens: More than a survey! ArXiv preprint.\n",
    "file_path": "/Users/colinsidberry/nlp-textbook-rag/data/extracted_text/16.txt",
    "file_size_kb": 47.32
  },
  {
    "id": "575ca7cf20c8b3ae",
    "source": "nlp_textbook",
    "chapter": "Sequence Labeling for Parts of 17 Speech and Named Entities To each word a warbling note",
    "filename": "17.Sequence Labeling for Parts of Speech and Named Entities.txt",
    "content": "rights reserved. Draft of August 24, 2025.\n\nCHAPTER\n\n Sequence Labeling for Parts of\n17 Speech and Named Entities\n To each word a warbling note\n A Midsummer Night‚Äôs Dream, V.I\n\n Dionysius Thrax of Alexandria (c. 100 B . C .), or perhaps someone else (it was a long\n time ago), wrote a grammatical sketch of Greek (a ‚ÄútechneÃÑ‚Äù) that summarized the\n linguistic knowledge of his day. This work is the source of an astonishing proportion\n of modern linguistic vocabulary, including the words syntax, diphthong, clitic, and\n parts of speech analogy. Also included are a description of eight parts of speech: noun, verb,\n pronoun, preposition, adverb, conjunction, participle, and article. Although earlier\n scholars (including Aristotle as well as the Stoics) had their own lists of parts of\n speech, it was Thrax‚Äôs set of eight that became the basis for descriptions of European\n languages for the next 2000 years. (All the way to the Schoolhouse Rock educational\n television shows of our childhood, which had songs about 8 parts of speech, like the\n late great Bob Dorough‚Äôs Conjunction Junction.) The durability of parts of speech\n through two millennia speaks to their centrality in models of human language.\n Proper names are another important and anciently studied linguistic category.\n While parts of speech are generally assigned to individual words or morphemes, a\n proper name is often an entire multiword phrase, like the name ‚ÄúMarie Curie‚Äù, the\n location ‚ÄúNew York City‚Äù, or the organization ‚ÄúStanford University‚Äù. We‚Äôll use the\n named entity term named entity for, roughly speaking, anything that can be referred to with a\n proper name: a person, a location, an organization, although as we‚Äôll see the term is\n commonly extended to include things that aren‚Äôt entities per se.\n POS Parts of speech (also known as POS) and named entities are useful clues to\n sentence structure and meaning. Knowing whether a word is a noun or a verb tells us\n about likely neighboring words (nouns in English are preceded by determiners and\n adjectives, verbs by nouns) and syntactic structure (verbs have dependency links to\n nouns), making part-of-speech tagging a key aspect of parsing. Knowing if a named\n entity like Washington is a name of a person, a place, or a university is important to\n many natural language processing tasks like question answering, stance detection,\n or information extraction.\n In this chapter we‚Äôll introduce the task of part-of-speech tagging, taking a sequence of words and assigning each word a part of speech like NOUN or VERB, and\n the task of named entity recognition (NER), assigning words or phrases tags like\n PERSON , LOCATION , or ORGANIZATION .\n Such tasks in which we assign, to each word xi in an input word sequence, a\n label yi , so that the output sequence Y has the same length as the input sequence X\n sequence\n labeling are called sequence labeling tasks. We‚Äôll introduce classic sequence labeling algorithms, one generative‚Äî the Hidden Markov Model (HMM)‚Äîand one discriminative‚Äî\n the Conditional Random Field (CRF). In following chapters we‚Äôll introduce modern\n sequence labelers based on RNNs and Transformers.\n2 C HAPTER 17 ‚Ä¢ S EQUENCE L ABELING FOR PARTS OF S PEECH AND NAMED E NTITIES\n\n17.1 (Mostly) English Word Classes\n Until now we have been using part-of-speech terms like noun and verb rather freely.\n In this section we give more complete definitions. While word classes do have\n semantic tendencies‚Äîadjectives, for example, often describe properties and nouns\n people‚Äî parts of speech are defined instead based on their grammatical relationship\n with neighboring words or the morphological properties about their affixes.\n\n Tag Description Example\n ADJ Adjective: noun modifiers describing properties red, young, awesome\n Open Class\n\n ADV Adverb: verb modifiers of time, place, manner very, slowly, home, yesterday\n NOUN words for persons, places, things, etc. algorithm, cat, mango, beauty\n VERB words for actions and processes draw, provide, go\n PROPN Proper noun: name of a person, organization, place, etc.. Regina, IBM, Colorado\n INTJ Interjection: exclamation, greeting, yes/no response, etc. oh, um, yes, hello\n ADP Adposition (Preposition/Postposition): marks a noun‚Äôs in, on, by, under\n spacial, temporal, or other relation\n Closed Class Words\n\n AUX Auxiliary: helping verb marking tense, aspect, mood, etc., can, may, should, are\n CCONJ Coordinating Conjunction: joins two phrases/clauses and, or, but\n DET Determiner: marks noun phrase properties a, an, the, this\n NUM Numeral one, two, 2026, 11:00, hundred\n PART Particle: a function word that must be associated with an- ‚Äôs, not, (infinitive) to\n other word\n PRON Pronoun: a shorthand for referring to an entity or event she, who, I, others\n SCONJ Subordinating Conjunction: joins a main clause with a whether, because\n subordinate clause such as a sentential complement\n PUNCT Punctuation ,Ãá , ()\n Other\n\n SYM Symbols like $ or emoji $, %\n X Other asdf, qwfg\ncan be added to make finer-grained distinctions (with properties like number, case, definiteness, and so on).\n\n closed class Parts of speech fall into two broad categories: closed class and open class.\n open class Closed classes are those with relatively fixed membership, such as prepositions‚Äî\n new prepositions are rarely coined. By contrast, nouns and verbs are open classes‚Äî\n new nouns and verbs like iPhone or to fax are continually being created or borrowed.\n function word Closed class words are generally function words like of, it, and, or you, which tend\n to be very short, occur frequently, and often have structuring uses in grammar.\n Four major open classes occur in the languages of the world: nouns (including\n proper nouns), verbs, adjectives, and adverbs, as well as the smaller open class of\n interjections. English has all five, although not every language does.\n noun Nouns are words for people, places, or things, but include others as well. Comcommon noun mon nouns include concrete terms like cat and mango, abstractions like algorithm\n and beauty, and verb-like terms like pacing as in His pacing to and fro became quite\n annoying. Nouns in English can occur with determiners (a goat, this bandwidth)\n take possessives (IBM‚Äôs annual revenue), and may occur in the plural (goats, abaci).\n count noun Many languages, including English, divide common nouns into count nouns and\n mass noun mass nouns. Count nouns can occur in the singular and plural (goat/goats, relationship/relationships) and can be counted (one goat, two goats). Mass nouns are\n used when something is conceptualized as a homogeneous group. So snow, salt, and\n proper noun communism are not counted (i.e., *two snows or *two communisms). Proper nouns,\n like Regina, Colorado, and IBM, are names of specific persons or entities.\n 17.1 ‚Ä¢ (M OSTLY ) E NGLISH W ORD C LASSES 3\n\n verb Verbs refer to actions and processes, including main verbs like draw, provide,\n and go. English verbs have inflections (non-third-person-singular (eat), third-personsingular (eats), progressive (eating), past participle (eaten)). While many scholars\n believe that all human languages have the categories of noun and verb, others have\n argued that some languages, such as Riau Indonesian and Tongan, don‚Äôt even make\n this distinction (Broschart 1997; Evans 2000; Gil 2000) .\n adjective Adjectives often describe properties or qualities of nouns, like color (white,\n black), age (old, young), and value (good, bad), but there are languages without\n adjectives. In Korean, for example, the words corresponding to English adjectives\n act as a subclass of verbs, so what is in English an adjective ‚Äúbeautiful‚Äù acts in\n Korean like a verb meaning ‚Äúto be beautiful‚Äù.\n adverb Adverbs are a hodge-podge. All the italicized words in this example are adverbs:\n Actually, I ran home extremely quickly yesterday\n Adverbs generally modify something (often verbs, hence the name ‚Äúadverb‚Äù, but\n locative also other adverbs and entire verb phrases). Directional adverbs or locative addegree verbs (home, here, downhill) specify the direction or location of some action; degree\n adverbs (extremely, very, somewhat) specify the extent of some action, process, or\n manner property; manner adverbs (slowly, slinkily, delicately) describe the manner of some\n temporal action or process; and temporal adverbs describe the time that some action or event\n took place (yesterday, Monday).\n interjection Interjections (oh, hey, alas, uh, um) are a smaller open class that also includes\n greetings (hello, goodbye) and question responses (yes, no, uh-huh).\n preposition English adpositions occur before nouns, hence are called prepositions. They can\n indicate spatial or temporal relations, whether literal (on it, before then, by the house)\n or metaphorical (on time, with gusto, beside herself), and relations like marking the\n agent in Hamlet was written by Shakespeare.\n particle A particle resembles a preposition or an adverb and is used in combination with\n a verb. Particles often have extended meanings that aren‚Äôt quite the same as the\n prepositions they resemble, as in the particle over in she turned the paper over. A\n phrasal verb verb and a particle acting as a single unit is called a phrasal verb. The meaning\n of phrasal verbs is often non-compositional‚Äînot predictable from the individual\n meanings of the verb and the particle. Thus, turn down means ‚Äòreject‚Äô, rule out\n ‚Äòeliminate‚Äô, and go on ‚Äòcontinue‚Äô.\n determiner Determiners like this and that (this chapter, that page) can mark the start of an\n article English noun phrase. Articles like a, an, and the, are a type of determiner that mark\n discourse properties of the noun and are quite frequent; the is the most common\n word in written English, with a and an right behind.\n conjunction Conjunctions join two phrases, clauses, or sentences. Coordinating conjunctions like and, or, and but join two elements of equal status. Subordinating conjunctions are used when one of the elements has some embedded status. For example,\n the subordinating conjunction that in ‚ÄúI thought that you might like some milk‚Äù links\n the main clause I thought with the subordinate clause you might like some milk. This\n clause is called subordinate because this entire clause is the ‚Äúcontent‚Äù of the main\n verb thought. Subordinating conjunctions like that which link a verb to its argument\ncomplementizer in this way are also called complementizers.\n pronoun Pronouns act as a shorthand for referring to an entity or event. Personal pronouns refer to persons or entities (you, she, I, it, me, etc.). Possessive pronouns are\n forms of personal pronouns that indicate either actual possession or more often just\n an abstract relation between the person and some object (my, your, his, her, its, one‚Äôs,\n wh our, their). Wh-pronouns (what, who, whom, whoever) are used in certain question\n4 C HAPTER 17 ‚Ä¢ S EQUENCE L ABELING FOR PARTS OF S PEECH AND NAMED E NTITIES\n\n forms, or act as complementizers (Frida, who married Diego. . . ).\n auxiliary Auxiliary verbs mark semantic features of a main verb such as its tense, whether\n it is completed (aspect), whether it is negated (polarity), and whether an action is\n necessary, possible, suggested, or desired (mood). English auxiliaries include the\n copula copula verb be, the two verbs do and have, forms, as well as modal verbs used to\n modal mark the mood associated with the event depicted by the main verb: can indicates\n ability or possibility, may permission or possibility, must necessity.\n An English-specific tagset, the Penn Treebank tagset (Marcus et al., 1993), shown\n in Fig. 17.2, has been used to label many syntactically annotated corpora like the\n Penn Treebank corpora, so it is worth knowing about.\n\nTag Description Example Tag Description Example Tag Description Example\nCC coord. conj. and, but, or NNP proper noun, sing. IBM TO infinitive to to\nCD cardinal number one, two NNPS proper noun, plu. Carolinas UH interjection ah, oops\nDT determiner a, the NNS noun, plural llamas VB verb base eat\nEX existential ‚Äòthere‚Äô there PDT predeterminer all, both VBD verb past tense ate\nFW foreign word mea culpa POS possessive ending ‚Äôs VBG verb gerund eating\nIN preposition/ of, in, by PRP personal pronoun I, you, he VBN verb past partici- eaten\n subordin-conj ple\nJJ adjective yellow PRP$ possess. pronoun your VBP verb non-3sg-pr eat\nJJR comparative adj bigger RB adverb quickly VBZ verb 3sg pres eats\nJJS superlative adj wildest RBR comparative adv faster WDT wh-determ. which, that\nLS list item marker 1, 2, One RBS superlatv. adv fastest WP wh-pronoun what, who\nMD modal can, should RP particle up, off WP$ wh-possess. whose\nNN sing or mass noun llama SYM symbol +, %, & WRB wh-adverb how, where\n\n Below we show some examples with each word tagged according to both the UD\n (in blue) and Penn (in red) tagsets. Notice that the Penn tagset distinguishes tense\n and participles on verbs, and has a special tag for the existential there construction in\n English. Note that since London Journal of Medicine is a proper noun, both tagsets\n mark its component nouns as PROPN/NNP, including journal and medicine, which\n might otherwise be labeled as common nouns (NOUN/NN).\n (17.1) There/PRON/EX are/VERB/VBP 70/NUM/CD children/NOUN/NNS\n there/ADV/RB ./PUNC/.\n (17.2) Preliminary/ADJ/JJ findings/NOUN/NNS were/AUX/VBD\n reported/VERB/VBN in/ADP/IN today/NOUN/NN ‚Äôs/PART/POS\n London/PROPN/NNP Journal/PROPN/NNP of/ADP/IN Medicine/PROPN/NNP\n\n part-of-speech\n tagging Part-of-speech tagging is the process of assigning a part-of-speech to each word in\n a text. The input is a sequence x1 , x2 , ..., xn of (tokenized) words and a tagset, and\n the output is a sequence y1 , y2 , ..., yn of tags, each output yi corresponding exactly to\n one input xi , as shown in the intuition in Fig. 17.3.\n ambiguous Tagging is a disambiguation task; words are ambiguous ‚Äîhave more than one\n possible part-of-speech‚Äîand the goal is to find the correct tag for the situation.\n For example, book can be a verb (book that flight) or a noun (hand me that book).\n That can be a determiner (Does that flight serve dinner) or a complementizer (I\n 17.2 ‚Ä¢ PART- OF -S PEECH TAGGING 5\n\n y1 y2 y3 y4 y5\n\n NOUN AUX VERB DET NOUN\n\n Part of Speech Tagger\n\n Janet will back the bill\n x1 x2 x3 x4 x5\n\n output POS tags y1 , y2 , ..., yn .\n\nambiguity thought that your flight was earlier). The goal of POS-tagging is to resolve these\nresolution\n ambiguities, choosing the proper tag for the context.\n accuracy The accuracy of part-of-speech tagging algorithms (the percentage of test set\n tags that match human gold labels) is extremely high. One study found accuracies\n over 97% across 15 languages from the Universal Dependency (UD) treebank (Wu\n and Dredze, 2019). Accuracies on various English treebanks are also 97% (no matter\n the algorithm; HMMs, CRFs, BERT perform similarly). This 97% number is also\n about the human performance on this task, at least for English (Manning, 2011).\n\n Types: WSJ Brown\n Unambiguous (1 tag) 44,432 (86%) 45,799 (85%)\n Ambiguous (2+ tags) 7,025 (14%) 8,050 (15%)\n Tokens:\n Unambiguous (1 tag) 577,421 (45%) 384,349 (33%)\n Ambiguous (2+ tags) 711,780 (55%) 786,646 (67%)\n\n We‚Äôll introduce algorithms for the task in the next few sections, but first let‚Äôs\n explore the task. Exactly how hard is it? Fig. 17.4 shows that most word types\n (85-86%) are unambiguous (Janet is always NNP, hesitantly is always RB). But the\n ambiguous words, though accounting for only 14-15% of the vocabulary, are very\n common, and 55-67% of word tokens in running text are ambiguous. Particularly\n ambiguous common words include that, back, down, put and set; here are some\n examples of the 6 different parts of speech for the word back:\n earnings growth took a back/JJ seat\n a small building in the back/NN\n a clear majority of senators back/VBP the bill\n Dave began to back/VB toward the door\n enable the country to buy back/RP debt\n I was twenty-one back/RB then\n Nonetheless, many words are easy to disambiguate, because their different tags\n aren‚Äôt equally likely. For example, a can be a determiner or the letter a, but the\n determiner sense is much more likely.\n This idea suggests a useful baseline: given an ambiguous word, choose the tag\n which is most frequent in the training corpus. This is a key concept:\n Most Frequent Class Baseline: Always compare a classifier against a baseline at\n least as good as the most frequent class baseline (assigning each token to the class\n it occurred in most often in the training set).\n6 C HAPTER 17 ‚Ä¢ S EQUENCE L ABELING FOR PARTS OF S PEECH AND NAMED E NTITIES\n\n The most-frequent-tag baseline has an accuracy of about 92%1 . The baseline\n thus differs from the state-of-the-art and human ceiling (97%) by only 5%.\n\n Part of speech tagging can tell us that words like Janet, Stanford University, and\n Colorado are all proper nouns; being a proper noun is a grammatical property of\n these words. But viewed from a semantic perspective, these proper nouns refer to\n different kinds of entities: Janet is a person, Stanford University is an organization,\n and Colorado is a location.\n named entity Here we re-introduce the concept of a named entity, which was also introduced\n in Section ?? for readers who haven‚Äôt yet read Chapter 10.\n named entity A named entity is, roughly speaking, anything that can be referred to with a\n proper name: a person, a location, an organization. The task of named entity recognamed entity\n recognition nition (NER) is to find spans of text that constitute proper names and tag the type of\n NER the entity. Four entity tags are most common: PER (person), LOC (location), ORG\n (organization), or GPE (geo-political entity). However, the term named entity is\n commonly extended to include things that aren‚Äôt entities per se, including dates,\n times, and other kinds of temporal expressions, and even numerical expressions like\n prices. Here‚Äôs an example of the output of an NER tagger:\n Citing high fuel prices, [ORG United Airlines] said [TIME Friday] it\n has increased fares by [MONEY $6] per round trip on flights to some\n cities also served by lower-cost carriers. [ORG American Airlines], a\n unit of [ORG AMR Corp.], immediately matched the move, spokesman\n [PER Tim Wagner] said. [ORG United], a unit of [ORG UAL Corp.],\n said the increase took effect [TIME Thursday] and applies to most\n routes where it competes against discount carriers, such as [LOC Chicago]\n to [LOC Dallas] and [LOC Denver] to [LOC San Francisco].\n The text contains 13 mentions of named entities including 5 organizations, 4 locations, 2 times, 1 person, and 1 mention of money. Figure 17.5 shows typical generic\n named entity types. Many applications will also need to use specific entity types like\n proteins, genes, commercial products, or works of art.\n\nType Tag Sample Categories Example sentences\nPeople PER people, characters Turing is a giant of computer science.\nOrganization ORG companies, sports teams The IPCC warned about the cyclone.\nLocation LOC regions, mountains, seas Mt. Sanitas is in Sunshine Canyon.\nGeo-Political Entity GPE countries, states Palo Alto is raising the fees for parking.\n\n Named entity tagging is a useful first step in lots of natural language processing\n tasks. In sentiment analysis we might want to know a consumer‚Äôs sentiment toward a\n particular entity. Entities are a useful first stage in question answering, or for linking\n text to information in structured knowledge sources like Wikipedia. And named\n entity tagging is also central to tasks involving building semantic representations,\n like extracting events and the relationship between participants.\n 1 In English, on the WSJ corpus, tested on sections 22-24.\n 17.3 ‚Ä¢ NAMED E NTITIES AND NAMED E NTITY TAGGING 7\n\n Unlike part-of-speech tagging, where there is no segmentation problem since\n each word gets one tag, the task of named entity recognition is to find and label\n spans of text, and is difficult partly because of the ambiguity of segmentation; we\n need to decide what‚Äôs an entity and what isn‚Äôt, and where the boundaries are. Indeed,\n most words in a text will not be named entities. Another difficulty is caused by type\n ambiguity. The mention JFK can refer to a person, the airport in New York, or any\n number of schools, bridges, and streets around the United States. Some examples of\n this kind of cross-type confusion are given in Figure 17.6.\n\n [PER Washington] was born into slavery on the farm of James Burroughs.\n [ORG Washington] went up 2 games to 1 in the four-game series.\n Blair arrived in [LOC Washington] for what may well be his last state visit.\n In June, [GPE Washington] passed a primary seatbelt law.\n\n The standard approach to sequence labeling for a span-recognition problem like\n NER is BIO tagging (Ramshaw and Marcus, 1995). This is a method that allows us\n to treat NER like a word-by-word sequence labeling task, via tags that capture both\n the boundary and the named entity type. Consider the following sentence:\n [PER Jane Villanueva ] of [ORG United] , a unit of [ORG United Airlines\n Holding] , said the fare applies to the [LOC Chicago ] route.\nBIO Figure 17.7 shows the same excerpt represented with BIO tagging, as well as\n variants called IO tagging and BIOES tagging. In BIO tagging we label any token\n that begins a span of interest with the label B, tokens that occur inside a span are\n tagged with an I, and any tokens outside of any span of interest are labeled O. While\n there is only one O tag, we‚Äôll have distinct B and I tags for each named entity class.\n The number of tags is thus 2n + 1 tags, where n is the number of entity types. BIO\n tagging can represent exactly the same information as the bracketed notation, but has\n the advantage that we can represent the task in the same simple sequence modeling\n way as part-of-speech tagging: assigning a single label yi to each input word xi :\n\n Words IO Label BIO Label BIOES Label\n Jane I-PER B-PER B-PER\n Villanueva I-PER I-PER E-PER\n of O O O\n United I-ORG B-ORG B-ORG\n Airlines I-ORG I-ORG I-ORG\n Holding I-ORG I-ORG E-ORG\n discussed O O O\n the O O O\n Chicago I-LOC B-LOC S-LOC\n route O O O\n . O O O\n\n We‚Äôve also shown two variant tagging schemes: IO tagging, which loses some\n information by eliminating the B tag, and BIOES tagging, which adds an end tag\n E for the end of a span, and a span tag S for a span consisting of only one word.\n A sequence labeler (HMM, CRF, RNN, Transformer, etc.) is trained to label each\n token in a text with tags that indicate the presence (or absence) of particular kinds\n of named entities.\n8 C HAPTER 17 ‚Ä¢ S EQUENCE L ABELING FOR PARTS OF S PEECH AND NAMED E NTITIES\n\n In this section we introduce our first sequence labeling algorithm, the Hidden Markov\n Model, and show how to apply it to part-of-speech tagging. Recall that a sequence\n labeler is a model whose job is to assign a label to each unit in a sequence, thus\n mapping a sequence of observations to a sequence of labels of the same length.\n The HMM is a classic model that introduces many of the key concepts of sequence\n modeling that we will see again in more modern models.\n An HMM is a probabilistic sequence model: given a sequence of units (words,\n letters, morphemes, sentences, whatever), it computes a probability distribution over\n possible sequences of labels and chooses the best label sequence.\n\n 17.4.1 Markov Chains\n Markov chain The HMM is based on augmenting the Markov chain. A Markov chain is a model\n that tells us something about the probabilities of sequences of random variables,\n states, each of which can take on values from some set. These sets can be words, or\n tags, or symbols representing anything, for example the weather. A Markov chain\n makes a very strong assumption that if we want to predict the future in the sequence,\n all that matters is the current state. All the states before the current state have no impact on the future except via the current state. It‚Äôs as if to predict tomorrow‚Äôs weather\n you could examine today‚Äôs weather but you weren‚Äôt allowed to look at yesterday‚Äôs\n weather.\n\n .8\n are .2\n .1 COLD2 .1 .4 .5\n .1 .5\n .1\n .3 uniformly charming\n HOT1 WARM3 .5\n\n .6 .3 .6 .1 .2\n .6\n (a) (b)\n transitions. A start distribution œÄ is required; setting œÄ = [0.1, 0.7, 0.2] for (a) would mean a\n probability 0.7 of starting in state 2 (cold), probability 0.1 of starting in state 1 (hot), etc.\n\n More formally, consider a sequence of state variables q1 , q2 , ..., qi . A Markov\n Markov\n assumption model embodies the Markov assumption on the probabilities of this sequence: that\n when predicting the future, the past doesn‚Äôt matter, only the present.\n Markov Assumption: P(qi = a|q1 ...qi‚àí1 ) = P(qi = a|qi‚àí1 ) (17.3)\n\n weather events, for which the vocabulary consists of HOT, COLD, and WARM. The\n states are represented as nodes in the graph, and the transitions, with their probabilities, as edges. The transitions are probabilities: the values of arcs leaving a given\n state must sum to 1. Figure 17.8b shows a Markov chain for assigning a probability to a sequence of words w1 ...wt . This Markov chain should be familiar; in fact,\n it represents a bigram language model, with each edge expressing the probability\n p(wi |w j )! Given the two models in Fig. 17.8, we can assign a probability to any\n sequence from our vocabulary.\n 17.4 ‚Ä¢ HMM PART- OF -S PEECH TAGGING 9\n\n Formally, a Markov chain is specified by the following components:\n Q = q1 q2 . . . qN a set of N states\n A = a11 a12 . . . aN1 . . . aNN a transition probability matrix A, each ai j representing\n Pn the probability of moving from state i to state j, s.t.\n j=1 ai j = 1 ‚àÄi\n œÄ = œÄ1 , œÄ2 , ..., œÄN an initial probability distribution over states. œÄi is the\n probability that the Markov chain will start in state i.\n Some states j may haveP œÄ j = 0, meaning that they cannot\n be initial states. Also, ni=1 œÄi = 1\n Before you go on, use the sample probabilities in Fig. 17.8a (with œÄ = [0.1, 0.7, 0.2])\n to compute the probability of each of the following sequences:\n (17.4) hot hot hot hot\n (17.5) cold hot cold hot\n What does the difference in these probabilities tell you about a real-world weather\n fact encoded in Fig. 17.8a?\n\n 17.4.2 The Hidden Markov Model\n A Markov chain is useful when we need to compute a probability for a sequence\n of observable events. In many cases, however, the events we are interested in are\n hidden hidden: we don‚Äôt observe them directly. For example we don‚Äôt normally observe\n part-of-speech tags in a text. Rather, we see words, and must infer the tags from the\n word sequence. We call the tags hidden because they are not observed.\nhidden Markov A hidden Markov model (HMM) allows us to talk about both observed events\n model\n (like words that we see in the input) and hidden events (like part-of-speech tags) that\n we think of as causal factors in our probabilistic model. An HMM is specified by\n the following components:\n Q = q1 q2 . . . qN a set of N states\n A = a11 . . . ai j . . . aNN a transition probability matrix A, each ai j representing the probability\n of moving from state i to state j, s.t. Nj=1 ai j = 1 ‚àÄi\n P\n\n B = bi (ot ) a sequence of observation likelihoods, also called emission probabilities, each expressing the probability of an observation ot (drawn from a\n vocabulary V = v1 , v2 , ..., vV ) being generated from a state qi\n œÄ = œÄ1 , œÄ2 , ..., œÄN an initial probability distribution over states. œÄi is the probability that\n the Markov chain will start in state i. Some states P j may have œÄ j = 0,\n meaning that they cannot be initial states. Also, ni=1 œÄi = 1\n\n The HMM is given as input O = o1 o2 . . . oT : a sequence of T observations, each\n one drawn from the vocabulary V .\n A first-order hidden Markov model instantiates two simplifying assumptions.\n First, as with a first-order Markov chain, the probability of a particular state depends\n only on the previous state:\n\n Markov Assumption: P(qi |q1 , ..., qi‚àí1 ) = P(qi |qi‚àí1 ) (17.6)\n Second, the probability of an output observation oi depends only on the state that\n produced the observation qi and not on any other states or any other observations:\n\n Output Independence: P(oi |q1 , . . . qi , . . . , qT , o1 , . . . , oi , . . . , oT ) = P(oi |qi ) (17.7)\n10 C HAPTER 17 ‚Ä¢ S EQUENCE L ABELING FOR PARTS OF S PEECH AND NAMED E NTITIES\n\n 17.4.3 The components of an HMM tagger\n An HMM has two components, the A and B probabilities, both estimated by counting\n on a tagged training corpus. (For this example we‚Äôll use the tagged WSJ corpus.)\n The A matrix contains the tag transition probabilities P(ti |ti‚àí1 ) which represent\n the probability of a tag occurring given the previous tag. For example, modal verbs\n like will are very likely to be followed by a verb in the base form, a VB, like race, so\n we expect this probability to be high. We compute the maximum likelihood estimate\n of this transition probability by counting, out of the times we see the first tag in a\n labeled corpus, how often the first tag is followed by the second:\n\n C(ti‚àí1 ,ti )\n P(ti |ti‚àí1 ) = (17.8)\n C(ti‚àí1 )\n In the WSJ corpus, for example, MD occurs 13124 times of which it is followed\n by VB 10471, for an MLE estimate of\n\n C(MD,V B) 10471\n P(V B|MD) = = = .80 (17.9)\n C(MD) 13124\n The B emission probabilities, P(wi |ti ), represent the probability, given a tag (say\n MD), that it will be associated with a given word (say will). The MLE of the emission probability is\n C(ti , wi )\n P(wi |ti ) = (17.10)\n C(ti )\n Of the 13124 occurrences of MD in the WSJ corpus, it is associated with will 4046\n times:\n C(MD, will) 4046\n P(will|MD) = = = .31 (17.11)\n C(MD) 13124\n We saw this kind of Bayesian modeling in Appendix K; recall that this likelihood\n term is not asking ‚Äúwhich is the most likely tag for the word will?‚Äù That would be\n the posterior P(MD|will). Instead, P(will|MD) answers the slightly counterintuitive\n question ‚ÄúIf we were going to generate a MD, how likely is it that this modal would\n be will?‚Äù\n\n B2 a22\n P(\"aardvark\" | MD)\n ...\n P(‚Äúwill‚Äù | MD)\n ...\n P(\"the\" | MD)\n ...\n MD2 B3\n P(‚Äúback‚Äù | MD)\n ... a12 a32 P(\"aardvark\" | NN)\n P(\"zebra\" | MD) ...\n a11 a21 a33 P(‚Äúwill‚Äù | NN)\n a23 ...\n P(\"the\" | NN)\n B1 a13 ...\n P(‚Äúback‚Äù | NN)\n P(\"aardvark\" | VB)\n ...\n VB1 a31\n NN3 ...\n P(\"zebra\" | NN)\n P(‚Äúwill‚Äù | VB)\n ...\n P(\"the\" | VB)\n ...\n P(‚Äúback‚Äù | VB)\n ...\n P(\"zebra\" | VB)\n\n probabilities used to compute the prior probability, and the B observation likelihoods that are\n associated with each state, one likelihood for each possible observation word.\n 17.4 ‚Ä¢ HMM PART- OF -S PEECH TAGGING 11\n\n The A transition probabilities, and B observation likelihoods of the HMM are\n illustrated in Fig. 17.9 for three states in an HMM part-of-speech tagger; the full\n tagger would have one state for each tag.\n\n 17.4.4 HMM tagging as decoding\n For any model, such as an HMM, that contains hidden variables, the task of determining the hidden variables sequence corresponding to the sequence of observations\ndecoding is called decoding. More formally,\n\n Decoding: Given as input an HMM Œª = (A, B) and a sequence of observations O = o1 , o2 , ..., oT , find the most probable sequence of states\n Q = q1 q2 q3 . . . qT .\n\n For part-of-speech tagging, the goal of HMM decoding is to choose the tag\n sequence t1 . . .tn that is most probable given the observation sequence of n words\n w1 . . . wn :\n tÀÜ1:n = argmax P(t1 . . .tn |w1 . . . wn ) (17.12)\n t1 ... tn\n\n The way we‚Äôll do this in the HMM is to use Bayes‚Äô rule to instead compute:\n\n P(w1 . . . wn |t1 . . .tn )P(t1 . . .tn )\n tÀÜ1:n = argmax (17.13)\n t1 ... tn P(w1 . . . wn )\n\n Furthermore, we simplify Eq. 17.13 by dropping the denominator P(wn1 ):\n\n tÀÜ1:n = argmax P(w1 . . . wn |t1 . . .tn )P(t1 . . .tn ) (17.14)\n t1 ... tn\n\n HMM taggers make two further simplifying assumptions. The first (output independence, from Eq. 17.7) is that the probability of a word appearing depends only\n on its own tag and is independent of neighboring words and tags:\n n\n Y\n P(w1 . . . wn |t1 . . .tn ) ‚âà P(wi |ti ) (17.15)\n i=1\n\n The second assumption (the Markov assumption, Eq. 17.6) is that the probability of\n a tag is dependent only on the previous tag, rather than the entire tag sequence;\n n\n Y\n P(t1 . . .tn ) ‚âà P(ti |ti‚àí1 ) (17.16)\n i=1\n\n Plugging the simplifying assumptions from Eq. 17.15 and Eq. 17.16 into Eq. 17.14\n results in the following equation for the most probable tag sequence from a bigram\n tagger:\n\n emission transition\n n z }| { z }| {\n Y\n tÀÜ1:n = argmax P(t1 . . .tn |w1 . . . wn ) ‚âà argmax P(wi |ti ) P(ti |ti‚àí1 ) (17.17)\n t1 ... tn t1 ... tn\n i=1\n\n The two parts of Eq. 17.17 correspond neatly to the B emission probability and A\n transition probability that we just defined above!\n12 C HAPTER 17 ‚Ä¢ S EQUENCE L ABELING FOR PARTS OF S PEECH AND NAMED E NTITIES\n\n function V ITERBI(observations of len T,state-graph of len N) returns best-path, path-prob\n\n create a path probability matrix viterbi[N,T]\n for each state s from 1 to N do ; initialization step\n viterbi[s,1] ‚Üê œÄs ‚àó bs (o1 )\n backpointer[s,1] ‚Üê 0\n for each time step t from 2 to T do ; recursion step\n for each state s from 1 to N do\n N\n viterbi[s,t] ‚Üê max\n viterbi[s0 ,t ‚àí 1] ‚àó as0 ,s ‚àó bs (ot )\n s =1\n N\n backpointer[s,t] ‚Üê argmax viterbi[s0 ,t ‚àí 1] ‚àó as0 ,s ‚àó bs (ot )\n s0 =1\n N\n bestpathprob ‚Üê max viterbi[s, T ] ; termination step\n s=1\n N\n bestpathpointer ‚Üê argmax viterbi[s, T ] ; termination step\n s=1\n bestpath ‚Üê the path starting at state bestpathpointer, that follows backpointer[] to states back in time\n return bestpath, bestpathprob\n\nan HMM Œª = (A, B), the algorithm returns the state path through the HMM that assigns maximum likelihood\nto the observation sequence.\n\n 17.4.5 The Viterbi Algorithm\n Viterbi\n algorithm The decoding algorithm for HMMs is the Viterbi algorithm shown in Fig. 17.10.\n As an instance of dynamic programming, Viterbi resembles the dynamic programming minimum edit distance algorithm of Chapter 2.\n The Viterbi algorithm first sets up a probability matrix or lattice, with one column for each observation ot and one row for each state in the state graph. Each column thus has a cell for each state qi in the single combined automaton. Figure 17.11\n shows an intuition of this lattice for the sentence Janet will back the bill.\n Each cell of the lattice, vt ( j), represents the probability that the HMM is in state\n j after seeing the first t observations and passing through the most probable state\n sequence q1 , ..., qt‚àí1 , given the HMM Œª . The value of each cell vt ( j) is computed\n by recursively taking the most probable path that could lead us to this cell. Formally,\n each cell expresses the probability\n\n vt ( j) = max P(q1 ...qt‚àí1 , o1 , o2 . . . ot , qt = j|Œª ) (17.18)\n q1 ,...,qt‚àí1\n\n We represent the most probable path by taking the maximum over all possible\n previous state sequences max . Like other dynamic programming algorithms,\n q1 ,...,qt‚àí1\n Viterbi fills each cell recursively. Given that we had already computed the probability of being in every state at time t ‚àí 1, we compute the Viterbi probability by taking\n the most probable of the extensions of the paths that lead to the current cell. For a\n given state q j at time t, the value vt ( j) is computed as\n\n N\n vt ( j) = max vt‚àí1 (i) ai j b j (ot ) (17.19)\n i=1\n\n The three factors that are multiplied in Eq. 17.19 for extending the previous paths to\n compute the Viterbi probability at time t are\n 17.4 ‚Ä¢ HMM PART- OF -S PEECH TAGGING 13\n\n DT DT DT DT DT\n\n RB RB RB RB RB\n\n NN NN NN NN NN\n\n JJ JJ JJ JJ JJ\n\n VB VB VB VB VB\n\n MD MD MD MD MD\n\n NNP NNP NNP NNP NNP\n\n Janet will back the bill\n(qi ) for each word and highlighting the path corresponding to the correct tag sequence through\nthe hidden states. States (parts of speech) which have a zero probability of generating a\nparticular word according to the B matrix (such as the probability that a determiner DT will\nbe realized as Janet) are greyed out.\n\n vt‚àí1 (i) the previous Viterbi path probability from the previous time step\n ai j the transition probability from previous state qi to current state q j\n b j (ot ) the state observation likelihood of the observation symbol ot given\n the current state j\n\n17.4.6 Working through an example\nLet‚Äôs tag the sentence Janet will back the bill; the goal is the correct series of tags\n(see also Fig. 17.11):\n(17.20) Janet/NNP will/MD back/VB the/DT bill/NN\n\n NNP MD VB JJ NN RB DT\n<s > 0.2767 0.0006 0.0031 0.0453 0.0449 0.0510 0.2026\nNNP 0.3777 0.0110 0.0009 0.0084 0.0584 0.0090 0.0025\nMD 0.0008 0.0002 0.7968 0.0005 0.0008 0.1698 0.0041\nVB 0.0322 0.0005 0.0050 0.0837 0.0615 0.0514 0.2231\nJJ 0.0366 0.0004 0.0001 0.0733 0.4509 0.0036 0.0036\nNN 0.0096 0.0176 0.0014 0.0086 0.1216 0.0177 0.0068\nRB 0.0068 0.0102 0.1011 0.1012 0.0120 0.0728 0.0479\nDT 0.1147 0.0021 0.0002 0.2157 0.4744 0.0102 0.0017\n<s > is the start token.\n\n Let the HMM be defined by the two tables in Fig. 17.12 and Fig. 17.13. Figure 17.12 lists the ai j probabilities for transitioning between the hidden states (partof-speech tags). Figure 17.13 expresses the bi (ot ) probabilities, the observation\nlikelihoods of words given tags. This table is (slightly simplified) from counts in the\nWSJ corpus. So the word Janet only appears as an NNP, back has 4 possible parts\n14 C HAPTER 17 ‚Ä¢ S EQUENCE L ABELING FOR PARTS OF S PEECH AND NAMED E NTITIES\n\n Janet will back the bill\n NNP 0.000032 0 0 0.000048 0\n MD 0 0.308431 0 0 0\n VB 0 0.000028 0.000672 0 0.000028\n JJ 0 0 0.000340 0 0\n NN 0 0.000200 0.000223 0 0.002337\n RB 0 0 0.010446 0 0\n DT 0 0 0 0.506099 0\n\n of speech, and the word the can appear as a determiner or as an NNP (in titles like\n ‚ÄúSomewhere Over the Rainbow‚Äù all words are tagged as NNP).\n\n v1(7) v2(7)\n q7 DT\n\n v1(6) v2(6) v3(6)=\n q6 RB max * .0104\n )\n N\n N\n B|\n (R\n *P\n\n v1(5) v2(5)= v3(5)=\n q5 NN max * .0002 * P(NN|NN) max * .\n = .0000000001 000223\n\n v1(4)= . v2(4) v3(4)=\n )=\n q4 JJ tart 045*0=0\n J |s max * .00034\n P(J .045\n *P =\n\n v1(3)= v2(3)= v3(3)=\n (M 0\n\n art)\n D\n\n q3 VB B|st\n |J\n\n .0031 x 0 max * .000028\n P(V 0031 max * .00067\n )J\n\n =. =0 = 2.5e-13\n * P\n (MD\n = 0 |VB)\n v2(2) =\n tart) v1(2)=\n q2 MD D|s\n P(M 0006 .0006 x 0 = * P(MD|M max * .308 =\n = . D) 2.772e-8\n 0 =0\n 8 1 =)\n .9 9*.0 NP\n\n v1(1) =\n 00 D|N\n\n v2(1)\n .0 P(M\n\n q1 NNP tart) .28* .000032\n P(NNP|s\n e-\n\n = .000009\n\n = .28\n\n backtrace\n start start start start\n start\n œÄ backtrace\n\n Janet will\n t back the bill\n o1 o2 o3 o4 o5\n\nthe probability of the best path so far and a pointer to the previous cell along that path. We have only filled out\ncolumns 1 and 2; to avoid clutter most cells with value 0 are left empty. The rest is left as an exercise for the\nreader. After the cells are filled in, backtracing from the end state, we should be able to reconstruct the correct\nstate sequence NNP MD VB DT NN.\n\n the Viterbi lattice for computing the best hidden state sequence for the observation\n sequence Janet will back the bill.\n There are N = 5 state columns. We begin in column 1 (for the word Janet) by\n setting the Viterbi value in each cell to the product of the œÄ transition probability (the\n start probability for that state i, which we get from the <s> entry of Fig. 17.12), and\n 17.5 ‚Ä¢ C ONDITIONAL R ANDOM F IELDS (CRF S ) 15\n\n the observation likelihood of the word Janet given the tag for that cell. Most of the\n cells in the column are zero since the word Janet cannot be any of those tags. The\n reader should find this in Fig. 17.14.\n Next, each cell in the will column gets updated. For each state, we compute the\n value viterbi[s,t] by taking the maximum over the extensions of all the paths from\n the previous column that lead to the current cell according to Eq. 17.19. We have\n shown the values for the MD, VB, and NN cells. Each cell gets the max of the 7 values from the previous column, multiplied by the appropriate transition probability;\n as it happens in this case, most of them are zero from the previous column. The remaining value is multiplied by the relevant observation probability, and the (trivial)\n max is taken. In this case the final value, 2.772e-8, comes from the NNP state at the\n previous column. The reader should fill in the rest of the lattice in Fig. 17.14 and\n backtrace to see whether or not the Viterbi algorithm returns the gold state sequence\n NNP MD VB DT NN.\n\n While the HMM is a useful and powerful model, it turns out that HMMs need a\n number of augmentations to achieve high accuracy. For example, in POS tagging\n unknown as in other tasks, we often run into unknown words: proper names and acronyms\n words\n are created very often, and even new common nouns and verbs enter the language\n at a surprising rate. It would be great to have ways to add arbitrary features to\n help with this, perhaps based on capitalization or morphology (words starting with\n capital letters are likely to be proper nouns, words ending with -ed tend to be past\n tense (VBD or VBN), etc.) Or knowing the previous or following words might be a\n useful feature (if the previous word is the, the current tag is unlikely to be a verb).\n Although we could try to hack the HMM to find ways to incorporate some of\n these, in general it‚Äôs hard for generative models like HMMs to add arbitrary features\n directly into the model in a clean way. We‚Äôve already seen a model for combining\n arbitrary features in a principled way: log-linear models like the logistic regression\n model of Chapter 4! But logistic regression isn‚Äôt a sequence model; it assigns a class\n to a single observation.\n Luckily, there is a discriminative sequence model based on log-linear models:\n CRF the conditional random field (CRF). We‚Äôll describe here the linear chain CRF,\n the version of the CRF most commonly used for language processing, and the one\n whose conditioning closely matches the HMM.\n Assuming we have a sequence of input words X = x1 ...xn and want to compute\n a sequence of output tags Y = y1 ...yn . In an HMM to compute the best tag sequence\n that maximizes P(Y |X) we rely on Bayes‚Äô rule and the likelihood P(X|Y ):\n\n YÃÇ = argmax p(Y |X)\n Y\n = argmax p(X|Y )p(Y )\n Y\n Y Y\n = argmax p(xi |yi ) p(yi |yi‚àí1 ) (17.21)\n Y i i\n\n In a CRF, by contrast, we compute the posterior p(Y |X) directly, training the CRF\n16 C HAPTER 17 ‚Ä¢ S EQUENCE L ABELING FOR PARTS OF S PEECH AND NAMED E NTITIES\n\n to discriminate among the possible tag sequences:\n\n YÃÇ = argmax P(Y |X) (17.22)\n Y ‚ààY\n\n However, the CRF does not compute a probability for each tag at each time step. Instead, at each time step the CRF computes log-linear functions over a set of relevant\n features, and these local features are aggregated and normalized to produce a global\n probability for the whole sequence.\n Let‚Äôs introduce the CRF more formally, again using X and Y as the input and\n output sequences. A CRF is a log-linear model that assigns a probability to an entire\n output (tag) sequence Y , out of all possible sequences Y , given the entire input\n (word) sequence X. We can think of a CRF as like a giant sequential version of\n the multinomial logistic regression algorithm we saw for text categorization. Recall\n that we introduced the feature function f in regular multinomial logistic regression\n for text categorization as a function of a tuple: the input text x and a single class y\n (page ??). In a CRF, we‚Äôre dealing with a sequence, so the function F maps an entire\n input sequence X and an entire output sequence Y to a feature vector. Let‚Äôs assume\n we have K features, with a weight wk for each feature Fk :\n K\n !\n X\n exp wk Fk (X,Y )\n k=1\n p(Y |X) = K\n ! (17.23)\n X X\n exp wk Fk (X,Y )\n Y 0 ‚ààY k=1\n\n It‚Äôs common to also describe the same equation by pulling out the denominator into\n a function Z(X):\n K\n !\n 1 X\n p(Y |X) = exp wk Fk (X,Y ) (17.24)\n Z(X)\n k=1\n K\n !\n X X\n Z(X) = exp wk Fk (X,Y ) (17.25)\n Y 0 ‚ààY k=1\n\n We‚Äôll call these K functions Fk (X,Y ) global features, since each one is a property\n of the entire input sequence X and output sequence Y . We compute them by decomposing into a sum of local features for each position i in Y :\n n\n X\n Fk (X,Y ) = fk (yi‚àí1 , yi , X, i) (17.26)\n i=1\n\n Each of these local features fk in a linear-chain CRF is allowed to make use of the\n current output token yi , the previous output token yi‚àí1 , the entire input string X (or\n any subpart of it), and the current position i. This constraint to only depend on\n the current and previous output tokens yi and yi‚àí1 are what characterizes a linear\n linear chain chain CRF. As we will see, this limitation makes it possible to use versions of the\n CRF\n efficient Viterbi and Forward-Backwards algorithms from the HMM. A general CRF,\n by contrast, allows a feature to make use of any output token, and are thus necessary\n for tasks in which the decision depend on distant output tokens, like yi‚àí4 . General\n CRFs require more complex inference, and are less commonly used for language\n processing.\n 17.5 ‚Ä¢ C ONDITIONAL R ANDOM F IELDS (CRF S ) 17\n\n 17.5.1 Features in a CRF POS Tagger\n Let‚Äôs look at some of these features in detail, since the reason to use a discriminative\n sequence model is that it‚Äôs easier to incorporate a lot of features.2\n Again, in a linear-chain CRF, each local feature fk at position i can depend on\n any information from: (yi‚àí1 , yi , X, i). So some legal features representing common\n situations might be the following:\n\n 1{xi = the, yi = DET}\n 1{yi = PROPN, xi+1 = Street, yi‚àí1 = NUM}\n 1{yi = VERB, yi‚àí1 = AUX}\n For simplicity, we‚Äôll assume all CRF features take on the value 1 or 0. Above, we\n explicitly use the notation 1{x} to mean ‚Äú1 if x is true, and 0 otherwise‚Äù. From now\n on, we‚Äôll leave off the 1 when we define features, but you can assume each feature\n has it there implicitly.\n Although the idea of what features to use is done by the system designer by hand,\n feature\n templates the specific features are automatically populated by using feature templates as we\n briefly mentioned in Chapter 4. Here are some templates that only use information\n from (yi‚àí1 , yi , X, i):\n\n hyi , xi i, hyi , yi‚àí1 i, hyi , xi‚àí1 , xi+2 i\n\n These templates automatically populate the set of features from every instance in\n the training and test set. Thus for our example Janet/NNP will/MD back/VB the/DT\n bill/NN, when xi is the word back, the following features would be generated and\n have the value 1 (we‚Äôve assigned them arbitrary feature numbers):\n f3743 : yi = VB and xi = back\n f156 : yi = VB and yi‚àí1 = MD\n f99732 : yi = VB and xi‚àí1 = will and xi+2 = bill\n It‚Äôs also important to have features that help with unknown words. One of the\nword shape most important is word shape features, which represent the abstract letter pattern\n of the word by mapping lower-case letters to ‚Äòx‚Äô, upper-case to ‚ÄòX‚Äô, numbers to\n ‚Äôd‚Äô, and retaining punctuation. Thus for example I.M.F. would map to X.X.X. and\n DC10-30 would map to XXdd-dd. A second class of shorter word shape features is\n also used. In these features consecutive character types are removed, so words in all\n caps map to X, words with initial-caps map to Xx, DC10-30 would be mapped to\n Xd-d but I.M.F would still map to X.X.X. Prefix and suffix features are also useful.\n In summary, here are some sample feature templates that help with unknown words:\n\n xi contains a particular prefix (perhaps from all prefixes of length ‚â§ 2)\n xi contains a particular suffix (perhaps from all suffixes of length ‚â§ 2)\n xi ‚Äôs word shape\n xi ‚Äôs short word shape\n\n For example the word well-dressed might generate the following non-zero valued feature values:\n 2 Because in HMMs all computation is based on the two probabilities P(tag|tag) and P(word|tag), if\n we want to include some source of knowledge into the tagging process, we must find a way to encode\n the knowledge into one of these two probabilities. Each time we add a feature we have to do a lot of\n complicated conditioning which gets harder and harder as we have more and more such features.\n18 C HAPTER 17 ‚Ä¢ S EQUENCE L ABELING FOR PARTS OF S PEECH AND NAMED E NTITIES\n\n prefix(xi ) = w\n prefix(xi ) = we\n suffix(xi ) = ed\n suffix(xi ) = d\n word-shape(xi ) = xxxx-xxxxxxx\n short-word-shape(xi ) = x-x\n\n The known-word templates are computed for every word seen in the training\n set; the unknown word features can also be computed for all words in training, or\n only on training words whose frequency is below some threshold. The result of the\n known-word templates and word-signature features is a very large set of features.\n Generally a feature cutoff is used in which features are thrown out if they have count\n < 5 in the training set.\n Remember that in a CRF we don‚Äôt learn weights for each of these local features\n fk . Instead, we first sum the values of each local feature (for example feature f3743 )\n over the entire sentence, to create each global feature (for example F3743 ). It is those\n global features that will then be multiplied by weight w3743 . Thus for training and\n inference there is always a fixed set of K features with K weights, even though the\n length of each sentence is different.\n\n 17.5.2 Features for CRF Named Entity Recognizers\n A CRF for NER makes use of very similar features to a POS tagger, as shown in\n\n identity of wi , identity of neighboring words\n embeddings for wi , embeddings for neighboring words\n part of speech of wi , part of speech of neighboring words\n presence of wi in a gazetteer\n wi contains a particular prefix (from all prefixes of length ‚â§ 4)\n wi contains a particular suffix (from all suffixes of length ‚â§ 4)\n word shape of wi , word shape of neighboring words\n short word shape of wi , short word shape of neighboring words\n gazetteer features\n\n gazetteer One feature that is especially useful for locations is a gazetteer, a list of place\n names, often providing millions of entries for locations with detailed geographical\n and political information.3 This can be implemented as a binary feature indicating a\n phrase appears in the list. Other related resources like name-lists, for example from\n the United States Census Bureau4 , can be used, as can other entity dictionaries like\n lists of corporations or products, although they may not be as helpful as a gazetteer\n (Mikheev et al., 1999).\n The sample named entity token L‚ÄôOccitane would generate the following nonzero valued feature values (assuming that L‚ÄôOccitane is neither in the gazetteer nor\n the census).\n\n 3 www.geonames.org\n 4 www.census.gov\n 17.5 ‚Ä¢ C ONDITIONAL R ANDOM F IELDS (CRF S ) 19\n\n prefix(xi ) = L suffix(xi ) = tane\n prefix(xi ) = L‚Äô suffix(xi ) = ane\n prefix(xi ) = L‚ÄôO suffix(xi ) = ne\n prefix(xi ) = L‚ÄôOc suffix(xi ) = e\n word-shape(xi ) = X‚ÄôXxxxxxxx short-word-shape(xi ) = X‚ÄôXx\ninformation to our earlier example.\n\nWords POS Short shape Gazetteer BIO Label\nJane NNP Xx 0 B-PER\nVillanueva NNP Xx 1 I-PER\nof IN x 0 O\nUnited NNP Xx 0 B-ORG\nAirlines NNP Xx 0 I-ORG\nHolding NNP Xx 0 I-ORG\ndiscussed VBD x 0 O\nthe DT x 0 O\nChicago NNP Xx 1 B-LOC\nroute NN x 0 O\n. . . 0 O\n1, so the first POS feature, for example, would be represented as 1{POS = NNP}.\n\n17.5.3 Inference and Training for CRFs\nHow do we find the best tag sequence YÃÇ for a given input X? We start with Eq. 17.22:\n YÃÇ = argmax P(Y |X)\n Y ‚ààY\n K\n !\n 1 X\n = argmax exp wk Fk (X,Y ) (17.27)\n Y ‚ààY Z(X) k=1\n K n\n !\n X X\n = argmax exp wk fk (yi‚àí1 , yi , X, i) (17.28)\n Y ‚ààY k=1 i=1\n K\n X Xn\n = argmax wk fk (yi‚àí1 , yi , X, i) (17.29)\n Y ‚ààY k=1 i=1\n Xn X K\n = argmax wk fk (yi‚àí1 , yi , X, i) (17.30)\n Y ‚ààY i=1 k=1\n\nWe can ignore the exp function and the denominator Z(X), as we do above, because\nexp doesn‚Äôt change the argmax, and the denominator Z(X) is constant for a given\nobservation sequence X.\n How should we decode to find this optimal tag sequence yÃÇ? Just as with HMMs,\nwe‚Äôll turn to the Viterbi algorithm, which works because, like the HMM, the linearchain CRF depends at each timestep on only one previous output token yi‚àí1 .\n Concretely, this involves filling an N √óT array with the appropriate values, maintaining backpointers as we proceed. As with HMM Viterbi, when the table is filled,\nwe simply follow pointers back from the maximum value in the final column to\nretrieve the desired set of labels.\n20 C HAPTER 17 ‚Ä¢ S EQUENCE L ABELING FOR PARTS OF S PEECH AND NAMED E NTITIES\n\n The requisite changes from HMM Viterbi have to do only with how we fill each\n cell. Recall from Eq. 17.19 that the recursive step of the Viterbi equation computes\n the Viterbi value of time t for state j as\n N\n vt ( j) = max vt‚àí1 (i) ai j b j (ot ); 1 ‚â§ j ‚â§ N, 1 < t ‚â§ T (17.31)\n i=1\n\n which is the HMM implementation of\n N\n vt ( j) = max vt‚àí1 (i) P(s j |si ) P(ot |s j ) 1 ‚â§ j ‚â§ N, 1 < t ‚â§ T (17.32)\n i=1\n\n The CRF requires only a slight change to this latter formula, replacing the a and b\n prior and likelihood probabilities with the CRF features:\n \" K\n #\n N X\n vt ( j) = max vt‚àí1 (i) + wk fk (yt‚àí1 , yt , X,t) 1 ‚â§ j ‚â§ N, 1 < t ‚â§ T (17.33)\n i=1\n k=1\n\n Learning in CRFs relies on the same supervised learning algorithms we presented\n for logistic regression. Given a sequence of observations, feature functions, and corresponding outputs, we use stochastic gradient descent to train the weights to maximize the log-likelihood of the training corpus. The local nature of linear-chain CRFs\n means that the forward-backward algorithm introduced for HMMs in Appendix A\n can be extended to a CRF version that will efficiently compute the necessary derivatives. As with logistic regression, L1 or L2 regularization is important.\n\n Part-of-speech taggers are evaluated by the standard metric of accuracy. Named\n entity recognizers are evaluated by recall, precision, and F1 measure. Recall that\n recall is the ratio of the number of correctly labeled responses to the total that should\n have been labeled; precision is the ratio of the number of correctly labeled responses\n to the total labeled; and F-measure is the harmonic mean of the two.\n To know if the difference between the F1 scores of two NER systems is a significant difference, we use the paired bootstrap test, or the similar randomization test\n (Section ??).\n For named entity tagging, the entity rather than the word is the unit of response.\n Thus in the example in Fig. 17.16, the two entities Jane Villanueva and United Airlines Holding and the non-entity discussed would each count as a single response.\n The fact that named entity tagging has a segmentation component which is not\n present in tasks like text categorization or part-of-speech tagging causes some problems with evaluation. For example, a system that labeled Jane but not Jane Villanueva as a person would cause two errors, a false positive for O and a false negative for I-PER. In addition, using entities as the unit of response but words as the unit\n of training means that there is a mismatch between the training and test conditions.\n\n In this section we summarize a few remaining details of the data and models for\n part-of-speech tagging and NER, beginning with data. Since the algorithms we have\n 17.7 ‚Ä¢ F URTHER D ETAILS 21\n\npresented are supervised, having labeled data is essential for training and testing. A\nwide variety of datasets exist for part-of-speech tagging and/or NER. The Universal\nDependencies (UD) dataset (de Marneffe et al., 2021) has POS tagged corpora in\nover a hundred languages, as do the Penn Treebanks in English, Chinese, and Arabic.\nOntoNotes has corpora labeled for named entities in English, Chinese, and Arabic\n(Hovy et al., 2006). Named entity tagged corpora are also available in particular\ndomains, such as for biomedical (Bada et al., 2012) and literary text (Bamman et al.,\n2019).\n\n17.7.1 Rule-based Methods\nWhile machine learned (neural or CRF) sequence models are the norm in academic\nresearch, commercial approaches to NER are often based on pragmatic combinations of lists and rules, with some smaller amount of supervised machine learning\n(Chiticariu et al., 2013). For example in the IBM System T architecture, a user\nspecifies declarative constraints for tagging tasks in a formal query language that\nincludes regular expressions, dictionaries, semantic constraints, and other operators,\nwhich the system compiles into an efficient extractor (Chiticariu et al., 2018).\n One common approach is to make repeated rule-based passes over a text, starting\nwith rules with very high precision but low recall, and, in subsequent stages, using\nmachine learning methods that take the output of the first pass into account (an\napproach first worked out for coreference (Lee et al., 2017)):\n 1. First, use high-precision rules to tag unambiguous entity mentions.\n 2. Then, search for substring matches of the previously detected names.\n 3. Use application-specific name lists to find likely domain-specific mentions.\n 4. Finally, apply supervised sequence labeling techniques that use tags from previous stages as additional features.\n Rule-based methods were also the earliest methods for part-of-speech tagging.\nRule-based taggers like the English Constraint Grammar system (Karlsson et al.\n1995, Voutilainen 1999) use a two-stage formalism invented in the 1950s and 1960s:\n(1) a morphological analyzer with tens of thousands of word stem entries returns all\nparts of speech for a word, then (2) a large set of thousands of constraints are applied\nto the input sentence to rule out parts of speech inconsistent with the context.\n\n17.7.2 POS Tagging for Morphologically Rich Languages\nAugmentations to tagging algorithms become necessary when dealing with languages with rich morphology like Czech, Hungarian and Turkish.\n These productive word-formation processes result in a large vocabulary for these\nlanguages: a 250,000 word token corpus of Hungarian has more than twice as many\nword types as a similarly sized corpus of English (Oravecz and Dienes, 2002), while\na 10 million word token corpus of Turkish contains four times as many word types\nas a similarly sized English corpus (Hakkani-TuÃàr et al., 2002). Large vocabularies mean many unknown words, and these unknown words cause significant performance degradations in a wide variety of languages (including Czech, Slovene,\nEstonian, and Romanian) (HajicÃå, 2000).\n Highly inflectional languages also have much more information than English\ncoded in word morphology, like case (nominative, accusative, genitive) or gender\n(masculine, feminine). Because this information is important for tasks like parsing and coreference resolution, part-of-speech taggers for morphologically rich lan-\n22 C HAPTER 17 ‚Ä¢ S EQUENCE L ABELING FOR PARTS OF S PEECH AND NAMED E NTITIES\n\n guages need to label words with case and gender information. Tagsets for morphologically rich languages are therefore sequences of morphological tags rather than a\n single primitive tag. Here‚Äôs a Turkish example, in which the word izin has three possible morphological/part-of-speech tags and meanings (Hakkani-TuÃàr et al., 2002):\n 1. Yerdeki izin temizlenmesi gerek. iz + Noun+A3sg+Pnon+Gen\n The trace on the floor should be cleaned.\n\n 2. UÃàzerinde parmak izin kalmisÃß. iz + Noun+A3sg+P2sg+Nom\n Your finger print is left on (it).\n\n 3. IcÃßeri girmek icÃßin izin alman gerekiyor. izin + Noun+A3sg+Pnon+Nom\n You need permission to enter.\n\n Using a morphological parse sequence like Noun+A3sg+Pnon+Gen as the partof-speech tag greatly increases the number of parts of speech, and so tagsets can\n be 4 to 10 times larger than the 50‚Äì100 tags we have seen for English. With such\n large tagsets, each word needs to be morphologically analyzed to generate the list\n of possible morphological tag sequences (part-of-speech tags) for the word. The\n role of the tagger is then to disambiguate among these tags. This method also helps\n with unknown words since morphological parsers can accept unknown stems and\n still segment the affixes properly.\n\n This chapter introduced parts of speech and named entities, and the tasks of partof-speech tagging and named entity recognition:\n\n ‚Ä¢ Languages generally have a small set of closed class words that are highly\n frequent, ambiguous, and act as function words, and open-class words like\n nouns, verbs, adjectives. Various part-of-speech tagsets exist, of between 40\n and 200 tags.\n ‚Ä¢ Part-of-speech tagging is the process of assigning a part-of-speech label to\n each of a sequence of words.\n ‚Ä¢ Named entities are words for proper nouns referring mainly to people, places,\n and organizations, but extended to many other types that aren‚Äôt strictly entities\n or even proper nouns.\n ‚Ä¢ Two common approaches to sequence modeling are a generative approach,\n HMM tagging, and a discriminative approach, CRF tagging. We will see a\n neural approach in following chapters.\n ‚Ä¢ The probabilities in HMM taggers are estimated by maximum likelihood estimation on tag-labeled training corpora. The Viterbi algorithm is used for\n decoding, finding the most likely tag sequence\n ‚Ä¢ Conditional Random Fields or CRF taggers train a log-linear model that can\n choose the best tag sequence given an observation sequence, based on features\n that condition on the output tag, the prior output tag, the entire input sequence,\n and the current timestep. They use the Viterbi algorithm for inference, to\n choose the best sequence of tags, and a version of the Forward-Backward\n algorithm (see Appendix A) for training,\n H ISTORICAL N OTES 23\n\nHistorical Notes\n What is probably the earliest part-of-speech tagger was part of the parser in Zellig\n Harris‚Äôs Transformations and Discourse Analysis Project (TDAP), implemented between June 1958 and July 1959 at the University of Pennsylvania (Harris, 1962),\n although earlier systems had used part-of-speech dictionaries. TDAP used 14 handwritten rules for part-of-speech disambiguation; the use of part-of-speech tag sequences and the relative frequency of tags for a word prefigures modern algorithms.\n The parser was implemented essentially as a cascade of finite-state transducers; see\n Joshi and Hopely (1999) and Karttunen (1999) for a reimplementation.\n The Computational Grammar Coder (CGC) of Klein and Simmons (1963) had\n three components: a lexicon, a morphological analyzer, and a context disambiguator. The small 1500-word lexicon listed only function words and other irregular\n words. The morphological analyzer used inflectional and derivational suffixes to assign part-of-speech classes. These were run over words to produce candidate parts\n of speech which were then disambiguated by a set of 500 context rules by relying on\n surrounding islands of unambiguous words. For example, one rule said that between\n an ARTICLE and a VERB, the only allowable sequences were ADJ-NOUN, NOUN-\nADVERB, or NOUN-NOUN. The TAGGIT tagger (Greene and Rubin, 1971) used\n the same architecture as Klein and Simmons (1963), with a bigger dictionary and\n more tags (87). TAGGIT was applied to the Brown corpus and, according to Francis\n and KucÃåera (1982, p. 9), accurately tagged 77% of the corpus; the remainder of the\n Brown corpus was then tagged by hand. All these early algorithms were based on\n a two-stage architecture in which a dictionary was first used to assign each word a\n set of potential parts of speech, and then lists of handwritten disambiguation rules\n winnowed the set down to a single part of speech per word.\n Probabilities were used in tagging by Stolz et al. (1965) and a complete probabilistic tagger with Viterbi decoding was sketched by Bahl and Mercer (1976). The\n Lancaster-Oslo/Bergen (LOB) corpus, a British English equivalent of the Brown corpus, was tagged in the early 1980‚Äôs with the CLAWS tagger (Marshall 1983; Marshall 1987; Garside 1987), a probabilistic algorithm that approximated a simplified\n HMM tagger. The algorithm used tag bigram probabilities, but instead of storing the\n word likelihood of each tag, the algorithm marked tags either as rare (P(tag|word) <\n .01) infrequent (P(tag|word) < .10) or normally frequent (P(tag|word) > .10).\n DeRose (1988) developed a quasi-HMM algorithm, including the use of dynamic programming, although computing P(t|w)P(w) instead of P(w|t)P(w). The\n same year, the probabilistic PARTS tagger of Church 1988, 1989 was probably the\n first implemented HMM tagger, described correctly in Church (1989), although\n Church (1988) also described the computation incorrectly as P(t|w)P(w) instead\n of P(w|t)P(w). Church (p.c.) explained that he had simplified for pedagogical purposes because using the probability P(t|w) made the idea seem more understandable\n as ‚Äústoring a lexicon in an almost standard form‚Äù.\n Later taggers explicitly introduced the use of the hidden Markov model (Kupiec\n 1992; Weischedel et al. 1993; SchuÃàtze and Singer 1994). Merialdo (1994) showed\n that fully unsupervised EM didn‚Äôt work well for the tagging task and that reliance\n on hand-labeled data was important. Charniak et al. (1993) showed the importance\n of the most frequent tag baseline; the 92.3% number we give above was from Abney\n et al. (1999). See Brants (2000) for HMM tagger implementation details, including the extension to trigram contexts, and the use of sophisticated unknown word\n features; its performance is still close to state of the art taggers.\n24 C HAPTER 17 ‚Ä¢ S EQUENCE L ABELING FOR PARTS OF S PEECH AND NAMED E NTITIES\n\n Log-linear models for POS tagging were introduced by Ratnaparkhi (1996),\n who introduced a system called MXPOST which implemented a maximum entropy\n Markov model (MEMM), a slightly simpler version of a CRF. Around the same\n time, sequence labelers were applied to the task of named entity tagging, first with\n HMMs (Bikel et al., 1997) and MEMMs (McCallum et al., 2000), and then once\n CRFs were developed (Lafferty et al. 2001), they were also applied to NER (Mc-\nCallum and Li, 2003). A wide exploration of features followed (Zhou et al., 2005).\n Neural approaches to NER mainly follow from the pioneering results of Collobert\n et al. (2011), who applied a CRF on top of a convolutional net. BiLSTMs with word\n and character-based embeddings as input followed shortly and became a standard\n neural algorithm for NER (Huang et al. 2015, Ma and Hovy 2016, Lample et al.\n 2016) followed by the more recent use of Transformers and BERT.\n The idea of using letter suffixes for unknown words is quite old; the early Klein\n and Simmons (1963) system checked all final letter suffixes of lengths 1-5. The\n unknown word features described on page 17 come mainly from Ratnaparkhi (1996),\n with augmentations from Toutanova et al. (2003) and Manning (2011).\n State of the art POS taggers use neural algorithms, either bidirectional RNNs or\n Transformers like BERT; see Chapter 13 to Chapter 10. HMM (Brants 2000; Thede\n and Harper 1999) and CRF tagger accuracies are likely just a tad lower.\n Manning (2011) investigates the remaining 2.7% of errors in a high-performing\n tagger (Toutanova et al., 2003). He suggests that a third or half of these remaining\n errors are due to errors or inconsistencies in the training data, a third might be solvable with richer linguistic models, and for the remainder the task is underspecified\n or unclear.\n Supervised tagging relies heavily on in-domain training data hand-labeled by\n experts. Ways to relax this assumption include unsupervised algorithms for clustering words into part-of-speech-like classes, summarized in Christodoulopoulos et al.\n (2010), and ways to combine labeled and unlabeled data, for example by co-training\n (Clark et al. 2003; S√∏gaard 2010).\n See Householder (1995) for historical notes on parts of speech, and Sampson\n (1987) and Garside et al. (1997) on the provenance of the Brown and other tagsets.\n\nExercises\n the Penn Treebank tagset:\n 1. I/PRP need/VBP a/DT flight/NN from/IN Atlanta/NN\n 2. Does/VBZ this/DT flight/NN serve/VB dinner/NNS\n 3. I/PRP have/VB a/DT friend/NN living/VBG in/IN Denver/NNP\n 4. Can/VBP you/PRP list/VB the/DT nonstop/JJ afternoon/NN flights/NNS\n from Damon Runyon‚Äôs short stories. You may ignore punctuation. Some of\n these are quite difficult; do your best.\n 1. It is a nice night.\n 2. This crap game is over a garage in Fifty-second Street. . .\n 3. . . . Nobody ever takes the newspapers she sells . . .\n 4. He is a tall, skinny guy with a long, sad, mean-looking kisser, and a\n mournful voice.\n E XERCISES 25\n\n 5. . . . I am sitting in Mindy‚Äôs restaurant putting on the gefillte fish, which is\n a dish I am very fond of, . . .\n 6. When a guy and a doll get to taking peeks back and forth at each other,\n why there you are indeed.\n answers. On which words did you disagree the most? Why?\n and use it to compute for each word the tag that maximizes p(t|w). You will\n need to implement a simple tokenizer to deal with sentence boundaries. Start\n by assuming that all unknown words are NN and compute your error rate on\n known and unknown words. Now write at least five rules to do a better job of\n tagging unknown words, and show the difference in error rates.\n First split the corpus into a training set and test set. From the labeled training\n set, train the transition and observation probabilities of the HMM tagger directly on the hand-tagged data. Then implement the Viterbi algorithm so you\n can decode a test sentence. Now run your algorithm on the test set. Report its\n error rate and compare its performance to the most frequent tag baseline.\n the most frequent errors. Propose some features for improving the performance of your tagger on these errors.\n described on page 17.\n possible one. For example, the B tag can be reserved only for those situations\n where an ambiguity exists between adjacent entities. Propose a new set of\n BIO tags for use with your NER system. Experiment with it and compare its\n performance with the schemes presented in this chapter.\n from the kinds of named entities we‚Äôve discussed in this chapter. Collect a\n list of names of works of art from a particular category from a Web-based\n source (e.g., gutenberg.org, amazon.com, imdb.com, etc.). Analyze your list\n and give examples of ways that the names in it are likely to be problematic for\n the techniques described in this chapter.\n in the last exercise. Evaluate your system on a collection of text likely to\n contain instances of these named entities.\n26 Chapter 17 ‚Ä¢ Sequence Labeling for Parts of Speech and Named Entities\n\nAbney, S. P., R. E. Schapire, and Y. Singer. 1999. Boosting Greene, B. B. and G. M. Rubin. 1971. Automatic grammatiapplied to tagging and PP attachment. EMNLP/VLC. cal tagging of English. Department of Linguistics, Brown\nBada, M., M. Eckert, D. Evans, K. Garcia, K. Shipley, D. Sit- University, Providence, Rhode Island.\n nikov, W. A. Baumgartner, K. B. Cohen, K. Verspoor, HajicÃå, J. 2000. Morphological tagging: Data vs. dictionaries.\n J. A. Blake, and L. E. Hunter. 2012. Concept annotation NAACL.\n in the craft corpus. BMC bioinformatics, 13(1):161. Hakkani-TuÃàr, D., K. Oflazer, and G. TuÃàr. 2002. Sta-\nBahl, L. R. and R. L. Mercer. 1976. Part of speech as- tistical morphological disambiguation for agglutinative\n signment by a statistical decision algorithm. Proceedings languages. Journal of Computers and Humanities,\n IEEE International Symposium on Information Theory. 36(4):381‚Äì410.\nBamman, D., S. Popat, and S. Shen. 2019. An annotated Harris, Z. S. 1962. String Analysis of Sentence Structure.\n dataset of literary entities. NAACL HLT. Mouton, The Hague.\nBikel, D. M., S. Miller, R. Schwartz, and R. Weischedel. Householder, F. W. 1995. Dionysius Thrax, the technai, and\n 1997. Nymble: A high-performance learning name- Sextus Empiricus. In E. F. K. Koerner and R. E. Asher,\n finder. ANLP. eds, Concise History of the Language Sciences, 99‚Äì103.\n Elsevier Science.\nBrants, T. 2000. TnT: A statistical part-of-speech tagger.\n ANLP. Hovy, E. H., M. P. Marcus, M. Palmer, L. A. Ramshaw,\n and R. Weischedel. 2006. OntoNotes: The 90% solution.\nBroschart, J. 1997. Why Tongan does it differently. Linguis-\nHLT-NAACL.\n tic Typology, 1:123‚Äì165.\n Huang, Z., W. Xu, and K. Yu. 2015. Bidirectional LSTM-\nCharniak, E., C. Hendrickson, N. Jacobson, and CRF models for sequence tagging. arXiv preprint\n M. Perkowitz. 1993. Equations for part-of-speech tag- arXiv:1508.01991.\n ging. AAAI.\n Joshi, A. K. and P. Hopely. 1999. A parser from antiquity.\nChiticariu, L., M. Danilevsky, Y. Li, F. Reiss, and H. Zhu. In A. Kornai, ed., Extended Finite State Models of Lan-\n2018. SystemT: Declarative text understanding for enter- guage, 6‚Äì15. Cambridge University Press.\n prise. NAACL HLT, volume 3.\n Karlsson, F., A. Voutilainen, J. HeikkilaÃà, and A. Anttila, eds.\nChiticariu, L., Y. Li, and F. R. Reiss. 2013. Rule-Based In- 1995. Constraint Grammar: A Language-Independent\n formation Extraction is Dead! Long Live Rule-Based In- System for Parsing Unrestricted Text. Mouton de Gruyter.\n formation Extraction Systems! EMNLP.\n Karttunen, L. 1999. Comments on Joshi. In A. Kornai, ed.,\nChristodoulopoulos, C., S. Goldwater, and M. Steedman. Extended Finite State Models of Language, 16‚Äì18. Cam-\n2010. Two decades of unsupervised POS induction: How bridge University Press.\n far have we come? EMNLP.\n Klein, S. and R. F. Simmons. 1963. A computational ap-\nChurch, K. W. 1988. A stochastic parts program and noun proach to grammatical coding of English words. Journal\n phrase parser for unrestricted text. ANLP. of the ACM, 10(3):334‚Äì347.\nChurch, K. W. 1989. A stochastic parts program and noun Kupiec, J. 1992. Robust part-of-speech tagging using a hidphrase parser for unrestricted text. ICASSP. den Markov model. Computer Speech and Language,\nClark, S., J. R. Curran, and M. Osborne. 2003. Bootstrapping 6:225‚Äì242.\n POS-taggers using unlabelled data. CoNLL. Lafferty, J. D., A. McCallum, and F. C. N. Pereira. 2001.\nCollobert, R., J. Weston, L. Bottou, M. Karlen, Conditional random fields: Probabilistic models for seg-\nK. Kavukcuoglu, and P. Kuksa. 2011. Natural language menting and labeling sequence data. ICML.\n processing (almost) from scratch. JMLR, 12:2493‚Äì2537. Lample, G., M. Ballesteros, S. Subramanian, K. Kawakami,\nDeRose, S. J. 1988. Grammatical category disambiguation and C. Dyer. 2016. Neural architectures for named entity\n by statistical optimization. Computational Linguistics, recognition. NAACL HLT.\n 14:31‚Äì39. Lee, H., M. Surdeanu, and D. Jurafsky. 2017. A scaffolding\nEvans, N. 2000. Word classes in the world‚Äôs languages. In approach to coreference resolution integrating statistical\n G. Booij, C. Lehmann, and J. Mugdan, eds, Morphology: and rule-based models. Natural Language Engineering,\n A Handbook on Inflection and Word Formation, 708‚Äì732. 23(5):733‚Äì762.\n Mouton. Ma, X. and E. H. Hovy. 2016. End-to-end sequence labeling\nFrancis, W. N. and H. KucÃåera. 1982. Frequency Analysis of via bi-directional LSTM-CNNs-CRF. ACL.\n English Usage. Houghton Mifflin, Boston. Manning, C. D. 2011. Part-of-speech tagging from 97% to\nGarside, R. 1987. The CLAWS word-tagging system. In 100%: Is it time for some linguistics? CICLing 2011.\n R. Garside, G. Leech, and G. Sampson, eds, The Compu- Marcus, M. P., B. Santorini, and M. A. Marcinkiewicz. 1993.\n tational Analysis of English, 30‚Äì41. Longman. Building a large annotated corpus of English: The Penn\nGarside, R., G. Leech, and A. McEnery. 1997. Corpus An- treebank. Computational Linguistics, 19(2):313‚Äì330.\n notation. Longman. de Marneffe, M.-C., C. D. Manning, J. Nivre, and D. Zeman.\n 2021. Universal Dependencies. Computational Linguis-\nGil, D. 2000. Syntactic categories, cross-linguistic variation\n tics, 47(2):255‚Äì308.\n and universal grammar. In P. M. Vogel and B. Comrie,\n eds, Approaches to the Typology of Word Classes, 173‚Äì Marshall, I. 1983. Choice of grammatical word-class with-\n216. Mouton. out global syntactic analysis: Tagging words in the LOB\n corpus. Computers and the Humanities, 17:139‚Äì150.\n Exercises 27\n\nMarshall, I. 1987. Tag selection using probabilistic methods. In R. Garside, G. Leech, and G. Sampson, eds, The\n Computational Analysis of English, 42‚Äì56. Longman.\nMcCallum, A., D. Freitag, and F. C. N. Pereira. 2000. Maximum entropy Markov models for information extraction\n and segmentation. ICML.\nMcCallum, A. and W. Li. 2003. Early results for named\n entity recognition with conditional random fields, feature\n induction and web-enhanced lexicons. CoNLL.\nMerialdo, B. 1994. Tagging English text with a probabilistic\n model. Computational Linguistics, 20(2):155‚Äì172.\nMikheev, A., M. Moens, and C. Grover. 1999. Named entity\n recognition without gazetteers. EACL.\nOravecz, C. and P. Dienes. 2002. Efficient stochastic partof-speech tagging for Hungarian. LREC.\nRamshaw, L. A. and M. P. Marcus. 1995. Text chunking\n using transformation-based learning. Proceedings of the\n 3rd Annual Workshop on Very Large Corpora.\nRatnaparkhi, A. 1996. A maximum entropy part-of-speech\n tagger. EMNLP.\nSampson, G. 1987. Alternative grammatical coding systems.\n In R. Garside, G. Leech, and G. Sampson, eds, The Computational Analysis of English, 165‚Äì183. Longman.\nSchuÃàtze, H. and Y. Singer. 1994. Part-of-speech tagging using a variable memory Markov model. ACL.\nS√∏gaard, A. 2010. Simple semi-supervised training of partof-speech taggers. ACL.\nStolz, W. S., P. H. Tannenbaum, and F. V. Carstensen. 1965.\n A stochastic approach to the grammatical coding of English. CACM, 8(6):399‚Äì405.\nThede, S. M. and M. P. Harper. 1999. A second-order hidden\n Markov model for part-of-speech tagging. ACL.\nToutanova, K., D. Klein, C. D. Manning, and Y. Singer.\n 2003. Feature-rich part-of-speech tagging with a cyclic\n dependency network. HLT-NAACL.\nVoutilainen, A. 1999. Handcrafted rules. In H. van Halteren,\n ed., Syntactic Wordclass Tagging, 217‚Äì246. Kluwer.\nWeischedel, R., M. Meteer, R. Schwartz, L. A. Ramshaw,\n and J. Palmucci. 1993. Coping with ambiguity and unknown words through probabilistic models. Computational Linguistics, 19(2):359‚Äì382.\nWu, S. and M. Dredze. 2019. Beto, Bentz, Becas: The surprising cross-lingual effectiveness of BERT. EMNLP.\nZhou, G., J. Su, J. Zhang, and M. Zhang. 2005. Exploring\n various knowledge in relation extraction. ACL.\n",
    "file_path": "/Users/colinsidberry/nlp-textbook-rag/data/extracted_text/17.Sequence Labeling for Parts of Speech and Named Entities.txt",
    "file_size_kb": 76.67
  },
  {
    "id": "6e071f9a2f0ae896",
    "source": "nlp_textbook",
    "chapter": "Context-Free Grammars and 18 Constituency Parsing Because the Night by Bruce Springsteen and Patti Smith",
    "filename": "18.Context-Free Grammars and Constituency Parsing.txt",
    "content": "rights reserved. Draft of August 24, 2025.\n\nCHAPTER\n\n Context-Free Grammars and\n18 Constituency Parsing\n Because the Night by Bruce Springsteen and Patti Smith\n The Fire Next Time by James Baldwin\n If on a winter‚Äôs night a traveler by Italo Calvino\n Love Actually by Richard Curtis\n Suddenly Last Summer by Tennessee Williams\n A Scanner Darkly by Philip K. Dick\n Six titles that are not constituents, from Geoffrey K. Pullum on\n Language Log (who was pointing out their incredible rarity).\n\n One morning I shot an elephant in my pajamas.\n How he got into my pajamas I don‚Äôt know.\n Groucho Marx, Animal Crackers, 1930\n\n The study of grammar has an ancient pedigree. The grammar of Sanskrit was\n described by the Indian grammarian PaÃÑn.ini sometime between the 7th and 4th censyntax turies BCE, in his famous treatise the As.t.aÃÑdhyaÃÑyƒ±ÃÑ (‚Äò8 books‚Äô). And our word syntax\n comes from the Greek syÃÅntaxis, meaning ‚Äúsetting out together or arrangement‚Äù, and\n refers to the way words are arranged together. We have seen syntactic notions in previous chapters like the use of part-of-speech categories (Chapter 17). In this chapter\n and the next one we introduce formal models for capturing more sophisticated notions of grammatical structure and algorithms for parsing these structures.\n Our focus in this chapter is context-free grammars and the CKY algorithm\n for parsing them. Context-free grammars are the backbone of many formal models of the syntax of natural language (and, for that matter, of computer languages).\n Syntactic parsing is the task of assigning a syntactic structure to a sentence. Parse\n trees (whether for context-free grammars or for the dependency or CCG formalisms\n we introduce in following chapters) can be used in applications such as grammar\n checking: sentence that cannot be parsed may have grammatical errors (or at least\n be hard to read). Parse trees can be an intermediate stage of representation for formal semantic analysis. And parsers and the grammatical structure they assign a\n sentence are a useful text analysis tool for text data science applications that require\n modeling the relationship of elements in sentences.\n In this chapter we introduce context-free grammars, give a small sample grammar of English, introduce more formal definitions of context-free grammars and\n grammar normal form, and talk about treebanks: corpora that have been annotated with syntactic structure. We then discuss parse ambiguity and the problems\n it presents, and turn to parsing itself, giving the famous Cocke-Kasami-Younger\n (CKY) algorithm (Kasami 1965, Younger 1967), the standard dynamic programming approach to syntactic parsing. The CKY algorithm returns an efficient representation of the set of parse trees for a sentence, but doesn‚Äôt tell us which parse tree\n is the right one. For that, we need to augment CKY with scores for each possible\n constituent. We‚Äôll see how to do this with neural span-based parsers. Finally, we‚Äôll\n introduce the standard set of metrics for evaluating parser accuracy.\n2 C HAPTER 18 ‚Ä¢ C ONTEXT-F REE G RAMMARS AND C ONSTITUENCY PARSING\n\n Syntactic constituency is the idea that groups of words can behave as single units,\n or constituents. Part of developing a grammar involves building an inventory of the\n constituents in the language. How do words group together in English? Consider\n noun phrase the noun phrase, a sequence of words surrounding at least one noun. Here are some\n examples of noun phrases (thanks to Damon Runyon):\n\n Harry the Horse a high-class spot such as Mindy‚Äôs\n the Broadway coppers the reason he comes into the Hot Box\n they three parties from Brooklyn\n\n What evidence do we have that these words group together (or ‚Äúform constituents‚Äù)?\n One piece of evidence is that they can all appear in similar syntactic environments,\n for example, before a verb.\n\n three parties from Brooklyn arrive. . .\n a high-class spot such as Mindy‚Äôs attracts. . .\n the Broadway coppers love. . .\n they sit\n\n But while the whole noun phrase can occur before a verb, this is not true of each\n of the individual words that make up a noun phrase. The following are not grammatical sentences of English (recall that we use an asterisk (*) to mark fragments that\n are not grammatical English sentences):\n\n *from arrive. . . *as attracts. . .\n *the is. . . *spot sat. . .\n\n Thus, to correctly describe facts about the ordering of these words in English, we\n must be able to say things like ‚ÄúNoun Phrases can occur before verbs‚Äù. Let‚Äôs now\n see how to do this in a more formal way!\n\n A widely used formal system for modeling constituent structure in natural lan-\nCFG guage is the context-free grammar, or CFG. Context-free grammars are also called\n phrase-structure grammars, and the formalism is equivalent to Backus-Naur form,\n or BNF. The idea of basing a grammar on constituent structure dates back to the psychologist Wilhelm Wundt (1900) but was not formalized until Chomsky (1956) and,\n independently, Backus (1959).\n rules A context-free grammar consists of a set of rules or productions, each of which\n expresses the ways that symbols of the language can be grouped and ordered tolexicon gether, and a lexicon of words and symbols. For example, the following productions\n NP express that an NP (or noun phrase) can be composed of either a ProperNoun or\n a determiner (Det) followed by a Nominal; a Nominal in turn can consist of one or\n 18.2 ‚Ä¢ C ONTEXT-F REE G RAMMARS 3\n\n more Nouns.1\n NP ‚Üí Det Nominal\n NP ‚Üí ProperNoun\n Nominal ‚Üí Noun | Nominal Noun\n Context-free rules can be hierarchically embedded, so we can combine the previous\n rules with others, like the following, that express facts about the lexicon:\n Det ‚Üí a\n Det ‚Üí the\n Noun ‚Üí flight\n The symbols that are used in a CFG are divided into two classes. The symbols\n terminal that correspond to words in the language (‚Äúthe‚Äù, ‚Äúnightclub‚Äù) are called terminal\n symbols; the lexicon is the set of rules that introduce these terminal symbols. The\nnon-terminal symbols that express abstractions over these terminals are called non-terminals. In\n each context-free rule, the item to the right of the arrow (‚Üí) is an ordered list of one\n or more terminals and non-terminals; to the left of the arrow is a single non-terminal\n symbol expressing some cluster or generalization. The non-terminal associated with\n each word in the lexicon is its lexical category, or part of speech.\n A CFG can be thought of in two ways: as a device for generating sentences\n and as a device for assigning a structure to a given sentence. Viewing a CFG as a\n generator, we can read the ‚Üí arrow as ‚Äúrewrite the symbol on the left with the string\n of symbols on the right‚Äù.\n So starting from the symbol: NP\n we can use our first rule to rewrite NP as: Det Nominal\n and then rewrite Nominal as: Noun\n and finally rewrite these parts-of-speech as: a flight\n We say the string a flight can be derived from the non-terminal NP. Thus, a CFG\n can be used to generate a set of strings. This sequence of rule expansions is called a\n derivation derivation of the string of words. It is common to represent a derivation by a parse\n parse tree tree (commonly shown inverted with the root at the top). Figure 18.1 shows the tree\n representation of this derivation.\n\n NP\n\n Det Nom\n\n a Noun\n\n flight\n\n dominates In the parse tree shown in Fig. 18.1, we can say that the node NP dominates\n all the nodes in the tree (Det, Nom, Noun, a, flight). We can say further that it\n immediately dominates the nodes Det and Nom.\n The formal language defined by a CFG is the set of strings that are derivable\nstart symbol from the designated start symbol. Each grammar must have one designated start\n 1 When talking about these rules we can pronounce the rightarrow ‚Üí as ‚Äúgoes to‚Äù, and so we might\n read the first rule above as ‚ÄúNP goes to Det Nominal‚Äù.\n4 C HAPTER 18 ‚Ä¢ C ONTEXT-F REE G RAMMARS AND C ONSTITUENCY PARSING\n\n symbol, which is often called S. Since context-free grammars are often used to define\n sentences, S is usually interpreted as the ‚Äúsentence‚Äù node, and the set of strings that\n are derivable from S is the set of sentences in some simplified version of English.\n Let‚Äôs add a few additional rules to our inventory. The following rule expresses\n verb phrase the fact that a sentence can consist of a noun phrase followed by a verb phrase:\n\n S ‚Üí NP VP I prefer a morning flight\n\n A verb phrase in English consists of a verb followed by assorted other things;\n for example, one kind of verb phrase consists of a verb followed by a noun phrase:\n\n VP ‚Üí Verb NP prefer a morning flight\n\n Or the verb may be followed by a noun phrase and a prepositional phrase:\n\n VP ‚Üí Verb NP PP leave Boston in the morning\n\n Or the verb phrase may have a verb followed by a prepositional phrase alone:\n\n VP ‚Üí Verb PP leaving on Thursday\n\n A prepositional phrase generally has a preposition followed by a noun phrase.\n For example, a common type of prepositional phrase in the ATIS corpus is used to\n indicate location or direction:\n\n PP ‚Üí Preposition NP from Los Angeles\n\n The NP inside a PP need not be a location; PPs are often used with times and\n dates, and with other nouns as well; they can be arbitrarily complex. Here are ten\n examples from the ATIS corpus:\n to Seattle on these flights\n in Minneapolis about the ground transportation in Chicago\n on Wednesday of the round trip flight on United Airlines\n in the evening of the AP fifty seven flight\n on the ninth of July with a stopover in Nashville\n we‚Äôve seen so far, which we‚Äôll call L0 . Note that we can use the or-symbol | to\n indicate that a non-terminal has alternate possible expansions.\n\n Noun ‚Üí flights | flight | breeze | trip | morning\n Verb ‚Üí is | prefer | like | need | want | fly | do\n Adjective ‚Üí cheapest | non-stop | first | latest\n | other | direct\n Pronoun ‚Üí me | I | you | it\n Proper-Noun ‚Üí Alaska | Baltimore | Los Angeles\n | Chicago | United | American\n Determiner ‚Üí the | a | an | this | these | that\n Preposition ‚Üí from | to | on | near | in\n Conjunction ‚Üí and | or | but\n\n We can use this grammar to generate sentences of this ‚ÄúATIS-language‚Äù. We\n start with S, expand it to NP VP, then choose a random expansion of NP (let‚Äôs say, to\n 18.2 ‚Ä¢ C ONTEXT-F REE G RAMMARS 5\n\n Grammar Rules Examples\n S ‚Üí NP VP I + want a morning flight\n\n NP ‚Üí Pronoun I\n | Proper-Noun Los Angeles\n | Det Nominal a + flight\n Nominal ‚Üí Nominal Noun morning + flight\n | Noun flights\n\n VP ‚Üí Verb do\n | Verb NP want + a flight\n | Verb NP PP leave + Boston + in the morning\n | Verb PP leaving + on Thursday\n\n PP ‚Üí Preposition NP from + Los Angeles\n\n S\n\n NP VP\n\n Pro Verb NP\n\n I prefer Det Nom\n\n a Nom Noun\n\n Noun flight\n\n morning\n\n I), and a random expansion of VP (let‚Äôs say, to Verb NP), and so on until we generate\n the string I prefer a morning flight. Figure 18.4 shows a parse tree that represents a\n complete derivation of I prefer a morning flight.\n We can also represent a parse tree in a more compact format called bracketed\n bracketed notation; here is the bracketed representation of the parse tree of Fig. 18.4:\n notation\n (18.1) [S [NP [Pro I]] [VP [V prefer] [NP [Det a] [Nom [N morning] [Nom [N flight]]]]]]\n A CFG like that of L0 defines a formal language. Sentences (strings of words)\n that can be derived by a grammar are in the formal language defined by that gramgrammatical mar, and are called grammatical sentences. Sentences that cannot be derived by\n a given formal grammar are not in the language defined by that grammar and are\nungrammatical referred to as ungrammatical. This hard line between ‚Äúin‚Äù and ‚Äúout‚Äù characterizes\n all formal languages but is only a very simplified model of how natural languages\n really work. This is because determining whether a given sentence is part of a given\n natural language (say, English) often depends on the context. In linguistics, the use\n generative\n grammar of formal languages to model natural languages is called generative grammar since\n the language is defined by the set of possible sentences ‚Äúgenerated‚Äù by the grammar.\n (Note that this is a different sense of the word ‚Äògenerate‚Äô than when we talk about\n6 C HAPTER 18 ‚Ä¢ C ONTEXT-F REE G RAMMARS AND C ONSTITUENCY PARSING\n\n language models generating text.)\n\n 18.2.1 Formal Definition of Context-Free Grammar\n We conclude this section with a quick, formal description of a context-free grammar and the language it generates. A context-free grammar G is defined by four\n parameters: N, Œ£, R, S (technically it is a ‚Äú4-tuple‚Äù).\n\n N a set of non-terminal symbols (or variables)\n Œ£ a set of terminal symbols (disjoint from N)\n R a set of rules or productions, each of the form A ‚Üí Œ≤ ,\n where A is a non-terminal,\n Œ≤ is a string of symbols from the infinite set of strings (Œ£ ‚à™ N)‚àó\n S a designated start symbol and a member of N\n\n For the remainder of the book we adhere to the following conventions when discussing the formal properties of context-free grammars (as opposed to explaining\n particular facts about English or other languages).\n Capital letters like A, B, and S Non-terminals\n S The start symbol\n Lower-case Greek letters like Œ±, Œ≤ , and Œ≥ Strings drawn from (Œ£ ‚à™ N)‚àó\n Lower-case Roman letters like u, v, and w Strings of terminals\n\n A language is defined through the concept of derivation. One string derives another one if it can be rewritten as the second one by some series of rule applications.\n More formally, following Hopcroft and Ullman (1979),\n if A ‚Üí Œ≤ is a production of R and Œ± and Œ≥ are any strings in the set\ndirectly derives (Œ£ ‚à™ N)‚àó , then we say that Œ±AŒ≥ directly derives Œ±Œ≤ Œ≥, or Œ±AŒ≥ ‚áí Œ±Œ≤ Œ≥.\n Derivation is then a generalization of direct derivation:\n Let Œ±1 , Œ±2 , . . . , Œ±m be strings in (Œ£ ‚à™ N)‚àó , m ‚â• 1, such that\n\n Œ±1 ‚áí Œ±2 , Œ±2 ‚áí Œ±3 , . . . , Œ±m‚àí1 ‚áí Œ±m\n ‚àó\n derives We say that Œ±1 derives Œ±m , or Œ±1 ‚áí Œ±m .\n We can then formally define the language LG generated by a grammar G as the\n set of strings composed of terminal symbols that can be derived from the designated\n start symbol S.\n ‚àó\n LG = {w|w is in Œ£‚àó and S ‚áí w}\n The problem of mapping from a string of words to its parse tree is called synsyntactic\n parsing tactic parsing, as we‚Äôll see in Section 18.6.\n\n treebank A corpus in which every sentence is annotated with a parse tree is called a treebank.\n 18.3 ‚Ä¢ T REEBANKS 7\n\n Treebanks play an important role in parsing as well as in linguistic investigations of\n syntactic phenomena.\n Treebanks are generally made by running a parser over each sentence and then\n having the resulting parse hand-corrected by human linguists. Figure 18.5 shows\nPenn Treebank sentences from the Penn Treebank project, which includes various treebanks in\n English, Arabic, and Chinese. The Penn Treebank part-of-speech tagset was defined\n in Chapter 17, but we‚Äôll see minor formatting differences across treebanks. The use\n of LISP-style parenthesized notation for trees is extremely common and resembles\n the bracketed notation we saw earlier in (18.1). For those who are not familiar with\n it we show a standard node-and-line tree representation in Fig. 18.6.\n\n ((S\n (NP-SBJ (DT That) ((S\n (JJ cold) (, ,) (NP-SBJ The/DT flight/NN )\n (JJ empty) (NN sky) ) (VP should/MD\n (VP (VBD was) (VP arrive/VB\n (ADJP-PRD (JJ full) (PP-TMP at/IN\n (PP (IN of) (NP eleven/CD a.m/RB ))\n (NP (NN fire) (NP-TMP tomorrow/NN )))))\n (CC and)\n (NN light) ))))\n (. .) ))\n (a) (b)\n\n S\n\n NP-SBJ VP .\n\n DT JJ , JJ NN VBD ADJP-PRD .\n\n That cold , empty sky was JJ PP\n\n full IN NP\n\n of NN CC NN\n\n fire and light\n\n The sentences in a treebank implicitly constitute a grammar of the language. For\n example, from the parsed sentences in Fig. 18.5 we can extract the CFG rules shown\n in Fig. 18.7 (with rule suffixes (-SBJ) stripped for simplicity). The grammar used\n to parse the Penn Treebank is very flat, resulting in very many rules. For example,\n8 C HAPTER 18 ‚Ä¢ C ONTEXT-F REE G RAMMARS AND C ONSTITUENCY PARSING\n\n Grammar Lexicon\n S ‚Üí NP VP . DT ‚Üí the | that\n S ‚Üí NP VP JJ ‚Üí cold | empty | full\n NP ‚Üí CD RB NN ‚Üí sky | fire | light | flight | tomorrow\n NP ‚Üí DT NN CC ‚Üí and\n NP ‚Üí NN CC NN IN ‚Üí of | at\n NP ‚Üí DT JJ , JJ NN CD ‚Üí eleven\n NP ‚Üí NN RB ‚Üí a.m.\n VP ‚Üí MD VP VB ‚Üí arrive\n VP ‚Üí VBD ADJP VBD ‚Üí was | said\n VP ‚Üí MD VP MD ‚Üí should | would\n VP ‚Üí VB PP NP\n ADJP ‚Üí JJ PP\n PP ‚Üí IN NP\n\n among the approximately 4,500 different rules for expanding VPs are separate rules\n for PP sequences of any length and every possible arrangement of verb arguments:\n\n VP ‚Üí VBD PP\n VP ‚Üí VBD PP PP\n VP ‚Üí VBD PP PP PP\n VP ‚Üí VBD PP PP PP PP\n VP ‚Üí VB ADVP PP\n VP ‚Üí VB PP ADVP\n VP ‚Üí ADVP VB PP\n\n A formal language is defined as a (possibly infinite) set of strings of words. This suggests that we could ask if two grammars are equivalent by asking if they generate the\n same set of strings. In fact, it is possible to have two distinct context-free grammars\n strongly\n equivalent generate the same language. We say that two grammars are strongly equivalent if\n they generate the same set of strings and if they assign the same phrase structure\n to each sentence (allowing merely for renaming of the non-terminal symbols). Two\n weakly\n equivalent grammars are weakly equivalent if they generate the same set of strings but do not\n assign the same phrase structure to each sentence.\n normal form It is sometimes useful to have a normal form for grammars, in which each of\n the productions takes a particular form. For example, a context-free grammar is in\n Chomsky Chomsky normal form (CNF) (Chomsky, 1963) if it is \u000f-free and if in addition\n normal form\n each production is either of the form A ‚Üí B C or A ‚Üí a. That is, the right-hand side\n of each rule either has two non-terminal symbols or one terminal symbol. Chomsky\n binary\n branching normal form grammars are binary branching, that is they have binary trees (down\n to the prelexical nodes). We make use of this binary branching property in the CKY\n parsing algorithm in Section 18.6.\n Any context-free grammar can be converted into a weakly equivalent Chomsky\n normal form grammar. For example, a rule of the form\n\n A ‚Üí B C D\n\n can be converted into the following two CNF rules (Exercise 18.1 asks the reader to\n 18.5 ‚Ä¢ A MBIGUITY 9\n\n Grammar Lexicon\n S ‚Üí NP VP Det ‚Üí that | this | the | a\n S ‚Üí Aux NP VP Noun ‚Üí book | flight | meal | money\n S ‚Üí VP Verb ‚Üí book | include | prefer\n NP ‚Üí Pronoun Pronoun ‚Üí I | she | me\n NP ‚Üí Proper-Noun Proper-Noun ‚Üí Houston | United\n NP ‚Üí Det Nominal Aux ‚Üí does\n Nominal ‚Üí Noun Preposition ‚Üí from | to | on | near | through\n Nominal ‚Üí Nominal Noun\n Nominal ‚Üí Nominal PP\n VP ‚Üí Verb\n VP ‚Üí Verb NP\n VP ‚Üí Verb NP PP\n VP ‚Üí Verb PP\n VP ‚Üí VP PP\n PP ‚Üí Preposition NP\n\n formulate the complete algorithm):\n\n A ‚Üí B X\n X ‚Üí C D\n\n Sometimes using binary branching can actually produce smaller grammars. For\n example, the sentences that might be characterized as\n VP -> VBD NP PP*\n are represented in the Penn Treebank by this series of rules:\n VP ‚Üí VBD NP PP\n VP ‚Üí VBD NP PP PP\n VP ‚Üí VBD NP PP PP PP\n VP ‚Üí VBD NP PP PP PP PP\n ...\n but could also be generated by the following two-rule grammar:\n VP ‚Üí VBD NP PP\n VP ‚Üí VP PP\n The generation of a symbol A with a potentially infinite sequence of symbols B with\n Chomskyadjunction a rule of the form A ‚Üí A B is known as Chomsky-adjunction.\n\n Ambiguity is the most serious problem faced by syntactic parsers. Chapter 17 introduced the notions of part-of-speech ambiguity and part-of-speech disambiguastructural\n ambiguity tion. Here, we introduce a new kind of ambiguity, called structural ambiguity,\n illustrated with a new toy grammar L1 , shown in Figure 18.8, which adds a few\n rules to the L0 grammar.\n Structural ambiguity occurs when the grammar can assign more than one parse\n to a sentence. Groucho Marx‚Äôs well-known line as Captain Spaulding in Animal\n10 C HAPTER 18 ‚Ä¢ C ONTEXT-F REE G RAMMARS AND C ONSTITUENCY PARSING\n\n S S\n\n NP VP NP VP\n\n Pronoun Verb NP Pronoun VP PP\n\n I shot Det Nominal I Verb NP in my pajamas\n\n an Nominal PP shot Det Nominal\n\n Noun in my pajamas an Noun\n\n elephant elephant\n\nreading in which the elephant is in the pajamas, the parse on the right corresponds to the reading in which\nCaptain Spaulding did the shooting in his pajamas.\n\n Crackers is ambiguous because the phrase in my pajamas can be part of the NP\n headed by elephant or a part of the verb phrase headed by shot. Figure 18.9 illustrates these two analyses of Marx‚Äôs line using rules from L1 .\n Structural ambiguity, appropriately enough, comes in many forms. Two common\n kinds of ambiguity are attachment ambiguity and coordination ambiguity. A\n attachment\n ambiguity sentence has an attachment ambiguity if a particular constituent can be attached to\n the parse tree at more than one place. The Groucho Marx sentence is an example\n PP-attachment\n ambiguity of PP-attachment ambiguity: the preposition phrase can be attached either as part\n of the NP or as part of the VP. Various kinds of adverbial phrases are also subject\n to this kind of ambiguity. For instance, in the following example the gerundive-VP\n flying to Paris can be part of a gerundive sentence whose subject is the Eiffel Tower\n or it can be an adjunct modifying the VP headed by saw:\n (18.2) We saw the Eiffel Tower flying to Paris.\n coordination\n ambiguity In coordination ambiguity phrases can be conjoined by a conjunction like and.\n For example, the phrase old men and women can be bracketed as [old [men and\n women]], referring to old men and old women, or as [old men] and [women], in\n which case it is only the men who are old. These ambiguities combine in complex\n ways in real sentences, like the following news sentence from the Brown corpus:\n (18.3) President Kennedy today pushed aside other White House business to\n devote all his time and attention to working on the Berlin crisis address he\n will deliver tomorrow night to the American people over nationwide\n television and radio.\n This sentence has a number of ambiguities, although since they are semantically\n unreasonable, it requires a careful reading to see them. The last noun phrase could be\n parsed [nationwide [television and radio]] or [[nationwide television] and radio].\n The direct object of pushed aside should be other White House business but could\n also be the bizarre phrase [other White House business to devote all his time and\n attention to working] (i.e., a structure like Kennedy affirmed [his intention to propose\n a new budget to address the deficit]). Then the phrase on the Berlin crisis address he\n 18.6 ‚Ä¢ CKY PARSING : A DYNAMIC P ROGRAMMING A PPROACH 11\n\n will deliver tomorrow night to the American people could be an adjunct modifying\n the verb pushed. A PP like over nationwide television and radio could be attached\n to any of the higher VPs or NPs (e.g., it could modify people or night).\n The fact that there are many grammatically correct but semantically unreasonable parses for naturally occurring sentences is an irksome problem that affects all\n parsers. Fortunately, the CKY algorithm below is designed to efficiently handle\n structural ambiguities. And as we‚Äôll see in the following section, we can augment\n CKY with neural methods to choose a single correct parse by syntactic disambiguasyntactic\ndisambiguation tion.\n\n Dynamic programming provides a powerful framework for addressing the problems caused by ambiguity in grammars. Recall that a dynamic programming approach systematically fills in a table of solutions to subproblems. The complete\n table has the solution to all the subproblems needed to solve the problem as a whole.\n In the case of syntactic parsing, these subproblems represent parse trees for all the\n constituents detected in the input.\n The dynamic programming advantage arises from the context-free nature of our\n grammar rules‚Äîonce a constituent has been discovered in a segment of the input we\n can record its presence and make it available for use in any subsequent derivation\n that might require it. This provides both time and storage efficiencies since subtrees\n can be looked up in a table, not reanalyzed. This section presents the Cocke-Kasami-\nYounger (CKY) algorithm, the most widely used dynamic-programming based approach to parsing. Chart parsing (Kaplan 1973, Kay 1982) is a related approach,\n chart parsing and dynamic programming methods are often referred to as chart parsing methods.\n\n 18.6.1 Conversion to Chomsky Normal Form\n The CKY algorithm requires grammars to first be in Chomsky Normal Form (CNF).\n Recall from Section 18.4 that grammars in CNF are restricted to rules of the form\n A ‚Üí B C or A ‚Üí w. That is, the right-hand side of each rule must expand either to\n two non-terminals or to a single terminal. Restricting a grammar to CNF does not\n lead to any loss in expressiveness, since any context-free grammar can be converted\n into a corresponding CNF grammar that accepts exactly the same set of strings as\n the original grammar.\n Let‚Äôs start with the process of converting a generic CFG into one represented in\n CNF. Assuming we‚Äôre dealing with an \u000f-free grammar, there are three situations we\n need to address in any generic grammar: rules that mix terminals with non-terminals\n on the right-hand side, rules that have a single non-terminal on the right-hand side,\n and rules in which the length of the right-hand side is greater than 2.\n The remedy for rules that mix terminals and non-terminals is to simply introduce\n a new dummy non-terminal that covers only the original terminal. For example, a\n rule for an infinitive verb phrase such as INF-VP ‚Üí to VP would be replaced by the\n two rules INF-VP ‚Üí TO VP and TO ‚Üí to.\n Unit\n productions Rules with a single non-terminal on the right are called unit productions. We\n can eliminate unit productions by rewriting the right-hand side of the original rules\n with the right-hand side of all the non-unit production rules that they ultimately lead\n ‚àó\n to. More formally, if A ‚áí B by a chain of one or more unit productions and B ‚Üí Œ≥\n12 C HAPTER 18 ‚Ä¢ C ONTEXT-F REE G RAMMARS AND C ONSTITUENCY PARSING\n\n is a non-unit production in our grammar, then we add A ‚Üí Œ≥ for each such rule in\n the grammar and discard all the intervening unit productions. As we demonstrate\n with our toy grammar, this can lead to a substantial flattening of the grammar and a\n consequent promotion of terminals to fairly high levels in the resulting trees.\n Rules with right-hand sides longer than 2 are normalized through the introduction of new non-terminals that spread the longer sequences over several new rules.\n Formally, if we have a rule like\n\n A ‚Üí BCŒ≥\n\n we replace the leftmost pair of non-terminals with a new non-terminal and introduce\n a new production, resulting in the following new rules:\n\n A ‚Üí X1 Œ≥\n X1 ‚Üí B C\n\n In the case of longer right-hand sides, we simply iterate this process until the offending rule has been replaced by rules of length 2. The choice of replacing the\n leftmost pair of non-terminals is purely arbitrary; any systematic scheme that results\n in binary rules would suffice.\n In our current grammar, the rule S ‚Üí Aux NP VP would be replaced by the two\n rules S ‚Üí X1 VP and X1 ‚Üí Aux NP.\n The entire conversion process can be summarized as follows:\n 1. Copy all conforming rules to the new grammar unchanged.\n 2. Convert terminals within rules to dummy non-terminals.\n 3. Convert unit productions.\n 4. Make all rules binary and add them to new grammar.\n the L1 grammar introduced earlier on page 9. Note that this figure doesn‚Äôt show\n the original lexical rules; since these original lexical rules are already in CNF, they\n all carry over unchanged to the new grammar. Figure 18.10 does, however, show\n the various places where the process of eliminating unit productions has, in effect,\n created new lexical rules. For example, all the original verbs have been promoted to\n both VPs and to Ss in the converted grammar.\n\n 18.6.2 CKY Recognition\n With our grammar now in CNF, each non-terminal node above the part-of-speech\n level in a parse tree will have exactly two daughters. A two-dimensional matrix can\n be used to encode the structure of an entire tree. For a sentence of length n, we will\n work with the upper-triangular portion of an (n + 1) √ó (n + 1) matrix. Each cell [i, j]\n in this matrix contains the set of non-terminals that represent all the constituents that\n span positions i through j of the input. Since our indexing scheme begins with 0, it‚Äôs\n natural to think of the indexes as pointing at the gaps between the input words (as in\n fenceposts 0 Book 1 that 2 flight 3 ). These gaps are often called fenceposts, on the metaphor of\n the posts between segments of fencing. It follows then that the cell that represents\n the entire input resides in position [0, n] in the matrix.\n Since each non-terminal entry in our table has two daughters in the parse, it follows that for each constituent represented by an entry [i, j], there must be a position\n in the input, k, where it can be split into two parts such that i < k < j. Given such\n 18.6 ‚Ä¢ CKY PARSING : A DYNAMIC P ROGRAMMING A PPROACH 13\n\n L1 Grammar L1 in CNF\nS ‚Üí NP VP S ‚Üí NP VP\nS ‚Üí Aux NP VP S ‚Üí X1 VP\n X1 ‚Üí Aux NP\nS ‚Üí VP S ‚Üí book | include | prefer\n S ‚Üí Verb NP\n S ‚Üí X2 PP\n S ‚Üí Verb PP\n S ‚Üí VP PP\nNP ‚Üí Pronoun NP ‚Üí I | she | me\nNP ‚Üí Proper-Noun NP ‚Üí United | Houston\nNP ‚Üí Det Nominal NP ‚Üí Det Nominal\nNominal ‚Üí Noun Nominal ‚Üí book | flight | meal | money\nNominal ‚Üí Nominal Noun Nominal ‚Üí Nominal Noun\nNominal ‚Üí Nominal PP Nominal ‚Üí Nominal PP\nVP ‚Üí Verb VP ‚Üí book | include | prefer\nVP ‚Üí Verb NP VP ‚Üí Verb NP\nVP ‚Üí Verb NP PP VP ‚Üí X2 PP\n X2 ‚Üí Verb NP\nVP ‚Üí Verb PP VP ‚Üí Verb PP\nVP ‚Üí VP PP VP ‚Üí VP PP\nPP ‚Üí Preposition NP PP ‚Üí Preposition NP\nhere, all the original lexical entries from L1 carry over unchanged as well.\n\na position k, the first constituent [i, k] must lie to the left of entry [i, j] somewhere\nalong row i, and the second entry [k, j] must lie beneath it, along column j.\n To make this more concrete, consider the following example with its completed\nparse matrix, shown in Fig. 18.11.\n(18.4) Book the flight through Houston.\nThe superdiagonal row in the matrix contains the parts of speech for each word in\nthe input. The subsequent diagonals above that superdiagonal contain constituents\nthat cover all the spans of increasing length in the input.\n Given this setup, CKY recognition consists of filling the parse table in the right\nway. To do this, we‚Äôll proceed in a bottom-up fashion so that at the point where we\nare filling any cell [i, j], the cells containing the parts that could contribute to this\nentry (i.e., the cells to the left and the cells below) have already been filled. The\nalgorithm given in Fig. 18.12 fills the upper-triangular matrix a column at a time\nworking from left to right, with each column filled from bottom to top, as the right\nside of Fig. 18.11 illustrates. This scheme guarantees that at each point in time we\nhave all the information we need (to the left, since all the columns to the left have\nalready been filled, and below since we‚Äôre filling bottom to top). It also mirrors online processing, since filling the columns from left to right corresponds to processing\neach word one at a time.\n The outermost loop of the algorithm given in Fig. 18.12 iterates over the columns,\nand the second loop iterates over the rows, from the bottom up. The purpose of the\ninnermost loop is to range over all the places where a substring spanning i to j in\nthe input might be split in two. As k ranges over the places where the string can be\nsplit, the pairs of cells we consider move, in lockstep, to the right along row i and\ndown along column j. Figure 18.13 illustrates the general case of filling cell [i, j].\n14 C HAPTER 18 ‚Ä¢ C ONTEXT-F REE G RAMMARS AND C ONSTITUENCY PARSING\n\n Book the flight through Houston\n\n S, VP, Verb, S,VP,X2 S,VP,X2\n Nominal,\n Noun\n [0,1] [0,2] [0,3] [0,4] [0,5]\n Det NP NP\n\n [1,2] [1,3] [1,4] [1,5]\n Nominal, Nominal\n Noun\n\n [2,3] [2,4] [2,5]\n Prep PP\n\n [3,4] [3,5]\n NP,\n Proper-\nNoun\n [4,5]\n\n function CKY-PARSE(words, grammar) returns table\n\n for j ‚Üê from 1 to L ENGTH(words) do\n for all {A | A ‚Üí words[ j] ‚àà grammar}\n table[ j ‚àí 1, j] ‚Üê table[ j ‚àí 1, j] ‚à™ A\n for i ‚Üê from j ‚àí 2 down to 0 do\n for k ‚Üê i + 1 to j ‚àí 1 do\n for all {A | A ‚Üí BC ‚àà grammar and B ‚àà table[i, k] and C ‚àà table[k, j]}\n table[i,j] ‚Üê table[i,j] ‚à™ A\n\n At each such split, the algorithm considers whether the contents of the two cells can\n be combined in a way that is sanctioned by a rule in the grammar. If such a rule\n exists, the non-terminal on its left-hand side is entered into the table.\n word Houston is read. The arrows point out the two spans that are being used to add\n an entry to the table. Note that the action in cell [0, 5] indicates the presence of three\n alternative parses for this input, one where the PP modifies the flight, one where\n it modifies the booking, and one that captures the second argument in the original\n VP ‚Üí Verb NP PP rule, now captured indirectly with the VP ‚Üí X2 PP rule.\n\n 18.6.3 CKY Parsing\n The algorithm given in Fig. 18.12 is a recognizer, not a parser. That is, it can tell\n us whether a valid parse exists for a given sentence based on whether or not if finds\n an S in cell [0, n], but it can‚Äôt provide the derivation, which is the actual job for a\n parser. To turn it into a parser capable of returning all possible parses for a given\n input, we can make two simple changes to the algorithm: the first change is to\n augment the entries in the table so that each non-terminal is paired with pointers to\n the table entries from which it was derived (more or less as shown in Fig. 18.14), the\n second change is to permit multiple versions of the same non-terminal to be entered\n into the table (again as shown in Fig. 18.14). With these changes, the completed\n table contains all the possible parses for a given input. Returning an arbitrary single\n 18.6 ‚Ä¢ CKY PARSING : A DYNAMIC P ROGRAMMING A PPROACH 15\n\n [0,1] [0,n]\n\n ...\n [i,j]\n\n [i,i+1] [i,i+2]\n ... [i,j-2] [i,j-1]\n\n [i+1,j]\n\n [i+2,j]\n\n [j-2,j]\n\n [j-1,j]\n\n ...\n\n [n-1, n]\n\nparse consists of choosing an S from cell [0, n] and then recursively retrieving its\ncomponent constituents from the table. Of course, instead of returning every parse\nfor a sentence, we usually want just the best parse; we‚Äôll see how to do that in the\nnext section.\n\n18.6.4 CKY in Practice\nFinally, we should note that while the restriction to CNF does not pose a problem\ntheoretically, it does pose some non-trivial problems in practice. The returned CNF\ntrees may not be consistent with the original grammar built by the grammar developers, and will complicate any syntax-driven approach to semantic analysis.\n One approach to getting around these problems is to keep enough information\naround to transform our trees back to the original grammar as a post-processing step\nof the parse. This is trivial in the case of the transformation used for rules with length\ngreater than 2. Simply deleting the new dummy non-terminals and promoting their\ndaughters restores the original tree.\n In the case of unit productions, it turns out to be more convenient to alter the basic CKY algorithm to handle them directly than it is to store the information needed\nto recover the correct trees. Exercise 18.3 asks you to make this change. Many of\nthe probabilistic parsers presented in Appendix C use the CKY algorithm altered in\n16 C HAPTER 18 ‚Ä¢ C ONTEXT-F REE G RAMMARS AND C ONSTITUENCY PARSING\n\n Book the flight through Houston Book the flight through Houston\n\n S, VP, Verb, S,VP,X2 S, VP, Verb, S,VP,X2\n Nominal, Nominal,\n Noun Noun\n [0,1] [0,2] [0,3] [0,4] [0,5] [0,1] [0,2] [0,3] [0,4] [0,5]\n Det NP Det NP NP\n\n [1,2] [1,3] [1,4] [1,5] [1,2] [1,3] [1,4] [1,5]\n Nominal, Nominal Nominal,\n Noun Noun\n\n [2,3] [2,4] [2,5] [2,3] [2,4] [2,5]\n Prep Prep PP\n\n [3,4] [3,5] [3,4] [3,5]\n NP, NP,\n Proper- Proper-\nNoun Noun\n [4,5] [4,5]\n\n Book the flight through Houston Book the flight through Houston\n\n S, VP, Verb, S,VP,X2 S, VP, Verb, S,VP,X2\n Nominal, Nominal,\n Noun Noun\n [0,1] [0,2] [0,3] [0,4] [0,5] [0,1] [0,2] [0,3] [0,4] [0,5]\n Det NP NP Det NP NP\n\n [1,2] [1,3] [1,4] [1,5] [1,2] [1,3] [1,4] [1,5]\n Nominal, Nominal Nominal, Nominal\n Noun Noun\n\n [2,3] [2,4] [2,5] [2,3] [2,4] [2,5]\n Prep PP Prep PP\n\n [3,4] [3,5] [3,4] [3,5]\n NP, NP,\n Proper- Proper-\nNoun Noun\n [4,5] [4,5]\n\n Book the flight through Houston\n\n S, VP, Verb, S1,VP, X2\n Nominal, S,\n Noun VP, S2, VP\n X2 S3\n [0,1] [0,2] [0,3] [0,4]\n Det NP NP\n\n [1,2] [1,3] [1,4] [1,5]\n Nominal, Nominal\n Noun\n\n [2,3] [2,4] [2,5]\n Prep PP\n\n [3,4] [3,5]\n NP,\n Proper-\nNoun\n [4,5]\n\n 18.7 ‚Ä¢ S PAN -BASED N EURAL C ONSTITUENCY PARSING 17\n\n just this manner.\n\n While the CKY parsing algorithm we‚Äôve seen so far does great at enumerating all\n the possible parse trees for a sentence, it has a large problem: it doesn‚Äôt tell us which\n parse is the correct one! That is, it doesn‚Äôt disambiguate among the possible parses.\n To solve the disambiguation problem we‚Äôll use a simple neural extension of the\n CKY algorithm. The intuition of such parsing algorithms (often called span-based\n constituency parsing, or neural CKY), is to train a neural classifier to assign a\n score to each constituent, and then use a modified version of CKY to combine these\n constituent scores to find the best-scoring parse tree.\n Here we‚Äôll describe a version of the algorithm from Kitaev et al. (2019). This\n parser learns to map a span of words to a constituent, and, like CKY, hierarchically\n combines larger and larger spans to build the parse-tree bottom-up. But unlike classic CKY, this parser doesn‚Äôt use the hand-written grammar to constrain what constituents can be combined, instead just relying on the learned neural representations\n of spans to encode likely combinations.\n\n 18.7.1 Computing Scores for a Span\n span Let‚Äôs begin by considering just the constituent (we‚Äôll call it a span) that lies between\n fencepost positions i and j with non-terminal symbol label l. We‚Äôll build a system\n to assign a score s(i, j, l) to this constituent span.\n\n CKY for computing best parse NP\n\n Compute score for span MLP\n\n Represent span hj-hi\n i=1 j=3\n\n 0 1 2 3 4 5\n\n postprocessing layers\n map back to words\n\n ENCODER\n map to subwords\n\n [START] Book the flight through Houston [END]\n the label NP.\n\n Fig. 18.15 sketches the architecture. The input word tokens are embedded by\n18 C HAPTER 18 ‚Ä¢ C ONTEXT-F REE G RAMMARS AND C ONSTITUENCY PARSING\n\n passing them through a pretrained language model like BERT. Because BERT operates on the level of subword (wordpiece) tokens rather than words, we‚Äôll first need to\n convert the BERT outputs to word representations. One standard way of doing this\n is to simply use the first subword unit as the representation for the entire word; using the last subword unit, or the sum of all the subword units are also common. The\n embeddings can then be passed through some postprocessing layers; Kitaev et al.\n (2019), for example, use 8 Transformer layers.\n The resulting word encoder outputs yt are then used to compute a span score.\n First, we must map the word encodings (indexed by word positions) to span encodings (indexed by fenceposts). We do this by representing each fencepost with two\n separate values; the intuition is that a span endpoint to the right of a word represents\n different information than a span endpoint to the left of a word. We convert each\n word output yt into a (leftward-pointing) value for spans ending at this fencepost,\n ‚Üê‚àí\n y t , and a (rightward-pointing) value ‚Üí ‚àíy t for spans beginning at this fencepost, by\n splitting yt into two halves. Each span then stretches from one double-vector fencepost to another, as in the following representation of the flight, which is span(1, 3):\n\n START 0 Book the flight through\n y0 ‚Üí\n ‚àí\n y0 ‚Üê\n y‚àí1 y ‚Üí ‚àí\n y ‚Üê\n y‚àí y2 ‚Üí\n ‚àí\n y2 ‚Üê\n y‚àí3 y ‚Üí ‚àíy ‚Üê\n y‚àí y ‚Üí ‚àíy ‚Üê\n y‚àí ...\n 1 1 2 3 3 4 4 4 5\n 0 1 2 3 4\n\n span(1,3)\n\n A traditional way to represent a span, developed originally for RNN-based models\n (Wang and Chang, 2016), but extended also to Transformers, is to take the difference between the embeddings of its start and end, i.e., representing span (i, j) by\n subtracting the embedding of i from the embedding of j. Here we represent a span\n by concatenating the difference of each of its fencepost components:\n\n v(i, j) = [‚Üí\n ‚àí\n yj ‚àí‚Üí\n ‚àí\n yi ; ‚Üê ‚àí‚àí ‚àí ‚Üê\n y j+1 ‚àí‚àí]\n yi+1 (18.5)\n\n The span vector v is then passed through an MLP span classifier, with two fullyconnected layers and one ReLU activation function, whose output dimensionality is\n the number of possible non-terminal labels:\n\n s(i, j, ¬∑) = W2 ReLU(LayerNorm(W1 v(i, j))) (18.6)\n\n The MLP then outputs a score for each possible non-terminal.\n\n 18.7.2 Integrating Span Scores into a Parse\n Now we have a score for each labeled constituent span s(i, j, l). But we need a score\n for an entire parse tree. Formally a tree T is represented as a set of |T | such labeled\n spans, with the t th span starting at position it and ending at position jt , with label lt :\n\n T = {(it , jt , lt ) : t = 1, . . . , |T |} (18.7)\n\n Thus once we have a score for each span, the parser can compute a score for the\n whole tree s(T ) simply by summing over the scores of its constituent spans:\n X\n s(T ) = s(i, j, l) (18.8)\n (i, j,l)‚ààT\n 18.8 ‚Ä¢ E VALUATING PARSERS 19\n\n And we can choose the final parse tree as the tree with the maximum score:\n\n TÃÇ = argmax s(T ) (18.9)\n T\n\n The simplest method to produce the most likely parse is to greedily choose the\n highest scoring label for each span. This greedy method is not guaranteed to produce\n a tree, since the best label for a span might not fit into a complete tree. In practice,\n however, the greedy method tends to find trees; in their experiments Gaddy et al.\n (2018) finds that 95% of predicted bracketings form valid trees.\n Nonetheless it is more common to use a variant of the CKY algorithm to find the\n full parse. The variant defined in Gaddy et al. (2018) works as follows. Let‚Äôs define\n sbest (i, j) as the score of the best subtree spanning (i, j). For spans of length one, we\n choose the best label:\n\n sbest (i, i + 1) = max s(i, i + 1, l) (18.10)\n l\n\n For other spans (i, j), the recursion is:\n\n sbest (i, j) = max s(i, j, l)\n l\n + max[sbest (i, k) + sbest (k, j)] (18.11)\n k\n\n Note that the parser is using the max label for span (i, j) + the max labels for spans\n (i, k) and (k, j) without worrying about whether those decisions make sense given a\n grammar. The role of the grammar in classical parsing is to help constrain possible\n combinations of constituents (NPs like to be followed by VPs). By contrast, the\n neural model seems to learn these kinds of contextual constraints during its mapping\n from spans to non-terminals.\n For more details on span-based parsing, including the margin-based training algorithm, see Stern et al. (2017), Gaddy et al. (2018), Kitaev and Klein (2018), and\n Kitaev et al. (2019).\n\n The standard tool for evaluating parsers that assign a single parse tree to a sentence\n PARSEVAL is the PARSEVAL metrics (Black et al., 1991). The PARSEVAL metric measures\n how much the constituents in the hypothesis parse tree look like the constituents in a\n hand-labeled, reference parse. PARSEVAL thus requires a human-labeled reference\n (or ‚Äúgold standard‚Äù) parse tree for each sentence in the test set; we generally draw\n these reference parses from a treebank like the Penn Treebank.\n A constituent in a hypothesis parse Ch of a sentence s is labeled correct if there\n is a constituent in the reference parse Cr with the same starting point, ending point,\n and non-terminal symbol. We can then measure the precision and recall just as for\n tasks we‚Äôve seen already like named entity tagging:\n\n # of correct constituents in hypothesis parse of s\n labeled recall: = # of total constituents in reference parse of s\n\n # of correct constituents in hypothesis parse of s\n labeled precision: = # of total constituents in hypothesis parse of s\n20 C HAPTER 18 ‚Ä¢ C ONTEXT-F REE G RAMMARS AND C ONSTITUENCY PARSING\n\n S(dumped)\n\n NP(workers) VP(dumped)\n\n NNS(workers) VBD(dumped) NP(sacks) PP(into)\n\n workers dumped NNS(sacks) P NP(bin)\n\n sacks into DT(a) NN(bin)\n\n a bin\n\n As usual, we often report a combination of the two, F1 :\n 2PR\n F1 = (18.12)\n P+R\n We additionally use a new metric, crossing brackets, for each sentence s:\n\n cross-brackets: the number of constituents for which the reference parse has a\n bracketing such as ((A B) C) but the hypothesis parse has a bracketing such\n as (A (B C)).\n For comparing parsers that use different grammars, the PARSEVAL metric includes a canonicalization algorithm for removing information likely to be grammarspecific (auxiliaries, pre-infinitival ‚Äúto‚Äù, etc.) and for computing a simplified score\n (Black et al., 1991). The canonical implementation of the PARSEVAL metrics is\n evalb called evalb (Sekine and Collins, 1997).\n\n Syntactic constituents can be associated with a lexical head; N is the head of an NP,\n V is the head of a VP. This idea of a head for each constituent dates back to Bloomfield 1914, and is central to the dependency grammars and dependency parsing we‚Äôll\n introduce in Chapter 19. Indeed, heads can be used as a way to map between constituency and dependency parses. Heads are also important in probabilistic parsing (Appendix C) and in constituent-based grammar formalisms like Head-Driven\n Phrase Structure Grammar (Pollard and Sag, 1994)..\n In one simple model of lexical heads, each context-free rule is associated with\n a head (Charniak 1997, Collins 1999). The head is the word in the phrase that is\n grammatically the most important. Heads are passed up the parse tree; thus, each\n non-terminal in a parse tree is annotated with a single word, which is its lexical head.\n non-terminal is annotated with its head.\n For the generation of such a tree, each CFG rule must be augmented to identify\n one right-side constituent to be the head child. The headword for a node is then set to\n the headword of its head child. Choosing these head children is simple for textbook\n examples (NN is the head of NP) but is complicated and indeed controversial for\n 18.10 ‚Ä¢ S UMMARY 21\n\n most phrases. (Should the complementizer to or the verb be the head of an infinite\n verb phrase?) Modern linguistic theories of syntax generally include a component\n that defines heads (see, e.g., (Pollard and Sag, 1994)).\n An alternative approach to finding a head is used in most practical computational\n systems. Instead of specifying head rules in the grammar itself, heads are identified\n dynamically in the context of trees for specific sentences. In other words, once\n a sentence is parsed, the resulting tree is walked to decorate each node with the\n appropriate head. Most current systems rely on a simple set of handwritten rules,\n such as a practical one for Penn Treebank grammars given in Collins (1999) but\n developed originally by Magerman (1995). For example, the rule for finding the\n head of an NP is as follows (Collins, 1999, p. 238):\n\n ‚Ä¢ If the last word is tagged POS, return last-word.\n ‚Ä¢ Else search from right to left for the first child which is an NN, NNP, NNPS, NX, POS,\n or JJR.\n ‚Ä¢ Else search from left to right for the first child which is an NP.\n ‚Ä¢ Else search from right to left for the first child which is a $, ADJP, or PRN.\n ‚Ä¢ Else search from right to left for the first child which is a CD.\n ‚Ä¢ Else search from right to left for the first child which is a JJ, JJS, RB or QP.\n ‚Ä¢ Else return the last word\n\n Selected other rules from this set are shown in Fig. 18.17. For example, for VP\n rules of the form VP ‚Üí Y1 ¬∑ ¬∑ ¬∑ Yn , the algorithm would start from the left of Y1 ¬∑ ¬∑ ¬∑\n Yn looking for the first Yi of type TO; if no TOs are found, it would search for the\n first Yi of type VBD; if no VBDs are found, it would search for a VBN, and so on.\n See Collins (1999) for more details.\n\nParent Direction Priority List\nADJP Left NNS QP NN $ ADVP JJ VBN VBG ADJP JJR NP JJS DT FW RBR RBS\n SBAR RB\nADVP Right RB RBR RBS FW ADVP TO CD JJR JJ IN NP JJS NN\nPRN Left\nPRT Right RP\nQP Left $ IN NNS NN JJ RB DT CD NCD QP JJR JJS\nS Left TO IN VP S SBAR ADJP UCP NP\nSBAR Left WHNP WHPP WHADVP WHADJP IN DT S SQ SINV SBAR FRAG\nVP Left TO VBD VBN MD VBZ VB VBG VBP VP ADJP NN NNS NP\n\n This chapter introduced constituency parsing. Here‚Äôs a summary of the main points:\n ‚Ä¢ In many languages, groups of consecutive words act as a group or a constituent, which can be modeled by context-free grammars (which are also\n known as phrase-structure grammars).\n ‚Ä¢ A context-free grammar consists of a set of rules or productions, expressed\n over a set of non-terminal symbols and a set of terminal symbols. Formally,\n a particular context-free language is the set of strings that can be derived\n from a particular context-free grammar.\n22 C HAPTER 18 ‚Ä¢ C ONTEXT-F REE G RAMMARS AND C ONSTITUENCY PARSING\n\n ‚Ä¢ Structural ambiguity is a significant problem for parsers. Common sources\n of structural ambiguity include PP-attachment and coordination ambiguity.\n ‚Ä¢ Dynamic programming parsing algorithms, such as CKY, use a table of\n partial parses to efficiently parse ambiguous sentences.\n ‚Ä¢ CKY restricts the form of the grammar to Chomsky normal form (CNF).\n ‚Ä¢ The basic CKY algorithm compactly represents all possible parses of the sentence but doesn‚Äôt choose a single best parse.\n ‚Ä¢ Choosing a single parse from all possible parses (disambiguation) can be\n done by neural constituency parsers.\n ‚Ä¢ Span-based neural constituency parses train a neural classifier to assign a score\n to each constituent, and then use a modified version of CKY to combine these\n constituent scores to find the best-scoring parse tree.\n ‚Ä¢ Parsers are evaluated with three metrics: labeled recall, labeled precision,\n and cross-brackets.\n ‚Ä¢ Partial parsing and chunking are methods for identifying shallow syntactic constituents in a text. They are solved by sequence models trained on\n syntactically-annotated data.\n\nHistorical Notes\n According to Percival (1976), the idea of breaking up a sentence into a hierarchy of\n constituents appeared in the VoÃàlkerpsychologie of the groundbreaking psychologist\n Wilhelm Wundt (Wundt, 1900):\n ...den sprachlichen Ausdruck fuÃàr die willkuÃàrliche Gliederung einer Gesammtvorstellung in ihre in logische Beziehung zueinander gesetzten\n Bestandteile\n [the linguistic expression for the arbitrary division of a total idea\n into its constituent parts placed in logical relations to one another]\n Wundt‚Äôs idea of constituency was taken up into linguistics by Leonard Bloomfield in his early book An Introduction to the Study of Language (Bloomfield, 1914).\n By the time of his later book, Language (Bloomfield, 1933), what was then called\n ‚Äúimmediate-constituent analysis‚Äù was a well-established method of syntactic study\n in the United States. By contrast, traditional European grammar, dating from the\n Classical period, defined relations between words rather than constituents, and European syntacticians retained this emphasis on such dependency grammars, the subject of Chapter 19. (And indeed, both dependency and constituency grammars have\n been in vogue in computational linguistics at different times).\n American Structuralism saw a number of specific definitions of the immediate\n constituent, couched in terms of their search for a ‚Äúdiscovery procedure‚Äù: a methodological algorithm for describing the syntax of a language. In general, these attempt\n to capture the intuition that ‚ÄúThe primary criterion of the immediate constituent\n is the degree in which combinations behave as simple units‚Äù (Bazell, 1952/1966, p.\n 284). The most well known of the specific definitions is Harris‚Äô idea of distributional\n similarity to individual units, with the substitutability test. Essentially, the method\n proceeded by breaking up a construction into constituents by attempting to substitute\n simple structures for possible constituents‚Äîif a substitution of a simple form, say,\n H ISTORICAL N OTES 23\n\n man, was substitutable in a construction for a more complex set (like intense young\n man), then the form intense young man was probably a constituent. Harris‚Äôs test was\n the beginning of the intuition that a constituent is a kind of equivalence class.\n The context-free grammar was a formalization of this idea of hierarchical\n constituency defined in Chomsky (1956) and further expanded upon (and argued\n against) in Chomsky (1957) and Chomsky (1956/1975). Shortly after Chomsky‚Äôs\n initial work, the context-free grammar was reinvented by Backus (1959) and independently by Naur et al. (1960) in their descriptions of the ALGOL programming\n language; Backus (1996) noted that he was influenced by the productions of Emil\n Post and that Naur‚Äôs work was independent of his (Backus‚Äô) own. After this early\n work, a great number of computational models of natural language processing were\n based on context-free grammars because of the early development of efficient parsing algorithms.\n Dynamic programming parsing has a history of independent discovery. According to the late Martin Kay (personal communication), a dynamic programming\n parser containing the roots of the CKY algorithm was first implemented by John\n Cocke in 1960. Later work extended and formalized the algorithm, as well as proving its time complexity (Kay 1967, Younger 1967, Kasami 1965). The related well-\nWFST formed substring table (WFST) seems to have been independently proposed by\n Kuno (1965) as a data structure that stores the results of all previous computations\n in the course of the parse. Based on a generalization of Cocke‚Äôs work, a similar\n data structure had been independently described in Kay (1967) (and Kay 1973). The\n top-down application of dynamic programming to parsing was described in Earley‚Äôs\n Ph.D. dissertation (Earley 1968, Earley 1970). Sheil (1976) showed the equivalence\n of the WFST and the Earley algorithm. Norvig (1991) shows that the efficiency offered by dynamic programming can be captured in any language with a memoization\n function (such as in LISP) simply by wrapping the memoization operation around a\n simple top-down parser.\nprobabilistic\n The earliest disambiguation algorithms for parsing were based on probabilistic\n context-free context-free grammars, first worked out by Booth (1969) and Salomaa (1969); see\n grammars\n Appendix C for more history. Neural methods were first applied to parsing at around\n the same time as statistical parsing methods were developed (Henderson, 1994). In\n the earliest work neural networks were used to estimate some of the probabilities for\n statistical constituency parsers (Henderson, 2003, 2004; Emami and Jelinek, 2005)\n . The next decades saw a wide variety of neural parsing algorithms, including recursive neural architectures (Socher et al., 2011, 2013), encoder-decoder models\n (Vinyals et al., 2015; Choe and Charniak, 2016), and the idea of focusing on spans\n (Cross and Huang, 2016). For more on the span-based self-attention approach we\n describe in this chapter see Stern et al. (2017), Gaddy et al. (2018), Kitaev and Klein\n (2018), and Kitaev et al. (2019). See Chapter 19 for the parallel history of neural\n dependency parsing.\n The classic reference for parsing algorithms is Aho and Ullman (1972); although\n the focus of that book is on computer languages, most of the algorithms have been\n applied to natural language.\n24 C HAPTER 18 ‚Ä¢ C ONTEXT-F REE G RAMMARS AND C ONSTITUENCY PARSING\n\nExercises\n Apply your program to the L1 grammar.\n grammars that contain unit productions.\n example, containing spelling errors or mistakes arising from automatic speech\n recognition.\n parser and a treebank, compare your metrics against a standard implementation. Analyze the errors in your approach.\n Exercises 25\n\nAho, A. V. and J. D. Ullman. 1972. The Theory of Parsing, Harris, Z. S. 1946. From morpheme to utterance. Language,\n Translation, and Compiling, volume 1. Prentice Hall. 22(3):161‚Äì183.\nBackus, J. W. 1959. The syntax and semantics of the Henderson, J. 1994. Description Based Parsing in a Connecproposed international algebraic language of the Zurich tionist Network. Ph.D. thesis, University of Pennsylvania,\n ACM-GAMM Conference. Information Processing: Pro- Philadelphia, PA.\n ceedings of the International Conference on Information Henderson, J. 2003. Inducing history representations for\n Processing, Paris. UNESCO. broad coverage statistical parsing. HLT-NAACL-03.\nBackus, J. W. 1996. Transcript of question and answer ses- Henderson, J. 2004. Discriminative training of a neural netsion. In R. L. Wexelblat, ed., History of Programming work statistical parser. ACL.\n Languages, page 162. Academic Press. Hopcroft, J. E. and J. D. Ullman. 1979. Introduction to Au-\nBazell, C. E. 1952/1966. The correspondence fallacy in tomata Theory, Languages, and Computation. Addisonstructural linguistics. In E. P. Hamp, F. W. Householder, Wesley.\n and R. Austerlitz, eds, Studies by Members of the En- Kaplan, R. M. 1973. A general syntactic processor. In\n glish Department, Istanbul University (3), reprinted in R. Rustin, ed., Natural Language Processing, 193‚Äì241.\n Readings in Linguistics II (1966), 271‚Äì298. University of Algorithmics Press.\n Chicago Press.\n Kasami, T. 1965. An efficient recognition and syntax anal-\nBlack, E., S. P. Abney, D. Flickinger, C. Gdaniec, R. Gr- ysis algorithm for context-free languages. Technical\n ishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek, Report AFCRL-65-758, Air Force Cambridge Research\n J. L. Klavans, M. Y. Liberman, M. P. Marcus, S. Roukos, Laboratory, Bedford, MA.\n B. Santorini, and T. Strzalkowski. 1991. A procedure for\n Kay, M. 1967. Experiments with a powerful parser. COLquantitatively comparing the syntactic coverage of En-\nING.\n glish grammars. Speech and Natural Language Workshop. Kay, M. 1973. The MIND system. In R. Rustin, ed., Natural\n Language Processing, 155‚Äì188. Algorithmics Press.\nBloomfield, L. 1914. An Introduction to the Study of Language. Henry Holt and Company. Kay, M. 1982. Algorithm schemata and data structures in\n syntactic processing. In S. AlleÃÅn, ed., Text Processing:\nBloomfield, L. 1933. Language. University of Chicago Text Analysis and Generation, Text Typology and Attribu-\nPress. tion, 327‚Äì358. Almqvist and Wiksell, Stockholm.\nBooth, T. L. 1969. Probabilistic representation of formal Kitaev, N., S. Cao, and D. Klein. 2019. Multilingual\n languages. IEEE Conference Record of the 1969 Tenth constituency parsing with self-attention and pre-training.\n Annual Symposium on Switching and Automata Theory. ACL.\nCharniak, E. 1997. Statistical parsing with a context-free Kitaev, N. and D. Klein. 2018. Constituency parsing with a\n grammar and word statistics. AAAI. self-attentive encoder. ACL.\nChoe, D. K. and E. Charniak. 2016. Parsing as language Kuno, S. 1965. The predictive analyzer and a path eliminamodeling. EMNLP. tion technique. CACM, 8(7):453‚Äì462.\nChomsky, N. 1956. Three models for the description of Magerman, D. M. 1995. Statistical decision-tree models for\n language. IRE Transactions on Information Theory, parsing. ACL.\n 2(3):113‚Äì124. Naur, P., J. W. Backus, F. L. Bauer, J. Green, C. Katz,\nChomsky, N. 1956/1975. The Logical Structure of Linguistic J. McCarthy, A. J. Perlis, H. Rutishauser, K. Samelson,\n Theory. Plenum. B. Vauquois, J. H. Wegstein, A. van Wijnagaarden, and\nChomsky, N. 1957. Syntactic Structures. Mouton. M. Woodger. 1960. Report on the algorithmic language\n ALGOL 60. CACM, 3(5):299‚Äì314. Revised in CACM\nChomsky, N. 1963. Formal properties of grammars. In R. D. 6:1, 1-17, 1963.\n Luce, R. Bush, and E. Galanter, eds, Handbook of Mathematical Psychology, volume 2, 323‚Äì418. Wiley. Norvig, P. 1991. Techniques for automatic memoization with\n applications to context-free parsing. Computational Lin-\nCollins, M. 1999. Head-Driven Statistical Models for Natu- guistics, 17(1):91‚Äì98.\n ral Language Parsing. Ph.D. thesis, University of Penn-\nPercival, W. K. 1976. On the historical source of immedisylvania, Philadelphia.\n ate constituent analysis. In J. D. McCawley, ed., Syntax\nCross, J. and L. Huang. 2016. Span-based constituency pars- and Semantics Volume 7, Notes from the Linguistic Uning with a structure-label system and provably optimal derground, 229‚Äì242. Academic Press.\n dynamic oracles. EMNLP.\n Pollard, C. and I. A. Sag. 1994. Head-Driven Phrase Struc-\nEarley, J. 1968. An Efficient Context-Free Parsing Algorithm. ture Grammar. University of Chicago Press.\n Ph.D. thesis, Carnegie Mellon University, Pittsburgh, PA. Salomaa, A. 1969. Probabilistic and weighted grammars.\nEarley, J. 1970. An efficient context-free parsing algorithm. Information and Control, 15:529‚Äì544.\n CACM, 6(8):451‚Äì455. Sekine, S. and M. Collins. 1997. The evalb software. http:\nEmami, A. and F. Jelinek. 2005. A neural syntactic language //cs.nyu.edu/cs/projects/proteus/evalb.\n model. Machine learning, 60(1):195‚Äì227. Sheil, B. A. 1976. Observations on context free parsing.\nGaddy, D., M. Stern, and D. Klein. 2018. What‚Äôs going on SMIL: Statistical Methods in Linguistics, 1:71‚Äì109.\n in neural constituency parsers? an analysis. NAACL HLT. Socher, R., J. Bauer, C. D. Manning, and A. Y. Ng. 2013.\n Parsing with compositional vector grammars. ACL.\n26 Chapter 18 ‚Ä¢ Context-Free Grammars and Constituency Parsing\n\nSocher, R., C. C.-Y. Lin, A. Y. Ng, and C. D. Manning. 2011.\n Parsing natural scenes and natural language with recursive neural networks. ICML.\nStern, M., J. Andreas, and D. Klein. 2017. A minimal spanbased neural constituency parser. ACL.\nVinyals, O., ≈Å. Kaiser, T. Koo, S. Petrov, I. Sutskever,\n and G. Hinton. 2015. Grammar as a foreign language.\n NeurIPS.\nWang, W. and B. Chang. 2016. Graph-based dependency\n parsing with bidirectional LSTM. ACL.\nWundt, W. 1900. VoÃàlkerpsychologie: eine Untersuchung der\n Entwicklungsgesetze von Sprache, Mythus, und Sitte. W.\n Engelmann, Leipzig. Band II: Die Sprache, Zweiter Teil.\nYounger, D. H. 1967. Recognition and parsing of contextfree languages in time n3 . Information and Control,\n 10:189‚Äì208.\n",
    "file_path": "/Users/colinsidberry/nlp-textbook-rag/data/extracted_text/18.Context-Free Grammars and Constituency Parsing.txt",
    "file_size_kb": 61.23
  },
  {
    "id": "94cd71396bf6de6a",
    "source": "nlp_textbook",
    "chapter": "Dependency Parsing",
    "filename": "19.Dependency Parsing.txt",
    "content": "rights reserved. Draft of August 24, 2025.\n\nCHAPTER\n\n Dependency Parsing\n19 Tout mot qui fait partie d‚Äôune phrase... Entre lui et ses voisins, l‚Äôesprit apercÃßoit\n des connexions, dont l‚Äôensemble forme la charpente de la phrase.\n\n [Between each word in a sentence and its neighbors, the mind perceives connections. These connections together form the scaffolding of the sentence.]\n Lucien TesnieÃÄre. 1959. EÃÅleÃÅments de syntaxe structurale, A.1.¬ß4\n\n The focus of the last chapter was on context-free grammars and constituentbased representations. Here we present another important family of grammar fordependency\n grammars malisms called dependency grammars. In dependency formalisms, phrasal constituents and phrase-structure rules do not play a direct role. Instead, the syntactic\n structure of a sentence is described solely in terms of directed binary grammatical\n relations between the words, as in the following dependency parse:\n root\n obj\n det nmod\n (19.1)\n nsubj compound case\n\n I prefer the morning flight through Denver\n\n Relations among the words are illustrated above the sentence with directed, labeled\n typed\n dependency arcs from heads to dependents. We call this a typed dependency structure because\n the labels are drawn from a fixed inventory of grammatical relations. A root node\n explicitly marks the root of the tree, the head of the entire structure.\n visualized as a tree, alongside its corresponding phrase-structure analysis of the kind\n given in the prior chapter. Note the absence of nodes corresponding to phrasal constituents or lexical categories in the dependency parse; the internal structure of the\n dependency parse consists solely of directed relations between words. These headdependent relationships directly encode important information that is often buried in\n the more complex phrase-structure parses. For example, the arguments to the verb\n prefer are directly linked to it in the dependency structure, while their connection\n to the main verb is more distant in the phrase-structure tree. Similarly, morning\n and Denver, modifiers of flight, are linked to it directly in the dependency structure.\n This fact that the head-dependent relations are a good proxy for the semantic relationship between predicates and their arguments is an important reason why dependency grammars are currently more common than constituency grammars in natural\n language processing.\n Another major advantage of dependency grammars is their ability to deal with\nfree word order languages that have a relatively free word order. For example, word order in Czech\n can be much more flexible than in English; a grammatical object might occur before\n or after a location adverbial. A phrase-structure grammar would need a separate rule\n2 C HAPTER 19 ‚Ä¢ D EPENDENCY PARSING\n\n prefer S\n\n I flight NP VP\n\n the morning Denver Pro Verb NP\n\n I prefer Det Nom\n\n through the Nom PP\n\n Nom Noun P NP\n\n Noun flight through Pro\n\n morning Denver\n\n for each possible place in the parse tree where such an adverbial phrase could occur.\n A dependency-based approach can have just one link type representing this particular adverbial relation; dependency grammar approaches can thus abstract away a bit\n more from word order information.\n In the following sections, we‚Äôll give an inventory of relations used in dependency\n parsing, discuss two families of parsing algorithms (transition-based, and graphbased), and discuss evaluation.\n\n grammatical The traditional linguistic notion of grammatical relation provides the basis for the\n relation\n binary relations that comprise these dependency structures. The arguments to these\n head relations consist of a head and a dependent. The head plays the role of the central\n dependent organizing word, and the dependent as a kind of modifier. The head-dependent relationship is made explicit by directly linking heads to the words that are immediately\n dependent on them.\n In addition to specifying the head-dependent pairs, dependency grammars allow\n grammatical us to classify the kinds of grammatical relations, or grammatical function that the\n function\n dependent plays with respect to its head. These include familiar notions such as\n subject, direct object and indirect object. In English these notions strongly correlate with, but by no means determine, both position in a sentence and constituent\n type and are therefore somewhat redundant with the kind of information found in\n phrase-structure trees. However, in languages with more flexible word order, the\n information encoded directly in these grammatical relations is critical since phrasebased constituent syntax provides little help.\n Linguists have developed taxonomies of relations that go well beyond the familiar notions of subject and object. While there is considerable variation from theory\n 19.1 ‚Ä¢ D EPENDENCY R ELATIONS 3\n\n Clausal Argument Relations Description\n NSUBJ Nominal subject\n OBJ Direct object\n IOBJ Indirect object\n CCOMP Clausal complement\n Nominal Modifier Relations Description\n NMOD Nominal modifier\n AMOD Adjectival modifier\n APPOS Appositional modifier\n DET Determiner\n CASE Prepositions, postpositions and other case markers\n Other Notable Relations Description\n CONJ Conjunct\n CC Coordinating conjunction\n\n to theory, there is enough commonality that cross-linguistic standards have been\n Universal\nDependencies developed. The Universal Dependencies (UD) project (de Marneffe et al., 2021),\n an open community effort to annotate dependencies and other aspects of grammar\n across more than 100 languages, provides an inventory of 37 dependency relations.\n Fig. 19.2 shows a subset of the UD relations and Fig. 19.3 provides some examples.\n The motivation for all of the relations in the Universal Dependency scheme is\n beyond the scope of this chapter, but the core set of frequently used relations can be\n broken into two sets: clausal relations that describe syntactic roles with respect to a\n predicate (often a verb), and modifier relations that categorize the ways that words\n can modify their heads.\n Consider, for example, the following sentence:\n root\n obj\n det nmod\n (19.2)\n nsubj compound case\n\n United canceled the morning flights to Houston\n\n Here the clausal relations NSUBJ and OBJ identify the subject and direct object of\n the predicate cancel, while the NMOD, DET, and CASE relations denote modifiers of\n the nouns flights and Houston.\n\n 19.1.1 Dependency Formalisms\n A dependency structure can be represented as a directed graph G = (V, A), consisting\n of a set of vertices V , and a set of ordered pairs of vertices A, which we‚Äôll call arcs.\n For the most part we will assume that the set of vertices, V , corresponds exactly\n to the set of words in a given sentence. However, they might also correspond to\n punctuation, or when dealing with morphologically complex languages the set of\n vertices might consist of stems and affixes. The set of arcs, A, captures the headdependent and grammatical function relationships between the elements in V .\n Different grammatical theories or formalisms may place further constraints on\n these dependency structures. Among the more frequent restrictions are that the structures must be connected, have a designated root node, and be acyclic or planar. Of\n most relevance to the parsing approaches discussed in this chapter is the common,\n4 C HAPTER 19 ‚Ä¢ D EPENDENCY PARSING\n\n Relation Examples with head and dependent\n NSUBJ United canceled the flight.\n OBJ United diverted the flight to Reno.\n We booked her the first flight to Miami.\n IOBJ We booked her the flight to Miami.\n COMPOUND We took the morning flight.\n NMOD flight to Houston.\n AMOD Book the cheapest flight.\n APPOS United, a unit of UAL, matched the fares.\n DET The flight was canceled.\n Which flight was delayed?\n CONJ We flew to Denver and drove to Steamboat.\n CC We flew to Denver and drove to Steamboat.\n CASE Book the flight through Houston.\n\n dependency computationally-motivated, restriction to rooted trees. That is, a dependency tree\n tree\n is a directed graph that satisfies the following constraints:\n 1. There is a single designated root node that has no incoming arcs.\n 2. With the exception of the root node, each vertex has exactly one incoming arc.\n 3. There is a unique path from the root node to each vertex in V .\n Taken together, these constraints ensure that each word has a single head, that the\n dependency structure is connected, and that there is a single root node from which\n one can follow a unique directed path to each of the words in the sentence.\n\n 19.1.2 Projectivity\n The notion of projectivity imposes an additional constraint that is derived from the\n order of the words in the input. An arc from a head to a dependent is said to be\n projective projective if there is a path from the head to every word that lies between the head\n and the dependent in the sentence. A dependency tree is then said to be projective if\n all the arcs that make it up are projective. All the dependency trees we‚Äôve seen thus\n far have been projective. There are, however, many valid constructions which lead\n to non-projective trees, particularly in languages with relatively flexible word order.\n Consider the following example.\n\n acl:relcl\n root obl\n\n obj cop (19.3)\n nsubj det det nsubj adv\n\n JetBlue canceled our flight this morning which was already late\n\n In this example, the arc from flight to its modifier late is non-projective since there\n is no path from flight to the intervening words this and morning. As we can see from\n this diagram, projectivity (and non-projectivity) can be detected in the way we‚Äôve\n been drawing our trees. A dependency tree is projective if it can be drawn with\n no crossing edges. Here there is no way to link flight to its dependent late without\n crossing the arc that links morning to its head.\n 19.1 ‚Ä¢ D EPENDENCY R ELATIONS 5\n\n Our concern with projectivity arises from two related issues. First, the most\nwidely used English dependency treebanks were automatically derived from phrasestructure treebanks through the use of head-finding rules. The trees generated in such\na fashion will always be projective, and hence will be incorrect when non-projective\nexamples like this one are encountered.\n Second, there are computational limitations to the most widely used families of\nparsing algorithms. The transition-based approaches discussed in Section 19.2 can\nonly produce projective trees, hence any sentences with non-projective structures\nwill necessarily contain some errors. This limitation is one of the motivations for\nthe more flexible graph-based parsing approach described in Section 19.3.\n\n19.1.3 Dependency Treebanks\n\nTreebanks play a critical role in the development and evaluation of dependency\nparsers. They are used for training parsers, they act as the gold labels for evaluating\nparsers, and they also provide useful information for corpus linguistics studies.\n Dependency treebanks are created by having human annotators directly generate\ndependency structures for a given corpus, or by hand-correcting the output of an\nautomatic parser. A few early treebanks were also based on using a deterministic\nprocess to translate existing constituent-based treebanks into dependency trees.\n The largest open community project for building dependency trees is the Universal Dependencies project at https://universaldependencies.org/ introduced\nabove, which currently has almost 200 dependency treebanks in more than 100 languages (de Marneffe et al., 2021). Here are a few UD examples showing dependency\ntrees for sentences in Spanish, Basque, and Mandarin Chinese:\n\n punct\n obl:tmod\n obl\n case case\n det det\n\n VERB ADP DET NOUN ADP DET NUM PUNCT\n Subiremos a el tren a las cinco .\n we-will-board on the train at the five .\n[Spanish] Subiremos al tren a las cinco. ‚ÄúWe will be boarding the train at five.‚Äù\n (19.4)\n\n nsubj punct\n obj aux\n\n NOUN NOUN VERB AUX PUNCT\n Ekaitzak itsasontzia hondoratu du .\n storm (Erg.) ship (Abs.) sunk has .\n [Basque] Ekaitzak itsasontzia hondoratu du. ‚ÄúThe storm has sunk the ship.‚Äù (19.5)\n6 C HAPTER 19 ‚Ä¢ D EPENDENCY PARSING\n\n adv\n nsubj\n obj:tmod obj\n advmod compound:vv\n\n ADV PRON NOUN ADV VERB VERB NOUN\n ‰ΩÜ Êàë Êò®Â§© Êâç Êî∂ Âà∞ ‰ø°\n but I yesterday only-then receive arrive letter .\n [Chinese] ‰ΩÜÊàëÊò®Â§©ÊâçÊî∂Âà∞‰ø° ‚ÄúBut I didn‚Äôt receive the letter until yesterday‚Äù(19.6)\n\ntransition-based Our first approach to dependency parsing is called transition-based parsing. This\n architecture draws on shift-reduce parsing, a paradigm originally developed for\n analyzing programming languages (Aho and Ullman, 1972). In transition-based\n parsing we‚Äôll have a stack on which we build the parse, a buffer of tokens to be\n parsed, and a parser which takes actions on the parse via a predictor called an oracle,\n as illustrated in Fig. 19.4.\n\n Input buÔ¨Äer\n w1 w2 wn\n\n s1 Dependency\n s2\n Parser LEFTARC Relations\n Action\n Stack ... Oracle RIGHTARC\n w3 w2\n SHIFT\n\n sn\n\n stack and selects an action by consulting an oracle that examines the current configuration.\n\n The parser walks through the sentence left-to-right, successively shifting items\n from the buffer onto the stack. At each time point we examine the top two elements\n on the stack, and the oracle makes a decision about what transition to apply to build\n the parse. The possible transitions correspond to the intuitive actions one might take\n in creating a dependency tree by examining the words in a single pass over the input\n from left to right (Covington, 2001):\n ‚Ä¢ Assign the current word as the head of some previously seen word,\n ‚Ä¢ Assign some previously seen word as the head of the current word,\n ‚Ä¢ Postpone dealing with the current word, storing it for later processing.\n We‚Äôll formalize this intuition with the following three transition operators that\n will operate on the top two elements of the stack:\n ‚Ä¢ LEFTA RC: Assert a head-dependent relation between the word at the top of\n the stack and the second word; remove the second word from the stack.\n ‚Ä¢ RIGHTA RC: Assert a head-dependent relation between the second word on\n the stack and the word at the top; remove the top word from the stack;\n 19.2 ‚Ä¢ T RANSITION -BASED D EPENDENCY PARSING 7\n\n ‚Ä¢ SHIFT: Remove the word from the front of the input buffer and push it onto\n the stack.\n We‚Äôll sometimes call operations like LEFTA RC and RIGHTA RC reduce operations,\n based on a metaphor from shift-reduce parsing, in which reducing means combining elements on the stack. There are some preconditions for using operators. The\n LEFTA RC operator cannot be applied when ROOT is the second element of the stack\n (since by definition the ROOT node cannot have any incoming arcs). And both the\n LEFTA RC and RIGHTA RC operators require two elements to be on the stack to be\n applied.\narc standard This particular set of operators implements what is known as the arc standard\n approach to transition-based parsing (Covington 2001, Nivre 2003). In arc standard\n parsing the transition operators only assert relations between elements at the top of\n the stack, and once an element has been assigned its head it is removed from the\n stack and is not available for further processing. As we‚Äôll see, there are alternative transition systems which demonstrate different parsing behaviors, but the arc\n standard approach is quite effective and is simple to implement.\n The specification of a transition-based parser is quite simple, based on repreconfiguration senting the current state of the parse as a configuration: the stack, an input buffer\n of words or tokens, and a set of relations representing a dependency tree. Parsing\n means making a sequence of transitions through the space of possible configurations. We start with an initial configuration in which the stack contains the ROOT\n node, the buffer has the tokens in the sentence, and an empty set of relations represents the parse. In the final goal state, the stack and the word list should be empty,\n and the set of relations will represent the final parse. Fig. 19.5 gives the algorithm.\n\n function DEPENDENCY PARSE(words) returns dependency tree\n\n state ‚Üê {[root], [words], [] } ; initial configuration\n while state not final\n t ‚Üê O RACLE(state) ; choose a transition operator to apply\n state ‚Üê A PPLY(t, state) ; apply it, creating a new state\n return state\n\n At each step, the parser consults an oracle (we‚Äôll come back to this shortly) that\n provides the correct transition operator to use given the current configuration. It then\n applies that operator to the current configuration, producing a new configuration.\n The process ends when all the words in the sentence have been consumed and the\n ROOT node is the only element remaining on the stack.\n The efficiency of transition-based parsers should be apparent from the algorithm.\n The complexity is linear in the length of the sentence since it is based on a single\n left to right pass through the words in the sentence. (Each word must first be shifted\n onto the stack and then later reduced.)\n Note that unlike the dynamic programming and search-based approaches discussed in Chapter 18, this approach is a straightforward greedy algorithm‚Äîthe oracle provides a single choice at each step and the parser proceeds with that choice,\n no other options are explored, no backtracking is employed, and a single parse is\n returned in the end.\n8 C HAPTER 19 ‚Ä¢ D EPENDENCY PARSING\n\n leading to a parse for the following example.\n\n root\n obj\n\n det\n (19.7)\n iobj compound\n\n Book me the morning flight\n\n Let‚Äôs consider the state of the configuration at Step 2, after the word me has been\n pushed onto the stack.\n\n Stack Word List Relations\n [root, book, me] [the, morning, flight]\n\n The correct operator to apply here is RIGHTA RC which assigns book as the head of\n me and pops me from the stack resulting in the following configuration.\n\n Stack Word List Relations\n [root, book] [the, morning, flight] (book ‚Üí me)\n\n After several subsequent applications of the SHIFT operator, the configuration in\n Step 6 looks like the following:\n\n Stack Word List Relations\n [root, book, the, morning, flight] [] (book ‚Üí me)\n\n Here, all the remaining words have been passed onto the stack and all that is left\n to do is to apply the appropriate reduce operators. In the current configuration, we\n employ the LEFTA RC operator resulting in the following state.\n\n Stack Word List Relations\n [root, book, the, flight] [] (book ‚Üí me)\n (morning ‚Üê flight)\n\n At this point, the parse for this sentence consists of the following structure.\n\n iobj compound\n (19.8)\n Book me the morning flight\n\n There are several important things to note when examining sequences such as\n the one in Figure 19.6. First, the sequence given is not the only one that might lead\n to a reasonable parse. In general, there may be more than one path that leads to the\n same result, and due to ambiguity, there may be other transition sequences that lead\n to different equally valid parses.\n Second, we are assuming that the oracle always provides the correct operator\n at each point in the parse‚Äîan assumption that is unlikely to be true in practice.\n As a result, given the greedy nature of this algorithm, incorrect choices will lead to\n incorrect parses since the parser has no opportunity to go back and pursue alternative\n choices. Section 19.2.4 will introduce several techniques that allow transition-based\n approaches to explore the search space more fully.\n 19.2 ‚Ä¢ T RANSITION -BASED D EPENDENCY PARSING 9\n\nStep Stack Word List Action Relation Added\n 0 [root] [book, me, the, morning, flight] SHIFT\n 1 [root, book] [me, the, morning, flight] SHIFT\n 2 [root, book, me] [the, morning, flight] RIGHTA RC (book ‚Üí me)\n 3 [root, book] [the, morning, flight] SHIFT\n 4 [root, book, the] [morning, flight] SHIFT\n 5 [root, book, the, morning] [flight] SHIFT\n 6 [root, book, the, morning, flight] [] LEFTA RC (morning ‚Üê flight)\n 7 [root, book, the, flight] [] LEFTA RC (the ‚Üê flight)\n 8 [root, book, flight] [] RIGHTA RC (book ‚Üí flight)\n 9 [root, book] [] RIGHTA RC (root ‚Üí book)\n 10 [root] [] Done\n\n Finally, for simplicity, we have illustrated this example without the labels on\n the dependency relations. To produce labeled trees, we can parameterize the LEFT-\nA RC and RIGHTA RC operators with dependency labels, as in LEFTA RC ( NSUBJ ) or\n RIGHTA RC ( OBJ ). This is equivalent to expanding the set of transition operators from\n our original set of three to a set that includes LEFTA RC and RIGHTA RC operators for\n each relation in the set of dependency relations being used, plus an additional one\n for the SHIFT operator. This, of course, makes the job of the oracle more difficult\n since it now has a much larger set of operators from which to choose.\n\n 19.2.1 Creating an Oracle\n The oracle for greedily selecting the appropriate transition is trained by supervised\n machine learning. As with all supervised machine learning methods, we will need\n training data: configurations annotated with the correct transition to take. We can\n draw these from dependency trees. And we need to extract features of the configuration. We‚Äôll introduce neural classifiers that represent the configuration via\n embeddings, as well as classic systems that use hand-designed features.\n\n Generating Training Data\n The oracle from the algorithm in Fig. 19.5 takes as input a configuration and returns a\n transition operator. Therefore, to train a classifier, we will need configurations paired\n with transition operators (i.e., LEFTA RC, RIGHTA RC, or SHIFT). Unfortunately,\n treebanks pair entire sentences with their corresponding trees, not configurations\n with transitions.\n To generate the required training data, we employ the oracle-based parsing algorithm in a clever way. We supply our oracle with the training sentences to be parsed\n along with their corresponding reference parses from the treebank. To produce training instances, we then simulate the operation of the parser by running the algorithm\n training oracle and relying on a new training oracle to give us correct transition operators for each\n successive configuration.\n To see how this works, let‚Äôs first review the operation of our parser. It begins with\n a default initial configuration where the stack contains the ROOT, the input list is just\n the list of words, and the set of relations is empty. The LEFTA RC and RIGHTA RC\n operators each add relations between the words at the top of the stack to the set of\n relations being accumulated for a given sentence. Since we have a gold-standard\n reference parse for each training sentence, we know which dependency relations are\n valid for a given sentence. Therefore, we can use the reference parse to guide the\n10 C HAPTER 19 ‚Ä¢ D EPENDENCY PARSING\n\nStep Stack Word List Predicted Action\n 0 [root] [book, the, flight, through, houston] SHIFT\n 1 [root, book] [the, flight, through, houston] SHIFT\n 2 [root, book, the] [flight, through, houston] SHIFT\n 3 [root, book, the, flight] [through, houston] LEFTA RC\n 4 [root, book, flight] [through, houston] SHIFT\n 5 [root, book, flight, through] [houston] SHIFT\n 6 [root, book, flight, through, houston] [] LEFTA RC\n 7 [root, book, flight, houston ] [] RIGHTA RC\n 8 [root, book, flight] [] RIGHTA RC\n 9 [root, book] [] RIGHTA RC\n 10 [root] [] Done\nwith a given reference parse.\n\n selection of operators as the parser steps through a sequence of configurations.\n To be more precise, given a reference parse and a configuration, the training\n oracle proceeds as follows:\n ‚Ä¢ Choose LEFTA RC if it produces a correct head-dependent relation given the\n reference parse and the current configuration,\n ‚Ä¢ Otherwise, choose RIGHTA RC if (1) it produces a correct head-dependent relation given the reference parse and (2) all of the dependents of the word at\n the top of the stack have already been assigned,\n ‚Ä¢ Otherwise, choose SHIFT.\n The restriction on selecting the RIGHTA RC operator is needed to ensure that a\n word is not popped from the stack, and thus lost to further processing, before all its\n dependents have been assigned to it.\n More formally, during training the oracle has access to the following:\n ‚Ä¢ A current configuration with a stack S and a set of dependency relations Rc\n ‚Ä¢ A reference parse consisting of a set of vertices V and a set of dependency\n relations R p\n Given this information, the oracle chooses transitions as follows:\n LEFTA RC (r): if (S1 r S2 ) ‚àà R p\n RIGHTA RC (r): if (S2 r S1 ) ‚àà R p and ‚àÄr0 , w s.t.(S1 r0 w) ‚àà R p then (S1 r0 w) ‚àà Rc\n SHIFT: otherwise\n\n Let‚Äôs walk through the processing of the following example as shown in Fig. 19.7.\n\n root\n\n obj nmod\n (19.9)\n det case\n\n Book the flight through Houston\n\n At Step 1, LEFTA RC is not applicable in the initial configuration since it asserts\n a relation, (root ‚Üê book), not in the reference answer; RIGHTA RC does assert a\n relation contained in the final answer (root ‚Üí book), however book has not been\n attached to any of its dependents yet, so we have to defer, leaving SHIFT as the only\n 19.2 ‚Ä¢ T RANSITION -BASED D EPENDENCY PARSING 11\n\n possible action. The same conditions hold in the next two steps. In step 3, LEFTA RC\n is selected to link the to its head.\n Now consider the situation in Step 4.\n\n Stack Word buffer Relations\n [root, book, flight] [through, Houston] (the ‚Üê flight)\n\n Here, we might be tempted to add a dependency relation between book and flight,\n which is present in the reference parse. But doing so now would prevent the later\n attachment of Houston since flight would have been removed from the stack. Fortunately, the precondition on choosing RIGHTA RC prevents this choice and we‚Äôre\n again left with SHIFT as the only viable option. The remaining choices complete the\n set of operators needed for this example.\n To recap, we derive appropriate training instances consisting of configurationtransition pairs from a treebank by simulating the operation of a parser in the context of a reference dependency tree. We can deterministically record correct parser\n actions at each step as we progress through each training example, thereby creating\n the training set we require.\n\n 19.2.2 A feature-based classifier\n We‚Äôll now introduce two classifiers for choosing transitions, here a classic featurebased algorithm and in the next section a neural classifier using embedding features.\n Featured-based classifiers generally use the same features we‚Äôve seen with partof-speech tagging and partial parsing: Word forms, lemmas, parts of speech, the\n head, and the dependency relation to the head. Other features may be relevant for\n some languages, for example morphosyntactic features like case marking on subjects\n or objects. The features are extracted from the training configurations, which consist\n of the stack, the buffer and the current set of relations. Most useful are features\n referencing the top levels of the stack, the words near the front of the buffer, and the\n dependency relations already associated with any of those elements.\n feature\ntemplate We‚Äôll use a feature template as we did for sentiment analysis and part-of-speech\n tagging. Feature templates allow us to automatically generate large numbers of specific features from a training set. For example, consider the following feature templates that are based on single positions in a configuration.\n\n hs1 .w, opi, hs2 .w, opihs1 .t, opi, hs2 .t, opi\n hb1 .w, opi, hb1 .t, opihs1 .wt, opi (19.10)\n\n Here features are denoted as location.property, where s = stack, b = the word\n buffer, w = word forms, t = part-of-speech, and op = operator. Thus the feature for\n the word form at the top of the stack would be s1 .w, the part of speech tag at the\n front of the buffer b1 .t, and the concatenated feature s1 .wt represents the word form\n concatenated with the part of speech of the word at the top of the stack. Consider\n applying these templates to the following intermediate configuration derived from a\n training oracle for (19.2).\n\n Stack Word buffer Relations\n [root, canceled, flights] [to Houston] (canceled ‚Üí United)\n (flights ‚Üí morning)\n (flights ‚Üí the)\n12 C HAPTER 19 ‚Ä¢ D EPENDENCY PARSING\n\n The correct transition here is SHIFT (you should convince yourself of this before\n proceeding). The application of our set of feature templates to this configuration\n would result in the following set of instantiated features.\n\n hs1 .w = flights, op = shifti (19.11)\n hs2 .w = canceled, op = shifti\n hs1 .t = NNS, op = shifti\n hs2 .t = VBD, op = shifti\n hb1 .w = to, op = shifti\n hb1 .t = TO, op = shifti\n hs1 .wt = flightsNNS, op = shifti\n\n Given that the left and right arc transitions operate on the top two elements of the\n stack, features that combine properties from these positions are even more useful.\n For example, a feature like s1 .t ‚ó¶ s2 .t concatenates the part of speech tag of the word\n at the top of the stack with the tag of the word beneath it.\n\n hs1 .t ‚ó¶ s2 .t = NNSVBD, op = shifti (19.12)\n\n Given the training data and features, any classifier, like multinomial logistic regression or support vector machines, can be used.\n\n 19.2.3 A neural classifier\n The oracle can also be implemented by a neural classifier. A standard architecture\n is simply to pass the sentence through an encoder, then take the presentation of the\n top 2 words on the stack and the first word of the buffer, concatenate them, and\n present to a feedforward network that predicts the transition to take (Kiperwasser\n and Goldberg, 2016; Kulmizev et al., 2019). Fig. 19.8 sketches this model. Learning\n can be done with cross-entropy loss.\n\n Input buÔ¨Äer\n Parser Oracle\n w ‚Ä¶\n w e(w) Dependency\n Action\n Relations\n Softmax\n\n s1 e(s1) FFN LEFTARC\n s1\n RIGHTARC w3 w2\n s2 e(s2)\n Stack s2 SHIFT\n\n ...\n\n ENCODER\n\n w1 w2 w3 w4 w5 w6\n\n the top 2 words on the stack and the first word of the buffer, represents them by their encodings\n (from running the whole sentence through the encoder), concatenates the embeddings and\n passes through a softmax to choose a parser action (transition).\n 19.2 ‚Ä¢ T RANSITION -BASED D EPENDENCY PARSING 13\n\n 19.2.4 Advanced Methods in Transition-Based Parsing\n The basic transition-based approach can be elaborated in a number of ways to improve performance by addressing some of the most obvious flaws in the approach.\n\n Alternative Transition Systems\n The arc-standard transition system described above is only one of many possible sysarc eager tems. A frequently used alternative is the arc eager transition system. The arc eager\n approach gets its name from its ability to assert rightward relations much sooner\n than in the arc standard approach. To see this, let‚Äôs revisit the arc standard trace of\n Example 19.9, repeated here.\n root\n\n obj nmod\n det case\n\n Book the flight through Houston\n Consider the dependency relation between book and flight in this analysis. As\n is shown in Fig. 19.7, an arc-standard approach would assert this relation at Step 8,\n despite the fact that book and flight first come together on the stack much earlier at\n Step 4. The reason this relation can‚Äôt be captured at this point is due to the presence\n of the postnominal modifier through Houston. In an arc-standard approach, dependents are removed from the stack as soon as they are assigned their heads. If flight\n had been assigned book as its head in Step 4, it would no longer be available to serve\n as the head of Houston.\n While this delay doesn‚Äôt cause any issues in this example, in general the longer\n a word has to wait to get assigned its head the more opportunities there are for\n something to go awry. The arc-eager system addresses this issue by allowing words\n to be attached to their heads as early as possible, before all the subsequent words\n dependent on them have been seen. This is accomplished through minor changes to\n the LEFTA RC and RIGHTA RC operators and the addition of a new REDUCE operator.\n ‚Ä¢ LEFTA RC: Assert a head-dependent relation between the word at the front of\n the input buffer and the word at the top of the stack; pop the stack.\n ‚Ä¢ RIGHTA RC: Assert a head-dependent relation between the word on the top of\n the stack and the word at the front of the input buffer; shift the word at the\n front of the input buffer to the stack.\n ‚Ä¢ SHIFT: Remove the word from the front of the input buffer and push it onto\n the stack.\n ‚Ä¢ REDUCE: Pop the stack.\n The LEFTA RC and RIGHTA RC operators are applied to the top of the stack and\n the front of the input buffer, instead of the top two elements of the stack as in the\n arc-standard approach. The RIGHTA RC operator now moves the dependent to the\n stack from the buffer rather than removing it, thus making it available to serve as the\n head of following words. The new REDUCE operator removes the top element from\n the stack. Together these changes permit a word to be eagerly assigned its head and\n still allow it to serve as the head for later dependents. The trace shown in Fig. 19.9\n illustrates the new decision sequence for this example.\n In addition to demonstrating the arc-eager transition system, this example demonstrates the power and flexibility of the overall transition-based approach. We were\n able to swap in a new transition system without having to make any changes to the\n14 C HAPTER 19 ‚Ä¢ D EPENDENCY PARSING\n\nStep Stack Word List Action Relation Added\n 0 [root] [book, the, flight, through, houston] RIGHTA RC (root ‚Üí book)\n 1 [root, book] [the, flight, through, houston] SHIFT\n 2 [root, book, the] [flight, through, houston] LEFTA RC (the ‚Üê flight)\n 3 [root, book] [flight, through, houston] RIGHTA RC (book ‚Üí flight)\n 4 [root, book, flight] [through, houston] SHIFT\n 5 [root, book, flight, through] [houston] LEFTA RC (through ‚Üê houston)\n 6 [root, book, flight] [houston] RIGHTA RC (flight ‚Üí houston)\n 7 [root, book, flight, houston] [] REDUCE\n 8 [root, book, flight] [] REDUCE\n 9 [root, book] [] REDUCE\n 10 [root] [] Done\n\n underlying parsing algorithm. This flexibility has led to the development of a diverse set of transition systems that address different aspects of syntax and semantics\n including: assigning part of speech tags (Choi and Palmer, 2011a), allowing the\n generation of non-projective dependency structures (Nivre, 2009), assigning semantic roles (Choi and Palmer, 2011b), and parsing texts containing multiple languages\n (Bhat et al., 2017).\n\n Beam Search\n The computational efficiency of the transition-based approach discussed earlier derives from the fact that it makes a single pass through the sentence, greedily making\n decisions without considering alternatives. Of course, this is also a weakness ‚Äì once\n a decision has been made it can not be undone, even in the face of overwhelming\n beam search evidence arriving later in a sentence. We can use beam search to explore alternative decision sequences. Recall from Chapter 8 that beam search uses a breadth-first\n search strategy with a heuristic filter that prunes the search frontier to stay within a\n beam width fixed-size beam width.\n In applying beam search to transition-based parsing, we‚Äôll elaborate on the algorithm given in Fig. 19.5. Instead of choosing the single best transition operator\n at each iteration, we‚Äôll apply all applicable operators to each state on an agenda and\n then score the resulting configurations. We then add each of these new configurations to the frontier, subject to the constraint that there has to be room within the\n beam. As long as the size of the agenda is within the specified beam width, we can\n add new configurations to the agenda. Once the agenda reaches the limit, we only\n add new configurations that are better than the worst configuration on the agenda\n (removing the worst element so that we stay within the limit). Finally, to insure that\n we retrieve the best possible state on the agenda, the while loop continues as long as\n there are non-final states on the agenda.\n The beam search approach requires a more elaborate notion of scoring than we\n used with the greedy algorithm. There, we assumed that the oracle would be a\n supervised classifier that chose the best transition operator based on features of the\n current configuration. This choice can be viewed as assigning a score to all the\n possible transitions and picking the best one.\n\n TÃÇ (c) = argmax Score(t, c)\n\n With beam search we are now searching through the space of decision sequences,\n so it makes sense to base the score for a configuration on its entire history. So we\n can define the score for a new configuration as the score of its predecessor plus the\n 19.3 ‚Ä¢ G RAPH -BASED D EPENDENCY PARSING 15\n\n score of the operator used to produce it.\n ConfigScore(c0 ) = 0.0\n ConfigScore(ci ) = ConfigScore(ci‚àí1 ) + Score(ti , ci‚àí1 )\n This score is used both in filtering the agenda and in selecting the final answer. The\n new beam search version of transition-based parsing is given in Fig. 19.10.\n\n function D EPENDENCY B EAM PARSE(words, width) returns dependency tree\n\n state ‚Üê {[root], [words], [], 0.0} ;initial configuration\n agenda ‚Üê hstatei ;initial agenda\n\n while agenda contains non-final states\n newagenda ‚Üê hi\n for each state ‚àà agenda do\n for all {t | t ‚àà VALID O PERATORS(state)} do\n child ‚Üê A PPLY(t, state)\n newagenda ‚Üê A DD T O B EAM(child, newagenda, width)\n agenda ‚Üê newagenda\n return B EST O F(agenda)\n\n function A DD T O B EAM(state, agenda, width) returns updated agenda\n\n if L ENGTH(agenda) < width then\n agenda ‚Üê I NSERT(state, agenda)\n else if S CORE(state) > S CORE(W ORST O F(agenda))\n agenda ‚Üê R EMOVE(W ORST O F(agenda))\n agenda ‚Üê I NSERT(state, agenda)\n return agenda\n\n Graph-based methods are the second important family of dependency parsing algorithms. Graph-based parsers are more accurate than transition-based parsers, especially on long sentences; transition-based methods have trouble when the heads are\n very far from the dependents (McDonald and Nivre, 2011). Graph-based methods\n avoid this difficulty by scoring entire trees, rather than relying on greedy local decisions. Furthermore, unlike transition-based approaches, graph-based parsers can\n produce non-projective trees. Although projectivity is not a significant issue for\n English, it is definitely a problem for many of the world‚Äôs languages.\n Graph-based dependency parsers search through the space of possible trees for a\n given sentence for a tree (or trees) that maximize some score. These methods encode\n the search space as directed graphs and employ methods drawn from graph theory\n to search the space for optimal solutions. More formally, given a sentence S we‚Äôre\n looking for the best dependency tree in Gs , the space of all possible trees for that\n sentence, that maximizes some score.\n TÃÇ (S) = argmax Score(t, S)\n t‚ààGS\n16 C HAPTER 19 ‚Ä¢ D EPENDENCY PARSING\n\n edge-factored We‚Äôll make the simplifying assumption that this score can be edge-factored,\n meaning that the overall score for a tree is the sum of the scores of each of the scores\n of the edges that comprise the tree.\n X\n Score(t, S) = Score(e)\n e‚ààt\n\n Graph-based algorithms have to solve two problems: (1) assigning a score to\n each edge, and (2) finding the best parse tree given the scores of all potential edges.\n In the next few sections we‚Äôll introduce solutions to these two problems, beginning\n with the second problem of finding trees, and then giving a feature-based and a\n neural algorithm for solving the first problem of assigning scores.\n\n 19.3.1 Parsing via finding the maximum spanning tree\n In graph-based parsing, given a sentence S we start by creating a graph G which is a\n fully-connected, weighted, directed graph where the vertices are the input words and\n the directed edges represent all possible head-dependent assignments. We‚Äôll include\n an additional ROOT node with outgoing edges directed at all of the other vertices.\n The weights of each edge in G reflect the score for each possible head-dependent\n relation assigned by some scoring algorithm.\n It turns out that finding the best dependency parse for S is equivalent to finding\n maximum\n spanning tree the maximum spanning tree over G. A spanning tree over a graph G is a subset\n of G that is a tree and covers all the vertices in G; a spanning tree over G that starts\n from the ROOT is a valid parse of S. A maximum spanning tree is the spanning tree\n with the highest score. Thus a maximum spanning tree of G emanating from the\n ROOT is the optimal dependency parse for the sentence.\n A directed graph for the example Book that flight is shown in Fig. 19.11, with the\n maximum spanning tree corresponding to the desired parse shown in blue. For ease\n of exposition, we‚Äôll describe here the algorithm for unlabeled dependency parsing.\n\n 5 8\n root Book that flight\n 6 7\n\n Before describing the algorithm it‚Äôs useful to consider two intuitions about directed graphs and their spanning trees. The first intuition begins with the fact that\n every vertex in a spanning tree has exactly one incoming edge. It follows from this\n that every connected component of a spanning tree (i.e., every set of vertices that\n are linked to each other by paths over edges) will also have one incoming edge.\n The second intuition is that the absolute values of the edge scores are not critical\n to determining its maximum spanning tree. Instead, it is the relative weights of the\n edges entering each vertex that matters. If we were to subtract a constant amount\n from each edge entering a given vertex it would have no impact on the choice of\n 19.3 ‚Ä¢ G RAPH -BASED D EPENDENCY PARSING 17\n\nthe maximum spanning tree since every possible spanning tree would decrease by\nexactly the same amount.\n The first step of the algorithm itself is quite straightforward. For each vertex\nin the graph, an incoming edge (representing a possible head assignment) with the\nhighest score is chosen. If the resulting set of edges produces a spanning tree then\nwe‚Äôre done. More formally, given the original fully-connected graph G = (V, E), a\nsubgraph T = (V, F) is a spanning tree if it has no cycles and each vertex (other than\nthe root) has exactly one edge entering it. If the greedy selection process produces\nsuch a tree then it is the best possible one.\n Unfortunately, this approach doesn‚Äôt always lead to a tree since the set of edges\nselected may contain cycles. Fortunately, in yet another case of multiple discovery,\nthere is a straightforward way to eliminate cycles generated during the greedy selection phase. Chu and Liu (1965) and Edmonds (1967) independently developed\nan approach that begins with greedy selection and follows with an elegant recursive\ncleanup phase that eliminates cycles.\n The cleanup phase begins by adjusting all the weights in the graph by subtracting\nthe score of the maximum edge entering each vertex from the score of all the edges\nentering that vertex. This is where the intuitions mentioned earlier come into play.\nWe have scaled the values of the edges so that the weights of the edges in the cycle\nhave no bearing on the weight of any of the possible spanning trees. Subtracting the\nvalue of the edge with maximum weight from each edge entering a vertex results\nin a weight of zero for all of the edges selected during the greedy selection phase,\nincluding all of the edges involved in the cycle.\n Having adjusted the weights, the algorithm creates a new graph by selecting a\ncycle and collapsing it into a single new node. Edges that enter or leave the cycle\nare altered so that they now enter or leave the newly collapsed node. Edges that do\nnot touch the cycle are included and edges within the cycle are dropped.\n Now, if we knew the maximum spanning tree of this new graph, we would have\nwhat we need to eliminate the cycle. The edge of the maximum spanning tree directed towards the vertex representing the collapsed cycle tells us which edge to\ndelete in order to eliminate the cycle. How do we find the maximum spanning tree\nof this new graph? We recursively apply the algorithm to the new graph. This will\neither result in a spanning tree or a graph with a cycle. The recursions can continue\nas long as cycles are encountered. When each recursion completes we expand the\ncollapsed vertex, restoring all the vertices and edges from the cycle with the exception of the single edge to be deleted.\n Putting all this together, the maximum spanning tree algorithm consists of greedy\nedge selection, re-scoring of edge costs and a recursive cleanup phase when needed.\nThe full algorithm is shown in Fig. 19.12.\n Fig. 19.13 steps through the algorithm with our Book that flight example. The\nfirst row of the figure illustrates greedy edge selection with the edges chosen shown\nin blue (corresponding to the set F in the algorithm). This results in a cycle between\nthat and flight. The scaled weights using the maximum value entering each node are\nshown in the graph to the right.\n Collapsing the cycle between that and flight to a single node (labelled tf) and\nrecursing with the newly scaled costs is shown in the second row. The greedy selection step in this recursion yields a spanning tree that links root to book, as well as an\nedge that links book to the contracted node. Expanding the contracted node, we can\nsee that this edge corresponds to the edge from book to flight in the original graph.\nThis in turn tells us which edge to drop to eliminate the cycle.\n18 C HAPTER 19 ‚Ä¢ D EPENDENCY PARSING\n\n function M AX S PANNING T REE(G=(V,E), root, score) returns spanning tree\n\n F ‚Üê []\n T‚Äô ‚Üê []\n score‚Äô ‚Üê []\n for each v ‚àà V do\n bestInEdge ‚Üê argmaxe=(u,v)‚àà E score[e]\n F ‚Üê F ‚à™ bestInEdge\n for each e=(u,v) ‚àà E do\n score‚Äô[e] ‚Üê score[e] ‚àí score[bestInEdge]\n\n if T=(V,F) is a spanning tree then return it\n else\n C ‚Üê a cycle in F\n G‚Äô ‚Üê C ONTRACT(G, C)\n T‚Äô ‚Üê M AX S PANNING T REE(G‚Äô, root, score‚Äô)\n T ‚Üê E XPAND(T‚Äô, C)\n return T\n\n function C ONTRACT(G, C) returns contracted graph\n\n function E XPAND(T, C) returns expanded graph\n\n weighted directed graph.\n\n On arbitrary directed graphs, this version of the CLE algorithm runs in O(mn)\n time, where m is the number of edges and n is the number of nodes. Since this particular application of the algorithm begins by constructing a fully connected graph\n m = n2 yielding a running time of O(n3 ). Gabow et al. (1986) present a more efficient implementation with a running time of O(m + nlogn).\n\n 19.3.2 A feature-based algorithm for assigning scores\n Recall that given a sentence, S, and a candidate tree, T , edge-factored parsing models\n make the simplification that the score for the tree is the sum of the scores of the edges\n that comprise the tree:\n X\n score(S, T ) = score(S, e)\n e‚ààT\n\n In a feature-based algorithm we compute the edge score as a weighted sum of features extracted from it:\n N\n X\n score(S, e) = wi fi (S, e)\n i=1\n\n Or more succinctly.\n\n score(S, e) = w ¬∑ f\n\n Given this formulation, we need to identify relevant features and train the weights.\n The features (and feature combinations) used to train edge-factored models mirror those used in training transition-based parsers, such as\n 19.3 ‚Ä¢ G RAPH -BASED D EPENDENCY PARSING 19\n\n -4\n 4 -3\n 12 0\n\n 5 8 -2 0\n Book that flight Book that flight\n root root\n 12 7 8 12 -6 7 8\n 6 7 0\n\n 7 -1\n 5 -7\n\n -4 -4\n\n -3 -3\n 0 0\n -2 -2\n Book tf\n root Book -6 tf root -6\n 0 -1\n -1 -1\n -7 -7\n\n Deleted from cycle\n\n root Book that flight\n\n ‚Ä¢ Wordforms, lemmas, and parts of speech of the headword and its dependent.\n ‚Ä¢ Corresponding features from the contexts before, after and between the words.\n ‚Ä¢ Word embeddings.\n ‚Ä¢ The dependency relation itself.\n ‚Ä¢ The direction of the relation (to the right or left).\n ‚Ä¢ The distance from the head to the dependent.\n\n Given a set of features, our next problem is to learn a set of weights corresponding to each. Unlike many of the learning problems discussed in earlier chapters,\n here we are not training a model to associate training items with class labels, or\n parser actions. Instead, we seek to train a model that assigns higher scores to correct trees than to incorrect ones. An effective framework for problems like this is to\ninference-based\n learning use inference-based learning combined with the perceptron learning rule. In this\n framework, we parse a sentence (i.e, perform inference) from the training set using\n some initially random set of initial weights. If the resulting parse matches the corresponding tree in the training data, we do nothing to the weights. Otherwise, we\n find those features in the incorrect parse that are not present in the reference parse\n and we lower their weights by a small amount based on the learning rate. We do this\n incrementally for each sentence in our training data until the weights converge.\n20 C HAPTER 19 ‚Ä¢ D EPENDENCY PARSING\n\n 19.3.3 A neural algorithm for assigning scores\n State-of-the-art graph-based multilingual parsers are based on neural networks. Instead of extracting hand-designed features to represent each edge between words wi\n and w j , these parsers run the sentence through an encoder, and then pass the encoded\n representation of the two words wi and w j through a network that estimates a score\n for the edge i ‚Üí j.\n\n score(h1head, h3dep)\n\n ‚àë\n BiaÔ¨Éne\n\n U W b\n h1 head h1 dep h2 head h2 dep h3 head h3 dep\n\n FFN FFN FFN FFN FFN FFN\n head dep head dep head dep\n\n r1 r2 r3\n\n ENCODER\n\n book that flight\n Dozat and Manning (2017); Dozat et al. (2017). The parser uses distinct feedforward networks to turn the encoder output for each word into a head and dependent representation for\n the word. The biaffine function turns the head embedding of the head and the dependent\n embedding of the dependent into a score for the dependency edge.\n\n Here we‚Äôll sketch the biaffine algorithm of Dozat and Manning (2017) and Dozat\n et al. (2017) shown in Fig. 19.14, drawing on the work of GruÃànewald et al. (2021)\n who tested many versions of the algorithm via their STEPS system. The algorithm\n first runs the sentence X = x1 , ..., xn through an encoder to produce a contextual\n embedding representation for each token R = r1 , ..., rn . The embedding for each\n token is now passed through two separate feedforward networks, one to produce a\n representation of this token as a head, and one to produce a representation of this\n token as a dependent:\n\n hhead\n i = FFNhead (ri ) (19.13)\n hdep\n i = FFNdep (ri ) (19.14)\n\n Now to assign a score to the directed edge i ‚Üí j, (wi is the head and w j is the dependent), we feed the head representation of i, hhead\n i , and the dependent representation\n of j, hdep\n j , into a biaffine scoring function:\n\n Score(i ‚Üí j) = Biaff(hhead\n i , hdep\n j ) (19.15)\n Biaff(x, y) = x Uy + W(x ‚äï y) + b (19.16)\n 19.4 ‚Ä¢ E VALUATION 21\n\n where U, W, and b are weights learned by the model. The idea of using a biaffine\n function is to allow the system to learn multiplicative interactions between the vectors x and y.\n If we pass Score(i ‚Üí j) through a softmax, we end up with a probability distribution, for each token j, over potential heads i (all other tokens in the sentence):\n\n p(i ‚Üí j) = softmax([Score(k ‚Üí j); ‚àÄk 6= j, 1 ‚â§ k ‚â§ n]) (19.17)\n\n This probability can then be passed to the maximum spanning tree algorithm of\n Section 19.3.1 to find the best tree.\n This p(i ‚Üí j) classifier is trained by optimizing the cross-entropy loss.\n Note that the algorithm as we‚Äôve described it is unlabeled. To make this into\n a labeled algorithm, the Dozat and Manning (2017) algorithm actually trains two\n classifiers. The first classifier, the edge-scorer, the one we described above, assigns\n a probability p(i ‚Üí j) to each word wi and w j . Then the Maximum Spanning Tree\n algorithm is run to get a single best dependency parse tree for the second. We then\n apply a second classifier, the label-scorer, whose job is to find the maximum probability label for each edge in this parse. This second classifier has the same form\n as (19.15-19.17), but instead of being trained to predict with binary softmax the\n probability of an edge existing between two words, it is trained with a softmax over\n dependency labels to predict the dependency label between the words.\n\n As with phrase structure-based parsing, the evaluation of dependency parsers proceeds by measuring how well they work on a test set. An obvious metric would be\n exact match (EM)‚Äîhow many sentences are parsed correctly. This metric is quite\n pessimistic, with most sentences being marked wrong. Such measures are not finegrained enough to guide the development process. Our metrics need to be sensitive\n enough to tell if actual improvements are being made.\n For these reasons, the most common method for evaluating dependency parsers\n are labeled and unlabeled attachment accuracy. Labeled attachment refers to the\n proper assignment of a word to its head along with the correct dependency relation.\n Unlabeled attachment simply looks at the correctness of the assigned head, ignoring the dependency relation. Given a system output and a corresponding reference\n parse, accuracy is simply the percentage of words in an input that are assigned the\n correct head with the correct relation. These metrics are usually referred to as the\n labeled attachment score (LAS) and unlabeled attachment score (UAS). Finally, we\n can make use of a label accuracy score (LS), the percentage of tokens with correct\n labels, ignoring where the relations are coming from.\n As an example, consider the reference parse and system parse for the following\n example shown in Fig. 19.15.\n (19.18) Book me the flight through Houston.\n The system correctly finds 4 of the 6 dependency relations present in the reference\n parse and receives an LAS of 2/3. However, one of the 2 incorrect relations found\n by the system holds between book and flight, which are in a head-dependent relation\n in the reference parse; the system therefore achieves a UAS of 5/6.\n Beyond attachment scores, we may also be interested in how well a system is\n performing on a particular kind of dependency relation, for example NSUBJ, across\n22 C HAPTER 19 ‚Ä¢ D EPENDENCY PARSING\n\n root root\n obj xcomp\n nmod nsubj nmod\n iobj det case det case\n\n Book me the flight through Houston Book me the flight through Houston\n (a) Reference (b) System\n\n2/3 and an UAS of 5/6.\n\n a development corpus. Here we can make use of the notions of precision and recall\n introduced in Chapter 17, measuring the percentage of relations labeled NSUBJ by\n the system that were correct (precision), and the percentage of the NSUBJ relations\n present in the development set that were in fact discovered by the system (recall).\n We can employ a confusion matrix to keep track of how often each dependency type\n was confused for another.\n\n This chapter has introduced the concept of dependency grammars and dependency\n parsing. Here‚Äôs a summary of the main points that we covered:\n\n ‚Ä¢ In dependency-based approaches to syntax, the structure of a sentence is described in terms of a set of binary relations that hold between the words in a\n sentence. Larger notions of constituency are not directly encoded in dependency analyses.\n ‚Ä¢ The relations in a dependency structure capture the head-dependent relationship among the words in a sentence.\n ‚Ä¢ Dependency-based analysis provides information directly useful in further\n language processing tasks including information extraction, semantic parsing\n and question answering.\n ‚Ä¢ Transition-based parsing systems employ a greedy stack-based algorithm to\n create dependency structures.\n ‚Ä¢ Graph-based methods for creating dependency structures are based on the use\n of maximum spanning tree methods from graph theory.\n ‚Ä¢ Both transition-based and graph-based approaches are developed using supervised machine learning techniques.\n ‚Ä¢ Treebanks provide the data needed to train these systems. Dependency treebanks can be created directly by human annotators or via automatic transformation from phrase-structure treebanks.\n ‚Ä¢ Evaluation of dependency parsers is based on labeled and unlabeled accuracy\n scores as measured against withheld development and test corpora.\n H ISTORICAL N OTES 23\n\nHistorical Notes\n The dependency-based approach to grammar is much older than the relatively recent\n phrase-structure or constituency grammars, which date only to the 20th century. Dependency grammar dates back to the Indian grammarian PaÃÑn.ini sometime between\n the 7th and 4th centuries BCE, as well as the ancient Greek linguistic traditions.\n Contemporary theories of dependency grammar all draw heavily on the 20th century work of TesnieÃÄre (1959).\n Automatic parsing using dependency grammars was first introduced into computational linguistics by early work on machine translation at the RAND Corporation\n led by David Hays. This work on dependency parsing closely paralleled work on\n constituent parsing and made explicit use of grammars to guide the parsing process.\n After this early period, computational work on dependency parsing remained intermittent over the following decades. Notable implementations of dependency parsers\n for English during this period include Link Grammar (Sleator and Temperley, 1993),\n Constraint Grammar (Karlsson et al., 1995), and MINIPAR (Lin, 2003).\n Dependency parsing saw a major resurgence in the late 1990‚Äôs with the appearance of large dependency-based treebanks and the associated advent of data driven\n approaches described in this chapter. Eisner (1996) developed an efficient dynamic\n programming approach to dependency parsing based on bilexical grammars derived\n from the Penn Treebank. Covington (2001) introduced the deterministic word by\n word approach underlying current transition-based approaches. Yamada and Matsumoto (2003) and Kudo and Matsumoto (2002) introduced both the shift-reduce\n paradigm and the use of supervised machine learning in the form of support vector\n machines to dependency parsing.\n Transition-based parsing is based on the shift-reduce parsing algorithm originally developed for analyzing programming languages (Aho and Ullman, 1972).\n Shift-reduce parsing also makes use of a context-free grammar. Input tokens are\n successively shifted onto the stack and the top two elements of the stack are matched\n against the right-hand side of the rules in the grammar; when a match is found the\n matched elements are replaced on the stack (reduced) by the non-terminal from the\n left-hand side of the rule being matched. In transition-based dependency parsing\n we skip the grammar, and alter the reduce operation to add a dependency relation\n between a word and its head.\n Nivre (2003) defined the modern, deterministic, transition-based approach to\n dependency parsing. Subsequent work by Nivre and his colleagues formalized and\n analyzed the performance of numerous transition systems, training methods, and\n methods for dealing with non-projective language (Nivre and Scholz 2004, Nivre\n 2006, Nivre and Nilsson 2005, Nivre et al. 2007b, Nivre 2007). The neural approach was pioneered by Chen and Manning (2014) and extended by Kiperwasser\n and Goldberg (2016); Kulmizev et al. (2019).\n The graph-based maximum spanning tree approach to dependency parsing was\n introduced by McDonald et al. 2005a, McDonald et al. 2005b. The neural classifier\n was introduced by (Kiperwasser and Goldberg, 2016).\n The long-running Prague Dependency Treebank project (HajicÃå, 1998) is the most\n significant effort to directly annotate a corpus with multiple layers of morphological,\n syntactic and semantic information. PDT 3.0 contains over 1.5 M tokens (BejcÃåek\n et al., 2013).\n Universal Dependencies (UD) (de Marneffe et al., 2021) is an open community\n24 C HAPTER 19 ‚Ä¢ D EPENDENCY PARSING\n\n project to create a framework for dependency treebank annotation, with nearly 200\n treebanks in over 100 languages. The UD annotation scheme evolved out of several\n distinct efforts including Stanford dependencies (de Marneffe et al. 2006, de Marneffe and Manning 2008, de Marneffe et al. 2014), Google‚Äôs universal part-of-speech\n tags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic tagsets\n (Zeman, 2008).\n The Conference on Natural Language Learning (CoNLL) has conducted an influential series of shared tasks related to dependency parsing over the years (Buchholz and Marsi 2006, Nivre et al. 2007a, Surdeanu et al. 2008, HajicÃå et al. 2009).\n More recent evaluations have focused on parser robustness with respect to morphologically rich languages (Seddah et al., 2013), and non-canonical language forms\n such as social media, texts, and spoken language (Petrov and McDonald, 2012).\n Choi et al. (2015) presents a performance analysis of 10 dependency parsers across\n a range of metrics, as well as DEPENDA BLE, a robust parser evaluation tool.\n\nExercises\n Exercises 25\n\nAho, A. V. and J. D. Ullman. 1972. The Theory of Parsing, HajicÃå, J., M. Ciaramita, R. Johansson, D. Kawahara, M. A.\n Translation, and Compiling, volume 1. Prentice Hall. Martƒ±ÃÅ, L. MaÃÄrquez, A. Meyers, J. Nivre, S. PadoÃÅ,\nBejcÃåek, E., E. HajicÃåovaÃÅ, J. HajicÃå, P. Jƒ±ÃÅnovaÃÅ, V. KettnerovaÃÅ, J. SÃåteÃåpaÃÅnek, P. StranaÃåkÃÅ, M. Surdeanu, N. Xue, and\n V. KolaÃÅrÃåovaÃÅ, M. MikulovaÃÅ, J. Mƒ±ÃÅrovskyÃÅ, A. Nedoluzhko, Y. Zhang. 2009. The conll-2009 shared task: Syntac-\nJ. PanevovaÃÅ, L. PolaÃÅkovaÃÅ, M. SÃåevcÃåƒ±ÃÅkovaÃÅ, J. SÃåteÃåpaÃÅnek, tic and semantic dependencies in multiple languages.\n and SÃå. ZikaÃÅnovaÃÅ. 2013. Prague dependency treebank CoNLL.\n 3.0. Technical report, Institute of Formal and Ap- Karlsson, F., A. Voutilainen, J. HeikkilaÃà, and A. Anttila, eds.\n plied Linguistics, Charles University in Prague. LIN- 1995. Constraint Grammar: A Language-Independent\n DAT/CLARIN digital library at Institute of Formal and System for Parsing Unrestricted Text. Mouton de Gruyter.\n Applied Linguistics, Charles University in Prague. Kiperwasser, E. and Y. Goldberg. 2016. Simple and accu-\nBhat, I., R. A. Bhat, M. Shrivastava, and D. Sharma. 2017. rate dependency parsing using bidirectional LSTM fea-\nJoining hands: Exploiting monolingual treebanks for ture representations. TACL, 4:313‚Äì327.\n parsing of code-mixing data. EACL. Kudo, T. and Y. Matsumoto. 2002. Japanese dependency\nBuchholz, S. and E. Marsi. 2006. Conll-x shared task on analysis using cascaded chunking. CoNLL.\n multilingual dependency parsing. CoNLL. Kulmizev, A., M. de Lhoneux, J. Gontrum, E. Fano, and\nChen, D. and C. Manning. 2014. A fast and accurate depen- J. Nivre. 2019. Deep contextualized word embeddings\n dency parser using neural networks. EMNLP. in transition-based and graph-based dependency parsing\nChoi, J. D. and M. Palmer. 2011a. Getting the most out of - a tale of two parsers revisited. EMNLP.\n transition-based dependency parsing. ACL. Lin, D. 2003. Dependency-based evaluation of minipar.\nChoi, J. D. and M. Palmer. 2011b. Transition-based semantic Workshop on the Evaluation of Parsing Systems.\n role labeling using predicate argument clustering. Pro- de Marneffe, M.-C., T. Dozat, N. Silveira, K. Haverinen,\n ceedings of the ACL 2011 Workshop on Relational Mod- F. Ginter, J. Nivre, and C. D. Manning. 2014. Univerels of Semantics. sal Stanford dependencies: A cross-linguistic typology.\nChoi, J. D., J. Tetreault, and A. Stent. 2015. It depends: LREC.\n Dependency parser comparison using a web-based evalu- de Marneffe, M.-C., B. MacCartney, and C. D. Manning.\n ation tool. ACL. 2006. Generating typed dependency parses from phrase\nChu, Y.-J. and T.-H. Liu. 1965. On the shortest arborescence structure parses. LREC.\n of a directed graph. Science Sinica, 14:1396‚Äì1400. de Marneffe, M.-C. and C. D. Manning. 2008. The Stanford\nCovington, M. 2001. A fundamental algorithm for depen- typed dependencies representation. COLING Workshop\n dency parsing. Proceedings of the 39th Annual ACM on Cross-Framework and Cross-Domain Parser Evalua-\nSoutheast Conference. tion.\nDozat, T. and C. D. Manning. 2017. Deep biaffine attention de Marneffe, M.-C., C. D. Manning, J. Nivre, and D. Zeman.\n for neural dependency parsing. ICLR. 2021. Universal Dependencies. Computational Linguistics, 47(2):255‚Äì308.\nDozat, T. and C. D. Manning. 2018. Simpler but more accurate semantic dependency parsing. ACL. McDonald, R., K. Crammer, and F. C. N. Pereira. 2005a. Online large-margin training of dependency parsers. ACL.\nDozat, T., P. Qi, and C. D. Manning. 2017. Stanford‚Äôs\n graph-based neural dependency parser at the CoNLL McDonald, R. and J. Nivre. 2011. Analyzing and inte-\n2017 shared task. Proceedings of the CoNLL 2017 Shared grating dependency parsers. Computational Linguistics,\n Task: Multilingual Parsing from Raw Text to Universal 37(1):197‚Äì230.\n Dependencies. McDonald, R., F. C. N. Pereira, K. Ribarov, and J. HajicÃå.\nEdmonds, J. 1967. Optimum branchings. Journal of Re- 2005b. Non-projective dependency parsing using spansearch of the National Bureau of Standards B, 71(4):233‚Äì ning tree algorithms. HLT-EMNLP.\n 240. Nivre, J. 2007. Incremental non-projective dependency pars-\nEisner, J. 1996. Three new probabilistic models for depen- ing. NAACL-HLT.\n dency parsing: An exploration. COLING. Nivre, J. 2003. An efficient algorithm for projective depen-\nGabow, H. N., Z. Galil, T. Spencer, and R. E. Tarjan. dency parsing. Proceedings of the 8th International Work-\n1986. Efficient algorithms for finding minimum spanning shop on Parsing Technologies (IWPT).\n trees in undirected and directed graphs. Combinatorica, Nivre, J. 2006. Inductive Dependency Parsing. Springer.\n 6(2):109‚Äì122. Nivre, J. 2009. Non-projective dependency parsing in ex-\nGruÃànewald, S., A. Friedrich, and J. Kuhn. 2021. Applying pected linear time. ACL IJCNLP.\n Occam‚Äôs razor to transformer-based dependency parsing: Nivre, J., J. Hall, S. KuÃàbler, R. McDonald, J. Nilsson,\n What works, what doesn‚Äôt, and what is really necessary. S. Riedel, and D. Yuret. 2007a. The conll 2007 shared\n IWPT. task on dependency parsing. EMNLP/CoNLL.\nHajicÃå, J. 1998. Building a Syntactically Annotated Corpus: Nivre, J., J. Hall, J. Nilsson, A. Chanev, G. Eryigit,\n The Prague Dependency Treebank, 106‚Äì132. Karolinum. S. KuÃàbler, S. Marinov, and E. Marsi. 2007b. Maltparser: A language-independent system for data-driven\n dependency parsing. Natural Language Engineering,\n 13(02):95‚Äì135.\n26 Chapter 19 ‚Ä¢ Dependency Parsing\n\nNivre, J. and J. Nilsson. 2005. Pseudo-projective dependency\n parsing. ACL.\nNivre, J. and M. Scholz. 2004. Deterministic dependency\n parsing of english text. COLING.\nPetrov, S., D. Das, and R. McDonald. 2012. A universal\n part-of-speech tagset. LREC.\nPetrov, S. and R. McDonald. 2012. Overview of the 2012\n shared task on parsing the web. Notes of the First Workshop on Syntactic Analysis of Non-Canonical Language\n (SANCL), volume 59.\nSeddah, D., R. Tsarfaty, S. KuÃàbler, M. Candito, J. D. Choi,\n R. Farkas, J. Foster, I. Goenaga, K. Gojenola, Y. Goldberg, S. Green, N. Habash, M. Kuhlmann, W. Maier,\n J. Nivre, A. PrzepioÃÅrkowski, R. Roth, W. Seeker, Y. Versley, V. Vincze, M. WolinÃÅski, A. WroÃÅblewska, and E. Villemonte de la CleÃÅrgerie. 2013. Overview of the SPMRL\n 2013 shared task: cross-framework evaluation of parsing\n morphologically rich languages. 4th Workshop on Statistical Parsing of Morphologically-Rich Languages.\nSleator, D. and D. Temperley. 1993. Parsing English with a\n link grammar. IWPT-93.\nSurdeanu, M., R. Johansson, A. Meyers, L. MaÃÄrquez, and\n J. Nivre. 2008. The CoNLL 2008 shared task on joint\n parsing of syntactic and semantic dependencies. CoNLL.\nTesnieÃÄre, L. 1959. EÃÅleÃÅments de Syntaxe Structurale. Librairie\n C. Klincksieck, Paris.\nYamada, H. and Y. Matsumoto. 2003. Statistical dependency\n analysis with support vector machines. IWPT-03.\nZeman, D. 2008. Reusable tagset conversion using tagset\n drivers. LREC.\n",
    "file_path": "/Users/colinsidberry/nlp-textbook-rag/data/extracted_text/19.Dependency Parsing.txt",
    "file_size_kb": 66.33
  },
  {
    "id": "a455ad9c260713b7",
    "source": "nlp_textbook",
    "chapter": "Information Extraction: 20 Relations, Events, and Time Time will explain. Jane Austen, Persuasion",
    "filename": "20.Information Extraction- Relations, Events, and Time.txt",
    "content": "rights reserved. Draft of August 24, 2025.\n\nCHAPTER\n\n Information Extraction:\n20 Relations, Events, and Time\n Time will explain.\n Jane Austen, Persuasion\n\n Imagine that you are an analyst with an investment firm that tracks airline stocks.\n You‚Äôre given the task of determining the relationship (if any) between airline announcements of fare increases and the behavior of their stocks the next day. Historical data about stock prices is easy to come by, but what about the airline announcements? You will need to know at least the name of the airline, the nature of\n the proposed fare hike, the dates of the announcement, and possibly the response of\n other airlines. Fortunately, these can be all found in news articles like this one:\n Citing high fuel prices, United Airlines said Friday it has increased fares\n by $6 per round trip on flights to some cities also served by lowercost carriers. American Airlines, a unit of AMR Corp., immediately\n matched the move, spokesman Tim Wagner said. United, a unit of UAL\n Corp., said the increase took effect Thursday and applies to most routes\n where it competes against discount carriers, such as Chicago to Dallas\n and Denver to San Francisco.\n This chapter presents techniques for extracting limited kinds of semantic coninformation tent from text. This process of information extraction (IE) turns the unstructured\n extraction\n information embedded in texts into structured data, for example for populating a\n relational database to enable further processing.\n relation We begin with the task of relation extraction: finding and classifying semantic\n extraction\n relations among entities mentioned in a text, like child-of (X is the child-of Y), or\n part-whole or geospatial relations. Relation extraction has close links to populatknowledge\n graphs ing a relational database, and knowledge graphs, datasets of structured relational\n knowledge, are a useful way for search engines to present information to users.\n event Next, we discuss event extraction, the task of finding events in which these enextraction\n tities participate, like, in our sample text, the fare increases by United and American\n and the reporting events said and cite. Events are also situated in time, occurring at\n a particular date or time, and events can be related temporally, happening before or\n after or simultaneously with each other. We‚Äôll need to recognize temporal expressions like Friday, Thursday or two days from now and times such as 3:30 P.M., and\n normalize them onto specific calendar dates or times. We‚Äôll need to link Friday to\n the time of United‚Äôs announcement, Thursday to the previous day‚Äôs fare increase,\n and we‚Äôll need to produce a timeline in which United‚Äôs announcement follows the\n fare increase and American‚Äôs announcement follows both of those events.\ntemplate filling The related task of template filling is to find recurring stereotypical events or\n situations in documents and fill in the template slots. These slot-fillers may consist\n of text segments extracted directly from the text, or concepts like times, amounts, or\n ontology entities that have been inferred through additional processing. Our airline\n2 C HAPTER 20 ‚Ä¢ I NFORMATION E XTRACTION : R ELATIONS , E VENTS , AND T IME\n\n PERSON- GENERAL PART-\nPHYSICAL\n SOCIAL AFFILIATION WHOLE\n\n Lasting Subsidiary\n Family Near Citizen-\nPersonal Resident- Geographical\n Located Ethnicity- Org-Location-\nBusiness\n Religion Origin\n\n ORG\n AFFILIATION ARTIFACT\n Investor\n Founder\n Student-Alum\n Ownership User-Owner-Inventor-\nEmployment Manufacturer\n Membership\n Sports-Affiliation\n\n text presents such a stereotypical situation since airlines often raise fares and then\n wait to see if competitors follow along. Here we can identify United as a lead airline that initially raised its fares, $6 as the amount, Thursday as the increase date,\n and American as an airline that followed along, leading to a filled template like the\n following:\n Ô£Æ Ô£π\n FARE -R AISE ATTEMPT: L EAD A IRLINE : U NITED A IRLINES\n Ô£ØA MOUNT: $6 Ô£∫\n Ô£Ø Ô£∫\n Ô£∞E FFECTIVE DATE : 2006-10-26\n Ô£Ø Ô£∫\n Ô£ª\n F OLLOWER : A MERICAN A IRLINES\n\n Let‚Äôs assume that we have detected the named entities in our sample text (perhaps\n using the techniques of Chapter 17), and would like to discern the relationships that\n exist among the detected entities:\n Citing high fuel prices, [ORG United Airlines] said [TIME Friday] it\n has increased fares by [MONEY $6] per round trip on flights to some\n cities also served by lower-cost carriers. [ORG American Airlines], a\n unit of [ORG AMR Corp.], immediately matched the move, spokesman\n [PER Tim Wagner] said. [ORG United], a unit of [ORG UAL Corp.],\n said the increase took effect [TIME Thursday] and applies to most\n routes where it competes against discount carriers, such as [LOC Chicago]\n to [LOC Dallas] and [LOC Denver] to [LOC San Francisco].\n The text tells us, for example, that Tim Wagner is a spokesman for American\n Airlines, that United is a unit of UAL Corp., and that American is a unit of AMR.\n These binary relations are instances of more generic relations such as part-of or\n employs that are fairly frequent in news-style texts. Figure 20.1 lists the 17 relations\n used in the ACE relation extraction evaluations and Fig. 20.2 shows some sample\n relations. We might also extract more domain-specific relations such as the notion of\n an airline route. For example from this text we can conclude that United has routes\n to Chicago, Dallas, Denver, and San Francisco.\n 20.1 ‚Ä¢ R ELATION E XTRACTION 3\n\n Relations Types Examples\n Physical-Located PER-GPE He was in Tennessee\n Part-Whole-Subsidiary ORG-ORG XYZ, the parent company of ABC\n Person-Social-Family PER-PER Yoko‚Äôs husband John\n Org-AFF-Founder PER-ORG Steve Jobs, co-founder of Apple...\n\n Sets of relations have been defined for many other domains as well. For example\n UMLS, the Unified Medical Language System from the US National Library of\n Medicine has a network that defines 134 broad subject categories, entity types, and\n 54 relations between the entities, such as the following:\n Entity Relation Entity\n Injury disrupts Physiological Function\n Bodily Location location-of Biologic Function\n Anatomical Structure part-of Organism\n Pharmacologic Substance causes Pathological Function\n Pharmacologic Substance treats Pathologic Function\n Given a medical sentence like this one:\n (20.1) Doppler echocardiography can be used to diagnose left anterior descending\n artery stenosis in patients with type 2 diabetes\n We could thus extract the UMLS relation:\n Echocardiography, Doppler Diagnoses Acquired stenosis\n infoboxes Wikipedia also offers a large supply of relations, drawn from infoboxes, structured tables associated with certain Wikipedia articles. For example, the Wikipedia\n infobox for Stanford includes structured facts like state = \"California\" or\n president = \"Marc Tessier-Lavigne\". These facts can be turned into rela-\nRDF tions like president-of or located-in. or into relations in a metalanguage called RDF\nRDF triple (Resource Description Framework). An RDF triple is a tuple of entity-relationentity, called a subject-predicate-object expression. Here‚Äôs a sample RDF triple:\n subject predicate object\n Golden Gate Park location San Francisco\n For example the crowdsourced DBpedia (Bizer et al., 2009) is an ontology derived from Wikipedia containing over 2 billion RDF triples. Another dataset from\n Freebase Wikipedia infoboxes, Freebase (Bollacker et al., 2008), now part of Wikidata (VrandecÃåicÃÅ\n and KroÃàtzsch, 2014), has relations between people and their nationality, or locations,\n and other locations they are contained in.\n WordNet or other ontologies offer useful ontological relations that express hieris-a archical relations between words or concepts. For example WordNet has the is-a or\nhypernym hypernym relation between classes,\n Giraffe is-a ruminant is-a ungulate is-a mammal is-a vertebrate ...\n WordNet also has Instance-of relation between individuals and classes, so that for\n example San Francisco is in the Instance-of relation with city. Extracting these\n relations is an important step in extending or building ontologies.\n Finally, there are large datasets that contain sentences hand-labeled with their\n relations, designed for training and testing relation extractors. The TACRED dataset\n (Zhang et al., 2017) contains 106,264 examples of relation triples about particular\n people or organizations, labeled in sentences from news and web text drawn from the\n4 C HAPTER 20 ‚Ä¢ I NFORMATION E XTRACTION : R ELATIONS , E VENTS , AND T IME\n\n annual TAC Knowledge Base Population (TAC KBP) challenges. TACRED contains\n 41 relation types (like per:city of birth, org:subsidiaries, org:member of, per:spouse),\n plus a no relation tag; examples are shown in Fig. 20.3. About 80% of all examples\n are annotated as no relation; having sufficient negative data is important for training\n supervised classifiers.\n\nExample Entity Types & Label\nCarey will succeed Cathleen P. Black, who held the position for 15 PERSON / TITLE\nyears and will take on a new role as chairwoman of Hearst Maga- Relation: per:title\nzines, the company said.\nIrene Morgan Kirkaldy, who was born and reared in Baltimore, lived PERSON / CITY\non Long Island and ran a child-care center in Queens with her second Relation: per:city of birth\nhusband, Stanley Kirkaldy.\nBaldwin declined further comment, and said JetBlue chief executive Types: PERSON / TITLE\nDave Barger was unavailable. Relation: no relation\n\n A standard dataset was also produced for the SemEval 2010 Task 8, detecting\n relations between nominals (Hendrickx et al., 2009). The dataset has 10,717 examples, each with a pair of nominals (untyped) hand-labeled with one of 9 directed\n relations like product-producer ( a factory manufactures suits) or component-whole\n (my apartment has a large kitchen).\n\n There are five main classes of algorithms for relation extraction: handwritten patterns, supervised machine learning, semi-supervised (via bootstrapping or distant supervision), and unsupervised. We‚Äôll introduce each of these in the next\n sections.\n\n 20.2.1 Using Patterns to Extract Relations\n The earliest and still common algorithm for relation extraction is lexico-syntactic\n patterns, first developed by Hearst (1992a), and therefore often called Hearst pat-\nHearst patterns terns. Consider the following sentence:\n Agar is a substance prepared from a mixture of red algae, such as Gelidium, for laboratory or industrial use.\n Hearst points out that most human readers will not know what Gelidium is, but that\n they can readily infer that it is a kind of (a hyponym of) red algae, whatever that is.\n She suggests that the following lexico-syntactic pattern\n\n NP0 such as NP1 {, NP2 . . . , (and|or)NPi }, i ‚â• 1 (20.2)\n\n implies the following semantics\n\n ‚àÄNPi , i ‚â• 1, hyponym(NPi , NP0 ) (20.3)\n\n allowing us to infer\n hyponym(Gelidium, red algae) (20.4)\n 20.2 ‚Ä¢ R ELATION E XTRACTION A LGORITHMS 5\n\nNP {, NP}* {,} (and|or) other NPH temples, treasuries, and other important civic buildings\nNPH such as {NP,}* {(or|and)} NP red algae such as Gelidium\nsuch NPH as {NP,}* {(or|and)} NP such authors as Herrick, Goldsmith, and Shakespeare\nNPH {,} including {NP,}* {(or|and)} NP common-law countries, including Canada and England\nNPH {,} especially {NP}* {(or|and)} NP European countries, especially France, England, and Spain\n1992a, Hearst 1998).\n\n the hyponym relation; we‚Äôve shown NPH as the parent/hyponym. Modern versions\n of the pattern-based approach extend it by adding named entity constraints. For\n example if our goal is to answer questions about ‚ÄúWho holds what office in which\n organization?‚Äù, we can use patterns like the following:\n PER, POSITION of ORG:\n George Marshall, Secretary of State of the United States\n\n PER (named|appointed|chose|etc.) PER Prep? POSITION\n Truman appointed Marshall Secretary of State\n\n PER [be]? (named|appointed|etc.) Prep? ORG POSITION\n George Marshall was named US Secretary of State\n Hand-built patterns have the advantage of high-precision and they can be tailored\n to specific domains. On the other hand, they are often low-recall, and it‚Äôs a lot of\n work to create them for all possible patterns.\n\n 20.2.2 Relation Extraction via Supervised Learning\n Supervised machine learning approaches to relation extraction follow a scheme that\n should be familiar by now. A fixed set of relations and entities is chosen, a training\n corpus is hand-annotated with the relations and entities, and the annotated texts are\n then used to train classifiers to annotate an unseen test set.\n The most straightforward approach, illustrated in Fig. 20.5 is: (1) Find pairs of\n named entities (usually in the same sentence). (2): Apply a relation-classification\n on each pair. The classifier can use any supervised technique (logistic regression,\n RNN, Transformer, random forest, etc.).\n An optional intermediate filtering classifier can be used to speed up the processing by making a binary decision on whether a given pair of named entities are related\n (by any relation). It‚Äôs trained on positive examples extracted directly from all relations in the annotated corpus, and negative examples generated from within-sentence\n entity pairs that are not annotated with a relation.\n Feature-based supervised relation classifiers. Let‚Äôs consider sample features for\n a feature-based classifier (like logistic regression or random forests), classifying the\n relationship between American Airlines (Mention 1, or M1) and Tim Wagner (Mention 2, M2) from this sentence:\n (20.5) American Airlines, a unit of AMR, immediately matched the move,\n spokesman Tim Wagner said\n These include word features (as embeddings, or 1-hot, stemmed or not):\n ‚Ä¢ The headwords of M1 and M2 and their concatenation\n Airlines Wagner Airlines-Wagner\n6 C HAPTER 20 ‚Ä¢ I NFORMATION E XTRACTION : R ELATIONS , E VENTS , AND T IME\n\n function F IND R ELATIONS(words) returns relations\n\n relations ‚Üê nil\n entities ‚Üê F IND E NTITIES(words)\n forall entity pairs he1, e2i in entities do\n if R ELATED ?(e1, e2)\n relations ‚Üê relations+C LASSIFY R ELATION(e1, e2)\n\n ‚Ä¢ Bag-of-words and bigrams in M1 and M2\n American, Airlines, Tim, Wagner, American Airlines, Tim Wagner\n ‚Ä¢ Words or bigrams in particular positions\n M2: -1 spokesman\n M2: +1 said\n ‚Ä¢ Bag of words or bigrams between M1 and M2:\n a, AMR, of, immediately, matched, move, spokesman, the, unit\n Named entity features:\n ‚Ä¢ Named-entity types and their concatenation\n (M1: ORG, M2: PER, M1M2: ORG-PER)\n ‚Ä¢ Entity Level of M1 and M2 (from the set NAME, NOMINAL, PRONOUN)\n M1: NAME [it or he would be PRONOUN]\n M2: NAME [the company would be NOMINAL]\n ‚Ä¢ Number of entities between the arguments (in this case 1, for AMR)\n Syntactic structure is a useful signal, often represented as the dependency or\n constituency syntactic path traversed through the tree between the entities.\n ‚Ä¢ Constituent paths between M1 and M2\n NP ‚Üë NP ‚Üë S ‚Üë S ‚Üì NP\n ‚Ä¢ Dependency-tree paths\n Airlines ‚Üêsub j matched ‚Üêcomp said ‚Üísub j Wagner\n Neural supervised relation classifiers Neural models for relation extraction similarly treat the task as supervised classification. Let‚Äôs consider a typical system applied to the TACRED relation extraction dataset and task (Zhang et al., 2017). In\n TACRED we are given a sentence and two spans within it: a subject, which is a\n person or organization, and an object, which is any other entity. The task is to assign\n a relation from the 42 TAC relations, including no relation.\n A typical Transformer-encoder algorithm, shown in Fig. 20.6, simply takes a\n pretrained encoder like BERT and adds a linear layer on top of the sentence representation (for example the BERT [CLS] token), a linear layer that is finetuned as a\n 1-of-N classifier to assign one of the 43 labels. The input to the BERT encoder is\n partially de-lexified; the subject and object entities are replaced in the input by their\n NER tags. This helps keep the system from overfitting to the individual lexical items\n (Zhang et al., 2017). When using BERT-type Transformers for relation extraction, it\n helps to use versions of BERT like RoBERTa (Liu et al., 2019) or spanBERT (Joshi\n et al., 2020) that don‚Äôt have two sequences separated by a [SEP] token, but instead\n form the input from a single long sequence of sentences.\n In general, if the test set is similar enough to the training set, and if there is\n enough hand-labeled data, supervised relation extraction systems can get high ac-\n20.2 ‚Ä¢ R ELATION E XTRACTION A LGORITHMS 7\n\n p(relation|SUBJ,OBJ)\n\n Linear\n Classifier\n\n ENCODER\n [CLS] [SUBJ_PERSON] was born in [OBJ_LOC] , Michigan\n\n with the subject and object entities replaced in the input by their NER tags (Zhang et al. 2017,\n Joshi et al. 2020).\n\n curacies. But labeling a large training set is extremely expensive and supervised\n models are brittle: they don‚Äôt generalize well to different text genres. For this reason, much research in relation extraction has focused on the semi-supervised and\n unsupervised approaches we turn to next.\n\n 20.2.3 Semisupervised Relation Extraction via Bootstrapping\n Supervised machine learning assumes that we have lots of labeled data. Unfortunately, this is expensive. But suppose we just have a few high-precision seed patseed patterns terns, like those in Section 20.2.1, or perhaps a few seed tuples. That‚Äôs enough\n seed tuples to bootstrap a classifier! Bootstrapping proceeds by taking the entities in the seed\nbootstrapping pair, and then finding sentences (on the web, or whatever dataset we are using) that\n contain both entities. From all such sentences, we extract and generalize the context\n around the entities to learn new patterns. Fig. 20.7 sketches a basic algorithm.\n\n function B OOTSTRAP(Relation R) returns new relation tuples\n\n tuples ‚Üê Gather a set of seed tuples that have relation R\n iterate\n sentences ‚Üê find sentences that contain entities in tuples\n patterns ‚Üê generalize the context between and around entities in sentences\n newpairs ‚Üê use patterns to identify more tuples\n newpairs ‚Üê newpairs with high confidence\n tuples ‚Üê tuples + newpairs\n return tuples\n\n Suppose, for example, that we need to create a list of airline/hub pairs, and we\n know only that Ryanair has a hub at Charleroi. We can use this seed fact to discover\n new patterns by finding other mentions of this relation in our corpus. We search\n for the terms Ryanair, Charleroi and hub in some proximity. Perhaps we find the\n following set of sentences:\n (20.6) Budget airline Ryanair, which uses Charleroi as a hub, scrapped all\n weekend flights out of the airport.\n (20.7) All flights in and out of Ryanair‚Äôs hub at Charleroi airport were grounded on\n Friday...\n (20.8) A spokesman at Charleroi, a main hub for Ryanair, estimated that 8000\n passengers had already been affected.\n8 C HAPTER 20 ‚Ä¢ I NFORMATION E XTRACTION : R ELATIONS , E VENTS , AND T IME\n\n From these results, we can use the context of words between the entity mentions,\n the words before mention one, the word after mention two, and the named entity\n types of the two mentions, and perhaps other features, to extract general patterns\n such as the following:\n / [ORG], which uses [LOC] as a hub /\n / [ORG]‚Äôs hub at [LOC] /\n / [LOC], a main hub for [ORG] /\n These new patterns can then be used to search for additional tuples.\n confidence Bootstrapping systems also assign confidence values to new tuples to avoid sevalues\n semantic drift mantic drift. In semantic drift, an erroneous pattern leads to the introduction of\n erroneous tuples, which, in turn, lead to the creation of problematic patterns and the\n meaning of the extracted relations ‚Äòdrifts‚Äô. Consider the following example:\n (20.9) Sydney has a ferry hub at Circular Quay.\n If accepted as a positive example, this expression could lead to the incorrect introduction of the tuple hSydney,CircularQuayi. Patterns based on this tuple could\n propagate further errors into the database.\n Confidence values for patterns are based on balancing two factors: the pattern‚Äôs\n performance with respect to the current set of tuples and the pattern‚Äôs productivity\n in terms of the number of matches it produces in the document collection. More\n formally, given a document collection D, a current set of tuples T , and a proposed\n pattern p, we need to track two factors:\n ‚Ä¢ hits(p): the set of tuples in T that p matches while looking in D\n ‚Ä¢ finds(p): The total set of tuples that p finds in D\n The following equation balances these considerations (Riloff and Jones, 1999).\n\n |hits(p)|\n Conf RlogF (p) = log(|finds(p)|) (20.10)\n |finds(p)|\n\n This metric is generally normalized to produce a probability.\n We can assess the confidence in a proposed new tuple by combining the evidence\n supporting it from all the patterns P0 that match that tuple in D (Agichtein and Granoisy-or vano, 2000). One way to combine such evidence is the noisy-or technique. Assume\n that a given tuple is supported by a subset of the patterns in P, each with its own\n confidence assessed as above. In the noisy-or model, we make two basic assumptions. First, that for a proposed tuple to be false, all of its supporting patterns must\n have been in error, and second, that the sources of their individual failures are all\n independent. If we loosely treat our confidence measures as probabilities, then the\n probability of any individual pattern p failing is 1 ‚àí Conf (p); the probability of all\n of the supporting patterns for a tuple being wrong is the product of their individual\n failure probabilities, leaving us with the following equation for our confidence in a\n new tuple.\n Y\n Conf (t) = 1 ‚àí (1 ‚àí Conf (p)) (20.11)\n p‚ààP0\n\n Setting conservative confidence thresholds for the acceptance of new patterns\n and tuples during the bootstrapping process helps prevent the system from drifting\n away from the targeted relation.\n 20.2 ‚Ä¢ R ELATION E XTRACTION A LGORITHMS 9\n\n 20.2.4 Distant Supervision for Relation Extraction\n Although hand-labeling text with relation labels is expensive to produce, there are\n distant\nsupervision ways to find indirect sources of training data. The distant supervision method\n (Mintz et al., 2009) combines the advantages of bootstrapping with supervised learning. Instead of just a handful of seeds, distant supervision uses a large database to\n acquire a huge number of seed examples, creates lots of noisy pattern features from\n all these examples and then combines them in a supervised classifier.\n For example suppose we are trying to learn the place-of-birth relationship between people and their birth cities. In the seed-based approach, we might have only\n 5 examples to start with. But Wikipedia-based databases like DBPedia or Freebase\n have tens of thousands of examples of many relations; including over 100,000 examples of place-of-birth, (<Edwin Hubble, Marshfield>, <Albert Einstein,\n Ulm>, etc.,). The next step is to run named entity taggers on large amounts of text‚Äî\n Mintz et al. (2009) used 800,000 articles from Wikipedia‚Äîand extract all sentences\n that have two named entities that match the tuple, like the following:\n ...Hubble was born in Marshfield...\n ...Einstein, born (1879), Ulm...\n ...Hubble‚Äôs birthplace in Marshfield...\n Training instances can now be extracted from this data, one training instance\n for each identical tuple <relation, entity1, entity2>. Thus there will be one\n training instance for each of:\n <born-in, Edwin Hubble, Marshfield>\n <born-in, Albert Einstein, Ulm>\n <born-year, Albert Einstein, 1879>\n and so on.\n We can then apply feature-based or neural classification. For feature-based\n classification, we can use standard supervised relation extraction features like the\n named entity labels of the two mentions, the words and dependency paths in between the mentions, and neighboring words. Each tuple will have features collected from many training instances; the feature vector for a single training instance\n like (<born-in,Albert Einstein, Ulm> will have lexical and syntactic features\n from many different sentences that mention Einstein and Ulm.\n Because distant supervision has very large training sets, it is also able to use very\n rich features that are conjunctions of these individual features. So we will extract\n thousands of patterns that conjoin the entity types with the intervening words or\n dependency paths like these:\n PER was born in LOC\n PER, born (XXXX), LOC\n PER‚Äôs birthplace in LOC\n To return to our running example, for this sentence:\n (20.12) American Airlines, a unit of AMR, immediately matched the move,\n spokesman Tim Wagner said\n we would learn rich conjunction features like this one:\n M1 = ORG & M2 = PER & nextword=‚Äúsaid‚Äù& path= NP ‚Üë NP ‚Üë S ‚Üë S ‚Üì NP\n The result is a supervised classifier that has a huge rich set of features to use\n in detecting relations. Since not every test sentence will have one of the training\n10 C HAPTER 20 ‚Ä¢ I NFORMATION E XTRACTION : R ELATIONS , E VENTS , AND T IME\n\n relations, the classifier will also need to be able to label an example as no-relation.\n This label is trained by randomly selecting entity pairs that do not appear in any\n Freebase relation, extracting features for them, and building a feature vector for\n each such tuple. The final algorithm is sketched in Fig. 20.8.\n\n function D ISTANT S UPERVISION(Database D, Text T) returns relation classifier C\n\n foreach relation R\n foreach tuple (e1,e2) of entities with relation R in D\n sentences ‚Üê Sentences in T that contain e1 and e2\n f ‚Üê Frequent features in sentences\n observations ‚Üê observations + new training tuple (e1, e2, f, R)\n C ‚Üê Train supervised classifier on observations\n return C\n\n would skip the feature set f .\n\n Distant supervision shares advantages with each of the methods we‚Äôve examined. Like supervised classification, distant supervision uses a classifier with lots\n of features, and supervised by detailed hand-created knowledge. Like pattern-based\n classifiers, it can make use of high-precision evidence for the relation between entities. Indeed, distance supervision systems learn patterns just like the hand-built\n patterns of early relation extractors. For example the is-a or hypernym extraction\n system of Snow et al. (2005) used hypernym/hyponym NP pairs from WordNet as\n distant supervision, and then learned new patterns from large amounts of text. Their\n system induced exactly the original 5 template patterns of Hearst (1992a), but also\n 70,000 additional patterns including these four:\n NPH like NP Many hormones like leptin...\n NPH called NP ...using a markup language called XHTML\n NP is a NPH Ruby is a programming language...\n NP, a NPH IBM, a company with a long...\n This ability to use a large number of features simultaneously means that, unlike the iterative expansion of patterns in seed-based systems, there‚Äôs no semantic\n drift. Like unsupervised classification, it doesn‚Äôt use a labeled training corpus of\n texts, so it isn‚Äôt sensitive to genre issues in the training corpus, and relies on very\n large amounts of unlabeled data. Distant supervision also has the advantage that it\n can create training tuples to be used with neural classifiers, where features are not\n required.\n The main problem with distant supervision is that it tends to produce low-precision\n results, and so current research focuses on ways to improve precision. Furthermore,\n distant supervision can only help in extracting relations for which a large enough\n database already exists. To extract new relations without datasets, or relations for\n new domains, purely unsupervised methods must be used.\n\n 20.2.5 Unsupervised Relation Extraction\n The goal of unsupervised relation extraction is to extract relations from the web\n open\n when we have no labeled training data, and not even any list of relations. This task\n information is often called open information extraction or Open IE. In Open IE, the relations\n extraction\n 20.2 ‚Ä¢ R ELATION E XTRACTION A LGORITHMS 11\n\nare simply strings of words (usually beginning with a verb).\n For example, the ReVerb system (Fader et al., 2011) extracts a relation from a\nsentence s in 4 steps:\n 1. Run a part-of-speech tagger and entity chunker over s\n 2. For each verb in s, find the longest sequence of words w that start with a verb\n and satisfy syntactic and lexical constraints, merging adjacent matches.\n 3. For each phrase w, find the nearest noun phrase x to the left which is not a\n relative pronoun, wh-word or existential ‚Äúthere‚Äù. Find the nearest noun phrase\n y to the right.\n 4. Assign confidence c to the relation r = (x, w, y) using a confidence classifier\n and return it.\n A relation is only accepted if it meets syntactic and lexical constraints. The\nsyntactic constraints ensure that it is a verb-initial sequence that might also include\nnouns (relations that begin with light verbs like make, have, or do often express the\ncore of the relation with a noun, like have a hub in):\n V | VP | VW*P\n V = verb particle? adv?\n W = (noun | adj | adv | pron | det )\n P = (prep | particle | infinitive ‚Äúto‚Äù)\nThe lexical constraints are based on a dictionary D that is used to prune very rare,\nlong relation strings. The intuition is to eliminate candidate relations that don‚Äôt occur with sufficient number of distinct argument types and so are likely to be bad\nexamples. The system first runs the above relation extraction algorithm offline on\n500 million web sentences and extracts a list of all the relations that occur after normalizing them (removing inflection, auxiliary verbs, adjectives, and adverbs). Each\nrelation r is added to the dictionary if it occurs with at least 20 different arguments.\nFader et al. (2011) used a dictionary of 1.7 million normalized relations.\n Finally, a confidence value is computed for each relation using a logistic regression classifier. The classifier is trained by taking 1000 random web sentences,\nrunning the extractor, and hand labeling each extracted relation as correct or incorrect. A confidence classifier is then trained on this hand-labeled data, using features\nof the relation and the surrounding words. Fig. 20.9 shows some sample features\nused in the classification.\n\n(x,r,y) covers all words in s\nthe last preposition in r is for\nthe last preposition in r is on\nlen(s) ‚â§ 10\nthere is a coordinating conjunction to the left of r in s\nr matches a lone V in the syntactic constraints\nthere is preposition to the left of x in s\nthere is an NP to the right of y in s\nOpen Information Extraction system REVERB (Fader et al., 2011).\n\n For example the following sentence:\n(20.13) United has a hub in Chicago, which is the headquarters of United\n Continental Holdings.\n12 C HAPTER 20 ‚Ä¢ I NFORMATION E XTRACTION : R ELATIONS , E VENTS , AND T IME\n\n has the relation phrases has a hub in and is the headquarters of (it also has has and\n is, but longer phrases are preferred). Step 3 finds United to the left and Chicago to\n the right of has a hub in, and skips over which to find Chicago to the left of is the\n headquarters of. The final output is:\n r1: <United, has a hub in, Chicago>\n r2: <Chicago, is the headquarters of, United Continental Holdings>\n The great advantage of unsupervised relation extraction is its ability to handle\n a huge number of relations without having to specify them in advance. The disadvantage is the need to map all the strings into some canonical form for adding\n to databases or knowledge graphs. Current methods focus heavily on relations expressed with verbs, and so will miss many relations that are expressed nominally.\n\n 20.2.6 Evaluation of Relation Extraction\n Supervised relation extraction systems are evaluated by using test sets with humanannotated, gold-standard relations and computing precision, recall, and F-measure.\n Labeled precision and recall require the system to classify the relation correctly,\n whereas unlabeled methods simply measure a system‚Äôs ability to detect entities that\n are related.\n Semi-supervised and unsupervised methods are much more difficult to evaluate, since they extract totally new relations from the web or a large text. Because\n these methods use very large amounts of text, it is generally not possible to run them\n solely on a small labeled test set, and as a result it‚Äôs not possible to pre-annotate a\n gold set of correct instances of relations.\n For these methods it‚Äôs possible to approximate (only) precision by drawing a\n random sample of relations from the output, and having a human check the accuracy\n of each of these relations. Usually this approach focuses on the tuples to be extracted\n from a body of text rather than on the relation mentions; systems need not detect\n every mention of a relation to be scored correctly. Instead, the evaluation is based\n on the set of tuples occupying the database when the system is finished. That is,\n we want to know if the system can discover that Ryanair has a hub at Charleroi; we\n don‚Äôt really care how many times it discovers it. The estimated precision PÃÇ is then\n # of correctly extracted relation tuples in the sample\n PÃÇ = (20.14)\n total # of extracted relation tuples in the sample.\n Another approach that gives us a little bit of information about recall is to compute precision at different levels of recall. Assuming that our system is able to\n rank the relations it produces (by probability, or confidence) we can separately compute precision for the top 1000 new relations, the top 10,000 new relations, the top\n 100,000, and so on. In each case we take a random sample of that set. This will\n show us how the precision curve behaves as we extract more and more tuples. But\n there is no way to directly evaluate recall.\n\n event The task of event extraction is to identify mentions of events in texts. For the\n extraction\n purposes of this task, an event mention is any expression denoting an event or state\n that can be assigned to a particular point, or interval, in time. The following markup\n of the sample text on page 1 shows all the events in this text.\n 20.4 ‚Ä¢ R EPRESENTING T IME 13\n\n [EVENT Citing] high fuel prices, United Airlines [EVENT said] Friday it has [EVENT increased] fares by $6 per round trip on flights to\n some cities also served by lower-cost carriers. American Airlines, a unit\n of AMR Corp., immediately [EVENT matched] [EVENT the move],\n spokesman Tim Wagner [EVENT said]. United, a unit of UAL Corp.,\n [EVENT said] [EVENT the increase] took effect Thursday and [EVENT\n applies] to most routes where it [EVENT competes] against discount\n carriers, such as Chicago to Dallas and Denver to San Francisco.\n In English, most event mentions correspond to verbs, and most verbs introduce\n events. However, as we can see from our example, this is not always the case. Events\n can be introduced by noun phrases, as in the move and the increase, and some verbs\n fail to introduce events, as in the phrasal verb took effect, which refers to when the\n light verbs event began rather than to the event itself. Similarly, light verbs such as make, take,\n and have often fail to denote events. A light verb is a verb that has very little meaning\n itself, and the associated event is instead expressed by its direct object noun. In light\n verb examples like took a flight, it‚Äôs the word flight that defines the event; these light\n verbs just provide a syntactic structure for the noun‚Äôs arguments.\n Various versions of the event extraction task exist, depending on the goal. For\n example in the TempEval shared tasks (Verhagen et al. 2009) the goal is to extract\n events and aspects like their aspectual and temporal properties. Events are to be\n reporting classified as actions, states, reporting events (say, report, tell, explain), perception\n events\n events, and so on. The aspect, tense, and modality of each event also needs to be\n extracted. Thus for example the various said events in the sample text would be\n annotated as (class=REPORTING, tense=PAST, aspect=PERFECTIVE).\n Event extraction is generally modeled via supervised learning, detecting events\n via IOB sequence models and assigning event classes and attributes with multi-class\n classifiers. The input can be neural models starting from encoders; or classic featurebased models using features like those in Fig. 20.10.\n\nFeature Explanation\nCharacter affixes Character-level prefixes and suffixes of target word\nNominalization suffix Character-level suffixes for nominalizations (e.g., -tion)\nPart of speech Part of speech of the target word\nLight verb Binary feature indicating that the target is governed by a light verb\nSubject syntactic category Syntactic category of the subject of the sentence\nMorphological stem Stemmed version of the target word\nVerb root Root form of the verb basis for a nominalization\nWordNet hypernyms Hypernym set for the target\n\n temporal logic Let‚Äôs begin by introducing the basics of temporal logic and how human languages\n convey temporal information. The most straightforward theory of time holds that it\n flows inexorably forward and that events are associated with either points or intervals in time, as on a timeline. We can order distinct events by situating them on the\n timeline; one event precedes another if the flow of time leads from the first event\n14 C HAPTER 20 ‚Ä¢ I NFORMATION E XTRACTION : R ELATIONS , E VENTS , AND T IME\n\n to the second. Accompanying these notions in most theories is the idea of the current moment in time. Combining this notion with the idea of a temporal ordering\n relationship yields the familiar notions of past, present, and future.\n Various kinds of temporal representation systems can be used to talk about temporal ordering relationship. One of the most commonly used in computational modinterval algebra eling is the interval algebra of Allen (1984). Allen models all events and time\n expressions as intervals there is no representation for points (although intervals can\n be very short). In order to deal with intervals without points, he identifies 13 primitive relations that can hold between these temporal intervals. Fig. 20.11 shows these\n Allen relations 13 Allen relations.\n\n A A\n\n A before B B\n A overlaps B B\n B after A B overlaps' A\n\n A\n A\n A equals B\n B (B equals A)\n A meets B\n B\n B meets' A\n\n A A\n A starts B A finishes B\n B starts' A B finishes' A\n\n B B\n\n A during B A\n B during' A\n\n B\n\n Time\n\n 20.4.1 Reichenbach‚Äôs reference point\n The relation between simple verb tenses and points in time is by no means straightforward. The present tense can be used to refer to a future event, as in this example:\n (20.15) Ok, we fly from San Francisco to Boston at 10.\n Or consider the following examples:\n (20.16) Flight 1902 arrived late.\n (20.17) Flight 1902 had arrived late.\n Although both refer to events in the past, representing them in the same way seems\n wrong. The second example seems to have another unnamed event lurking in the\n background (e.g., Flight 1902 had already arrived late when something else happened).\n 20.4 ‚Ä¢ R EPRESENTING T IME 15\n\n To account for this phenomena, Reichenbach (1947) introduced the notion of\nreference point a reference point. In our simple temporal scheme, the current moment in time is\n equated with the time of the utterance and is used as a reference point for when\n the event occurred (before, at, or after). In Reichenbach‚Äôs approach, the notion of\n the reference point is separated from the utterance time and the event time. The\n following examples illustrate the basics of this approach:\n (20.18) When Mary‚Äôs flight departed, I ate lunch.\n (20.19) When Mary‚Äôs flight departed, I had eaten lunch.\n In both of these examples, the eating event has happened in the past, that is, prior\n to the utterance. However, the verb tense in the first example indicates that the eating\n event began when the flight departed, while the second example indicates that the\n eating was accomplished prior to the flight‚Äôs departure. Therefore, in Reichenbach‚Äôs\n terms the departure event specifies the reference point. These facts can be accommodated by additional constraints relating the eating and departure events. In the\n first example, the reference point precedes the eating event, and in the second example, the eating precedes the reference point. Figure 20.12 illustrates Reichenbach‚Äôs\n approach with the primary English tenses. Exercise 20.4 asks you to represent these\n examples in FOL.\n\n Past Perfect Simple Past Present Perfect\n\n E R U R,E U E R,U\n\n Present Simple Future Future Perfect\n\n U,R,E U,R E U E R\n\n time flows from left to right, E denotes the time of the event, R denotes the reference time,\n and U denotes the time of the utterance.\n\n Languages have many other ways to convey temporal information besides tense.\n Most useful for our purposes will be temporal expressions like in the morning or\n 6:45 or afterwards.\n (20.20) I‚Äôd like to go at 6:45 in the morning.\n (20.21) Somewhere around noon, please.\n (20.22) I want to take the train back afterwards.\n Incidentally, temporal expressions display a fascinating metaphorical conceptual\n organization. Temporal expressions in English are frequently expressed in spatial\n terms, as is illustrated by the various uses of at, in, somewhere, and near in these\n examples (Lakoff and Johnson 1980, Jackendoff 1983). Metaphorical organizations\n such as these, in which one domain is systematically expressed in terms of another,\n are very common in languages of the world.\n16 C HAPTER 20 ‚Ä¢ I NFORMATION E XTRACTION : R ELATIONS , E VENTS , AND T IME\n\n aspect A related notion to time is aspect, which is what we call the way events can be\n categorized by their internal temporal structure or temporal contour. By this we\n mean questions like whether events are ongoing or have ended, or whether they are\n conceptualized as happening at a point in time or over some interval. Such notions\n of temporal contour have been used to divide event expressions into classes since\n Aristotle, although the set of four classes we‚Äôll introduce here is due to Vendler\n aktionsart (1967) (you may also see the German term aktionsart used to refer to these classes).\n events The most basic aspectual distinction is between events (which involve change)\n states and states (which do not involve change). Stative expressions represent the notion\n stative of an event participant being in a state, or having a particular property, at a given\n point in time. Stative expressions capture aspects of the world at a single point in\n time, and conceptualize the participant as unchanging and continuous. Consider the\n following ATIS examples.\n (20.23) I like express trains.\n (20.24) I need the cheapest fare.\n (20.25) I want to go first class.\n In examples like these, the event participant denoted by the subject can be seen as\n experiencing something at a specific point in time, and don‚Äôt involve any kind of\n internal change over time (the liking or needing is conceptualized as continuous and\n unchanging).\n Non-states (which we‚Äôll refer to as events) are divided into subclasses; we‚Äôll\n activity introduce three here. Activity expressions describe events undertaken by a participant that occur over a span of time (rather than being conceptualized as a single\n point in time like stative expressions), and have no particular end point. Of course\n in practice all things end, but the meaning of the expression doesn‚Äôt represent this\n fact. Consider the following examples:\n (20.26) She drove a Mazda.\n (20.27) I live in Brooklyn.\n These examples both specify that the subject is engaged in, or has engaged in, the\n activity specified by the verb for some period of time, but doesn‚Äôt specify when the\n driving or living might have stopped.\n Two more classes of expressions, achievement expressions and accomplishment expressions, describe events that take place over time, but also conceptualize\n the event as having a particular kind of endpoint or goal. The Greek word telos\n means ‚Äòend‚Äô or ‚Äôgoal‚Äô and so the events described by these kinds of expressions are\n telic often called telic events.\naccomplishment Accomplishment expressions describe events that have a natural end point and\n expressions\n result in a particular state. Consider the following examples:\n (20.28) He booked me a reservation.\n (20.29) The 7:00 train got me to New York City.\n In these examples, an event is seen as occurring over some period of time that ends\n when the intended state is accomplished (i.e., the state of me having a reservation,\n or me being in New York City).\n achievement\n expressions The final aspectual class, achievement expressions, is only subtly different than\n accomplishments. Consider the following:\n 20.6 ‚Ä¢ T EMPORALLY A NNOTATED DATASETS : T IME BANK 17\n\n (20.30) She found her gate.\n (20.31) I reached New York.\n Like accomplishment expressions, achievement expressions result in a state. But\n unlike accomplishments, achievement events are ‚Äòpunctual‚Äô: they are thought of as\n happening in an instant and the verb doesn‚Äôt conceptualize the process or activity leading up the state. Thus the events in these examples may in fact have been\n preceded by extended searching or traveling events, but the verb doesn‚Äôt conceptualize these preceding processes, but rather conceptualizes the events corresponding\n to finding and reaching as points, not intervals.\n In summary, a standard way of categorizing event expressions by their temporal\n contours is via these four general classes:\n Stative: I know my departure gate.\n Activity: John is flying.\n Accomplishment: Sally booked her flight.\n Achievement: She found her gate.\n Before moving on, note that event expressions can easily be shifted from one\n class to another. Consider the following examples:\n (20.32) I flew.\n (20.33) I flew to New York.\n The first example is a simple activity; it has no natural end point. The second example is clearly an accomplishment event since it has an end point, and results in a\n particular state. Clearly, the classification of an event is not solely governed by the\n verb, but by the semantics of the entire expression in context.\n\n TimeBank The TimeBank corpus consists of American English text annotated with temporal\n information (Pustejovsky et al., 2003). The annotations use TimeML (Saurƒ±ÃÅ et al.,\n 2006), a markup language for time based on Allen‚Äôs interval algebra discussed above\n (Allen, 1984). There are three types of TimeML objects: an E VENT represent events\n and states, a T IME represents time expressions like dates, and a L INK represents\n various relationships between events and times (event-event, event-time, and timetime). The links include temporal links (TL INK) for the 13 Allen relations, aspectual links (AL INK) for aspectual relationships between events and subevents, and\n SL INKS which mark factuality.\n Consider the following sample sentence and its corresponding markup shown in\n Fig. 20.13, selected from one of the TimeBank documents.\n (20.34) Delta Air Lines earnings soared 33% to a record in the fiscal first quarter,\n bucking the industry trend toward declining profits.\n This text has three events and two temporal expressions (including the creation\n time of the article, which serves as the document time), and four temporal links that\n capture the using the Allen relations:\n ‚Ä¢ Soaringe1 is included in the fiscal first quartert58\n ‚Ä¢ Soaringe1 is before 1989-10-26t57\n ‚Ä¢ Soaringe1 is simultaneous with the buckinge3\n18 C HAPTER 20 ‚Ä¢ I NFORMATION E XTRACTION : R ELATIONS , E VENTS , AND T IME\n\n<TIMEX3 tid=\"t57\" type=\"DATE\" value=\"1989-10-26\" functionInDocument=\"CREATION_TIME\">\n10/26/89 </TIMEX3>\n\nDelta Air Lines earnings <EVENT eid=\"e1\" class=\"OCCURRENCE\"> soared </EVENT> 33% to a\nrecord in <TIMEX3 tid=\"t58\" type=\"DATE\" value=\"1989-Q1\" anchorTimeID=\"t57\"> the\nfiscal first quarter </TIMEX3>, <EVENT eid=\"e3\" class=\"OCCURRENCE\">bucking</EVENT>\nthe industry trend toward <EVENT eid=\"e4\" class=\"OCCURRENCE\">declining</EVENT>\nprofits.\n\n ‚Ä¢ Declininge4 includes soaringe1\n We can also visualize the links as a graph. The TimeBank snippet in Eq. 20.35\n would be represented with a graph like Fig. 20.14.\n (20.35) [DCT:11/02/891]1 : Pacific First Financial Corp. said2 shareholders\n approved3 its acquisition4 by Royal Trustco Ltd. of Toronto for $27 a share,\n or $212 million. The thrift holding company said5 it expects6 to obtain7\n regulatory approval8 and complete9 the transaction10 by year-end11 .\n\n BEFORE BEFORE AFTER\n 1 S\n 2 3 4\n OU EVIDENTIAL MODAL\n NE\n ULTA\n SIM EVIDENTIAL MODAL FACTIVE\n 5 6 7 8\n MODAL\n\n BEFORE ENDS\n 11 9 10\n CULMINATES\n\n are shown in blue, AL INKS in red, and SL INKS in green.\n\n Here we introduce the three common steps used in analyzing time in text:\n 1. Extracting temporal expressions\n 2. Normalizing these expressions, by converting them to a standard format.\n 3. Linking events to times and extracting time graphs and timelines\n\n 20.7.1 Extracting Temporal Expressions\n Temporal expressions are phrases that refer to absolute points in time, relative times,\n absolute durations, and sets of these. Absolute temporal expressions are those that can be\n relative mapped directly to calendar dates, times of day, or both. Relative temporal expressions map to particular times through some other reference point (as in a week from\n duration last Tuesday). Finally, durations denote spans of time at varying levels of granularity (seconds, minutes, days, weeks, centuries, etc.). Figure 20.15 lists some sample\n temporal expressions in each of these categories.\n Temporal expressions are grammatical constructions that often have temporal\nlexical triggers lexical triggers as their heads, making them easy to find. Lexical triggers might\n be nouns, proper nouns, adjectives, and adverbs; full temporal expressions consist\n 20.7 ‚Ä¢ AUTOMATIC T EMPORAL A NALYSIS 19\n\n Absolute Relative Durations\n April 24, 1916 yesterday four hours\n The summer of ‚Äô77 next semester three weeks\n 10:15 AM two weeks from yesterday six days\n The 3rd quarter of 2006 last quarter the last three quarters\n\n of their phrasal projections: noun phrases, adjective phrases, and adverbial phrases\n (Figure 20.16).\n\n Category Examples\n Noun morning, noon, night, winter, dusk, dawn\n Proper Noun January, Monday, Ides, Easter, Rosh Hashana, Ramadan, Tet\n Adjective recent, past, annual, former\n Adverb hourly, daily, monthly, yearly\n\n The task is to detect temporal expressions in running text, like this examples,\n shown with TIMEX3 tags (Pustejovsky et al. 2005, Ferro et al. 2005).\n A fare increase initiated <TIMEX3>last week</TIMEX3> by UAL\n Corp‚Äôs United Airlines was matched by competitors over <TIMEX3>the\n weekend</TIMEX3>, marking the second successful fare increase in\n <TIMEX3>two weeks</TIMEX3>.\n Rule-based approaches use cascades of regular expressions to recognize larger\n and larger chunks from previous stages, based on patterns containing parts of speech,\n trigger words (e.g., February) or classes (e.g., MONTH) (Chang and Manning, 2012;\n StroÃàtgen and Gertz, 2013; Chambers, 2013). Here‚Äôs a rule from SUTime (Chang and\n Manning, 2012) for detecting expressions like 3 years old:\n /(\\d+)[-\\s]($TEUnits)(s)?([-\\s]old)?/\n Sequence-labeling approaches use the standard IOB scheme, marking words\n that are either (I)nside, (O)utside or at the (B)eginning of a temporal expression:\n A fare increase initiated last week by UAL Corp‚Äôs...\n OO O O B I O O O\n A statistical sequence labeler is trained, using either embeddings or a fine-tuned\n encoder, or classic features extracted from the token and context including words,\n lexical triggers, and POS.\n Temporal expression recognizers are evaluated with the usual recall, precision,\n and F-measures. A major difficulty for all of these very lexicalized approaches is\n avoiding expressions that trigger false positives:\n (20.36) 1984 tells the story of Winston Smith...\n (20.37) ...U2‚Äôs classic Sunday Bloody Sunday\n\n 20.7.2 Temporal Normalization\n temporal Temporal normalization is the task of mapping a temporal expression to a point\nnormalization\n in time or to a duration. Points in time correspond to calendar dates, to times of\n day, or both. Durations primarily consist of lengths of time. Normalized times\n are represented via the ISO 8601 standard for encoding temporal values (ISO8601,\n 2004). Fig. 20.17 reproduces our earlier example with these value attributes.\n20 C HAPTER 20 ‚Ä¢ I NFORMATION E XTRACTION : R ELATIONS , E VENTS , AND T IME\n\n<TIMEX3 i d =‚Äù t 1 ‚Äô ‚Äô t y p e =‚ÄùDATE‚Äù v a l u e =‚Äù 2007 ‚àí07 ‚àí02 ‚Äù f u n c t i o n I n D o c u m e n t =‚ÄùCREATION TIME‚Äù>\n J u l y 2 , 2007 </TIMEX3> A f a r e i n c r e a s e i n i t i a t e d <TIMEX3 i d =‚Äù t 2 ‚Äù t y p e =‚ÄùDATE‚Äù\n v a l u e =‚Äù 2007 ‚àíW26‚Äù a n c h o r T i m e I D =‚Äù t 1 ‚Äù> l a s t week </TIMEX3> by U n i t e d A i r l i n e s was\n m a t c h e d by c o m p e t i t o r s o v e r <TIMEX3 i d =‚Äù t 3 ‚Äù t y p e =‚ÄùDURATION‚Äù v a l u e =‚ÄùP1WE‚Äù\n a n c h o r T i m e I D =‚Äù t 1 ‚Äù> t h e weekend </TIMEX3>, m a r k i n g t h e s e c o n d s u c c e s s f u l f a r e\n i n c r e a s e i n <TIMEX3 i d =‚Äù t 4 ‚Äù t y p e =‚ÄùDURATION‚Äù v a l u e =‚ÄùP2W‚Äù a n c h o r T i m e I D =‚Äù t 1 ‚Äù> two\n weeks </TIMEX3>.\n\n The dateline, or document date, for this text was July 2, 2007. The ISO representation for this kind of expression is YYYY-MM-DD, or in this case, 2007-07-02.\n The encodings for the temporal expressions in our sample text all follow from this\n date, and are shown here as values for the VALUE attribute.\n The first temporal expression in the text proper refers to a particular week of the\n year. In the ISO standard, weeks are numbered from 01 to 53, with the first week\n of the year being the one that has the first Thursday of the year. These weeks are\n represented with the template YYYY-Wnn. The ISO week for our document date is\n week 27; thus the value for last week is represented as ‚Äú2007-W26‚Äù.\n The next temporal expression is the weekend. ISO weeks begin on Monday;\n thus, weekends occur at the end of a week and are fully contained within a single\n week. Weekends are treated as durations, so the value of the VALUE attribute has\n to be a length. Durations are represented according to the pattern Pnx, where n is\n an integer denoting the length and x represents the unit, as in P3Y for three years\n or P2D for two days. In this example, one weekend is captured as P1WE. In this\n case, there is also sufficient information to anchor this particular weekend as part of\n a particular week. Such information is encoded in the ANCHORT IME ID attribute.\n Finally, the phrase two weeks also denotes a duration captured as P2W. Figure 20.18\n give some more examples, but there is a lot more to the various temporal annotation\n standards; consult ISO8601 (2004), Ferro et al. (2005), and Pustejovsky et al. (2005)\n for more details.\n Unit Pattern Sample Value\n Fully specified dates YYYY-MM-DD 1991-09-28\n Weeks YYYY-Wnn 2007-W27\n Weekends PnWE P1WE\n 24-hour clock times HH:MM:SS 11:13:45\n Dates and times YYYY-MM-DDTHH:MM:SS 1991-09-28T11:00:00\n Financial quarters Qn 1999-Q3\n\n Most current approaches to temporal normalization are rule-based (Chang and\n Manning 2012, StroÃàtgen and Gertz 2013). Patterns that match temporal expressions\n are associated with semantic analysis procedures. For example, the pattern above for\n recognizing phrases like 3 years old can be associated with the predicate Duration\n that takes two arguments, the length and the unit of time:\n pattern: /(\\d+)[-\\s]($TEUnits)(s)?([-\\s]old)?/\n result: Duration($1, $2)\n The task is difficult because fully qualified temporal expressions are fairly rare\n in real texts. Most temporal expressions in news articles are incomplete and are only\n implicitly anchored, often with respect to the dateline of the article, which we refer\n temporal to as the document‚Äôs temporal anchor. The values of temporal expressions such\n anchor\n as today, yesterday, or tomorrow can all be computed with respect to this temporal\n 20.7 ‚Ä¢ AUTOMATIC T EMPORAL A NALYSIS 21\n\nanchor. The semantic procedure for today simply assigns the anchor, and the attachments for tomorrow and yesterday add a day and subtract a day from the anchor,\nrespectively. Of course, given the cyclic nature of our representations for months,\nweeks, days, and times of day, our temporal arithmetic procedures must use modulo\narithmetic appropriate to the time unit being used.\n Unfortunately, even simple expressions such as the weekend or Wednesday introduce a fair amount of complexity. In our current example, the weekend clearly\nrefers to the weekend of the week that immediately precedes the document date. But\nthis won‚Äôt always be the case, as is illustrated in the following example.\n(20.38) Random security checks that began yesterday at Sky Harbor will continue\n at least through the weekend.\nIn this case, the expression the weekend refers to the weekend of the week that the\nanchoring date is part of (i.e., the coming weekend). The information that signals\nthis meaning comes from the tense of continue, the verb governing the weekend.\n Relative temporal expressions are handled with temporal arithmetic similar to\nthat used for today and yesterday. The document date indicates that our example\narticle is ISO week 27, so the expression last week normalizes to the current week\nminus 1. To resolve ambiguous next and last expressions we consider the distance\nfrom the anchoring date to the nearest unit. Next Friday can refer either to the\nimmediately next Friday or to the Friday following that, but the closer the document\ndate is to a Friday, the more likely it is that the phrase will skip the nearest one. Such\nambiguities are handled by encoding language and domain-specific heuristics into\nthe temporal attachments.\n\n20.7.3 Temporal Ordering of Events\nThe goal of temporal analysis, is to link times to events and then fit all these events\ninto a complete timeline. This ambitious task is the subject of considerable current\nresearch but solving it with a high level of accuracy is beyond the capabilities of\ncurrent systems. A somewhat simpler, but still useful, task is to impose a partial ordering on the events and temporal expressions mentioned in a text. Such an ordering\ncan provide many of the same benefits as a true timeline. An example of such a partial ordering is the determination that the fare increase by American Airlines came\nafter the fare increase by United in our sample text. Determining such an ordering\ncan be viewed as a binary relation detection and classification task.\n Even this partial ordering task assumes that in addition to the detecting and normalizing time expressions steps described above, we have already detected all the\nevents in the text. Indeed, many temporal expressions are anchored to events mentioned in a text and not directly to other temporal expressions. Consider the following example:\n(20.39) One week after the storm, JetBlue issued its customer bill of rights.\nTo determine when JetBlue issued its customer bill of rights we need to determine\nthe time of the storm event, and then we need to modify that time by the temporal\nexpression one week after.\n Thus once the events and times have been detected, our goal next is to assert links\nbetween all the times and events: i.e. creating event-event, event-time, time-time,\nDCT-event, and DCT-time TimeML TL INKS. This can be done by training time\nrelation classifiers to predict the correct T: INK between each pair of times/events,\nsupervised by the gold labels in the TimeBank corpus with features like words/embeddings, parse paths, tense and aspect The sieve-based architecture using precision-\n22 C HAPTER 20 ‚Ä¢ I NFORMATION E XTRACTION : R ELATIONS , E VENTS , AND T IME\n\n ranked sets of classifiers, which we‚Äôll introduce in Chapter 23, is also commonly\n used.\n Systems that perform all 4 tasks (time extraction creation and normalization,\n event extraction, and time/event linking) include TARSQI (Verhagen et al., 2005)\n C LEARTK (Bethard, 2013), CAEVO (Chambers et al., 2014), and CATENA (Mirza\n and Tonelli, 2016).\n\n Many texts contain reports of events, and possibly sequences of events, that often\n correspond to fairly common, stereotypical situations in the world. These abstract\n scripts situations or stories, related to what have been called scripts (Schank and Abelson, 1977), consist of prototypical sequences of sub-events, participants, and their\n roles. The strong expectations provided by these scripts can facilitate the proper\n classification of entities, the assignment of entities into roles and relations, and most\n critically, the drawing of inferences that fill in things that have been left unsaid. In\n templates their simplest form, such scripts can be represented as templates consisting of fixed\n sets of slots that take as values slot-fillers belonging to particular classes. The task\ntemplate filling of template filling is to find documents that invoke particular scripts and then fill the\n slots in the associated templates with fillers extracted from the text. These slot-fillers\n may consist of text segments extracted directly from the text, or they may consist of\n concepts that have been inferred from text elements through some additional processing.\n A filled template from our original airline story might look like the following.\n Ô£Æ Ô£π\n FARE -R AISE ATTEMPT: L EAD A IRLINE : U NITED A IRLINES\n Ô£ØA MOUNT: $6 Ô£∫\n Ô£Ø Ô£∫\n Ô£∞E FFECTIVE DATE : 2006-10-26\n Ô£Ø Ô£∫\n Ô£ª\n F OLLOWER : A MERICAN A IRLINES\n\n This template has four slots (LEAD AIRLINE, AMOUNT, EFFECTIVE DATE, FOL -\n LOWER ). The next section describes a standard sequence-labeling approach to filling\n slots. Section 20.8.2 then describes an older system based on the use of cascades of\n finite-state transducers and designed to address a more complex template-filling task\n that current learning-based systems don‚Äôt yet address.\n\n 20.8.1 Machine Learning Approaches to Template Filling\n In the standard paradigm for template filling, we are given training documents with\n text spans annotated with predefined templates and their slot fillers. Our goal is to\n create one template for each event in the input, filling in the slots with text spans.\n The task is generally modeled by training two separate supervised systems. The\n first system decides whether the template is present in a particular sentence. This\n template\n recognition task is called template recognition or sometimes, in a perhaps confusing bit of\n terminology, event recognition. Template recognition can be treated as a text classification task, with features extracted from every sequence of words that was labeled\n in training documents as filling any slot from the template being detected. The usual\n set of features can be used: tokens, embeddings, word shapes, part-of-speech tags,\n syntactic chunk tags, and named entity tags.\n 20.8 ‚Ä¢ T EMPLATE F ILLING 23\n\n role-filler The second system has the job of role-filler extraction. A separate classifier is\n extraction\n trained to detect each role (LEAD - AIRLINE, AMOUNT, and so on). This can be a\n binary classifier that is run on every noun-phrase in the parsed input sentence, or a\n sequence model run over sequences of words. Each role classifier is trained on the\n labeled data in the training set. Again, the usual set of features can be used, but now\n trained only on an individual noun phrase or the fillers of a single slot.\n Multiple non-identical text segments might be labeled with the same slot label. For example in our sample text, the strings United or United Airlines might be\n labeled as the L EAD A IRLINE. These are not incompatible choices and the coreference resolution techniques introduced in Chapter 23 can provide a path to a solution.\n A variety of annotated collections have been used to evaluate this style of approach to template filling, including sets of job announcements, conference calls for\n papers, restaurant guides, and biological texts. A key open question is extracting\n templates in cases where there is no training data or even predefined templates, by\n inducing templates as sets of linked events (Chambers and Jurafsky, 2011).\n\n 20.8.2 Earlier Finite-State Template-Filling Systems\n The templates above are relatively simple. But consider the task of producing a\n template that contained all the information in a text like this one (Grishman and\n Sundheim, 1995):\n Bridgestone Sports Co. said Friday it has set up a joint venture in Taiwan\n with a local concern and a Japanese trading house to produce golf clubs to be\n shipped to Japan. The joint venture, Bridgestone Sports Taiwan Co., capitalized at 20 million new Taiwan dollars, will start production in January 1990\n with production of 20,000 iron and ‚Äúmetal wood‚Äù clubs a month.\n The MUC-5 ‚Äòjoint venture‚Äô task (the Message Understanding Conferences were\n a series of U.S. government-organized information-extraction evaluations) was to\n produce hierarchically linked templates describing joint ventures. Figure 20.19\n shows a structure produced by the FASTUS system (Hobbs et al., 1997). Note how\n the filler of the ACTIVITY slot of the TIE - UP template is itself a template with slots.\n\nTie-up-1 Activity-1:\nR ELATIONSHIP tie-up C OMPANY Bridgestone Sports Taiwan Co.\nE NTITIES Bridgestone Sports Co. P RODUCT iron and ‚Äúmetal wood‚Äù clubs\n a local concern S TART DATE DURING: January 1990\n a Japanese trading house\nJ OINT V ENTURE Bridgestone Sports Taiwan Co.\nACTIVITY Activity-1\nA MOUNT NT$20000000\n\n Early systems for dealing with these complex templates were based on cascades\n of transducers based on handwritten rules, as sketched in Fig. 20.20.\n The first four stages use handwritten regular expression and grammar rules to\n do basic tokenization, chunking, and parsing. Stage 5 then recognizes entities and\n events with a recognizer based on finite-state transducers (FSTs), and inserts the recognized objects into the appropriate slots in templates. This FST recognizer is based\n on hand-built regular expressions like the following (NG indicates Noun-Group and\n VG Verb-Group), which matches the first sentence of the news story above.\n24 C HAPTER 20 ‚Ä¢ I NFORMATION E XTRACTION : R ELATIONS , E VENTS , AND T IME\n\n No. Step Description\n 1 Tokens Tokenize input stream of characters\n 2 Complex Words Multiword phrases, numbers, and proper names.\n 3 Basic phrases Segment sentences into noun and verb groups\n 4 Complex phrases Identify complex noun groups and verb groups\n 5 Semantic Patterns Identify entities and events, insert into templates.\n 6 Merging Merge references to the same entity or event\n specific type of information which is then passed on to the next higher level.\n\n NG(Company/ies) VG(Set-up) NG(Joint-Venture) with NG(Company/ies)\n VG(Produce) NG(Product)\n The result of processing these two sentences is the five draft templates (Fig. 20.21)\n that must then be merged into the single hierarchical structure shown in Fig. 20.19.\n The merging algorithm, after performing coreference resolution, merges two activities that are likely to be describing the same events.\n\n # Template/Slot Value\n 1 R ELATIONSHIP : TIE - UP\n E NTITIES : Bridgestone Co., a local concern, a Japanese trading house\n 2 ACTIVITY: PRODUCTION\n P RODUCT: ‚Äúgolf clubs‚Äù\n 3 R ELATIONSHIP : TIE - UP\n J OINT V ENTURE : ‚ÄúBridgestone Sports Taiwan Co.‚Äù\n A MOUNT: NT$20000000\n 4 ACTIVITY: PRODUCTION\n C OMPANY: ‚ÄúBridgestone Sports Taiwan Co.‚Äù\n S TART DATE : DURING : January 1990\n 5 ACTIVITY: PRODUCTION\n P RODUCT: ‚Äúiron and ‚Äúmetal wood‚Äù clubs‚Äù\n are merged in stage 6 to produce the final template shown in Fig. 20.19 on page 23.\n\n This chapter has explored techniques for extracting limited forms of semantic content from texts.\n ‚Ä¢ Relations among entities can be extracted by pattern-based approaches, supervised learning methods when annotated training data is available, lightly\n supervised bootstrapping methods when small numbers of seed tuples or\n seed patterns are available, distant supervision when a database of relations\n is available, and unsupervised or Open IE methods.\n ‚Ä¢ Reasoning about time can be facilitated by detection and normalization of\n temporal expressions.\n ‚Ä¢ Events can be ordered in time using sequence models and classifiers trained\n on temporally- and event-labeled data like the TimeBank corpus.\n H ISTORICAL N OTES 25\n\n ‚Ä¢ Template-filling applications can recognize stereotypical situations in texts\n and assign elements from the text to roles represented as fixed sets of slots.\n\nHistorical Notes\n The earliest work on information extraction addressed the template-filling task in the\n context of the Frump system (DeJong, 1982). Later work was stimulated by the U.S.\n government-sponsored MUC conferences (Sundheim 1991, Sundheim 1992, Sundheim 1993, Sundheim 1995). Early MUC systems like CIRCUS system (Lehnert\n et al., 1991) and SCISOR (Jacobs and Rau, 1990) were quite influential and inspired\n later systems like FASTUS (Hobbs et al., 1997). Chinchor et al. (1993) describe the\n MUC evaluation techniques.\n Due to the difficulty of porting systems from one domain to another, attention\n shifted to machine learning approaches. Early supervised learning approaches to\n IE (Cardie 1993, Cardie 1994, Riloff 1993, Soderland et al. 1995, Huffman 1996)\n focused on automating the knowledge acquisition process, mainly for finite-state\n rule-based systems. Their success, and the earlier success of HMM-based speech\n recognition, led to the use of sequence labeling (HMMs: Bikel et al. 1997; MEMMs\n McCallum et al. 2000; CRFs: Lafferty et al. 2001), and a wide exploration of features (Zhou et al., 2005). Neural approaches followed from the pioneering results of\n Collobert et al. (2011), who applied a CRF on top of a convolutional net.\n Progress in this area continues to be stimulated by formal evaluations with shared\n benchmark datasets, including the Automatic Content Extraction (ACE) evaluations\n of 2000-2007 on named entity recognition, relation extraction, and temporal ex-\nKBP pressions1 , the KBP (Knowledge Base Population) evaluations (Ji et al. 2010, Surslot filling deanu 2013) of relation extraction tasks like slot filling (extracting attributes (‚Äòslots‚Äô)\n like age, birthplace, and spouse for a given entity) and a series of SemEval workshops (Hendrickx et al., 2009).\n Semisupervised relation extraction was first proposed by Hearst (1992b), and\n extended by systems like AutoSlog-TS (Riloff, 1996), DIPRE (Brin, 1998), SNOW-\nBALL (Agichtein and Gravano, 2000), and Jones et al. (1999). The distant supervision algorithm we describe was drawn from Mintz et al. (2009), who first used\n the term ‚Äòdistant supervision‚Äô (which was suggested to them by Chris Manning)\n but similar ideas had occurred in earlier systems like Craven and Kumlien (1999)\n and Morgan et al. (2004) under the name weakly labeled data, as well as in Snow\n et al. (2005) and Wu and Weld (2007). Among the many extensions are Wu and\n Weld (2010), Riedel et al. (2010), and Ritter et al. (2013). Open IE systems include\n K NOW I TA LL Etzioni et al. (2005), TextRunner (Banko et al., 2007), and R E V ERB\n (Fader et al., 2011). See Riedel et al. (2013) for a universal schema that combines\n the advantages of distant supervision and Open IE.\n\n 1 www.nist.gov/speech/tests/ace/\n26 C HAPTER 20 ‚Ä¢ I NFORMATION E XTRACTION : R ELATIONS , E VENTS , AND T IME\n\nExercises\n be accomplished by a simple form of relational analysis. Develop a system\n based on the relation analysis approaches described in this chapter to populate\n a database of acronym expansions. If you focus on English Three Letter\n Acronyms (TLAs) you can evaluate your system‚Äôs performance by comparing\n it to Wikipedia‚Äôs TLA page.\n using any of the techniques mentioned in Section 20.8. Analyze how well\n your system performs as compared with state-of-the-art results on this corpus.\n to associate temporal expressions connected with events in email (doctor‚Äôs\n appointments, meeting planning, party invitations, etc.) with specific calendar\n entries. Collect a corpus of email containing temporal expressions related to\n event planning. How do these expressions compare to the kinds of expressions\n commonly found in news text that we‚Äôve been discussing in this chapter?\n relationships between the events.\n 1. When Mary‚Äôs flight departed, I ate lunch.\n 2. When Mary‚Äôs flight departed, I had eaten lunch.\n Exercises 27\n\nAgichtein, E. and L. Gravano. 2000. Snowball: Extracting Fader, A., S. Soderland, and O. Etzioni. 2011. Identifying\n relations from large plain-text collections. Proceedings relations for open information extraction. EMNLP.\n of the 5th ACM International Conference on Digital Li- Ferro, L., L. Gerber, I. Mani, B. Sundheim, and G. Wilson.\n braries. 2005. Tides 2005 standard for the annotation of temporal\nAllen, J. 1984. Towards a general theory of action and time. expressions. Technical report, MITRE.\n Artificial Intelligence, 23(2):123‚Äì154. Grishman, R. and B. Sundheim. 1995. Design of the MUC-6\nBanko, M., M. Cafarella, S. Soderland, M. Broadhead, and evaluation. MUC-6.\n O. Etzioni. 2007. Open information extraction for the Hearst, M. A. 1992a. Automatic acquisition of hyponyms\n web. IJCAI. from large text corpora. COLING.\nBethard, S. 2013. ClearTK-TimeML: A minimalist approach Hearst, M. A. 1992b. Automatic acquisition of hyponyms\n to TempEval 2013. SemEval-13. from large text corpora. COLING.\nBikel, D. M., S. Miller, R. Schwartz, and R. Weischedel. Hearst, M. A. 1998. Automatic discovery of WordNet rela-\n1997. Nymble: A high-performance learning name- tions. In C. Fellbaum, ed., WordNet: An Electronic Lexifinder. ANLP. cal Database. MIT Press.\nBizer, C., J. Lehmann, G. Kobilarov, S. Auer, C. Becker, Hendrickx, I., S. N. Kim, Z. Kozareva, P. Nakov,\n R. Cyganiak, and S. Hellmann. 2009. DBpedia‚ÄîA crys- D. OÃÅ SeÃÅaghdha, S. PadoÃÅ, M. Pennacchiotti, L. Romano,\n tallization point for the Web of Data. Web Semantics: and S. Szpakowicz. 2009. Semeval-2010 task 8: Multiscience, services and agents on the world wide web, way classification of semantic relations between pairs of\n 7(3):154‚Äì165. nominals. 5th International Workshop on Semantic Eval-\nBollacker, K., C. Evans, P. Paritosh, T. Sturge, and J. Taylor. uation.\n 2008. Freebase: a collaboratively created graph database Hobbs, J. R., D. E. Appelt, J. Bear, D. Israel, M. Kameyama,\n for structuring human knowledge. SIGMOD 2008. M. E. Stickel, and M. Tyson. 1997. FASTUS: A cas-\nBrin, S. 1998. Extracting patterns and relations from caded finite-state transducer for extracting information\n the World Wide Web. Proceedings World Wide Web from natural-language text. In E. Roche and Y. Schand Databases International Workshop, Number 1590 in abes, eds, Finite-State Language Processing, 383‚Äì406.\n LNCS. Springer. MIT Press.\n Huffman, S. 1996. Learning information extraction pat-\nCardie, C. 1993. A case-based approach to knowledge acterns from examples. In S. Wertmer, E. Riloff, and\n quisition for domain specific sentence analysis. AAAI.\n G. Scheller, eds, Connectionist, Statistical, and Symbolic\nCardie, C. 1994. Domain-Specific Knowledge Acquisition Approaches to Learning Natural Language Processing,\n for Conceptual Sentence Analysis. Ph.D. thesis, Univer- 246‚Äì260. Springer.\n sity of Massachusetts, Amherst, MA. Available as CMP-\nISO8601. 2004. Data elements and interchange formats‚Äî\n SCI Technical Report 94-74.\n information interchange‚Äîrepresentation of dates and\nChambers, N. 2013. NavyTime: Event and time ordering times. Technical report, International Organization for\n from raw text. SemEval-13. Standards (ISO).\nChambers, N., T. Cassidy, B. McDowell, and S. Bethard. Jackendoff, R. 1983. Semantics and Cognition. MIT Press.\n 2014. Dense event ordering with a multi-pass architec- Jacobs, P. S. and L. F. Rau. 1990. SCISOR: A system\n ture. TACL, 2:273‚Äì284. for extracting information from on-line news. CACM,\nChambers, N. and D. Jurafsky. 2011. Template-based infor- 33(11):88‚Äì97.\n mation extraction without the templates. ACL. Ji, H., R. Grishman, and H. T. Dang. 2010. Overview of the\nChang, A. X. and C. D. Manning. 2012. SUTime: A library tac 2011 knowledge base population track. TAC-11.\n for recognizing and normalizing time expressions. LREC. Jones, R., A. McCallum, K. Nigam, and E. Riloff. 1999.\nChinchor, N., L. Hirschman, and D. L. Lewis. 1993. Eval- Bootstrapping for text learning tasks. IJCAI-99 Workshop\n uating Message Understanding systems: An analysis of on Text Mining: Foundations, Techniques and Applicathe third Message Understanding Conference. Computa- tions.\n tional Linguistics, 19(3):409‚Äì449. Joshi, M., D. Chen, Y. Liu, D. S. Weld, L. Zettlemoyer, and\nCollobert, R., J. Weston, L. Bottou, M. Karlen, O. Levy. 2020. SpanBERT: Improving pre-training by\n K. Kavukcuoglu, and P. Kuksa. 2011. Natural language representing and predicting spans. TACL, 8:64‚Äì77.\n processing (almost) from scratch. JMLR, 12:2493‚Äì2537. Lafferty, J. D., A. McCallum, and F. C. N. Pereira. 2001.\nCraven, M. and J. Kumlien. 1999. Constructing biologi- Conditional random fields: Probabilistic models for segcal knowledge bases by extracting information from text menting and labeling sequence data. ICML.\n sources. ISMB-99. Lakoff, G. and M. Johnson. 1980. Metaphors We Live By.\nDeJong, G. F. 1982. An overview of the FRUMP system. University of Chicago Press, Chicago, IL.\n In W. G. Lehnert and M. H. Ringle, eds, Strategies for Lehnert, W. G., C. Cardie, D. Fisher, E. Riloff, and\n Natural Language Processing, 149‚Äì176. LEA. R. Williams. 1991. Description of the CIRCUS system\nEtzioni, O., M. Cafarella, D. Downey, A.-M. Popescu, as used for MUC-3. MUC-3.\n T. Shaked, S. Soderland, D. S. Weld, and A. Yates. 2005. Liu, Y., M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen,\n Unsupervised named-entity extraction from the web: An O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov.\n experimental study. Artificial Intelligence, 165(1):91‚Äì 2019. RoBERTa: A robustly optimized BERT pretraining\n 134. approach. ArXiv preprint arXiv:1907.11692.\n28 Chapter 20 ‚Ä¢ Information Extraction: Relations, Events, and Time\n\nMcCallum, A., D. Freitag, and F. C. N. Pereira. 2000. Max- Sundheim, B., ed. 1993. Proceedings of MUC-5. Baltimore,\n imum entropy Markov models for information extraction MD.\n and segmentation. ICML. Sundheim, B., ed. 1995. Proceedings of MUC-6.\nMintz, M., S. Bills, R. Snow, and D. Jurafsky. 2009. Distant Surdeanu, M. 2013. Overview of the TAC2013 Knowledge\n supervision for relation extraction without labeled data. Base Population evaluation: English slot filling and tem-\nACL IJCNLP. poral slot filling. TAC-13.\nMirza, P. and S. Tonelli. 2016. CATENA: CAusal and TEm- Vendler, Z. 1967. Linguistics in Philosophy. Cornell Univerporal relation extraction from NAtural language texts. sity Press.\n COLING.\n Verhagen, M., R. Gaizauskas, F. Schilder, M. Hepple,\nMorgan, A. A., L. Hirschman, M. Colosimo, A. S. Yeh, and J. Moszkowicz, and J. Pustejovsky. 2009. The TempE-\nJ. B. Colombe. 2004. Gene name identification and nor- val challenge: Identifying temporal relations in text. Lanmalization using a model organism database. Journal of guage Resources and Evaluation, 43(2):161‚Äì179.\n Biomedical Informatics, 37(6):396‚Äì410.\n Verhagen, M., I. Mani, R. Sauri, R. Knippen, S. B. Jang,\nOcal, M., A. Perez, A. Radas, and M. Finlayson. 2022. J. Littman, A. Rumshisky, J. Phillips, and J. Pustejovsky.\n Holistic evaluation of automatic TimeML annotators. 2005. Automating temporal annotation with TARSQI.\n LREC. ACL.\nPustejovsky, J., P. Hanks, R. Saurƒ±ÃÅ, A. See, R. Gaizauskas, VrandecÃåicÃÅ, D. and M. KroÃàtzsch. 2014. Wikidata: a free col-\nA. Setzer, D. Radev, B. Sundheim, D. S. Day, L. Ferro, laborative knowledge base. CACM, 57(10):78‚Äì85.\n and M. Lazo. 2003. The TIMEBANK corpus. Proceedings of Corpus Linguistics 2003 Conference. UCREL Wu, F. and D. S. Weld. 2007. Autonomously semantifying\n Technical Paper number 16. Wikipedia. CIKM-07.\n\nPustejovsky, J., R. Ingria, R. Saurƒ±ÃÅ, J. CastanÃÉo, J. Littman, Wu, F. and D. S. Weld. 2010. Open information extraction\n R. Gaizauskas, A. Setzer, G. Katz, and I. Mani. 2005. The using Wikipedia. ACL.\n Specification Language TimeML, chapter 27. Oxford. Zhang, Y., V. Zhong, D. Chen, G. Angeli, and C. D. Man-\nReichenbach, H. 1947. Elements of Symbolic Logic. Macmil- ning. 2017. Position-aware attention and supervised data\n lan, New York. improve slot filling. EMNLP.\n\nRiedel, S., L. Yao, and A. McCallum. 2010. Modeling rela- Zhou, G., J. Su, J. Zhang, and M. Zhang. 2005. Exploring\n tions and their mentions without labeled text. In Machine various knowledge in relation extraction. ACL.\n Learning and Knowledge Discovery in Databases, 148‚Äì\n 163. Springer.\nRiedel, S., L. Yao, A. McCallum, and B. M. Marlin. 2013.\n Relation extraction with matrix factorization and universal schemas. NAACL HLT.\nRiloff, E. 1993. Automatically constructing a dictionary for\n information extraction tasks. AAAI.\nRiloff, E. 1996. Automatically generating extraction patterns\n from untagged text. AAAI.\nRiloff, E. and R. Jones. 1999. Learning dictionaries for information extraction by multi-level bootstrapping. AAAI.\nRitter, A., L. Zettlemoyer, Mausam, and O. Etzioni. 2013.\n Modeling missing data in distant supervision for information extraction. TACL, 1:367‚Äì378.\nSaurƒ±ÃÅ, R., J. Littman, B. Knippen, R. Gaizauskas, A. Setzer,\n and J. Pustejovsky. 2006. TimeML annotation guidelines\n version 1.2.1. Manuscript.\nSchank, R. C. and R. P. Abelson. 1977. Scripts, Plans, Goals\n and Understanding. Lawrence Erlbaum.\nSnow, R., D. Jurafsky, and A. Y. Ng. 2005. Learning syntactic patterns for automatic hypernym discovery. NeurIPS.\nSoderland, S., D. Fisher, J. Aseltine, and W. G. Lehnert. 1995. CRYSTAL: Inducing a conceptual dictionary.\n IJCAI-95.\nStroÃàtgen, J. and M. Gertz. 2013. Multilingual and crossdomain temporal tagging. Language Resources and Evaluation, 47(2):269‚Äì298.\nSundheim, B., ed. 1991. Proceedings of MUC-3.\nSundheim, B., ed. 1992. Proceedings of MUC-4.\n",
    "file_path": "/Users/colinsidberry/nlp-textbook-rag/data/extracted_text/20.Information Extraction- Relations, Events, and Time.txt",
    "file_size_kb": 80.37
  },
  {
    "id": "aaf3bb935c18e3ea",
    "source": "nlp_textbook",
    "chapter": "Semantic Role Labeling",
    "filename": "21.Semantic Role Labeling.txt",
    "content": "rights reserved. Draft of August 24, 2025.\n\nCHAPTER\n\n Semantic Role Labeling\n ‚ÄúWho, What, Where, When, With what, Why, How‚Äù\n The seven circumstances, associated with Hermagoras and Aristotle (Sloan, 2010)\n\n Sometime between the 7th and 4th centuries BCE, the Indian grammarian PaÃÑn.ini1\n wrote a famous treatise on Sanskrit grammar, the As.t.aÃÑdhyaÃÑyƒ±ÃÑ (‚Äò8 books‚Äô), a treatise\n that has been called ‚Äúone of the greatest monuments of human intelligence‚Äù (Bloomfield, 1933, 11). The work describes the linguistics of the Sanskrit language in the form\n of 3959 sutras, each very efficiently (since it had to be\n memorized!) expressing part of a formal rule system that\n brilliantly prefigured modern mechanisms of formal language theory (Penn and Kiparsky, 2012). One set of rules\n describes the kaÃÑrakas, semantic relationships between a\n verb and noun arguments, roles like agent, instrument, or\n destination. PaÃÑn.ini‚Äôs work was the earliest we know of\n that modeled the linguistic realization of events and their\n participants. This task of understanding how participants relate to events‚Äîbeing\n able to answer the question ‚ÄúWho did what to whom‚Äù (and perhaps also ‚Äúwhen and\n where‚Äù)‚Äîis a central question of natural language processing.\n Let‚Äôs move forward 2.5 millennia to the present and consider the very mundane\n goal of understanding text about a purchase of stock by XYZ Corporation. This\n purchasing event and its participants can be described by a wide variety of surface\n forms. The event can be described by a verb (sold, bought) or a noun (purchase),\n and XYZ Corp can be the syntactic subject (of bought), the indirect object (of sold),\n or in a genitive or noun compound relation (with the noun purchase) despite having\n notionally the same role in all of them:\n ‚Ä¢ XYZ corporation bought the stock.\n ‚Ä¢ They sold the stock to XYZ corporation.\n ‚Ä¢ The stock was bought by XYZ corporation.\n ‚Ä¢ The purchase of the stock by XYZ corporation...\n ‚Ä¢ The stock purchase by XYZ corporation...\n In this chapter we introduce a level of representation that captures the commonality between these sentences: there was a purchase event, the participants were\n XYZ Corp and some stock, and XYZ Corp was the buyer. These shallow semantic\n representations , semantic roles, express the role that arguments of a predicate take\n in the event, codified in databases like PropBank and FrameNet. We‚Äôll introduce\n semantic role labeling, the task of assigning roles to spans in sentences, and selectional restrictions, the preferences that predicates express about their arguments,\n such as the fact that the theme of eat is generally something edible.\n 1 Figure shows a birch bark manuscript from Kashmir of the Rupavatra, a grammatical textbook based\n on the Sanskrit grammar of Panini. Image from the Wellcome Collection.\n2 C HAPTER 21 ‚Ä¢ S EMANTIC ROLE L ABELING\n\n Consider the meanings of the arguments Sasha, Pat, the window, and the door in\n these two sentences.\n (21.1) Sasha broke the window.\n (21.2) Pat opened the door.\n The subjects Sasha and Pat, what we might call the breaker of the windowbreaking event and the opener of the door-opening event, have something in common. They are both volitional actors, often animate, and they have direct causal\n responsibility for their events.\n thematic roles Thematic roles are a way to capture this semantic commonality between breakagents ers and openers. We say that the subjects of both these verbs are agents. Thus,\n AGENT is the thematic role that represents an abstract idea such as volitional causation. Similarly, the direct objects of both these verbs, the BrokenThing and OpenedThing,\n are both prototypically inanimate objects that are affected in some way by the action.\n theme The semantic role for these participants is theme.\n\n Thematic Role Definition\n AGENT The volitional causer of an event\n EXPERIENCER The experiencer of an event\n FORCE The non-volitional causer of the event\n THEME The participant most directly affected by an event\n RESULT The end product of an event\n CONTENT The proposition or content of a propositional event\n INSTRUMENT An instrument used in an event\n BENEFICIARY The beneficiary of an event\n SOURCE The origin of the object of a transfer event\n GOAL The destination of an object of a transfer event\n\n Although thematic roles are one of the oldest linguistic models, as we saw above,\n their modern formulation is due to Fillmore (1968) and Gruber (1965). Although\n there is no universally agreed-upon set of roles, Figs. 21.1 and 21.2 list some thematic roles that have been used in various computational papers, together with rough\n definitions and examples. Most thematic role sets have about a dozen roles, but we‚Äôll\n see sets with smaller numbers of roles with even more abstract meanings, and sets\n with very large numbers of roles that are specific to situations. We‚Äôll use the general\n semantic roles term semantic roles for all sets of roles, whether small or large.\n\n The main reason computational systems use semantic roles is to act as a shallow\n meaning representation that can let us make simple inferences that aren‚Äôt possible\n from the pure surface string of words, or even from the parse tree. To extend the\n earlier examples, if a document says that Company A acquired Company B, we‚Äôd\n like to know that this answers the query Was Company B acquired? despite the fact\n that the two sentences have very different surface syntax. Similarly, this shallow\n semantics might act as a useful intermediate language in machine translation.\n 21.2 ‚Ä¢ D IATHESIS A LTERNATIONS 3\n\n Thematic Role Example\n AGENT The waiter spilled the soup.\n EXPERIENCER John has a headache.\n FORCE The wind blows debris from the mall into our yards.\n THEME Only after Benjamin Franklin broke the ice...\n RESULT The city built a regulation-size baseball diamond...\n CONTENT Mona asked ‚ÄúYou met Mary Ann at a supermarket?‚Äù\n INSTRUMENT He poached catfish, stunning them with a shocking device...\n BENEFICIARY Whenever Ann Callahan makes hotel reservations for her boss...\n SOURCE I flew in from Boston.\n GOAL I drove to Portland.\n\n Semantic roles thus help generalize over different surface realizations of predicate arguments. For example, while the AGENT is often realized as the subject of\n the sentence, in other cases the THEME can be the subject. Consider these possible\n realizations of the thematic arguments of the verb break:\n (21.3) John broke the window.\n AGENT THEME\n (21.4) John broke the window with a rock.\n AGENT THEME INSTRUMENT\n (21.5) The rock broke the window.\n INSTRUMENT THEME\n (21.6) The window broke.\n THEME\n (21.7) The window was broken by John.\n THEME AGENT\n These examples suggest that break has (at least) the possible arguments AGENT,\n THEME , and INSTRUMENT. The set of thematic role arguments taken by a verb is\nthematic grid often called the thematic grid, Œ∏ -grid, or case frame. We can see that there are\n case frame (among others) the following possibilities for the realization of these arguments of\n break:\n AGENT/Subject, THEME /Object\n AGENT/Subject, THEME /Object, INSTRUMENT/PPwith\n INSTRUMENT/Subject, THEME /Object\n THEME /Subject\n\n It turns out that many verbs allow their thematic roles to be realized in various\n syntactic positions. For example, verbs like give can realize the THEME and GOAL\n arguments in two different ways:\n (21.8) a. Doris gave the book to Cary.\n AGENT THEME GOAL\n\n b. Doris gave Cary the book.\n AGENT GOAL THEME\n\n These multiple argument structure realizations (the fact that break can take AGENT,\n INSTRUMENT, or THEME as subject, and give can realize its THEME and GOAL in\n verb either order) are called verb alternations or diathesis alternations. The alternation\n alternation\n dative we showed above for give, the dative alternation, seems to occur with particular sealternation\n mantic classes of verbs, including ‚Äúverbs of future having‚Äù (advance, allocate, offer,\n4 C HAPTER 21 ‚Ä¢ S EMANTIC ROLE L ABELING\n\n owe), ‚Äúsend verbs‚Äù (forward, hand, mail), ‚Äúverbs of throwing‚Äù (kick, pass, throw),\n and so on. Levin (1993) lists for 3100 English verbs the semantic classes to which\n they belong (47 high-level classes, divided into 193 more specific classes) and the\n various alternations in which they participate. These lists of verb classes have been\n incorporated into the online resource VerbNet (Kipper et al., 2000), which links each\n verb to both WordNet and FrameNet entries.\n\n Representing meaning at the thematic role level seems like it should be useful in\n dealing with complications like diathesis alternations. Yet it has proved quite difficult to come up with a standard set of roles, and equally difficult to produce a formal\n definition of roles like AGENT, THEME, or INSTRUMENT.\n For example, researchers attempting to define role sets often find they need to\n fragment a role like AGENT or THEME into many specific roles. Levin and Rappaport Hovav (2005) summarize a number of such cases, such as the fact there seem\n to be at least two kinds of INSTRUMENTS, intermediary instruments that can appear\n as subjects and enabling instruments that cannot:\n (21.9) a. Shelly cut the banana with a knife.\n b. The knife cut the banana.\n (21.10) a. Shelly ate the sliced banana with a fork.\n b. *The fork ate the sliced banana.\n In addition to the fragmentation problem, there are cases in which we‚Äôd like to\n reason about and generalize across semantic roles, but the finite discrete lists of roles\n don‚Äôt let us do this.\n Finally, it has proved difficult to formally define the thematic roles. Consider the\n AGENT role; most cases of AGENTS are animate, volitional, sentient, causal, but any\n individual noun phrase might not exhibit all of these properties.\n semantic role These problems have led to alternative semantic role models that use either\n many fewer or many more roles.\n The first of these options is to define generalized semantic roles that abstract\n proto-agent over the specific thematic roles. For example, PROTO - AGENT and PROTO - PATIENT\n proto-patient are generalized roles that express roughly agent-like and roughly patient-like meanings. These roles are defined, not by necessary and sufficient conditions, but rather\n by a set of heuristic features that accompany more agent-like or more patient-like\n meanings. Thus, the more an argument displays agent-like properties (being volitionally involved in the event, causing an event or a change of state in another participant, being sentient or intentionally involved, moving) the greater the likelihood\n that the argument can be labeled a PROTO - AGENT. The more patient-like the properties (undergoing change of state, causally affected by another participant, stationary\n relative to other participants, etc.), the greater the likelihood that the argument can\n be labeled a PROTO - PATIENT.\n The second direction is instead to define semantic roles that are specific to a\n particular verb or a particular group of semantically related verbs or nouns.\n In the next two sections we describe two commonly used lexical resources that\n make use of these alternative versions of semantic roles. PropBank uses both protoroles and verb-specific semantic roles. FrameNet uses semantic roles that are specific to a general semantic idea called a frame.\n 21.4 ‚Ä¢ T HE P ROPOSITION BANK 5\n\n PropBank The Proposition Bank, generally referred to as PropBank, is a resource of sentences annotated with semantic roles. The English PropBank labels all the sentences\n in the Penn TreeBank; the Chinese PropBank labels sentences in the Penn Chinese\n TreeBank. Because of the difficulty of defining a universal set of thematic roles,\n the semantic roles in PropBank are defined with respect to an individual verb sense.\n Each sense of each verb thus has a specific set of roles, which are given only numbers\n rather than names: Arg0, Arg1, Arg2, and so on. In general, Arg0 represents the\n PROTO - AGENT, and Arg1, the PROTO - PATIENT . The semantics of the other roles\n are less consistent, often being defined specifically for each verb. Nonetheless there\n are some generalization; the Arg2 is often the benefactive, instrument, attribute, or\n end state, the Arg3 the start point, benefactive, instrument, or attribute, and the Arg4\n the end point.\n Here are some slightly simplified PropBank entries for one sense each of the\n verbs agree and fall. Such PropBank entries are called frame files; note that the\n definitions in the frame file for each role (‚ÄúOther entity agreeing‚Äù, ‚ÄúExtent, amount\n fallen‚Äù) are informal glosses intended to be read by humans, rather than being formal\n definitions.\n (21.11) agree.01\n Arg0: Agreer\n Arg1: Proposition\n Arg2: Other entity agreeing\n\n Ex1: [Arg0 The group] agreed [Arg1 it wouldn‚Äôt make an offer].\n Ex2: [ArgM-TMP Usually] [Arg0 John] agrees [Arg2 with Mary]\n [Arg1 on everything].\n (21.12) fall.01\n Arg1: Logical subject, patient, thing falling\n Arg2: Extent, amount fallen\n Arg3: start point\n Arg4: end point, end state of arg1\n Ex1: [Arg1 Sales] fell [Arg4 to $25 million] [Arg3 from $27 million].\n Ex2: [Arg1 The average junk bond] fell [Arg2 by 4.2%].\n Note that there is no Arg0 role for fall, because the normal subject of fall is a\n PROTO - PATIENT .\n The PropBank semantic roles can be useful in recovering shallow semantic information about verbal arguments. Consider the verb increase:\n (21.13) increase.01 ‚Äúgo up incrementally‚Äù\n Arg0: causer of increase\n Arg1: thing increasing\n Arg2: amount increased by, EXT, or MNR\n Arg3: start point\n Arg4: end point\n A PropBank semantic role labeling would allow us to infer the commonality in\n the event structures of the following three examples, that is, that in each case Big\n Fruit Co. is the AGENT and the price of bananas is the THEME, despite the differing\n surface forms.\n6 C HAPTER 21 ‚Ä¢ S EMANTIC ROLE L ABELING\n\n (21.14) [Arg0 Big Fruit Co. ] increased [Arg1 the price of bananas].\n (21.15) [Arg1 The price of bananas] was increased again [Arg0 by Big Fruit Co. ]\n (21.16) [Arg1 The price of bananas] increased [Arg2 5%].\n PropBank also has a number of non-numbered arguments called ArgMs, (ArgM-\nTMP, ArgM-LOC, etc.) which represent modification or adjunct meanings. These\n are relatively stable across predicates, so aren‚Äôt listed with each frame file. Data\n labeled with these modifiers can be helpful in training systems to detect temporal,\n location, or directional modification across predicates. Some of the ArgMs include:\n TMP when? yesterday evening, now\n LOC where? at the museum, in San Francisco\n DIR where to/from? down, to Bangkok\n MNR how? clearly, with much enthusiasm\n PRP/CAU why? because ... , in response to the ruling\n REC themselves, each other\n ADV miscellaneous\n PRD secondary predication ...ate the meat raw\n NomBank While PropBank focuses on verbs, a related project, NomBank (Meyers et al.,\n 2004) adds annotations to noun predicates. For example the noun agreement in\n Apple‚Äôs agreement with IBM would be labeled with Apple as the Arg0 and IBM as\n the Arg2. This allows semantic role labelers to assign labels to arguments of both\n verbal and nominal predicates.\n\n While making inferences about the semantic commonalities across different sentences with increase is useful, it would be even more useful if we could make such\n inferences in many more situations, across different verbs, and also between verbs\n and nouns. For example, we‚Äôd like to extract the similarity among these three sentences:\n (21.17) [Arg1 The price of bananas] increased [Arg2 5%].\n (21.18) [Arg1 The price of bananas] rose [Arg2 5%].\n (21.19) There has been a [Arg2 5%] rise [Arg1 in the price of bananas].\n Note that the second example uses the different verb rise, and the third example\n uses the noun rather than the verb rise. We‚Äôd like a system to recognize that the\n price of bananas is what went up, and that 5% is the amount it went up, no matter\n whether the 5% appears as the object of the verb increased or as a nominal modifier\n of the noun rise.\n FrameNet The FrameNet project is another semantic-role-labeling project that attempts\n to address just these kinds of problems (Baker et al. 1998, Fillmore et al. 2003,\n Fillmore and Baker 2009, Ruppenhofer et al. 2016). Whereas roles in the PropBank\n project are specific to an individual verb, roles in the FrameNet project are specific\n to a frame.\n What is a frame? Consider the following set of words:\n reservation, flight, travel, buy, price, cost, fare, rates, meal, plane\n There are many individual lexical relations of hyponymy, synonymy, and so on\n between many of the words in this list. The resulting set of relations does not,\n 21.5 ‚Ä¢ F RAME N ET 7\n\n however, add up to a complete account of how these words are related. They are\n clearly all defined with respect to a coherent chunk of common-sense background\n information concerning air travel.\n frame We call the holistic background knowledge that unites these words a frame (Fillmore, 1985). The idea that groups of words are defined with respect to some background information is widespread in artificial intelligence and cognitive science,\n model where besides frame we see related works like a model (Johnson-Laird, 1983), or\n script even script (Schank and Abelson, 1977).\n A frame in FrameNet is a background knowledge structure that defines a set of\nframe elements frame-specific semantic roles, called frame elements, and includes a set of predicates that use these roles. Each word evokes a frame and profiles some aspect of the\n frame and its elements. The FrameNet dataset includes a set of frames and frame\n elements, the lexical units associated with each frame, and a set of labeled example sentences. For example, the change position on a scale frame is defined as\n follows:\n This frame consists of words that indicate the change of an Item‚Äôs position on a scale (the Attribute) from a starting point (Initial value) to an\n end point (Final value).\n Some of the semantic roles (frame elements) in the frame are defined as in\n core roles Fig. 21.3. Note that these are separated into core roles, which are frame specific, and\n non-core roles non-core roles, which are more like the Arg-M arguments in PropBank, expressing\n more general properties of time, location, and so on.\n\n Core Roles\nATTRIBUTE The ATTRIBUTE is a scalar property that the I TEM possesses.\nD IFFERENCE The distance by which an I TEM changes its position on the scale.\nF INAL STATE A description that presents the I TEM‚Äôs state after the change in the ATTRIBUTE‚Äôs\n value as an independent predication.\nF INAL VALUE The position on the scale where the I TEM ends up.\nI NITIAL STATE A description that presents the I TEM‚Äôs state before the change in the AT-\nTRIBUTE ‚Äôs value as an independent predication.\nI NITIAL VALUE The initial position on the scale from which the I TEM moves away.\nI TEM The entity that has a position on the scale.\nVALUE RANGE A portion of the scale, typically identified by its end points, along which the\n values of the ATTRIBUTE fluctuate.\n Some Non-Core Roles\nD URATION The length of time over which the change takes place.\nS PEED The rate of change of the VALUE.\nG ROUP The G ROUP in which an I TEM changes the value of an\n ATTRIBUTE in a specified way.\nGuide (Ruppenhofer et al., 2016).\n\n Here are some example sentences:\n (21.20) [I TEM Oil] rose [ATTRIBUTE in price] [D IFFERENCE by 2%].\n (21.21) [I TEM It] has increased [F INAL STATE to having them 1 day a month].\n (21.22) [I TEM Microsoft shares] fell [F INAL VALUE to 7 5/8].\n (21.23) [I TEM Colon cancer incidence] fell [D IFFERENCE by 50%] [G ROUP among\n men].\n8 C HAPTER 21 ‚Ä¢ S EMANTIC ROLE L ABELING\n\n (21.24) a steady increase [I NITIAL VALUE from 9.5] [F INAL VALUE to 14.3] [I TEM\n in dividends]\n (21.25) a [D IFFERENCE 5%] [I TEM dividend] increase...\n Note from these example sentences that the frame includes target words like rise,\n fall, and increase. In fact, the complete frame consists of the following words:\n VERBS: dwindle move soar escalation shift\n advance edge mushroom swell explosion tumble\n climb explode plummet swing fall\n decline fall reach triple fluctuation ADVERBS:\n decrease fluctuate rise tumble gain increasingly\n diminish gain rocket growth\n dip grow shift NOUNS: hike\n double increase skyrocket decline increase\n drop jump slide decrease rise\n FrameNet also codes relationships between frames, allowing frames to inherit\n from each other, or representing relations between frames like causation (and generalizations among frame elements in different frames can be represented by inheritance as well). Thus, there is a Cause change of position on a scale frame that is\n linked to the Change of position on a scale frame by the cause relation, but that\n adds an AGENT role and is used for causative examples such as the following:\n (21.26) [AGENT They] raised [I TEM the price of their soda] [D IFFERENCE by 2%].\n Together, these two frames would allow an understanding system to extract the\n common event semantics of all the verbal and nominal causative and non-causative\n usages.\n FrameNets have also been developed for many other languages including Spanish, German, Japanese, Portuguese, Italian, and Chinese.\n\n semantic role\n labeling Semantic role labeling (sometimes shortened as SRL) is the task of automatically\n finding the semantic roles of each argument of each predicate in a sentence. Current approaches to semantic role labeling are based on supervised machine learning,\n often using the FrameNet and PropBank resources to specify what counts as a predicate, define the set of roles used in the task, and provide training and test sets.\n Recall that the difference between these two models of semantic roles is that\n FrameNet (21.27) employs many frame-specific frame elements as roles, while Prop-\nBank (21.28) uses a smaller number of numbered argument labels that can be interpreted as verb-specific labels, along with the more general ARGM labels. Some\n examples:\n [You] can‚Äôt [blame] [the program] [for being unable to identify it]\n (21.27)\n COGNIZER TARGET EVALUEE REASON\n [The San Francisco Examiner] issued [a special edition] [yesterday]\n (21.28)\n ARG 0 TARGET ARG 1 ARGM - TMP\n\n 21.6.1 A Feature-based Algorithm for Semantic Role Labeling\n A simplified feature-based semantic role labeling algorithm is sketched in Fig. 21.4.\n Feature-based algorithms‚Äîfrom the very earliest systems like (Simmons, 1973)‚Äî\n begin by parsing, using broad-coverage parsers to assign a parse to the input string.\n 21.6 ‚Ä¢ S EMANTIC ROLE L ABELING 9\n\n words that are predicates.\n For each of these predicates, the algorithm examines each node in the parse\n tree and uses supervised classification to decide the semantic role (if any) it plays\n for this predicate. Given a labeled training set such as PropBank or FrameNet, a\n feature vector is extracted for each node, using feature templates described in the\n next subsection. A 1-of-N classifier is then trained to predict a semantic role for\n each constituent given these features, where N is the number of potential semantic\n roles plus an extra NONE role for non-role constituents. Any standard classification\n algorithms can be used. Finally, for each test sentence to be labeled, the classifier is\n run on each relevant constituent.\n\n function S EMANTIC ROLE L ABEL(words) returns labeled tree\n\n parse ‚Üê PARSE(words)\n for each predicate in parse do\n for each node in parse do\n featurevector ‚Üê E XTRACT F EATURES(node, predicate, parse)\n C LASSIFY N ODE(node, featurevector, parse)\n\n such as FrameNet or PropBank.\n\n S\n\n NP-SBJ = ARG0 VP\n\n DT NNP NNP NNP\n\n The San Francisco Examiner\n\n VBD = TARGET NP = ARG1 PP-TMP = ARGM-TMP\n\n issued DT JJ NN IN NP\n\n a special edition around NN NP-TMP\n\n noon yesterday\n\nshows the path feature NP‚ÜëS‚ÜìVP‚ÜìVBD for ARG0, the NP-SBJ constituent The San Francisco Examiner.\n\n Instead of training a single-stage classifier as in Fig. 21.5, the node-level classification task can be broken down into multiple steps:\n 1. Pruning: Since only a small number of the constituents in a sentence are\n arguments of any given predicate, many systems use simple heuristics to prune\n unlikely constituents.\n 2. Identification: a binary classification of each node as an argument to be labeled or a NONE.\n 3. Classification: a 1-of-N classification of all the constituents that were labeled\n as arguments by the previous stage\n10 C HAPTER 21 ‚Ä¢ S EMANTIC ROLE L ABELING\n\n The separation of identification and classification may lead to better use of features (different features may be useful for the two tasks) or to computational efficiency.\n\n Global Optimization\n The classification algorithm of Fig. 21.5 classifies each argument separately (‚Äòlocally‚Äô), making the simplifying assumption that each argument of a predicate can be\n labeled independently. This assumption is false; there are interactions between arguments that require a more ‚Äòglobal‚Äô assignment of labels to constituents. For example,\n constituents in FrameNet and PropBank are required to be non-overlapping. More\n significantly, the semantic roles of constituents are not independent. For example\n PropBank does not allow multiple identical arguments; two constituents of the same\n verb cannot both be labeled ARG 0 .\n Role labeling systems thus often add a fourth step to deal with global consistency\n across the labels in a sentence. For example, the local classifiers can return a list of\n possible labels associated with probabilities for each constituent, and a second-pass\n Viterbi decoding or re-ranking approach can be used to choose the best consensus\n label. Integer linear programming (ILP) is another common way to choose a solution\n that conforms best to multiple constraints.\n\n Features for Semantic Role Labeling\n Most systems use some generalization of the core set of features introduced by\n Gildea and Jurafsky (2000). Common basic features templates (demonstrated on\n the NP-SBJ constituent The San Francisco Examiner in Fig. 21.5) include:\n ‚Ä¢ The governing predicate, in this case the verb issued. The predicate is a crucial feature since labels are defined only with respect to a particular predicate.\n ‚Ä¢ The phrase type of the constituent, in this case, NP (or NP-SBJ). Some semantic roles tend to appear as NPs, others as S or PP, and so on.\n ‚Ä¢ The headword of the constituent, Examiner. The headword of a constituent\n can be computed with standard head rules, such as those given in Appendix D\n in Fig. ??. Certain headwords (e.g., pronouns) place strong constraints on the\n possible semantic roles they are likely to fill.\n ‚Ä¢ The headword part of speech of the constituent, NNP.\n ‚Ä¢ The path in the parse tree from the constituent to the predicate. This path is\n marked by the dotted line in Fig. 21.5. Following Gildea and Jurafsky (2000),\n we can use a simple linear representation of the path, NP‚ÜëS‚ÜìVP‚ÜìVBD. ‚Üë and\n ‚Üì represent upward and downward movement in the tree, respectively. The\n path is very useful as a compact representation of many kinds of grammatical\n function relationships between the constituent and the predicate.\n ‚Ä¢ The voice of the clause in which the constituent appears, in this case, active\n (as contrasted with passive). Passive sentences tend to have strongly different\n linkings of semantic roles to surface form than do active ones.\n ‚Ä¢ The binary linear position of the constituent with respect to the predicate,\n either before or after.\n ‚Ä¢ The subcategorization of the predicate, the set of expected arguments that\n appear in the verb phrase. We can extract this information by using the phrasestructure rule that expands the immediate parent of the predicate; VP ‚Üí VBD\n NP PP for the predicate in Fig. 21.5.\n ‚Ä¢ The named entity type of the constituent.\n 21.6 ‚Ä¢ S EMANTIC ROLE L ABELING 11\n\n ‚Ä¢ The first words and the last word of the constituent.\n The following feature vector thus represents the first NP in our example (recall\nthat most observations will have the value NONE rather than, for example, ARG 0,\nsince most constituents in the parse tree will not bear a semantic role):\n\n ARG 0: [issued, NP, Examiner, NNP, NP‚ÜëS‚ÜìVP‚ÜìVBD, active, before, VP ‚Üí NP PP,\n ORG, The, Examiner]\n\n Other features are often used in addition, such as sets of n-grams inside the\nconstituent, or more complex versions of the path features (the upward or downward\nhalves, or whether particular nodes occur in the path).\n It‚Äôs also possible to use dependency parses instead of constituency parses as the\nbasis of features, for example using dependency parse paths instead of constituency\npaths.\n\n21.6.2 A Neural Algorithm for Semantic Role Labeling\nA simple neural approach to SRL is to treat it as a sequence labeling task like namedentity recognition, using the BIO approach. Let‚Äôs assume that we are given the\npredicate and the task is just detecting and labeling spans. Recall that with BIO\ntagging, we have a begin and end tag for each possible role (B - ARG 0, I - ARG 0; B -\nARG 1, I - ARG 1, and so on), plus an outside tag O .\n\n B-ARG0 I-ARG0 B-PRED B-ARG1\n\n Softmax\n\n FFN FFN FFN FFN FFN\n\n concatenate\n with predicate\n\n ENCODER\n\n [CLS] the cats love hats [SEP] love [SEP]\nfollowed by [SEP] and an extra input for the predicate, in this case love. The encoder outputs\nare concatenated to an indicator variable which is 1 for the predicate and 0 for all other words\nAfter He et al. (2017) and Shi and Lin (2019).\n\n As with all the taggers, the goal is to compute the highest probability tag sequence yÃÇ, given the input sequence of words w:\n\n yÃÇ = argmax P(y|w)\n y‚ààT\n\nFig. 21.6 shows a sketch of a standard algorithm from He et al. (2017). Here each\ninput word is mapped to pretrained embeddings, and then each token is concatenated\nwith the predicate embedding and then passed through a feedforward network with\na softmax which outputs a distribution over each SRL label. For decoding, a CRF\nlayer can be used instead of the MLP layer on top of the biLSTM output to do global\ninference, but in practice this doesn‚Äôt seem to provide much benefit.\n12 C HAPTER 21 ‚Ä¢ S EMANTIC ROLE L ABELING\n\n 21.6.3 Evaluation of Semantic Role Labeling\n The standard evaluation for semantic role labeling is to require that each argument\n label must be assigned to the exactly correct word sequence or parse constituent, and\n then compute precision, recall, and F-measure. Identification and classification can\n also be evaluated separately. Two common datasets used for evaluation are CoNLL-\n2005 (Carreras and MaÃÄrquez, 2005) and CoNLL-2012 (Pradhan et al., 2013).\n\n We turn in this section to another way to represent facts about the relationship beselectional tween predicates and arguments. A selectional restriction is a semantic type conrestriction\n straint that a verb imposes on the kind of concepts that are allowed to fill its argument\n roles. Consider the two meanings associated with the following example:\n (21.29) I want to eat someplace nearby.\n There are two possible parses and semantic interpretations for this sentence. In\n the sensible interpretation, eat is intransitive and the phrase someplace nearby is\n an adjunct that gives the location of the eating event. In the nonsensical speaker-as-\nGodzilla interpretation, eat is transitive and the phrase someplace nearby is the direct\n object and the THEME of the eating, like the NP Malaysian food in the following\n sentences:\n (21.30) I want to eat Malaysian food.\n How do we know that someplace nearby isn‚Äôt the direct object in this sentence?\n One useful cue is the semantic fact that the THEME of E ATING events tends to be\n something that is edible. This restriction placed by the verb eat on the filler of its\n THEME argument is a selectional restriction.\n Selectional restrictions are associated with senses, not entire lexemes. We can\n see this in the following examples of the lexeme serve:\n (21.31) The restaurant serves green-lipped mussels.\n (21.32) Which airlines serve Denver?\n Example (21.31) illustrates the offering-food sense of serve, which ordinarily restricts its THEME to be some kind of food Example (21.32) illustrates the provides a\n commercial service to sense of serve, which constrains its THEME to be some type\n of appropriate location.\n Selectional restrictions vary widely in their specificity. The verb imagine, for\n example, imposes strict requirements on its AGENT role (restricting it to humans\n and other animate entities) but places very few semantic requirements on its THEME\n role. A verb like diagonalize, on the other hand, places a very specific constraint\n on the filler of its THEME role: it has to be a matrix, while the arguments of the\n adjective odorless are restricted to concepts that could possess an odor:\n (21.33) In rehearsal, I often ask the musicians to imagine a tennis game.\n (21.34) Radon is an odorless gas that can‚Äôt be detected by human senses.\n (21.35) To diagonalize a matrix is to find its eigenvalues.\n These examples illustrate that the set of concepts we need to represent selectional\n restrictions (being a matrix, being able to possess an odor, etc) is quite open ended.\n This distinguishes selectional restrictions from other features for representing lexical\n knowledge, like parts-of-speech, which are quite limited in number.\n 21.7 ‚Ä¢ S ELECTIONAL R ESTRICTIONS 13\n\n21.7.1 Representing Selectional Restrictions\nOne way to capture the semantics of selectional restrictions is to use and extend the\nevent representation of Appendix F. Recall that the neo-Davidsonian representation\nof an event consists of a single variable that stands for the event, a predicate denoting\nthe kind of event, and variables and relations for the event roles. Ignoring the issue of\nthe Œª -structures and using thematic roles rather than deep event roles, the semantic\ncontribution of a verb like eat might look like the following:\n\n ‚àÉe, x, y Eating(e) ‚àß Agent(e, x) ‚àß T heme(e, y)\n\nWith this representation, all we know about y, the filler of the THEME role, is that\nit is associated with an Eating event through the Theme relation. To stipulate the\nselectional restriction that y must be something edible, we simply add a new term to\nthat effect:\n\n ‚àÉe, x, y Eating(e) ‚àß Agent(e, x) ‚àß T heme(e, y) ‚àß EdibleT hing(y)\n\nWhen a phrase like ate a hamburger is encountered, a semantic analyzer can form\nthe following kind of representation:\n\n ‚àÉe, x, y Eating(e) ‚àß Eater(e, x) ‚àß T heme(e, y) ‚àß EdibleT hing(y) ‚àß Hamburger(y)\n\nThis representation is perfectly reasonable since the membership of y in the category\nHamburger is consistent with its membership in the category EdibleThing, assuming\na reasonable set of facts in the knowledge base. Correspondingly, the representation\nfor a phrase such as ate a takeoff would be ill-formed because membership in an\nevent-like category such as Takeoff would be inconsistent with membership in the\ncategory EdibleThing.\n While this approach adequately captures the semantics of selectional restrictions,\nthere are two problems with its direct use. First, using FOL to perform the simple\ntask of enforcing selectional restrictions is overkill. Other, far simpler, formalisms\ncan do the job with far less computational cost. The second problem is that this\napproach presupposes a large, logical knowledge base of facts about the concepts\nthat make up selectional restrictions. Unfortunately, although such common-sense\nknowledge bases are being developed, none currently have the kind of coverage\nnecessary to the task.\n A more practical approach is to state selectional restrictions in terms of WordNet\nsynsets rather than as logical concepts. Each predicate simply specifies a WordNet\nsynset as the selectional restriction on each of its arguments. A meaning representation is well-formed if the role filler word is a hyponym (subordinate) of this synset.\n For our ate a hamburger example, for instance, we could set the selectional\nrestriction on the THEME role of the verb eat to the synset {food, nutrient}, glossed\nas any substance that can be metabolized by an animal to give energy and build\ntissue. Luckily, the chain of hypernyms for hamburger shown in Fig. 21.7 reveals\nthat hamburgers are indeed food. Again, the filler of a role need not match the\nrestriction synset exactly; it just needs to have the synset as one of its superordinates.\n We can apply this approach to the THEME roles of the verbs imagine, lift, and diagonalize, discussed earlier. Let us restrict imagine‚Äôs THEME to the synset {entity},\nlift‚Äôs THEME to {physical entity}, and diagonalize to {matrix}. This arrangement\ncorrectly permits imagine a hamburger and lift a hamburger, while also correctly\nruling out diagonalize a hamburger.\n14 C HAPTER 21 ‚Ä¢ S EMANTIC ROLE L ABELING\n\n Sense 1\n hamburger, beefburger --\n (a fried cake of minced beef served on a bun)\n => sandwich\n => snack food\n => dish\n => nutriment, nourishment, nutrition...\n => food, nutrient\n => substance\n => matter\n => physical entity\n => entity\n\n 21.7.2 Selectional Preferences\n In the earliest implementations, selectional restrictions were considered strict constraints on the kind of arguments a predicate could take (Katz and Fodor 1963,\n Hirst 1987). For example, the verb eat might require that its THEME argument\n be [+FOOD]. Early word sense disambiguation systems used this idea to rule out\n senses that violated the selectional restrictions of their governing predicates.\n Very quickly, however, it became clear that these selectional restrictions were\n better represented as preferences rather than strict constraints (Wilks 1975b, Wilks\n 1975a). For example, selectional restriction violations (like inedible arguments of\n eat) often occur in well-formed sentences, for example because they are negated\n (21.36), or because selectional restrictions are overstated (21.37):\n\n (21.36) But it fell apart in 1931, perhaps because people realized you can‚Äôt eat\n gold for lunch if you‚Äôre hungry.\n\n (21.37) In his two championship trials, Mr. Kulkarni ate glass on an empty\n stomach, accompanied only by water and tea.\n\n Modern systems for selectional preferences therefore specify the relation between a predicate and its possible arguments with soft constraints of some kind.\n\n Selectional Association\n\n selectional\n One of the most influential has been the selectional association model of Resnik\n preference (1993). Resnik defines the idea of selectional preference strength as the general\n strength\n amount of information that a predicate tells us about the semantic class of its arguments. For example, the verb eat tells us a lot about the semantic class of its direct\n objects, since they tend to be edible. The verb be, by contrast, tells us less about\n its direct objects. The selectional preference strength can be defined by the difference in information between two distributions: the distribution of expected semantic\n classes P(c) (how likely is it that a direct object will fall into class c) and the distribution of expected semantic classes for the particular verb P(c|v) (how likely is\n it that the direct object of the specific verb v will fall into semantic class c). The\n greater the difference between these distributions, the more information the verb\n is giving us about possible objects. The difference between these two distributions\nrelative entropy can be quantified by relative entropy, or the Kullback-Leibler divergence (Kullback\n KL divergence and Leibler, 1951). The Kullback-Leibler or KL divergence D(P||Q) expresses the\n 21.7 ‚Ä¢ S ELECTIONAL R ESTRICTIONS 15\n\n difference between two probability distributions P and Q\n X P(x)\n D(P||Q) = P(x) log (21.38)\n x\n Q(x)\n\n The selectional preference SR (v) uses the KL divergence to express how much information, in bits, the verb v expresses about the possible semantic class of its argument.\n SR (v) = D(P(c|v)||P(c))\n X P(c|v)\n = P(c|v) log (21.39)\n c\n P(c)\nselectional Resnik then defines the selectional association of a particular class and verb as the\nassociation\n relative contribution of that class to the general selectional preference of the verb:\n 1 P(c|v)\n AR (v, c) = P(c|v) log (21.40)\n SR (v) P(c)\n The selectional association is thus a probabilistic measure of the strength of association between a predicate and a class dominating the argument to the predicate.\n Resnik estimates the probabilities for these associations by parsing a corpus, counting all the times each predicate occurs with each argument word, and assuming\n that each word is a partial observation of all the WordNet concepts containing the\n word. The following table from Resnik (1996) shows some sample high and low\n selectional associations for verbs and some WordNet semantic classes of their direct\n objects.\n Direct Object Direct Object\n Verb Semantic Class Assoc Semantic Class Assoc\n read WRITING 6.80 ACTIVITY -.20\n write WRITING 7.26 COMMERCE 0\n see ENTITY 5.79 METHOD -0.01\n\n Selectional Preference via Conditional Probability\n An alternative to using selectional association between a verb and the WordNet class\n of its arguments is to use the conditional probability of an argument word given a\n predicate verb, directly modeling the strength of association of one verb (predicate)\n with one noun (argument).\n The conditional probability model can be computed by parsing a very large corpus (billions of words), and computing co-occurrence counts: how often a given\n verb occurs with a given noun in a given relation. The conditional probability of an\n argument noun given a verb for a particular relation P(n|v, r) can then be used as a\n selectional preference metric for that pair of words (Brockmann and Lapata 2003,\n Keller and Lapata 2003):\n (\n C(n,v,r)\n P(n|v, r) = C(v,r) if C(n, v, r) > 0\n 0 otherwise\n The inverse probability P(v|n, r) was found to have better performance in some cases\n (Brockmann and Lapata, 2003):\n (\n C(n,v,r)\n P(v|n, r) = C(n,r) if C(n, v, r) > 0\n 0 otherwise\n16 C HAPTER 21 ‚Ä¢ S EMANTIC ROLE L ABELING\n\n An even simpler approach is to use the simple log co-occurrence frequency of\n the predicate with the argument log count(v, n, r) instead of conditional probability;\n this seems to do better for extracting preferences for syntactic subjects rather than\n objects (Brockmann and Lapata, 2003).\n\n Evaluating Selectional Preferences\n pseudowords One way to evaluate models of selectional preferences is to use pseudowords (Gale\n et al. 1992, SchuÃàtze 1992). A pseudoword is an artificial word created by concatenating a test word in some context (say banana) with a confounder word (say door)\n to create banana-door). The task of the system is to identify which of the two words\n is the original word. To evaluate a selectional preference model (for example on the\n relationship between a verb and a direct object) we take a test corpus and select all\n verb tokens. For each verb token (say drive) we select the direct object (e.g., car),\n concatenated with a confounder word that is its nearest neighbor, the noun with the\n frequency closest to the original (say house), to make car/house). We then use the\n selectional preference model to choose which of car and house are more preferred\n objects of drive, and compute how often the model chooses the correct original object (e.g., car) (Chambers and Jurafsky, 2010).\n Another evaluation metric is to get human preferences for a test set of verbargument pairs, and have them rate their degree of plausibility. This is usually done\n by using magnitude estimation, a technique from psychophysics, in which subjects\n rate the plausibility of an argument proportional to a modulus item. A selectional\n preference model can then be evaluated by its correlation with the human preferences (Keller and Lapata, 2003).\n\n One way of thinking about the semantic roles we have discussed through the chapter\n is that they help us define the roles that arguments play in a decompositional way,\n based on finite lists of thematic roles (agent, patient, instrument, proto-agent, protopatient, etc.). This idea of decomposing meaning into sets of primitive semantic\n componential\n analysis elements or features, called primitive decomposition or componential analysis,\n has been taken even further, and focused particularly on predicates.\n Consider these examples of the verb kill:\n (21.41) Jim killed his philodendron.\n (21.42) Jim did something to cause his philodendron to become not alive.\n There is a truth-conditional (‚Äòpropositional semantics‚Äô) perspective from which these\n two sentences have the same meaning. Assuming this equivalence, we could represent the meaning of kill as:\n (21.43) KILL(x,y) ‚áî CAUSE(x, BECOME(NOT(ALIVE(y))))\n thus using semantic primitives like do, cause, become not, and alive.\n Indeed, one such set of potential semantic primitives has been used to account\n for some of the verbal alternations discussed in Section 21.2 (Lakoff 1965, Dowty\n 1979). Consider the following examples.\n (21.44) John opened the door. ‚áí CAUSE(John, BECOME(OPEN(door)))\n (21.45) The door opened. ‚áí BECOME(OPEN(door))\n 21.9 ‚Ä¢ S UMMARY 17\n\n (21.46) The door is open. ‚áí OPEN(door)\n The decompositional approach asserts that a single state-like predicate associated with open underlies all of these examples. The differences among the meanings\n of these examples arises from the combination of this single predicate with the primitives CAUSE and BECOME.\n While this approach to primitive decomposition can explain the similarity between states and actions or causative and non-causative predicates, it still relies on\n having a large number of predicates like open. More radical approaches choose to\n break down these predicates as well. One such approach to verbal predicate decomposition that played a role in early natural language systems is conceptual depenconceptual\n dependency dency (CD), a set of ten primitive predicates, shown in Fig. 21.8.\n\n Primitive Definition\n ATRANS The abstract transfer of possession or control from one entity to\n another\n P TRANS The physical transfer of an object from one location to another\n M TRANS The transfer of mental concepts between entities or within an\n entity\n M BUILD The creation of new information within an entity\n P ROPEL The application of physical force to move an object\n M OVE The integral movement of a body part by an animal\n I NGEST The taking in of a substance by an animal\n E XPEL The expulsion of something from an animal\n S PEAK The action of producing a sound\n ATTEND The action of focusing a sense organ\n\n Below is an example sentence along with its CD representation. The verb brought\n is translated into the two primitives ATRANS and PTRANS to indicate that the waiter\n both physically conveyed the check to Mary and passed control of it to her. Note\n that CD also associates a fixed set of thematic roles with each primitive to represent\n the various participants in the action.\n (21.47) The waiter brought Mary the check.\n\n ‚àÉx, y Atrans(x) ‚àß Actor(x,Waiter) ‚àß Ob ject(x,Check) ‚àß To(x, Mary)\n ‚àßPtrans(y) ‚àß Actor(y,Waiter) ‚àß Ob ject(y,Check) ‚àß To(y, Mary)\n\n ‚Ä¢ Semantic roles are abstract models of the role an argument plays in the event\n described by the predicate.\n ‚Ä¢ Thematic roles are a model of semantic roles based on a single finite list of\n roles. Other semantic role models include per-verb semantic role lists and\n proto-agent/proto-patient, both of which are implemented in PropBank,\n and per-frame role lists, implemented in FrameNet.\n18 C HAPTER 21 ‚Ä¢ S EMANTIC ROLE L ABELING\n\n ‚Ä¢ Semantic role labeling is the task of assigning semantic role labels to the\n constituents of a sentence. The task is generally treated as a supervised machine learning task, with models trained on PropBank or FrameNet. Algorithms generally start by parsing a sentence and then automatically tag each\n parse tree node with a semantic role. Neural models map straight from words\n end-to-end.\n ‚Ä¢ Semantic selectional restrictions allow words (particularly predicates) to post\n constraints on the semantic properties of their argument words. Selectional\n preference models (like selectional association or simple conditional probability) allow a weight or probability to be assigned to the association between\n a predicate and an argument word or class.\n\nHistorical Notes\n Although the idea of semantic roles dates back to PaÃÑn.ini, they were re-introduced\n into modern linguistics by Gruber (1965), Fillmore (1966) and Fillmore (1968). Fillmore had become interested in argument structure by studying Lucien TesnieÃÄre‚Äôs\n groundbreaking EÃÅleÃÅments de Syntaxe Structurale (TesnieÃÄre, 1959) in which the term\n ‚Äòdependency‚Äô was introduced and the foundations were laid for dependency grammar. Following TesnieÃÄre‚Äôs terminology, Fillmore first referred to argument roles as\n actants (Fillmore, 1966) but quickly switched to the term case, (see Fillmore (2003))\n and proposed a universal list of semantic roles or cases (Agent, Patient, Instrument,\n etc.), that could be taken on by the arguments of predicates. Verbs would be listed in\n the lexicon with their case frame, the list of obligatory (or optional) case arguments.\n The idea that semantic roles could provide an intermediate level of semantic\n representation that could help map from syntactic parse structures to deeper, more\n fully-specified representations of meaning was quickly adopted in natural language\n processing, and systems for extracting case frames were created for machine translation (Wilks, 1973), question-answering (Hendrix et al., 1973), spoken-language processing (Nash-Webber, 1975), and dialogue systems (Bobrow et al., 1977). Generalpurpose semantic role labelers were developed. The earliest ones (Simmons, 1973)\n first parsed a sentence by means of an ATN (Augmented Transition Network) parser.\n Each verb then had a set of rules specifying how the parse should be mapped to semantic roles. These rules mainly made reference to grammatical functions (subject,\n object, complement of specific prepositions) but also checked constituent internal\n features such as the animacy of head nouns. Later systems assigned roles from prebuilt parse trees, again by using dictionaries with verb-specific case frames (Levin\n 1977, Marcus 1980).\n By 1977 case representation was widely used and taught in AI and NLP courses,\n and was described as a standard of natural language processing in the first edition of\n Winston‚Äôs 1977 textbook Artificial Intelligence.\n In the 1980s Fillmore proposed his model of frame semantics, later describing\n the intuition as follows:\n ‚ÄúThe idea behind frame semantics is that speakers are aware of possibly quite complex situation types, packages of connected expectations,\n that go by various names‚Äîframes, schemas, scenarios, scripts, cultural\n narratives, memes‚Äîand the words in our language are understood with\n such frames as their presupposed background.‚Äù (Fillmore, 2012, p. 712)\n H ISTORICAL N OTES 19\n\n The word frame seemed to be in the air for a suite of related notions proposed at\n about the same time by Minsky (1974), Hymes (1974), and Goffman (1974), as\n well as related notions with other names like scripts (Schank and Abelson, 1975)\n and schemata (Bobrow and Norman, 1975) (see Tannen (1979) for a comparison).\n Fillmore was also influenced by the semantic field theorists and by a visit to the Yale\n AI lab where he took notice of the lists of slots and fillers used by early information\n extraction systems like DeJong (1982) and Schank and Abelson (1977). In the 1990s\n Fillmore drew on these insights to begin the FrameNet corpus annotation project.\n At the same time, Beth Levin drew on her early case frame dictionaries (Levin,\n 1977) to develop her book which summarized sets of verb classes defined by shared\n argument realizations (Levin, 1993). The VerbNet project built on this work (Kipper\n et al., 2000), leading soon afterwards to the PropBank semantic-role-labeled corpus\n created by Martha Palmer and colleagues (Palmer et al., 2005).\n The combination of rich linguistic annotation and corpus-based approach instantiated in FrameNet and PropBank led to a revival of automatic approaches to\n semantic role labeling, first on FrameNet (Gildea and Jurafsky, 2000) and then on\n PropBank data (Gildea and Palmer, 2002, inter alia). The problem first addressed in\n the 1970s by handwritten rules was thus now generally recast as one of supervised\n machine learning enabled by large and consistent databases. Many popular features\n used for role labeling are defined in Gildea and Jurafsky (2002), Surdeanu et al.\n (2003), Xue and Palmer (2004), Pradhan et al. (2005), Che et al. (2009), and Zhao\n et al. (2009). The use of dependency rather than constituency parses was introduced\n in the CoNLL-2008 shared task (Surdeanu et al., 2008). For surveys see Palmer\n et al. (2010) and MaÃÄrquez et al. (2008).\n The use of neural approaches to semantic role labeling was pioneered by Collobert et al. (2011), who applied a CRF on top of a convolutional net. Early work\n like Foland, Jr. and Martin (2015) focused on using dependency features. Later work\n eschewed syntactic features altogether; Zhou and Xu (2015) introduced the use of\n a stacked (6-8 layer) biLSTM architecture, and (He et al., 2017) showed how to\n augment the biLSTM architecture with highway networks and also replace the CRF\n with A* decoding that make it possible to apply a wide variety of global constraints\n in SRL decoding.\n Most semantic role labeling schemes only work within a single sentence, focusing on the object of the verbal (or nominal, in the case of NomBank) predicate.\n However, in many cases, a verbal or nominal predicate may have an implicit arguimplicit\nargument ment: one that appears only in a contextual sentence, or perhaps not at all and must\n be inferred. In the two sentences This house has a new owner. The sale was finalized\n 10 days ago. the sale in the second sentence has no A RG 1, but a reasonable reader\n would infer that the Arg1 should be the house mentioned in the prior sentence. FindiSRL ing these arguments, implicit argument detection (sometimes shortened as iSRL)\n was introduced by Gerber and Chai (2010) and Ruppenhofer et al. (2010). See Do\n et al. (2017) for more recent neural models.\n To avoid the need for huge labeled training sets, unsupervised approaches for\n semantic role labeling attempt to induce the set of semantic roles by clustering over\n arguments. The task was pioneered by Riloff and Schmelzenbach (1998) and Swier\n and Stevenson (2004); see Grenager and Manning (2006), Titov and Klementiev\n (2012), Lang and Lapata (2014), Woodsend and Lapata (2015), and Titov and Khoddam (2014).\n Recent innovations in frame labeling include connotation frames, which mark\n richer information about the argument of predicates. Connotation frames mark the\n20 C HAPTER 21 ‚Ä¢ S EMANTIC ROLE L ABELING\n\n sentiment of the writer or reader toward the arguments (for example using the verb\n survive in he survived a bombing expresses the writer‚Äôs sympathy toward the subject\n he and negative sentiment toward the bombing. See Chapter 22 for more details.\n Selectional preference has been widely studied beyond the selectional association models of Resnik (1993) and Resnik (1996). Methods have included clustering\n (Rooth et al., 1999), discriminative learning (Bergsma et al., 2008), and topic models (SeÃÅaghdha 2010, Ritter et al. 2010), and constraints can be expressed at the level\n of words or classes (Agirre and Martinez, 2001). Selectional preferences have also\n been successfully integrated into semantic role labeling (Erk 2007, Zapirain et al.\n 2013, Do et al. 2017).\n\nExercises\n Exercises 21\n\nAgirre, E. and D. Martinez. 2001. Learning class-to-class Fillmore, C. J. and C. F. Baker. 2009. A frames approach\n selectional preferences. CoNLL. to semantic analysis. In B. Heine and H. Narrog, eds,\nBaker, C. F., C. J. Fillmore, and J. B. Lowe. 1998. The Berke- The Oxford Handbook of Linguistic Analysis, 313‚Äì340.\n ley FrameNet project. COLING/ACL. Oxford University Press.\nBergsma, S., D. Lin, and R. Goebel. 2008. Discriminative Fillmore, C. J., C. R. Johnson, and M. R. L. Petruck. 2003.\n learning of selectional preference from unlabeled text. Background to FrameNet. International journal of lexi-\nEMNLP. cography, 16(3):235‚Äì250.\n Foland, Jr., W. R. and J. H. Martin. 2015. Dependency-\nBloomfield, L. 1933. Language. University of Chicago\n based semantic role labeling using convolutional neural\n Press.\n networks. *SEM 2015.\nBobrow, D. G., R. M. Kaplan, M. Kay, D. A. Norman,\n Gale, W. A., K. W. Church, and D. Yarowsky. 1992. Work on\n H. Thompson, and T. Winograd. 1977. GUS, A frame\n statistical methods for word sense disambiguation. AAAI\n driven dialog system. Artificial Intelligence, 8:155‚Äì173.\n Fall Symposium on Probabilistic Approaches to Natural\nBobrow, D. G. and D. A. Norman. 1975. Some principles of Language.\n memory schemata. In D. G. Bobrow and A. Collins, eds, Gerber, M. and J. Y. Chai. 2010. Beyond nombank: A study\n Representation and Understanding. Academic Press. of implicit arguments for nominal predicates. ACL.\nBrockmann, C. and M. Lapata. 2003. Evaluating and com- Gildea, D. and D. Jurafsky. 2000. Automatic labeling of sebining approaches to selectional preference acquisition. mantic roles. ACL.\n EACL.\n Gildea, D. and D. Jurafsky. 2002. Automatic labeling of se-\nCarreras, X. and L. MaÃÄrquez. 2005. Introduction to mantic roles. Computational Linguistics, 28(3):245‚Äì288.\n the CoNLL-2005 shared task: Semantic role labeling.\n Gildea, D. and M. Palmer. 2002. The necessity of syntactic\n CoNLL.\n parsing for predicate argument recognition. ACL.\nChambers, N. and D. Jurafsky. 2010. Improving the use\n Goffman, E. 1974. Frame analysis: An essay on the organiof pseudo-words for evaluating selectional preferences.\n zation of experience. Harvard University Press.\n ACL.\n Grenager, T. and C. D. Manning. 2006. Unsupervised dis-\nChe, W., Z. Li, Y. Li, Y. Guo, B. Qin, and T. Liu. 2009. Mul- covery of a statistical verb lexicon. EMNLP.\n tilingual dependency-based syntactic and semantic parsing. CoNLL. Gruber, J. S. 1965. Studies in Lexical Relations. Ph.D. thesis,\n MIT.\nCollobert, R., J. Weston, L. Bottou, M. Karlen,\n He, L., K. Lee, M. Lewis, and L. Zettlemoyer. 2017. Deep\n K. Kavukcuoglu, and P. Kuksa. 2011. Natural language\n semantic role labeling: What works and what‚Äôs next.\n processing (almost) from scratch. JMLR, 12:2493‚Äì2537.\n ACL.\nDeJong, G. F. 1982. An overview of the FRUMP system.\n Hendrix, G. G., C. W. Thompson, and J. Slocum. 1973. Lan-\nIn W. G. Lehnert and M. H. Ringle, eds, Strategies for\n guage processing via canonical verbs and semantic mod-\nNatural Language Processing, 149‚Äì176. LEA.\n els. Proceedings of IJCAI-73.\nDo, Q. N. T., S. Bethard, and M.-F. Moens. 2017. Improv- Hirst, G. 1987. Semantic Interpretation and the Resolution\n ing implicit semantic role labeling by predicting semantic of Ambiguity. Cambridge University Press.\n frame arguments. IJCNLP.\n Hymes, D. 1974. Ways of speaking. In R. Bauman and\nDowty, D. R. 1979. Word Meaning and Montague Grammar. J. Sherzer, eds, Explorations in the ethnography of speak-\nD. Reidel. ing, 433‚Äì451. Cambridge University Press.\nErk, K. 2007. A simple, similarity-based model for selec- Johnson-Laird, P. N. 1983. Mental Models. Harvard Univertional preferences. ACL. sity Press, Cambridge, MA.\nFillmore, C. J. 1966. A proposal concerning English prepo- Katz, J. J. and J. A. Fodor. 1963. The structure of a semantic\n sitions. In F. P. Dinneen, ed., 17th annual Round Table, theory. Language, 39:170‚Äì210.\n volume 17 of Monograph Series on Language and Lin-\nKeller, F. and M. Lapata. 2003. Using the web to obtain freguistics, 19‚Äì34. Georgetown University Press.\n quencies for unseen bigrams. Computational Linguistics,\nFillmore, C. J. 1968. The case for case. In E. W. Bach and 29:459‚Äì484.\n R. T. Harms, eds, Universals in Linguistic Theory, 1‚Äì88. Kipper, K., H. T. Dang, and M. Palmer. 2000. Class-based\n Holt, Rinehart & Winston. construction of a verb lexicon. AAAI.\nFillmore, C. J. 1985. Frames and the semantics of under- Kullback, S. and R. A. Leibler. 1951. On information and\n standing. Quaderni di Semantica, VI(2):222‚Äì254. sufficiency. Annals of Mathematical Statistics, 22:79‚Äì86.\nFillmore, C. J. 2003. Valency and semantic roles: the con- Lakoff, G. 1965. On the Nature of Syntactic Irregularity.\n cept of deep structure case. In V. Agel, L. M. Eichinger, Ph.D. thesis, Indiana University. Published as Irregularity\n H. W. Eroms, P. Hellwig, H. J. Heringer, and H. Lobin, in Syntax. Holt, Rinehart, and Winston, New York, 1970.\n eds, Dependenz und Valenz: Ein internationales Hand-\nLang, J. and M. Lapata. 2014. Similarity-driven semantic\n buch der zeitgenoÃàssischen Forschung, chapter 36, 457‚Äì\n role induction via graph partitioning. Computational Lin-\n475. Walter de Gruyter.\n guistics, 40(3):633‚Äì669.\nFillmore, C. J. 2012. ACL lifetime achievement award: Levin, B. 1977. Mapping sentences to case frames. Techni-\nEncounters with language. Computational Linguistics, cal Report 167, MIT AI Laboratory. AI Working Paper\n 38(4):701‚Äì718. 143.\n22 Chapter 21 ‚Ä¢ Semantic Role Labeling\n\nLevin, B. 1993. English Verb Classes and Alternations: A SchuÃàtze, H. 1992. Context space. AAAI Fall Symposium on\n Preliminary Investigation. University of Chicago Press. Probabilistic Approaches to Natural Language.\nLevin, B. and M. Rappaport Hovav. 2005. Argument Real- SeÃÅaghdha, D. O. 2010. Latent variable models of selectional\n ization. Cambridge University Press. preference. ACL.\nMarcus, M. P. 1980. A Theory of Syntactic Recognition for Shi, P. and J. Lin. 2019. Simple BERT models for relation\n Natural Language. MIT Press. extraction and semantic role labeling. ArXiv.\nMaÃÄrquez, L., X. Carreras, K. C. Litkowski, and S. Steven- Simmons, R. F. 1973. Semantic networks: Their compuson. 2008. Semantic role labeling: An introduction to the tation and use for understanding English sentences. In\n special issue. Computational linguistics, 34(2):145‚Äì159. R. C. Schank and K. M. Colby, eds, Computer Models of\nMeyers, A., R. Reeves, C. Macleod, R. Szekely, V. Zielinska, Thought and Language, 61‚Äì113. W.H. Freeman & Co.\n B. Young, and R. Grishman. 2004. The nombank project: Sloan, M. C. 2010. Aristotle‚Äôs Nicomachean Ethics as the\n An interim report. NAACL/HLT Workshop: Frontiers in original locus for the Septem Circumstantiae. Classical\n Corpus Annotation. Philology, 105(3):236‚Äì251.\nMinsky, M. 1974. A framework for representing knowledge. Surdeanu, M., S. Harabagiu, J. Williams, and P. Aarseth.\n Technical Report 306, MIT AI Laboratory. Memo 306. 2003. Using predicate-argument structures for informa-\nNash-Webber, B. L. 1975. The role of semantics in automatic tion extraction. ACL.\n speech understanding. In D. G. Bobrow and A. Collins, Surdeanu, M., R. Johansson, A. Meyers, L. MaÃÄrquez, and\n eds, Representation and Understanding, 351‚Äì382. Aca- J. Nivre. 2008. The CoNLL 2008 shared task on joint\n demic Press. parsing of syntactic and semantic dependencies. CoNLL.\nPalmer, M., D. Gildea, and N. Xue. 2010. Semantic role Swier, R. and S. Stevenson. 2004. Unsupervised semantic\n labeling. Synthesis Lectures on Human Language Tech- role labelling. EMNLP.\n nologies, 3(1):1‚Äì103. Tannen, D. 1979. What‚Äôs in a frame? Surface evidence for\nPalmer, M., P. Kingsbury, and D. Gildea. 2005. The proposi- underlying expectations. In R. Freedle, ed., New Direction bank: An annotated corpus of semantic roles. Com- tions in Discourse Processing, 137‚Äì181. Ablex.\n putational Linguistics, 31(1):71‚Äì106.\n TesnieÃÄre, L. 1959. EÃÅleÃÅments de Syntaxe Structurale. Librairie\nPenn, G. and P. Kiparsky. 2012. On PaÃÑn.ini and the gen- C. Klincksieck, Paris.\n erative capacity of contextualized replacement systems.\n Titov, I. and E. Khoddam. 2014. Unsupervised induction of\n COLING.\n semantic roles within a reconstruction-error minimization\nPradhan, S., A. Moschitti, N. Xue, H. T. Ng, A. BjoÃàrkelund, framework. NAACL HLT.\n O. Uryupina, Y. Zhang, and Z. Zhong. 2013. Towards\n Titov, I. and A. Klementiev. 2012. A Bayesian approach to\n robust linguistic analysis using OntoNotes. CoNLL.\n unsupervised semantic role induction. EACL.\nPradhan, S., W. Ward, K. Hacioglu, J. H. Martin, and D. Ju-\nWilks, Y. 1973. An artificial intelligence approach to marafsky. 2005. Semantic role labeling using different synchine translation. In R. C. Schank and K. M. Colby, eds,\n tactic views. ACL.\n Computer Models of Thought and Language, 114‚Äì151.\nResnik, P. 1993. Semantic classes and syntactic ambiguity. W.H. Freeman.\n HLT.\n Wilks, Y. 1975a. Preference semantics. In E. L. Keenan,\nResnik, P. 1996. Selectional constraints: An information- ed., The Formal Semantics of Natural Language, 329‚Äì\n theoretic model and its computational realization. Cogni- 350. Cambridge Univ. Press.\n tion, 61:127‚Äì159.\n Wilks, Y. 1975b. A preferential, pattern-seeking, seman-\nRiloff, E. and M. Schmelzenbach. 1998. An empirical ap- tics for natural language inference. Artificial Intelligence,\n proach to conceptual case frame acquisition. Proceedings 6(1):53‚Äì74.\n of the Sixth Workshop on Very Large Corpora.\n Winston, P. H. 1977. Artificial Intelligence. Addison Wesley.\nRitter, A., O. Etzioni, and Mausam. 2010. A latent dirichlet\n Woodsend, K. and M. Lapata. 2015. Distributed representaallocation method for selectional preferences. ACL.\n tions for unsupervised semantic role labeling. EMNLP.\nRooth, M., S. Riezler, D. Prescher, G. Carroll, and F. Beil.\n Xue, N. and M. Palmer. 2004. Calibrating features for se-\n1999. Inducing a semantically annotated lexicon via EMmantic role labeling. EMNLP.\n based clustering. ACL.\n Zapirain, B., E. Agirre, L. MaÃÄrquez, and M. Surdeanu. 2013.\nRuppenhofer, J., M. Ellsworth, M. R. L. Petruck, C. R. John-\nSelectional preferences for semantic role classification.\n son, C. F. Baker, and J. Scheffczyk. 2016. FrameNet II:\n Computational Linguistics, 39(3):631‚Äì663.\n Extended theory and practice.\nRuppenhofer, J., C. Sporleder, R. Morante, C. F. Baker, Zhao, H., W. Chen, C. Kit, and G. Zhou. 2009. Multilingual\n and M. Palmer. 2010. Semeval-2010 task 10: Linking dependency learning: A huge feature engineering method\n events and their participants in discourse. 5th Interna- to semantic dependency parsing. CoNLL.\n tional Workshop on Semantic Evaluation. Zhou, J. and W. Xu. 2015. End-to-end learning of semantic\nSchank, R. C. and R. P. Abelson. 1975. Scripts, plans, and role labeling using recurrent neural networks. ACL.\n knowledge. Proceedings of IJCAI-75.\nSchank, R. C. and R. P. Abelson. 1977. Scripts, Plans, Goals\n and Understanding. Lawrence Erlbaum.\n",
    "file_path": "/Users/colinsidberry/nlp-textbook-rag/data/extracted_text/21.Semantic Role Labeling.txt",
    "file_size_kb": 65.27
  },
  {
    "id": "b169d73ea1c09177",
    "source": "nlp_textbook",
    "chapter": "22 Lexicons for Sentiment, Affect,",
    "filename": "22.Lexicons for Sentiment, Affect, and Connotation.txt",
    "content": "rights reserved. Draft of August 24, 2025.\n\nCHAPTER\n\n22 Lexicons for Sentiment, Affect,\n and Connotation\n Some day we‚Äôll be able to measure the power of words\n Maya Angelou\n\n affective In this chapter we turn to tools for interpreting affective meaning, extending our\n study of sentiment analysis in Appendix K. We use the word ‚Äòaffective‚Äô, following\n the tradition in affective computing (Picard, 1995) to mean emotion, sentiment, persubjectivity sonality, mood, and attitudes. Affective meaning is closely related to subjectivity,\n the study of a speaker or writer‚Äôs evaluations, opinions, emotions, and speculations\n (Wiebe et al., 1999).\n How should affective meaning be defined? One influential typology of affective states comes from Scherer (2000), who defines each class of affective states by\n factors like its cognitive realization and time course (Fig. 22.1).\n\n Emotion: Relatively brief episode of response to the evaluation of an external\n or internal event as being of major significance.\n (angry, sad, joyful, fearful, ashamed, proud, elated, desperate)\n Mood: Diffuse affect state, most pronounced as change in subjective feeling, of\n low intensity but relatively long duration, often without apparent cause.\n (cheerful, gloomy, irritable, listless, depressed, buoyant)\n Interpersonal stance: Affective stance taken toward another person in a specific interaction, coloring the interpersonal exchange in that situation.\n (distant, cold, warm, supportive, contemptuous, friendly)\n Attitude: Relatively enduring, affectively colored beliefs, preferences, and predispositions towards objects or persons.\n (liking, loving, hating, valuing, desiring)\n Personality traits: Emotionally laden, stable personality dispositions and behavior tendencies, typical for a person.\n (nervous, anxious, reckless, morose, hostile, jealous)\n\n We can design extractors for each of these kinds of affective states. Appendix K\n already introduced sentiment analysis, the task of extracting the positive or negative\n orientation that a writer expresses in a text. This corresponds in Scherer‚Äôs typology\n to the extraction of attitudes: figuring out what people like or dislike, from affectrich texts like consumer reviews of books or movies, newspaper editorials, or public\n sentiment in blogs or tweets.\n Detecting emotion and moods is useful for detecting whether a student is confused, engaged, or certain when interacting with a tutorial system, whether a caller\n to a help line is frustrated, whether someone‚Äôs blog posts or tweets indicated depression. Detecting emotions like fear in novels, for example, could help us trace what\n groups or situations are feared and how that changes over time.\n2 C HAPTER 22 ‚Ä¢ L EXICONS FOR S ENTIMENT, A FFECT, AND C ONNOTATION\n\n Detecting different interpersonal stances can be useful when extracting information from human-human conversations. The goal here is to detect stances like\n friendliness or awkwardness in interviews or friendly conversations, for example for\n summarizing meetings or finding parts of a conversation where people are especially\n excited or engaged, conversational hot spots that can help in meeting summarization. Detecting the personality of a user‚Äîsuch as whether the user is an extrovert\n or the extent to which they are open to experience‚Äî can help improve conversational agents, which seem to work better if they match users‚Äô personality expectations (Mairesse and Walker, 2008). And affect is important for generation as well\n as recognition; synthesizing affect is important for conversational agents in various\n domains, including literacy tutors such as children‚Äôs storybooks, or computer games.\n In Appendix K we introduced the use of naive Bayes classification to classify a\n document‚Äôs sentiment. Various classifiers have been successfully applied to many of\n these tasks, using all the words in the training set as input to a classifier which then\n determines the affect status of the text.\n In this chapter we focus on an alternative model, in which instead of using every\n word as a feature, we focus only on certain words, ones that carry particularly strong\n cues to affect or sentiment. We call these lists of words affective lexicons or sentiment lexicons. These lexicons presuppose a fact about semantics: that words have\n connotations affective meanings or connotations. The word connotation has different meanings\n in different fields, but here we use it to mean the aspects of a word‚Äôs meaning that\n are related to a writer or reader‚Äôs emotions, sentiment, opinions, or evaluations. In\n addition to their ability to help determine the affective status of a text, connotation\n lexicons can be useful features for other kinds of affective tasks, and for computational social science analysis.\n In the next sections we introduce basic theories of emotion, show how sentiment\n lexicons are a special case of emotion lexicons, and mention some useful lexicons.\n We then survey three ways for building lexicons: human labeling, semi-supervised,\n and supervised. Finally, we talk about how to detect affect toward a particular entity,\n and introduce connotation frames.\n\n emotion One of the most important affective classes is emotion, which Scherer (2000) defines\n as a ‚Äúrelatively brief episode of response to the evaluation of an external or internal\n event as being of major significance‚Äù.\n Detecting emotion has the potential to improve a number of language processing\n tasks. Emotion recognition could help dialogue systems like tutoring systems detect\n that a student was unhappy, bored, hesitant, confident, and so on. Automatically\n detecting emotions in reviews or customer responses (anger, dissatisfaction, trust)\n could help businesses recognize specific problem areas or ones that are going well.\n Emotion can play a role in medical NLP tasks like helping diagnose depression or\n suicidal intent. Detecting emotions expressed toward characters in novels might\n play a role in understanding how different social groups were viewed by society at\n different times.\n Computational models of emotion in NLP have mainly been based on two families of theories of emotion (out of the many studied in the field of affective science).\n In one of these families, emotions are viewed as fixed atomic units, limited in numbasic emotions ber, and from which others are generated, often called basic emotions (Tomkins\n 22.1 ‚Ä¢ D EFINING E MOTION 3\n\n1962, Plutchik 1962), a model dating back to Darwin. Perhaps the most well-known\nof this family of theories are the 6 emotions proposed by Ekman (e.g., Ekman 1999)\nto be universally present in all cultures: surprise, happiness, anger, fear, disgust,\nsadness. Another atomic theory is the Plutchik (1980) wheel of emotion, consisting\nof 8 basic emotions in four opposing pairs: joy‚Äìsadness, anger‚Äìfear, trust‚Äìdisgust,\nand anticipation‚Äìsurprise, together with the emotions derived from them, shown in\nFig. 22.2.\n\n The second class of emotion theories widely used in NLP views emotion as a\nspace in 2 or 3 dimensions (Russell, 1980). Most models include the two dimensions\nvalence and arousal, and many add a third, dominance. These can be defined as:\n valence: the pleasantness of the stimulus\n arousal: the level of alertness, activeness, or energy provoked by the stimulus\n dominance: the degree of control or dominance exerted by the stimulus or the\n emotion\nSentiment can be viewed as a special case of this second view of emotions as points\nin space. In particular, the valence dimension, measuring how pleasant or unpleasant\na word is, is often used directly as a measure of sentiment.\n In these lexicon-based models of affect, the affective meaning of a word is generally fixed, irrespective of the linguistic context in which a word is used, or the\ndialect or culture of the speaker. By contrast, other models in affective science represent emotions as much richer processes involving cognition (Barrett et al., 2007). In\nappraisal theory, for example, emotions are complex processes, in which a person\nconsiders how an event is congruent with their goals, taking into account variables\nlike the agency, certainty, urgency, novelty and control associated with the event\n(Moors et al., 2013). Computational models in NLP taking into account these richer\ntheories of emotion will likely play an important role in future work.\n4 C HAPTER 22 ‚Ä¢ L EXICONS FOR S ENTIMENT, A FFECT, AND C ONNOTATION\n\n A wide variety of affect lexicons have been created and released. The most basic\n lexicons label words along one dimension of semantic variability, generally called\n ‚Äúsentiment‚Äù or ‚Äúvalence‚Äù.\n In the simplest lexicons this dimension is represented in a binary fashion, with\n a wordlist for positive words and a wordlist for negative words. The oldest is the\n General\n Inquirer General Inquirer (Stone et al., 1966), which drew on content analysis and on early\n work in the cognitive psychology of word meaning (Osgood et al., 1957). The General Inquirer has a lexicon of 1915 positive words and a lexicon of 2291 negative\n words (as well as other lexicons discussed below). The MPQA Subjectivity lexicon\n (Wilson et al., 2005) has 2718 positive and 4912 negative words drawn from prior\n lexicons plus a bootstrapped list of subjective words and phrases (Riloff and Wiebe,\n 2003). Each entry in the lexicon is hand-labeled for sentiment and also labeled for\n reliability (strongly subjective or weakly subjective). The polarity lexicon of Hu\n and Liu (2004) gives 2006 positive and 4783 negative words, drawn from product\n reviews, labeled using a bootstrapping method from WordNet.\n\nPositive admire, amazing, assure, celebration, charm, eager, enthusiastic, excellent, fancy, fantastic, frolic, graceful, happy, joy, luck, majesty, mercy, nice, patience, perfect, proud,\n rejoice, relief, respect, satisfactorily, sensational, super, terrific, thank, vivid, wise, wonderful, zest\nNegative abominable, anger, anxious, bad, catastrophe, cheap, complaint, condescending, deceit,\n defective, disappointment, embarrass, fake, fear, filthy, fool, guilt, hate, idiot, inflict, lazy,\n miserable, mourn, nervous, objection, pest, plot, reject, scream, silly, terrible, unfriendly,\n vile, wicked\nMPQA Subjectivity lexicon (Wilson et al., 2005), and the polarity lexicon of Hu and Liu (2004).\n\n Slightly more general than these sentiment lexicons are lexicons that assign each\n word a value on all three affective dimensions. The NRC Valence, Arousal, and\n Dominance (VAD) lexicon (Mohammad, 2018a) assigns valence, arousal, and dominance scores to 20,000 words. Some examples are shown in Fig. 22.4.\n\n Valence Arousal Dominance\n vacation .840 enraged .962 powerful .991\n delightful .918 party .840 authority .935\n whistle .653 organized .337 saxophone .482\n consolation .408 effortless .120 discouraged .0090\n torture .115 napping .046 weak .045\n\n EmoLex The NRC Word-Emotion Association Lexicon, also called EmoLex (Mohammad and Turney, 2013), uses the Plutchik (1980) 8 basic emotions defined above.\n The lexicon includes around 14,000 words including words from prior lexicons as\n well as frequent nouns, verbs, adverbs and adjectives. Values from the lexicon for\n some sample words:\n 22.3 ‚Ä¢ C REATING A FFECT L EXICONS BY H UMAN L ABELING 5\n\n anticipation\n\n negative\n surprise\n\n positive\n sadness\n disgust\n anger\n\n trust\n fear\n joy\n Word\n reward 0 1 0 0 1 0 1 1 1 0\n worry 0 1 0 1 0 1 0 0 0 1\n tenderness 0 0 0 0 1 0 0 0 1 0\n sweetheart 0 1 0 0 1 1 0 1 1 0\n suddenly 0 0 0 0 0 0 1 0 0 0\n thirst 0 1 0 0 0 1 1 0 0 0\n garbage 0 0 1 0 0 0 0 0 0 1\n\n For a smaller set of 5,814 words, the NRC Emotion/Affect Intensity Lexicon\n (Mohammad, 2018b) contains real-valued scores of association for anger, fear, joy,\n and sadness; Fig. 22.5 shows examples.\n\n Anger Fear Joy Sadness\n outraged 0.964 horror 0.923 superb 0.864 sad 0.844\n violence 0.742 anguish 0.703 cheered 0.773 guilt 0.750\n coup 0.578 pestilence 0.625 rainbow 0.531 unkind 0.547\n oust 0.484 stressed 0.531 gesture 0.387 difficulties 0.421\n suspicious 0.484 failing 0.531 warms 0.391 beggar 0.422\n nurture 0.059 confident 0.094 hardship .031 sing 0.017\n Mohammad (2018b).\n\n LIWC LIWC, Linguistic Inquiry and Word Count, is a widely used set of 73 lexicons containing over 2300 words (Pennebaker et al., 2007), designed to capture\n aspects of lexical meaning relevant for social psychological tasks. In addition to\n sentiment-related lexicons like ones for negative emotion (bad, weird, hate, problem, tough) and positive emotion (love, nice, sweet), LIWC includes lexicons for\n categories like anger, sadness, cognitive mechanisms, perception, tentative, and inhibition, shown in Fig. 22.6.\n There are various other hand-built affective lexicons. The General Inquirer includes additional lexicons for dimensions like strong vs. weak, active vs. passive,\n overstated vs. understated, as well as lexicons for categories like pleasure, pain,\n virtue, vice, motivation, and cognitive orientation.\n concrete Another useful feature for various tasks is the distinction between concrete\n abstract words like banana or bathrobe and abstract words like belief and although. The\n lexicon in Brysbaert et al. (2014) used crowdsourcing to assign a rating from 1 to 5\n of the concreteness of 40,000 words, thus assigning banana, bathrobe, and bagel 5,\n belief 1.19, although 1.07, and in between words like brisk a 2.5.\n\n The earliest method used to build affect lexicons, and still in common use, is to have\ncrowdsourcing humans label each word. This is now most commonly done via crowdsourcing:\n breaking the task into small pieces and distributing them to a large number of anno-\n6 C HAPTER 22 ‚Ä¢ L EXICONS FOR S ENTIMENT, A FFECT, AND C ONNOTATION\n\n Positive Negative\n Emotion Emotion Insight Inhibition Family Negate\n appreciat* anger* aware* avoid* brother* aren‚Äôt\n comfort* bore* believe careful* cousin* cannot\n great cry decid* hesitat* daughter* didn‚Äôt\n happy despair* feel limit* family neither\n interest fail* figur* oppos* father* never\n joy* fear know prevent* grandf* no\n perfect* griev* knew reluctan* grandm* nobod*\n please* hate* means safe* husband none\n safe* panic* notice* stop mom nor\n terrific suffers recogni* stubborn* mother nothing\n value terrify sense wait niece* nowhere\n wow* violent* think wary wife without\n The * means the previous letters are a word prefix and all words with that prefix are included\n in the category.\n\n tators. Let‚Äôs take a look at some of the methodological choices for two crowdsourced\n emotion lexicons.\n The NRC Emotion Lexicon (EmoLex) (Mohammad and Turney, 2013), labeled\n emotions in two steps. To ensure that the annotators were judging the correct sense\n of the word, they first answered a multiple-choice synonym question that primed\n the correct sense of the word (without requiring the annotator to read a potentially\n confusing sense definition). These were created automatically using the headwords\n associated with the thesaurus category of the sense in question in the Macquarie\n dictionary and the headwords of 3 random distractor categories. An example:\n Which word is closest in meaning (most related) to startle?\n ‚Ä¢ automobile\n ‚Ä¢ shake\n ‚Ä¢ honesty\n ‚Ä¢ entertain\n For each word (e.g. startle), the annotator was then asked to rate how associated\n that word is with each of the 8 emotions (joy, fear, anger, etc.). The associations\n were rated on a scale of not, weakly, moderately, and strongly associated. Outlier\n ratings were removed, and then each term was assigned the class chosen by the majority of the annotators, with ties broken by choosing the stronger intensity, and then\n the 4 levels were mapped into a binary label for each word (no and weak mapped to\n 0, moderate and strong mapped to 1).\n The NRC VAD Lexicon (Mohammad, 2018a) was built by selecting words and\n emoticons from prior lexicons and annotating them with crowd-sourcing using bestbest-worst\n scaling worst scaling (Louviere et al. 2015, Kiritchenko and Mohammad 2017). In bestworst scaling, annotators are given N items (usually 4) and are asked which item is\n the best (highest) and which is the worst (lowest) in terms of some property. The\n set of words used to describe the ends of the scales are taken from prior literature.\n For valence, for example, the raters were asked:\n Q1. Which of the four words below is associated with the MOST happiness / pleasure / positiveness / satisfaction / contentedness / hopefulness\n OR LEAST unhappiness / annoyance / negativeness / dissatisfaction /\n 22.4 ‚Ä¢ S EMI - SUPERVISED I NDUCTION OF A FFECT L EXICONS 7\n\n melancholy / despair? (Four words listed as options.)\n Q2. Which of the four words below is associated with the LEAST happiness / pleasure / positiveness / satisfaction / contentedness / hopefulness OR MOST unhappiness / annoyance / negativeness / dissatisfaction\n / melancholy / despair? (Four words listed as options.)\n The score for each word in the lexicon is the proportion of times the item was chosen\n as the best (highest V/A/D) minus the proportion of times the item was chosen as the\n worst (lowest V/A/D). The agreement between annotations are evaluated by splitsplit-half\n reliability half reliability: split the corpus in half and compute the correlations between the\n annotations in the two halves.\n\n Another common way to learn sentiment lexicons is to start from a set of seed words\n that define two poles of a semantic axis (words like good or bad), and then find ways\n to label each word w by its similarity to the two seed sets. Here we summarize two\n families of seed-based semi-supervised lexicon induction algorithms, axis-based and\n graph-based.\n\n 22.4.1 Semantic Axis Methods\n One of the most well-known lexicon induction methods, the Turney and Littman\n (2003) algorithm, is given seed words like good or bad, and then for each word w to\n be labeled, measures both how similar it is to good and how different it is from bad.\n Here we describe a slight extension of the algorithm due to An et al. (2018), which\n is based on computing a semantic axis.\n In the first step, we choose seed words by hand. There are two methods for\n dealing with the fact that the affect of a word is different in different contexts: (1)\n start with a single large seed lexicon and rely on the induction algorithm to finetune\n it to the domain, or (2) choose different seed words for different genres. Hellrich\n et al. (2019) suggests that for modeling affect across different historical time periods,\n starting with a large modern affect dictionary is better than small seedsets tuned to\n be stable across time. As an example of the second approach, Hamilton et al. (2016)\n define one set of seed words for general sentiment analysis, a different set for Twitter,\n and yet another set for sentiment in financial text:\n\nDomain Positive seeds Negative seeds\nGeneral good, lovely, excellent, fortunate, pleas- bad, horrible, poor, unfortunate, unant, delightful, perfect, loved, love, pleasant, disgusting, evil, hated, hate,\n happy unhappy\nTwitter love, loved, loves, awesome, nice, hate, hated, hates, terrible, nasty, awful,\n amazing, best, fantastic, correct, happy worst, horrible, wrong, sad\nFinance successful, excellent, profit, beneficial, negligent, loss, volatile, wrong, losses,\n improving, improved, success, gains, damages, bad, litigation, failure, down,\n positive negative\n In the second step, we compute embeddings for each of the pole words. These\n embeddings can be off-the-shelf word2vec embeddings, or can be computed directly\n8 C HAPTER 22 ‚Ä¢ L EXICONS FOR S ENTIMENT, A FFECT, AND C ONNOTATION\n\n on a specific corpus (for example using a financial corpus if a finance lexicon is the\n goal), or we can finetune off-the-shelf embeddings to a corpus. Fine-tuning is especially important if we have a very specific genre of text but don‚Äôt have enough data\n to train good embeddings. In finetuning, we begin with off-the-shelf embeddings\n like word2vec, and continue training them on the small target corpus.\n Once we have embeddings for each pole word, we create an embedding that\n represents each pole by taking the centroid of the embeddings of each of the seed\n words; recall that the centroid is the multidimensional version of the mean. Given\n a set of embeddings for the positive seed words S+ = {E(w+ +\n 1 ), E(w2 ), ..., E(wn )},\n ‚àí ‚àí ‚àí ‚àí\n and embeddings for the negative seed words S = {E(w1 ), E(w2 ), ..., E(wm )}, the\n pole centroids are:\n n\n 1X\n V+ = E(w+\n i )\n n\n m\n 1X\n V‚àí = E(w‚àí\n i ) (22.1)\n m\n\n The semantic axis defined by the poles is computed just by subtracting the two vectors:\n\n Vaxis = V+ ‚àí V‚àí (22.2)\n\n Vaxis , the semantic axis, is a vector in the direction of positive sentiment. Finally,\n we compute (via cosine similarity) the angle between the vector in the direction of\n positive sentiment and the direction of w‚Äôs embedding. A higher cosine means that\n w is more aligned with S+ than S‚àí .\n\n score(w) = cos E(w), Vaxis\n \u0001\n\n E(w) ¬∑ Vaxis\n = (22.3)\n kE(w)kkVaxis k\n\n If a dictionary of words with sentiment scores is sufficient, we‚Äôre done! Or if we\n need to group words into a positive and a negative lexicon, we can use a threshold\n or other method to give us discrete lexicons.\n\n 22.4.2 Label Propagation\n An alternative family of methods defines lexicons by propagating sentiment labels\n on graphs, an idea suggested in early work by Hatzivassiloglou and McKeown\n (1997). We‚Äôll describe the simple SentProp (Sentiment Propagation) algorithm of\n Hamilton et al. (2016), which has four steps:\n 1. Define a graph: Given word embeddings, build a weighted lexical graph by\n connecting each word with its k nearest neighbors (according to cosine similarity). The weights of the edge between words wi and w j are set as:\n !\n wi > wj\n Ei, j = arccos ‚àí . (22.4)\n kwi kkwj k\n\n 2. Define a seed set: Choose positive and negative seed words.\n 22.4 ‚Ä¢ S EMI - SUPERVISED I NDUCTION OF A FFECT L EXICONS 9\n\n 3. Propagate polarities from the seed set: Now we perform a random walk on\n this graph, starting at the seed set. In a random walk, we start at a node and\n then choose a node to move to with probability proportional to the edge probability. A word‚Äôs polarity score for a seed set is proportional to the probability\n of a random walk from the seed set landing on that word (Fig. 22.7).\n 4. Create word scores: We walk from both positive and negative seed sets,\n resulting in positive (rawscore+ (wi )) and negative (rawscore‚àí (wi )) raw label\n scores. We then combine these values into a positive-polarity score as:\n\n rawscore+ (wi )\n score+ (wi ) = (22.5)\n rawscore+ (wi ) + rawscore‚àí (wi )\n It‚Äôs often helpful to standardize the scores to have zero mean and unit variance\n within a corpus.\n 5. Assign confidence to each score: Because sentiment scores are influenced by\n the seed set, we‚Äôd like to know how much the score of a word would change if\n a different seed set is used. We can use bootstrap sampling to get confidence\n regions, by computing the propagation B times over random subsets of the\n positive and negative seed sets (for example using B = 50 and choosing 7 of\n the 10 seed words each time). The standard deviation of the bootstrap sampled\n polarity scores gives a confidence measure.\n\n loathe loathe\n like like\n abhor abhor\n\n idolize find idolize find\n love hate love hate\n dislike dislike\n see uncover see uncover\n adore despise adore despise\n disapprove disapprove\n appreciate notice appreciate notice\n\n (a) (b)\npolarity scores (shown here as colors green or red) based on the frequency of random walk visits.\n\n 22.4.3 Other Methods\n The core of semisupervised algorithms is the metric for measuring similarity with\n the seed words. The Turney and Littman (2003) and Hamilton et al. (2016) approaches above used embedding cosine as the distance metric: words were labeled\n as positive basically if their embeddings had high cosines with positive seeds and\n low cosines with negative seeds. Other methods have chosen other kinds of distance\n metrics besides embedding cosine.\n For example the Hatzivassiloglou and McKeown (1997) algorithm uses syntactic\n cues; two adjectives are considered similar if they were frequently conjoined by and\n and rarely conjoined by but. This is based on the intuition that adjectives conjoined\n by the words and tend to have the same polarity; positive adjectives are generally\n coordinated with positive, negative with negative:\n fair and legitimate, corrupt and brutal\n but less often positive adjectives coordinated with negative:\n *fair and brutal, *corrupt and legitimate\n10 C HAPTER 22 ‚Ä¢ L EXICONS FOR S ENTIMENT, A FFECT, AND C ONNOTATION\n\n By contrast, adjectives conjoined by but are likely to be of opposite polarity:\n fair but brutal\n Another cue to opposite polarity comes from morphological negation (un-, im-,\n -less). Adjectives with the same root but differing in a morphological negative (adequate/inadequate, thoughtful/thoughtless) tend to be of opposite polarity.\n Yet another method for finding words that have a similar polarity to seed words\n is to make use of a thesaurus like WordNet (Kim and Hovy 2004, Hu and Liu 2004).\n A word‚Äôs synonyms presumably share its polarity while a word‚Äôs antonyms probably\n have the opposite polarity. After a seed lexicon is built, each lexicon is updated as\n follows, possibly iterated.\n Lex+ : Add synonyms of positive words (well) and antonyms (like fine) of negative\n words\n Lex‚àí : Add synonyms of negative words (awful) and antonyms (like evil) of positive\n words\n An extension of this algorithm assigns polarity to WordNet senses, called Senti-\nSentiWordNet WordNet (Baccianella et al., 2010). Fig. 22.8 shows some examples.\n\nSynset Pos Neg Obj\ngood#6 ‚Äòagreeable or pleasing‚Äô 1 0 0\nrespectable#2 honorable#4 good#4 estimable#2 ‚Äòdeserving of esteem‚Äô 0.75 0 0.25\nestimable#3 computable#1 ‚Äòmay be computed or estimated‚Äô 0 0 1\nsting#1 burn#4 bite#2 ‚Äòcause a sharp or stinging pain‚Äô 0 0.875 .125\nacute#6 ‚Äòof critical importance and consequence‚Äô 0.625 0.125 .250\nacute#4 ‚Äòof an angle; less than 90 degrees‚Äô 0 0 1\nacute#1 ‚Äòhaving or experiencing a rapid onset and short but severe course‚Äô 0 0.5 0.5\nof homonymous words: estimable#3 is purely objective, while estimable#2 is positive; acute can be positive\n(acute#6), negative (acute#1), or neutral (acute #4).\n\n In this algorithm, polarity is assigned to entire synsets rather than words. A\n positive lexicon is built from all the synsets associated with 7 positive words, and a\n negative lexicon from synsets associated with 7 negative words. A classifier is then\n trained from this data to take a WordNet gloss and decide if the sense being defined\n is positive, negative or neutral. A further step (involving a random-walk algorithm)\n assigns a score to each WordNet synset for its degree of positivity, negativity, and\n neutrality.\n In summary, semisupervised algorithms use a human-defined set of seed words\n for the two poles of a dimension, and use similarity metrics like embedding cosine,\n coordination, morphology, or thesaurus structure to score words by how similar they\n are to the positive seeds and how dissimilar to the negative seeds.\n\n Semi-supervised methods require only minimal human supervision (in the form of\n seed sets). But sometimes a supervision signal exists in the world and can be made\n use of. One such signal is the scores associated with online reviews.\n The web contains an enormous number of online reviews for restaurants, movies,\n books, or other products, each of which have the text of the review along with an\n 22.5 ‚Ä¢ S UPERVISED L EARNING OF W ORD S ENTIMENT 11\n\n associated review score: a value that may range from 1 star to 5 stars, or scoring 1\n to 10. Fig. 22.9 shows samples extracted from restaurant, book, and movie reviews.\n\n Movie review excerpts (IMDb)\n10 A great movie. This film is just a wonderful experience. It‚Äôs surreal, zany, witty and slapstick\n all at the same time. And terrific performances too.\n1 This was probably the worst movie I have ever seen. The story went nowhere even though they\n could have done some interesting stuff with it.\n Restaurant review excerpts (Yelp)\n5 The service was impeccable. The food was cooked and seasoned perfectly... The watermelon\n was perfectly square ... The grilled octopus was ... mouthwatering...\n2 ...it took a while to get our waters, we got our entree before our starter, and we never received\n silverware or napkins until we requested them...\n Book review excerpts (GoodReads)\n1 I am going to try and stop being deceived by eye-catching titles. I so wanted to like this book\n and was so disappointed by it.\n5 This book is hilarious. I would recommend it to anyone looking for a satirical read with a\n romantic twist and a narrator that keeps butting in\n Product review excerpts (Amazon)\n5 The lid on this blender though is probably what I like the best about it... enables you to pour\n into something without even taking the lid off! ... the perfect pitcher! ... works fantastic.\n1 I hate this blender... It is nearly impossible to get frozen fruit and ice to turn into a smoothie...\n You have to add a TON of liquid. I also wish it had a spout ...\nIMDb, which is on a scale of 1 to 10 stars.\n\n We can use this review score as supervision: positive words are more likely to\n appear in 5-star reviews; negative words in 1-star reviews. And instead of just a\n binary polarity, this kind of supervision allows us to assign a word a more complex\n representation of its polarity: its distribution over stars (or other scores).\n Thus in a ten-star system we could represent the sentiment of each word as a\n 10-tuple, each number a score representing the word‚Äôs association with that polarity\n level. This association can be a raw count, or a likelihood P(w|c), or some other\n function of the count, for each class c from 1 to 10.\n For example, we could compute the IMDb likelihood of a word like disappoint(ed/ing) occurring in a 1 star review by dividing the number of times disappoint(ed/ing) occurs in 1-star reviews in the IMDb dataset (8,557) by the total number of words occurring in 1-star reviews (25,395,214), so the IMDb estimate of\n P(disappointing|1) is .0003.\n A slight modification of this weighting, the normalized likelihood, can be used\n as an illuminating visualization (Potts, 2011)1\n count(w, c)\n P(w|c) = P\n w‚ààC count(w, c)\n P(w|c)\n PottsScore(w) = P (22.6)\n c P(w|c)\n\n Dividing the IMDb estimate P(disappointing|1) of .0003 by the sum of the likelihood P(w|c) over all categories gives a Potts score of 0.10. The word disappointing\n 1 Each element of the Potts score of a word w and category c can be shown to be a variant of the\n pointwise mutual information pmi(w, c) without the log term; see Exercise 22.1.\n Example: atten\n IMDB\n\n Cat = 0.3\n Cat^2 = -\n\n12 C HAPTER 22 ‚Ä¢ L EXICONS FOR S ENTIMENT, A FFECT, AND C ONNOTATION\n 0.15\n\n 0.09\n\n thus is associated with the vector [.10, .12, .14, .14, .13, .11, .08, .06, .06, .05]. The 0.05\n\n Potts diagram Potts diagram (Potts, 2011) is a visualization of these word scores, representing the\n\n -0.50\n -0.39\n -0.28\n prior sentiment of a word as a distribution over the rating categories.\n Fig. 22.10 shows the Potts diagrams for 3 positive and 3 negative scalar adjectives. Note that the curve for strongly positive scalars have the shape of the letter\n J, while strongly negative scalars look like a reverse J. By contrast, weakly positive and negative scalars have a hump-shape, with the maximum either below the\n\n ‚ÄúPotts&diagrams‚Äù\n mean (weakly negative words like disappointing) or above the mean (weakly positive words like good). These shapes offer an illuminating typology of affective\n IMDB\n Potts,&Christopher.& 2011.&NSF&wor\n restructuring& adjectives. Ca\n meaning. Cat^\n\n Negative scalars Emphatics 0.17\n Atten\n Positive scalars 0.09\n totally 0.04 som\n good disappointing\n\n -0.50\n -0.39\n -0.28\n 1 2 3 4 5 6 7 8 9 10 1 2 3 4\n 1 2 3 4 5 6 7 8 9 10 rating\n rating 1 2 3 4 5 6 7 8 9 10\n rating\n absolutely f\n great bad\n IMDB\n\n 1 2 3 4 5 6 7 8 9 10 1 2 3 4Ca\n rating Cat\n 1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10\n rating rating\n utterly p\n excellent terrible 0.13\n 0.09\n 0.05\n\n 1 2 3 4 5 6 7 8 9 10 1 2 3 4\n\n -0.50\n -0.39\n -0.28\n rating\n 1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10\n rating rating\n\n hump-shape for more weakly polarized adjectives.\n\n Fig. 22.11 shows the Potts diagrams for emphasizing and attenuating adverbs.\n Note that emphatics tend to have a J-shape (most likely to occur in the most positive reviews) or a U-shape (most likely to occur in the strongly positive and negative). Attenuators all have the hump-shape, emphasizing the middle of the scale and\n downplaying both extremes. The diagrams can be used both as a typology of lexical\n sentiment, and also play a role in modeling sentiment compositionality.\n In addition to functions like posterior P(c|w), likelihood P(w|c), or normalized\n likelihood (Eq. 22.6) many other functions of the count of a word occurring with a\n sentiment label have been used. We‚Äôll introduce some of these on page 16, including\n ideas like normalizing the counts per writer in Eq. 22.14.\n\n 22.5.1 Log Odds Ratio Informative Dirichlet Prior\n One thing we often want to do with word polarity is to distinguish between words\n that are more likely to be used in one category of texts than in another. We may, for\n example, want to know the words most associated with 1 star reviews versus those\n associated with 5 star reviews. These differences may not be just related to sentiment. We might want to find words used more often by Democratic than Republican\n -0.\n -0.\n -0.\n -0.\n -0.\n\n 0.\n 0.\n 0.\n 0.\n 0.\n\n -0.\n\n -0.\n\n 0.\n\n 0.\n\n 0.\n\n -0.\n\n -0.\n\n 0.\n Category Category Category\n\n fairly/r\n\n ‚ÄúPotts&diagrams‚Äù Potts,&Christopher.& 2011.&NSF&workshop&on&\n restructuring&\n ‚Ä¢ S UPERVISED L EARNINGCat^2 W ORD\n IMDB ‚Äì 33,515 tokens\n\n S ENTIMENT\n Cat = -0.13 (p = 0.284)\n OF= -5.37 (p < 0.001) 13 (p = 0.007)\n Cat = 0.2 (p = 0.265)\n Cat^2 = -4.16\n OpenTable ‚Äì 2,829 tokens Goodreads ‚Äì 1,806\n\n Cat = -0.87 (p\n Cat^2 = -5.74 (p\n 0.35\n 0.31\n\n Negative scalars Emphatics 0.17\n Attenuators 0.18\nve scalars 0.09 0.12\n totally 0.04 somewhat 0.08\n 0.05\ngood disappointing\n\n -0.50\n -0.39\n -0.28\n -0.17\n -0.06\n\n 0.06\n 0.17\n 0.28\n 0.39\n 0.50\n\n -0.50\n\n -0.25\n\n 0.00\n\n 0.25\n\n 0.50\n\n -0.50\n\n -0.25\n\n 0.00\n Category Category Category\n\n 1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10\n 5 6 7 8 9 10 rating\n rating 1 2 3 4 5 6 7 8 9 10 rating\n rating\n absolutely fairly\ngreat bad pretty/r\n IMDB ‚Äì 176,264 tokens OpenTable ‚Äì 8,982 tokens Goodreads ‚Äì 11,89\n\n 1 2 3 4 5 6 7 8 9 10 1 2 3 4Cat5= -0.43\n 6 7(p 8< 0.001)\n 9 10 Cat = -0.64 (p = 0.035) Cat = -0.71 (p\n rating Cat^2 = -3.6 (p < 0.001)\n rating Cat^2 = -4.47 (p = 0.007) Cat^2 = -4.59 (p\n 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10\n 0.34\n rating rating 0.32\n\n utterly pretty\nxcellent terrible 0.13\n 0.19\n 0.15\n 0.14\n 0.09\n\n 1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10\n\n -0.50\n -0.39\n -0.28\n -0.17\n -0.06\n\n 0.06\n 0.17\n 0.28\n 0.39\n 0.50\n\n -0.50\n\n -0.25\n\n 0.00\n\n 0.25\n\n 0.50\n\n -0.50\n\n -0.25\n\n 0.00\n rating rating\n Category Category Category\n4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10\n rating rating Figure 22.11 Potts diagrams (Potts, 2011) for emphatic and attenuating adverbs.\n\n members of Congress, or words used more often in menus of expensive restaurants\n than cheap restaurants.\n Given two classes of documents, to find words more associated with one category than another, we could measure the difference in frequencies (is a word w more\n frequent in class A or class B?). Or instead of the difference in frequencies we could\n compute the ratio of frequencies, or compute the log odds ratio (the log of the ratio\n between the odds of the two words). We could then sort words by whichever association measure we pick, ranging from words overrepresented in category A to words\n overrepresented in category B.\n The problem with simple log-likelihood or log odds methods is that they overemphasize differences in very rare words, and often also in very frequent words. Very\n rare words will seem to occur very differently in the two corpora since with tiny\n counts there may be statistical fluctuations, or even zero occurrences in one corpus\n compared to non-zero occurrences in the other. Very frequent words will also seem\n different since all counts are large.\n In this section we walk through the details of one solution to this problem: the\n ‚Äúlog odds ratio informative Dirichlet prior‚Äù method of Monroe et al. (2008) that is a\n particularly useful method for finding words that are statistically overrepresented in\n one particular category of texts compared to another. It‚Äôs based on the idea of using\n another large corpus to get a prior estimate of what we expect the frequency of each\n word to be.\n Let‚Äôs start with the goal: assume we want to know whether the word horrible\n log likelihood occurs more in corpus i or corpus j. We could compute the log likelihood ratio,\n ratio\n using f i (w) to mean the frequency of word w in corpus i, and ni to mean the total\n number of words in corpus i:\n\n Pi (horrible)\n llr(horrible) = log\n P j (horrible)\n = log Pi (horrible) ‚àí log P j (horrible)\n fi (horrible) f j (horrible)\n = log i\n ‚àí log (22.7)\n n nj\n14 C HAPTER 22 ‚Ä¢ L EXICONS FOR S ENTIMENT, A FFECT, AND C ONNOTATION\n\n log odds ratio Instead, let‚Äôs compute the log odds ratio: does horrible have higher odds in i or in\n j:\n\n Pi (horrible) P j (horrible)\n \u0012 \u0013 \u0012 \u0013\n lor(horrible) = log ‚àí log\n 1 ‚àí Pi (horrible) 1 ‚àí P j (horrible)\n Ô£´ i Ô£´ j\n f (horrible) f (horrible)\n Ô£∂ Ô£∂\n\n = log Ô£≠\n Ô£¨ ni Ô£∏ ‚àí log Ô£≠\n Ô£∑ Ô£¨ nj Ô£∑\n fi (horrible) f j (horrible)\n Ô£∏\n 1‚àí i 1‚àí j\n n n\n \u0012 i\n f (horrible) f j (horrible)\n \u0013 \u0012 \u0013\n = log i i ‚àí log (22.8)\n n ‚àí f (horrible) n j ‚àí f j (horrible)\n The Dirichlet intuition is to use a large background corpus to get a prior estimate of\n what we expect the frequency of each word w to be. We‚Äôll do this very simply by\n adding the counts from that corpus to the numerator and denominator, so that we‚Äôre\n essentially shrinking the counts toward that prior. It‚Äôs like asking how large are the\n differences between i and j given what we would expect given their frequencies in\n a well-estimated large background corpus.\n The method estimates the difference between the frequency of word w in two\n (i‚àí j)\n corpora i and j via the prior-modified log odds ratio for w, Œ¥w , which is estimated\n as:\n\n fwj + Œ±w\n !\n fwi + Œ±w\n \u0012 \u0013\n (i‚àí j)\n Œ¥w = log i ‚àí log (22.9)\n n + Œ±0 ‚àí ( fwi + Œ±w ) n j + Œ±0 ‚àí ( fwj + Œ±w )\n\n (where ni is the size of corpus i, n j is the size of corpus j, fwi is the count of word\n w in corpus i, fwj is the count of word w in corpus j, Œ±0 is the scaled size of the\n background corpus, and Œ±w is the scaled count of word w in the background corpus.)\n In addition, Monroe et al. (2008) make use of an estimate for the variance of the\n log‚Äìodds‚Äìratio:\n \u0010\n (i‚àí j)\n \u0011 1 1\n œÉ 2 Œ¥ÃÇw ‚âà i + j (22.10)\n fw + Œ±w fw + Œ±w\n\n The final statistic for a word is then the z‚Äìscore of its log‚Äìodds‚Äìratio:\n (i‚àí j)\n Œ¥ÃÇw\n r (22.11)\n (i‚àí j)\n \u0010 \u0011\n œÉ 2 Œ¥ÃÇw\n\n The Monroe et al. (2008) method thus modifies the commonly used log odds ratio\n in two ways: it uses the z-scores of the log odds ratio, which controls for the amount\n of variance in a word‚Äôs frequency, and it uses counts from a background corpus to\n provide a prior count for words.\n Fig. 22.12 shows the method applied to a dataset of restaurant reviews from\n Yelp, comparing the words used in 1-star reviews to the words used in 5-star reviews\n (Jurafsky et al., 2014). The largest difference is in obvious sentiment words, with the\n 1-star reviews using negative sentiment words like worse, bad, awful and the 5-star\n reviews using positive sentiment words like great, best, amazing. But there are other\n illuminating differences. 1-star reviews use logical negation (no, not), while 5-star\n reviews use emphatics and emphasize universality (very, highly, every, always). 1star reviews use first person plurals (we, us, our) while 5 star reviews use the second\n 22.6 ‚Ä¢ U SING L EXICONS FOR S ENTIMENT R ECOGNITION 15\n\n person. 1-star reviews talk about people (manager, waiter, customer) while 5-star\n reviews talk about dessert and properties of expensive restaurants like courses and\n atmosphere. See Jurafsky et al. (2014) for more details.\n\nClass Words in 1-star reviews Class Words in 5-star reviews\nNegative worst, rude, terrible, horrible, bad, Positive great, best, love(d), delicious, amazing,\n awful, disgusting, bland, tasteless, favorite, perfect, excellent, awesome,\n gross, mediocre, overpriced, worse, friendly, fantastic, fresh, wonderful, inpoor credible, sweet, yum(my)\nNegation no, not Emphatics/ very, highly, perfectly, definitely, absouniversals lutely, everything, every, always\n1Pl pro we, us, our 2 pro you\n3 pro she, he, her, him Articles a, the\nPast verb was, were, asked, told, said, did, Advice try, recommend\n charged, waited, left, took\n Sequencers after, then Conjunct also, as, well, with, and\n Nouns manager, waitress, waiter, customer, Nouns atmosphere, dessert, chocolate, wine,\n customers, attitude, waste, poisoning, course, menu\n money, bill, minutes\n Irrealis would, should Auxiliaries is/‚Äôs, can, ‚Äôve, are\n modals\n Comp to, that Prep, other in, of, die, city, mouth\n900,000 reviews, using the Monroe et al. (2008) method (Jurafsky et al., 2014).\n\n In Appendix K we introduced the naive Bayes algorithm for sentiment analysis. The\n lexicons we have focused on throughout the chapter so far can be used in a number\n of ways to improve sentiment detection.\n In the simplest case, lexicons can be used when we don‚Äôt have sufficient training\n data to build a supervised sentiment analyzer; it can often be expensive to have a\n human assign sentiment to each document to train the supervised classifier.\n In such situations, lexicons can be used in a rule-based algorithm for classification. The simplest version is just to use the ratio of positive to negative words: if a\n document has more positive than negative words (using the lexicon to decide the polarity of each word in the document), it is classified as positive. Often a threshold Œª\n is used, in which a document is classified as positive only if the ratio is greater than\n Œª . If the sentiment lexicon includes positive and negative weights for each word,\n Œ∏w+ and Œ∏w‚àí , these can be used as well. Here‚Äôs a simple such sentiment algorithm:\n X\n f+ = Œ∏w+ count(w)\n w s.t. w‚ààpositivelexicon\n X\n ‚àí\n f = Œ∏w‚àí count(w)\n w s.t. w‚àànegativelexicon\n if ff ‚àí > Œª\n Ô£±\n Ô£¥\n Ô£¥ +\n Ô£¥\n ‚àí\n Ô£≤\n sentiment = ‚àí if ff + > Œª (22.12)\n Ô£¥\n Ô£≥0 otherwise.\n Ô£¥\n Ô£¥\n16 C HAPTER 22 ‚Ä¢ L EXICONS FOR S ENTIMENT, A FFECT, AND C ONNOTATION\n\n If supervised training data is available, these counts computed from sentiment lexicons, sometimes weighted or normalized in various ways, can also be used as features in a classifier along with other lexical or non-lexical features. We return to\n such algorithms in Section 22.7.\n\n Detection of emotion (and the other kinds of affective meaning described by Scherer\n (2000)) can be done by generalizing the algorithms described above for detecting\n sentiment.\n The most common algorithms involve supervised classification: a training set is\n labeled for the affective meaning to be detected, and a classifier is built using features\n extracted from the training set. As with sentiment analysis, if the training set is large\n enough, and the test set is sufficiently similar to the training set, simply using all the\n words or all the bigrams as features in a powerful classifier like logistic regression or\n SVM is an excellent algorithm whose performance is hard to beat. Thus we can treat\n affective meaning classification of a text sample as simple document classification.\n Some modifications are nonetheless often necessary for very large datasets. For\n example, the Schwartz et al. (2013) study of personality, gender, and age using 700\n million words of Facebook posts used only a subset of the n-grams of lengths 1-\n3. Only words and phrases used by at least 1% of the subjects were included as\n features, and 2-grams and 3-grams were only kept if they had sufficiently high PMI\n (PMI greater than 2 ‚àó length, where length is the number of words):\n\n p(phrase)\n pmi(phrase) = log Y (22.13)\n p(w)\n w‚ààphrase\n\n Various weights can be used for the features, including the raw count in the training\n set, or some normalized probability or log probability. Schwartz et al. (2013), for\n example, turn feature counts into phrase likelihoods by normalizing them by each\n subject‚Äôs total word use.\n\n freq(phrase, subject)\n p(phrase|subject) = X (22.14)\n freq(phrase0 , subject)\n phrase ‚ààvocab(subject)\n\n If the training data is sparser, or not as similar to the test set, any of the lexicons\n we‚Äôve discussed can play a helpful role, either alone or in combination with all the\n words and n-grams.\n Many possible values can be used for lexicon features. The simplest is just an\n indicator function, in which the value of a feature fL takes the value 1 if a particular\n text has any word from the relevant lexicon L. Using the notation of Appendix K, in\n which a feature value is defined for a particular output class c and document x.\n\n 1 if ‚àÉw : w ‚àà L & w ‚àà x & class = c\n \u001a\n fL (c, x) =\n 0 otherwise\n\n Alternatively the value of a feature fL for a particular lexicon L can be the total\n 22.8 ‚Ä¢ L EXICON - BASED METHODS FOR E NTITY-C ENTRIC A FFECT 17\n\n number of word tokens in the document that occur in L:\n X\n fL = count(w)\n w‚ààL\n\n For lexica in which each word is associated with a score or weight, the count can be\n multiplied by a weight Œ∏wL :\n\n Œ∏wL count(w)\n X\n fL =\n w‚ààL\n\n Counts can alternatively be logged or normalized per writer as in Eq. 22.14.\n However they are defined, these lexicon features are then used in a supervised\n classifier to predict the desired affective category for the text or document. Once\n a classifier is trained, we can examine which lexicon features are associated with\n which classes. For a classifier like logistic regression the feature weight gives an\n indication of how associated the feature is with the class.\n\n What if we want to get an affect score not for an entire document, but for a particular\n entity in the text? The entity-centric method of Field and Tsvetkov (2019) combines\n affect lexicons with contextual embeddings to assign an affect score to an entity in\n text. In the context of affect about people, they relabel the Valence/Arousal/Dominance\n dimension as Sentiment/Agency/Power. The algorithm first trains classifiers to map\n embeddings to scores:\n 1. For each word w in the training corpus:\n (a) Use off-the-shelf pretrained encoders (like BERT) to extract a contextual\n embedding e for each instance of the word. No additional finetuning is\n done.\n (b) Average over the e embeddings of each instance of w to obtain a single\n embedding vector for one training point w.\n (c) Use the NRC VAD Lexicon to get S, A, and P scores for w.\n 2. Train (three) regression models on all words w to predict V, A, D scores from\n a word‚Äôs average embedding.\n Now given an entity mention m in a text, we assign affect scores as follows:\n 1. Use the same pretrained LM to get contextual embeddings for m in context.\n 2. Feed this embedding through the 3 regression models to get S, A, P scores for\n the entity.\n This results in a (S,A,P) tuple for a given entity mention; To get scores for the representation of an entity in a complete document, we can run coreference resolution\n and average the (S,A,P) scores for all the mentions. Fig. 22.13 shows the scores\n from their algorithm for characters from the movie The Dark Knight when run on\n Wikipedia plot summary texts with gold coreference.\n18 C HAPTER 22 ‚Ä¢ L EXICONS FOR S ENTIMENT, A FFECT, AND C ONNOTATION\n\n weakly Rachel Dent Gordan Batman Joker powerfully weakly Rachel Joker Dent Gordan Batm\n\n Power Score Power Score\n\n negative Joker Dent Gordan Rachel Batman positive negative Joker Gordan Batman Dent Rach\n\n Sentiment Score Sentiment Score\n\n dull Rachel Dent GordanBatman Joker\n dull Dent Gordan Rachel Batman Joker scary\n\n Agency Score\n Agency Score\n\n characters\n in the movie TheFigure 1: Power,\n Dark Knight sentiment,\n computed and agency\n from embeddings scores\n trained onfor\n thechar-\nNRC VADacters\n Lexicon.\n in The Dark Night as learned throug\n acters(Batman)\n Note the protagonist in The Dark Night\n and the as learned\n antagonist through\n (the Joker) thehigh\n have regrespower and agency\n ELMo embeddings. These scores reflect th\n sion model with ELMo embeddings. Scores generally\n scores but differ in sentiment, while the love interest Rachel has low power and agency but\n terns as the regression model with greater\n high sentiment. align with character archetypes, i.e. the antagonist has\n between characters.\n the lowest sentiment score.\n ment have resulted in his effective removal from vey Dent (ally to Batman who turns\n the industry.\n The lexicons we‚Äôve described so While articles\n far define a wordabout\n as a the in affective Rachel\n point#MeToo space. ADawes (primary love interest).\n connotation\n frame connotation frame, by contrast,\n movement portray is amen\n lexicon\n likethat incorporates\n Weinstein itate\n a richer kind of gramas unpow- extracting example sentences, we\n matical structure, by combining\n erful, affective lexicons\n we can speculate that thewith the frame\n corpora usedsemantic\n to instance\n lexicons of these entities in the narrative\n of Chapter 21. The basic insight of connotation frame lexicons is that a and predicate\n average across instances to obtain\n train ELMo and BERT portray them as powerful.\n like a verb expresses connotations about the verb‚Äôs arguments (Rashkin et score al. 2016,for the document.9 To maximiz\n Thus, in a corpus where traditional power roles\n Rashkin et al. 2017).\n have been by capturing every mention of an entit\n Consider sentences like: inverted, the embeddings extracted\n from ELMo and BERT perform worse than ran- form co-reference resolution by hand.\n (22.15) Country A violated the sovereignty of Country B ally, based on our results from Table 3\n dom, as they are biased towards the power struc-\n (22.16) the teenager ... survived the Boston Marathon bombing‚Äù the use of Wikipedia data in training\n tures in the data they are trained on. Further ev-\nBy using the verb violate\n idence of in\n this(22.15),\n existstheinauthor is expressing their\n the performance model\n sympathies\n of the with(Peters et al., 2018), we use ELM\n Country B, portraying Country B as a victim, and expressing antagonism toward\n dings for our analysis.\n BERT-masked embeddings - whereas these emthe agent Country A. By contrast, in using the verb survive, the author of (22.16) is\n Figures 1 and 2 show results.\n expressing thatbeddings\n the bombing generally capture\n is a negative power and\n experience, poorly as comthe subject of the sentence,\n the teenager, ispared to the character.\n sympathetic unmaskedThese embeddings\n aspects of (Table ence,\n 2), are inherent\n connotation we show the entity scores as co\n in the meaningthey outperform\n of the verbs violate theand\n unmasked\n survive, asembeddings\n shown in Fig. on22.14.\n this one polar opposite pair identified by\n The connotation frame\n task, and evenlexicons of Rashkin\n outperform et al. (2016)\n the frequency and Rashkin\n baseline the etregression\n al. model and ASP show s\n (2017) also express\n in oneother connotative\n setting. aspects of\n Nevertheless, theythedo\n predicate terns.\n toward each\n not outper- argu-Batman has high power, while R\n ment, including the effect (something bad happened to x) value: (x is valuable), low and\n power. Additionally, the Joker is\n form Field et al. (2019), likely because they do not\n mental state: (x is distressed by the event). Connotation frames can also with mark the the most negative sentiment, but\n capture\n power differential affecttheinformation\n between as wellthe\n arguments (using as verb\n the unmasked\n implore means that the\n embeddings (Tablethan est\n 2). the agent), and the agency of each argument agency. Throughout the plot sum\n theme argument has greater power\n (waited is low agency). Fig. 22.15 shows a visualization from Sap et al. (2017). movie progresses by the Joker taking\n Connotation frames can be built by hand (Sap et al., 2017), or they can be sive action and the other characters r\n learned\n by supervised learning (Rashkin et al., 2016), for example using hand-labeled train- see this dynamic reflected in t\n We can\n Finally, we qualitatively analyze how well our\n ing data to supervise classifiers for each of the individual relations, e.g.,profile whetherscore, as a high-powered, hi\n method captures affect dimensions by analyzing\n S(writer ‚Üí Role1) is + or -, and then improving accuracy via global constraints low-sentiment character, who is the pri\n single documents in detail. We conduct this analacross all relations. driver. In general, ASP shows a greater\n ysis in a domain where we expect entities to fulfill\n between characters than the regression m\n traditional power roles and where entity portrayhypothesize that this occurs because AS\n als are known. Following Bamman et al. (2013),\n the dimensions of interest, while the reg\n we analyze the Wikipedia plot summary of the\n proach captures other confounds, such\n movie The Dark Knight,7 focusing on Batman\n (protagonist),8 the Joker (antagonist), Jim Gordan 9\n When we used this averaging metric in othe\n 22.10 ‚Ä¢ S UMMARY 19\n\n Connotation Frame for ‚ÄúRole1 survives Role2‚Äù Connotation Frame for ‚ÄúRole1 violates Role2‚Äù\n\n )\n\n S(\n le1\n )\n\n S(\n le1\n\n wr\n ro\n wr\n ro\n Writer\n _ +\n\n ite\n r‚Üí\n Writer\n\n ite\n r‚Üí\n\n r‚Üí\n r‚Üí\n\n ite\n ite\n\n ro\n wr\n ro\n wr\n\n le2\n le2\n\n S(\n S(role1‚Üírole2)\n S(\n\n S(role1‚Üírole2)\n\n )\n )\n Role1 is a _ There is Role1 is the\n antagonist Role1\n _ Role2\n Role2 is a\n sympathetic\n sympathetic Role1 Role2 some type\n victim of hardship victim\n\n _ _ +\n + Reader\n Reader\n\n (a) (b)\nsentiment toward Role1, the subject, and negative sentiment toward Role2, the direct object. (b) For violate, the\nwriter and reader have positive sentiment instead toward Role2, the direct object.\n\n He implored the tribunal to show mercy. power(AG<TH) power(AG>TH)\n\n VERB\n AGENT THEME\n implore\n\n power(AG < TH)\n\n The princess waited for her prince.\n VERB\n AGENT THEME\n wait agency(AG)= agency(AG)=+\n agency(AG) = -\n\n implies the agent has Figure 2: The\n lower power thanformal notation\n the theme of the connotation\n (in contrast, say, with a verb like demanded),\n and showing the low frames\n level of of power\n agency of and agency.of The\n the subject firstFigure\n waited. example\n from Sap et al. (2017).\n shows the relative power differential implied by\n a position of less power than the theme (‚Äúthe tri- Figure 3: Sample verbs in the connotation frame\n bunal‚Äù). In contrast, ‚ÄúHe demanded the tribunal with high annotator agreement. Size is indicativ\n show mercy‚Äù implies that the agent has authority of verb frequency in our corpus (bigger = mor\n ‚Ä¢ Many kinds of affective states can be distinguished, including emotions, moods,\n over the theme. The second example shows the frequent), color differences are only for legibility\n attitudes (which include sentiment), interpersonal stance, and personality.\n low level of agency implied by the verb ‚Äúwaited‚Äù.\n ‚Ä¢ Emotion can be represented by fixed atomic units often called basic emoone another. For example, if the agent ‚Äúdom\n tions, or as points in space defined by dimensions like valenceinates‚Äù and arousal.\n the theme (denoted as power(AG>TH)\n interactive demo website of our findings (see Fig-\n ‚Ä¢ Words have ure connotational\n 5 in the appendix aspects\n for related to these\n a screenshot). affective states,\n 2 Further- then theand this\n agent is implied to have a level of contro\n connotationalmore,\n aspect of word meaning can be represented\n as will be seen in Section 4.1, connotation in lexicons.\n over the theme. Alternatively, if the agent ‚Äúhon\n ‚Ä¢ Affective lexicons\n frames can\n offerbenew built by hand,\n insights using crowd\n that complement and sourcing\n de- ors‚Äùtothe theme\n label the(denoted as power(AG<TH)), th\n affective content\n viateof each\n from theword.\n well-known Bechdel test (Bechdel, writer implies that the theme is more important o\n ‚Ä¢ Lexicons can1986).be builtInwith\n particular, high-agency from seed words used AMT crowdsourcing to la\n we find thatbootstrapping\n semi-supervised, authoritative. We\n women through bel 1700 transitive verbs for power differential\n using similarity metrics likethe lens of connotation\n embedding cosine. frames are\n With three annotators per verb, the inter-annotato\n ‚Ä¢ Lexicons canrare be inlearned\n modern films. It is, in part, because some\n in a fully supervised manner, whenagreement a convenient\n is 0.34 (Krippendorff‚Äôs ‚Üµ).\n movies (e.g., Snow White) accidentally pass the\n training signal can be found in the world, such as ratings assigned by users on\n Bechdel test and also because even movies with Agency The agency attributed to the agent of th\n a review site.\n strong female characters are not entirely free from verb denotes whether the action being describe\n ‚Ä¢ Words can betheassigned weights inbiases\n deeply ingrained a lexicon by using\n in social norms.various functions of word\n implies that the agent is powerful, decisive, an\n counts in training texts, and ratio metrics like log odds ratio informative\n capable of pushing forward their own storyline\n 2 Connotation Frames of Power and\n Dirichlet prior.\n For example, a person who is described as ‚Äúex\n Agencyjust like sentiment, by using standard supervised text\n ‚Ä¢ Affect can be detected, periencing‚Äù things does not seem as active and de\n classificationWetechniques, using all the words or bigrams\n create two new connotation relations, power in a text as as\n cisive features.\n someone who is described as ‚Äúdetermin\n and agency (examples in Figure 3), as an expan- ing‚Äù things. AMT workers labeled 2000 trans\n sion of the existing connotation frame lexicons.3 tive verbs for implying high/moderate/low agenc\n Three AMT crowdworkers annotated the verbs (inter-annotator agreement of 0.27). We denot\n with placeholders to avoid gender bias in the con- high agency as agency(AG)=+, and low agenc\n text (e.g., X rescued Y; an example task is shown as agency(AG)= .\n in the appendix in Figure 7). We define the anno-\n20 C HAPTER 22 ‚Ä¢ L EXICONS FOR S ENTIMENT, A FFECT, AND C ONNOTATION\n\n Additional features can be drawn from counts of words in lexicons.\n ‚Ä¢ Lexicons can also be used to detect affect in a rule-based classifier by picking\n the simple majority sentiment based on counts of words in each lexicon.\n ‚Ä¢ Connotation frames express richer relations of affective meaning that a predicate encodes about its arguments.\n\nHistorical Notes\n The idea of formally representing the subjective meaning of words began with Osgood et al. (1957), the same pioneering study that first proposed the vector space\n model of meaning described in Chapter 5. Osgood et al. (1957) had participants rate\n words on various scales, and ran factor analysis on the ratings. The most significant\n factor they uncovered was the evaluative dimension, which distinguished between\n pairs like good/bad, valuable/worthless, pleasant/unpleasant. This work influenced\n the development of early dictionaries of sentiment and affective meaning in the field\n of content analysis (Stone et al., 1966).\n subjectivity Wiebe (1994) began an influential line of work on detecting subjectivity in text,\n beginning with the task of identifying subjective sentences and the subjective characters who are described in the text as holding private states, beliefs or attitudes.\n Learned sentiment lexicons such as the polarity lexicons of Hatzivassiloglou and\n McKeown (1997) were shown to be a useful feature in subjectivity detection (Hatzivassiloglou and Wiebe 2000, Wiebe 2000).\n The term sentiment seems to have been introduced in 2001 by Das and Chen\n (2001), to describe the task of measuring market sentiment by looking at the words in\n stock trading message boards. In the same paper Das and Chen (2001) also proposed\n the use of a sentiment lexicon. The list of words in the lexicon was created by\n hand, but each word was assigned weights according to how much it discriminated\n a particular class (say buy versus sell) by maximizing across-class variation and\n minimizing within-class variation. The term sentiment, and the use of lexicons,\n caught on quite quickly (e.g., inter alia, Turney 2002). Pang et al. (2002) first showed\n the power of using all the words without a sentiment lexicon; see also Wang and\n Manning (2012).\n Most of the semi-supervised methods we describe for extending sentiment dictionaries drew on the early idea that synonyms and antonyms tend to co-occur in the\n same sentence (Miller and Charles 1991, Justeson and Katz 1991, Riloff and Shepherd 1997). Other semi-supervised methods for learning cues to affective meaning rely on information extraction techniques, like the AutoSlog pattern extractors\n (Riloff and Wiebe, 2003). Graph based algorithms for sentiment were first suggested by Hatzivassiloglou and McKeown (1997), and graph propagation became\n a standard method (Zhu and Ghahramani 2002, Zhu et al. 2003, Zhou et al. 2004,\n Velikovich et al. 2010). Crowdsourcing can also be used to improve precision by\n filtering the result of semi-supervised lexicon learning (Riloff and Shepherd 1997,\n Fast et al. 2016).\n Much recent work focuses on ways to learn embeddings that directly encode sentiment or other properties, such as the D ENSIFIER algorithm of Rothe et al. (2016)\n that learns to transform the embedding space to focus on sentiment (or other) information.\n E XERCISES 21\n\nExercises\n Score in Eq. 22.6 is a variant of the pointwise mutual information pmi(w, c)\n without the log term.\n22 Chapter 22 ‚Ä¢ Lexicons for Sentiment, Affect, and Connotation\n\nAn, J., H. Kwak, and Y.-Y. Ahn. 2018. SemAxis: A Mohammad, S. M. 2018a. Obtaining reliable human ratings\n lightweight framework to characterize domain-specific of valence, arousal, and dominance for 20,000 English\n word semantics beyond sentiment. ACL. words. ACL.\nBaccianella, S., A. Esuli, and F. Sebastiani. 2010. Senti- Mohammad, S. M. 2018b. Word affect intensities. LREC.\n wordnet 3.0: An enhanced lexical resource for sentiment Mohammad, S. M. and P. D. Turney. 2013. Crowdsourcing a\n analysis and opinion mining. LREC. word-emotion association lexicon. Computational Intel-\nBarrett, L. F., B. Mesquita, K. N. Ochsner, and J. J. Gross. ligence, 29(3):436‚Äì465.\n 2007. The experience of emotion. Annual Review of Psy- Monroe, B. L., M. P. Colaresi, and K. M. Quinn. 2008.\n chology, 58:373‚Äì403. Fightin‚Äôwords: Lexical feature selection and evaluation\nBrysbaert, M., A. B. Warriner, and V. Kuperman. 2014. for identifying the content of political conflict. Political\n Concreteness ratings for 40 thousand generally known Analysis, 16(4):372‚Äì403.\n English word lemmas. Behavior Research Methods, Moors, A., P. C. Ellsworth, K. R. Scherer, and N. H. Frijda.\n 46(3):904‚Äì911. 2013. Appraisal theories of emotion: State of the art and\nDas, S. R. and M. Y. Chen. 2001. Yahoo! for Ama- future development. Emotion Review, 5(2):119‚Äì124.\n zon: Sentiment parsing from small talk on the web. Osgood, C. E., G. J. Suci, and P. H. Tannenbaum. 1957. The\n EFA 2001 Barcelona Meetings. http://ssrn.com/ Measurement of Meaning. University of Illinois Press.\n abstract=276189. Pang, B., L. Lee, and S. Vaithyanathan. 2002. Thumbs\nEkman, P. 1999. Basic emotions. In T. Dalgleish and M. J. up? Sentiment classification using machine learning tech-\nPower, eds, Handbook of Cognition and Emotion, 45‚Äì60. niques. EMNLP.\n Wiley. Pennebaker, J. W., R. J. Booth, and M. E. Francis. 2007.\nFast, E., B. Chen, and M. S. Bernstein. 2016. Empath: Un- Linguistic Inquiry and Word Count: LIWC 2007. Austin,\n derstanding Topic Signals in Large-Scale Text. CHI. TX.\nField, A. and Y. Tsvetkov. 2019. Entity-centric contextual Picard, R. W. 1995. Affective computing. Technical Reaffective analysis. ACL. port 321, MIT Media Lab Perceputal Computing Technical Report. Revised November 26, 1995.\nHamilton, W. L., K. Clark, J. Leskovec, and D. Jurafsky.\n 2016. Inducing domain-specific sentiment lexicons from Plutchik, R. 1962. The emotions: Facts, theories, and a new\n unlabeled corpora. EMNLP. model. Random House.\nHatzivassiloglou, V. and K. McKeown. 1997. Predicting the Plutchik, R. 1980. A general psychoevolutionary theory of\n semantic orientation of adjectives. ACL. emotion. In R. Plutchik and H. Kellerman, eds, Emotion:\n Theory, Research, and Experience, Volume 1, 3‚Äì33. Aca-\nHatzivassiloglou, V. and J. Wiebe. 2000. Effects of adjecdemic Press.\n tive orientation and gradability on sentence subjectivity.\n COLING. Potts, C. 2011. On the negativity of negation. In N. Li and\n D. Lutz, eds, Proceedings of Semantics and Linguistic\nHellrich, J., S. Buechel, and U. Hahn. 2019. Modeling word Theory 20, 636‚Äì659. CLC Publications, Ithaca, NY.\n emotion in historical language: Quantity beats supposed\n stability in seed word selection. 3rd Joint SIGHUM Work- Rashkin, H., E. Bell, Y. Choi, and S. Volkova. 2017. Multishop on Computational Linguistics for Cultural Heritage, lingual connotation frames: A case study on social media\n Social Sciences, Humanities and Literature. for targeted sentiment analysis and forecast. ACL.\n Rashkin, H., S. Singh, and Y. Choi. 2016. Connotation\nHu, M. and B. Liu. 2004. Mining and summarizing customer\n frames: A data-driven investigation. ACL.\n reviews. SIGKDD-04.\n Riloff, E. and J. Shepherd. 1997. A corpus-based approach\nJurafsky, D., V. Chahuneau, B. R. Routledge, and N. A.\n for building semantic lexicons. EMNLP.\n Smith. 2014. Narrative framing of consumer sentiment\n in online restaurant reviews. First Monday, 19(4). Riloff, E. and J. Wiebe. 2003. Learning extraction patterns\n for subjective expressions. EMNLP.\nJusteson, J. S. and S. M. Katz. 1991. Co-occurrences of\n antonymous adjectives and their contexts. Computational Rothe, S., S. Ebert, and H. SchuÃàtze. 2016. Ultradense Word\n linguistics, 17(1):1‚Äì19. Embeddings by Orthogonal Transformation. NAACL\n HLT.\nKim, S. M. and E. H. Hovy. 2004. Determining the sentiment\n of opinions. COLING. Russell, J. A. 1980. A circumplex model of affect. Journal\n of personality and social psychology, 39(6):1161‚Äì1178.\nKiritchenko, S. and S. M. Mohammad. 2017. Best-worst\n scaling more reliable than rating scales: A case study on Sap, M., M. C. Prasettio, A. Holtzman, H. Rashkin, and\n sentiment intensity annotation. ACL. Y. Choi. 2017. Connotation frames of power and agency\n in modern films. EMNLP.\nLouviere, J. J., T. N. Flynn, and A. A. J. Marley. 2015. Best-\nScherer, K. R. 2000. Psychological models of emotion. In\n worst scaling: Theory, methods and applications. Cam-\nJ. C. Borod, ed., The neuropsychology of emotion, 137‚Äì\n bridge University Press.\n 162. Oxford.\nMairesse, F. and M. A. Walker. 2008. Trainable generation of\n Schwartz, H. A., J. C. Eichstaedt, M. L. Kern, L. Dziurzynbig-five personality styles through data-driven parameter\n ski, S. M. Ramones, M. Agrawal, A. Shah, M. Kosinestimation. ACL.\n ski, D. Stillwell, M. E. P. Seligman, and L. H. Ungar.\nMiller, G. A. and W. G. Charles. 1991. Contextual corre- 2013. Personality, gender, and age in the language of\n lates of semantics similarity. Language and Cognitive social media: The open-vocabulary approach. PloS one,\n Processes, 6(1):1‚Äì28. 8(9):e73791.\n Exercises 23\n\nStone, P., D. Dunphry, M. Smith, and D. Ogilvie. 1966.\n The General Inquirer: A Computer Approach to Content\n Analysis. MIT Press.\nTomkins, S. S. 1962. Affect, imagery, consciousness: Vol. I.\n The positive affects. Springer.\nTurney, P. D. 2002. Thumbs up or thumbs down? Semantic\n orientation applied to unsupervised classification of reviews. ACL.\nTurney, P. D. and M. Littman. 2003. Measuring praise and\n criticism: Inference of semantic orientation from association. ACM Transactions on Information Systems (TOIS),\n 21:315‚Äì346.\nVelikovich, L., S. Blair-Goldensohn, K. Hannan, and R. Mc-\nDonald. 2010. The viability of web-derived polarity lexicons. NAACL HLT.\nWang, S. and C. D. Manning. 2012. Baselines and bigrams:\n Simple, good sentiment and topic classification. ACL.\nWiebe, J. 1994. Tracking point of view in narrative. Computational Linguistics, 20(2):233‚Äì287.\nWiebe, J. 2000. Learning subjective adjectives from corpora.\n AAAI.\nWiebe, J., R. F. Bruce, and T. P. O‚ÄôHara. 1999. Development and use of a gold-standard data set for subjectivity\n classifications. ACL.\nWilson, T., J. Wiebe, and P. Hoffmann. 2005. Recognizing contextual polarity in phrase-level sentiment analysis.\n EMNLP.\nZhou, D., O. Bousquet, T. N. Lal, J. Weston, and\n B. SchoÃàlkopf. 2004. Learning with local and global consistency. NeurIPS.\nZhu, X. and Z. Ghahramani. 2002. Learning from labeled\n and unlabeled data with label propagation. Technical Report CMU-CALD-02, CMU.\nZhu, X., Z. Ghahramani, and J. Lafferty. 2003. Semisupervised learning using gaussian fields and harmonic\n functions. ICML.\n",
    "file_path": "/Users/colinsidberry/nlp-textbook-rag/data/extracted_text/22.Lexicons for Sentiment, Affect, and Connotation.txt",
    "file_size_kb": 67.83
  },
  {
    "id": "24f234de430d11bd",
    "source": "nlp_textbook",
    "chapter": "Coreference Resolution and 23 Entity Linking",
    "filename": "23.Coreference Resolution and Entity Linking.txt",
    "content": "rights reserved. Draft of August 24, 2025.\n\nCHAPTER\n\n Coreference Resolution and\n23 Entity Linking\n and even Stigand, the patriotic archbishop of Canterbury, found it advisable‚Äì‚Äù‚Äô\n ‚ÄòFound WHAT?‚Äô said the Duck.\n ‚ÄòFound IT,‚Äô the Mouse replied rather crossly: ‚Äòof course you know what ‚Äúit‚Äùmeans.‚Äô\n ‚ÄòI know what ‚Äúit‚Äùmeans well enough, when I find a thing,‚Äô said the Duck: ‚Äòit‚Äôs generally a frog or a worm. The question is, what did the archbishop find?‚Äô\n\n Lewis Carroll, Alice in Wonderland\n\n An important component of language processing is knowing who is being talked\n about in a text. Consider the following passage:\n (23.1) Victoria Chen, CFO of Megabucks Banking, saw her pay jump to $2.3\n million, as the 38-year-old became the company‚Äôs president. It is widely\n known that she came to Megabucks from rival Lotsabucks.\n Each of the underlined phrases in this passage is used by the writer to refer to\n a person named Victoria Chen. We call linguistic expressions like her or Victoria\n mention Chen mentions or referring expressions, and the discourse entity that is referred\n referent to (Victoria Chen) the referent. (To distinguish between referring expressions and\n their referents, we italicize the former.)1 Two or more referring expressions that are\n corefer used to refer to the same discourse entity are said to corefer; thus, Victoria Chen\n and she corefer in (23.1).\n Coreference is an important component of natural language processing. A dialogue system that has just told the user ‚ÄúThere is a 2pm flight on United and a 4pm\n one on Cathay Pacific‚Äù must know which flight the user means by ‚ÄúI‚Äôll take the second one‚Äù. A question answering system that uses Wikipedia to answer a question\n about Marie Curie must know who she was in the sentence ‚ÄúShe was born in Warsaw‚Äù. And a machine translation system translating from a language like Spanish, in\n which pronouns can be dropped, must use coreference from the previous sentence to\n decide whether the Spanish sentence ‚Äò‚ÄúMe encanta el conocimiento‚Äù, dice.‚Äô should\n be translated as ‚Äò‚ÄúI love knowledge‚Äù, he says‚Äô, or ‚Äò‚ÄúI love knowledge‚Äù, she says‚Äô.\n Indeed, this example comes from an actual news article in El Paƒ±ÃÅs about a female\n professor and was mistranslated as ‚Äúhe‚Äù in machine translation because of inaccurate\n coreference resolution (Schiebinger, 2013).\n Natural language processing systems (and humans) interpret linguistic expresdiscourse sions with respect to a discourse model (Karttunen, 1969). A discourse model\n model\n (Fig. 23.1) is a mental model that the understander builds incrementally when interpreting a text, containing representations of the entities referred to in the text,\n as well as properties of the entities and relations among them. When a referent is\n evoked first mentioned in a discourse, we say that a representation for it is evoked into the\n accessed model. Upon subsequent mention, this representation is accessed from the model.\n 1 As a convenient shorthand, we sometimes speak of a referring expression referring to a referent, e.g.,\n saying that she refers to Victoria Chen. However, the reader should keep in mind that what we really\n mean is that the speaker is performing the act of referring to Victoria Chen by uttering she.\n2 C HAPTER 23 ‚Ä¢ C OREFERENCE R ESOLUTION AND E NTITY L INKING\n\n Discourse Model\n\n Lotsabucks\n V\n Megabucks\n $ pay refer (access)\n refer (evoke)\n ‚ÄúVictoria‚Äù corefer ‚Äúshe‚Äù\n\n Reference in a text to an entity that has been previously introduced into the\n anaphora discourse is called anaphora, and the referring expression used is said to be an\n anaphor anaphor, or anaphoric.2 In passage (23.1), the pronouns she and her and the definite NP the 38-year-old are therefore anaphoric. The anaphor corefers with a prior\n antecedent mention (in this case Victoria Chen) that is called the antecedent. Not every referring expression is an antecedent. An entity that has only a single mention in a text\n singleton (like Lotsabucks in (23.1)) is called a singleton.\n coreference In this chapter we focus on the task of coreference resolution. Coreference\n resolution\n resolution is the task of determining whether two mentions corefer, by which we\n mean they refer to the same entity in the discourse model (the same discourse entity).\n coreference The set of coreferring expressions is often called a coreference chain or a cluster.\n chain\n cluster For example, in processing (23.1), a coreference resolution algorithm would need\n to find at least four coreference chains, corresponding to the four entities in the\n discourse model in Fig. 23.1.\n 1. {Victoria Chen, her, the 38-year-old, She}\n 2. {Megabucks Banking, the company, Megabucks}\n 3. {her pay}\n 4. {Lotsabucks}\n Note that mentions can be nested; for example the mention her is syntactically\n part of another mention, her pay, referring to a completely different discourse entity.\n Coreference resolution thus comprises two tasks (although they are often performed jointly): (1) identifying the mentions, and (2) clustering them into coreference chains/discourse entities.\n We said that two mentions corefered if they are associated with the same discourse entity. But often we‚Äôd like to go further, deciding which real world entity is\n associated with this discourse entity. For example, the mention Washington might\n refer to the US state, or the capital city, or the person George Washington; the interpretation of the sentence will of course be very different for each of these. The task\n entity linking of entity linking (Ji and Grishman, 2011) or entity resolution is the task of mapping\n a discourse entity to some real-world individual.3 We usually operationalize entity\n 2 We will follow the common NLP usage of anaphor to mean any mention that has an antecedent, rather\n than the more narrow usage to mean only mentions (like pronouns) whose interpretation depends on the\n antecedent (under the narrower interpretation, repeated names are not anaphors).\n 3 Computational linguistics/NLP thus differs in its use of the term reference from the field of formal\n semantics, which uses the words reference and coreference to describe the relation between a mention\n and a real-world entity. By contrast, we follow the functional linguistics tradition in which a mention\n refers to a discourse entity (Webber, 1978) and the relation between a discourse entity and the real world\n individual requires an additional step of linking.\n\n linking or resolution by mapping to an ontology: a list of entities in the world, like\n a gazeteer (Appendix F). Perhaps the most common ontology used for this task is\n Wikipedia; each Wikipedia page acts as the unique id for a particular entity. Thus\n the entity linking task of wikification (Mihalcea and Csomai, 2007) is the task of deciding which Wikipedia page corresponding to an individual is being referred to by\n a mention. But entity linking can be done with any ontology; for example if we have\n an ontology of genes, we can link mentions of genes in text to the disambiguated\n gene name in the ontology.\n In the next sections we introduce the task of coreference resolution in more detail, and survey a variety of architectures for resolution. We also introduce two\n architectures for the task of entity linking.\n Before turning to algorithms, however, we mention some important tasks we\n will only touch on briefly at the end of this chapter. First are the famous Winograd\n Schema problems (so-called because they were first pointed out by Terry Winograd\n in his dissertation). These entity coreference resolution problems are designed to be\n too difficult to be solved by the resolution methods we describe in this chapter, and\n the kind of real-world knowledge they require has made them a kind of challenge\n task for natural language processing. For example, consider the task of determining\n the correct antecedent of the pronoun they in the following example:\n (23.2) The city council denied the demonstrators a permit because\n a. they feared violence.\n b. they advocated violence.\n Determining the correct antecedent for the pronoun they requires understanding\n that the second clause is intended as an explanation of the first clause, and also\n that city councils are perhaps more likely than demonstrators to fear violence and\n that demonstrators might be more likely to advocate violence. Solving Winograd\n Schema problems requires finding way to represent or discover the necessary real\n world knowledge.\n A problem we won‚Äôt discuss in this chapter is the related task of event coreferevent ence, deciding whether two event mentions (such as the buy and the acquisition in\n coreference\n these two sentences from the ECB+ corpus) refer to the same event:\n (23.3) AMD agreed to [buy] Markham, Ontario-based ATI for around $5.4 billion\n in cash and stock, the companies announced Monday.\n (23.4) The [acquisition] would turn AMD into one of the world‚Äôs largest providers\n of graphics chips.\n Event mentions are much harder to detect than entity mentions, since they can be verbal as well as nominal. Once detected, the same mention-pair and mention-ranking\n models used for entities are often applied to events.\ndiscourse deixis An even more complex kind of coreference is discourse deixis (Webber, 1988),\n in which an anaphor refers back to a discourse segment, which can be quite hard to\n delimit or categorize, like the examples in (23.5) adapted from Webber (1991):\n (23.5) According to Soleil, Beau just opened a restaurant\n a. But that turned out to be a lie.\n b. But that was false.\n c. That struck me as a funny way to describe the situation.\n The referent of that is a speech act (see Chapter 25) in (23.5a), a proposition in\n (23.5b), and a manner of description in (23.5c). We don‚Äôt give algorithms in this\n chapter for these difficult types of non-nominal antecedents, but see Kolhatkar\n et al. (2018) for a survey.\n4 C HAPTER 23 ‚Ä¢ C OREFERENCE R ESOLUTION AND E NTITY L INKING\n\n We now offer some linguistic background on reference phenomena. We introduce\n the four types of referring expressions (definite and indefinite NPs, pronouns, and\n names), describe how these are used to evoke and access entities in the discourse\n model, and talk about linguistic features of the anaphor/antecedent relation (like\n number/gender agreement, or properties of verb semantics).\n\n 23.1.1 Types of Referring Expressions\n Indefinite Noun Phrases: The most common form of indefinite reference in English is marked with the determiner a (or an), but it can also be marked by a quantifier such as some or even the determiner this. Indefinite reference generally introduces into the discourse context entities that are new to the hearer.\n (23.6) a. Mrs. Martin was so very kind as to send Mrs. Goddard a beautiful goose.\n b. He had gone round one day to bring her some walnuts.\n c. I saw this beautiful cauliflower today.\n Definite Noun Phrases: Definite reference, such as via NPs that use the English\n article the, refers to an entity that is identifiable to the hearer. An entity can be\n identifiable to the hearer because it has been mentioned previously in the text and\n thus is already represented in the discourse model:\n (23.7) It concerns a white stallion which I have sold to an officer. But the pedigree\n of the white stallion was not fully established.\n Alternatively, an entity can be identifiable because it is contained in the hearer‚Äôs\n set of beliefs about the world, or the uniqueness of the object is implied by the\n description itself, in which case it evokes a representation of the referent into the\n discourse model, as in (23.9):\n (23.8) I read about it in the New York Times.\n (23.9) Have you seen the car keys?\n These last uses are quite common; more than half of definite NPs in newswire\n texts are non-anaphoric, often because they are the first time an entity is mentioned\n (Poesio and Vieira 1998, Bean and Riloff 1999).\n Pronouns: Another form of definite reference is pronominalization, used for entities that are extremely salient in the discourse, (as we discuss below):\n (23.10) Emma smiled and chatted as cheerfully as she could,\n cataphora Pronouns can also participate in cataphora, in which they are mentioned before\n their referents are, as in (23.11).\n (23.11) Even before she saw it, Dorothy had been thinking about the Emerald City\n every day.\n Here, the pronouns she and it both occur before their referents are introduced.\n Pronouns also appear in quantified contexts in which they are considered to be\n bound bound, as in (23.12).\n (23.12) Every dancer brought her left arm forward.\n Under the relevant reading, her does not refer to some woman in context, but instead\n behaves like a variable bound to the quantified expression every dancer. We are not\n concerned with the bound interpretation of pronouns in this chapter.\n 23.1 ‚Ä¢ C OREFERENCE P HENOMENA : L INGUISTIC BACKGROUND 5\n\n In some languages, pronouns can appear as clitics attached to a word, like lo\n (‚Äòit‚Äô) in this Spanish example from AnCora (Recasens and Martƒ±ÃÅ, 2010):\n (23.13) La intencioÃÅn es reconocer el gran prestigio que tiene la maratoÃÅn y unirlo\n con esta gran carrera.\n ‚ÄòThe aim is to recognize the great prestige that the Marathon has and join|it\n with this great race.‚Äù\n Demonstrative Pronouns: Demonstrative pronouns this and that can appear either alone or as determiners, for instance, this ingredient, that spice:\n (23.14) I just bought a copy of Thoreau‚Äôs Walden. I had bought one five years ago.\n That one had been very tattered; this one was in much better condition.\n Note that this NP is ambiguous; in colloquial spoken English, it can be indefinite,\n as in (23.6), or definite, as in (23.14).\n Zero Anaphora: Instead of using a pronoun, in some languages (including Chinese, Japanese, and Italian) it is possible to have an anaphor that has no lexical\nzero anaphor realization at all, called a zero anaphor or zero pronoun, as in the following Italian\n and Japanese examples from Poesio et al. (2016):\n (23.15) EN [John]i went to visit some friends. On the way [he]i bought some\n wine.\n IT [Giovanni]i andoÃÄ a far visita a degli amici. Per via œÜi comproÃÄ del vino.\n JA [John]i -wa yujin-o houmon-sita. Tochu-de œÜi wain-o ka-tta.\n or this Chinese example:\n (23.16) [Êàë] Ââç‰∏Ä‰ºöÁ≤æÁ•û‰∏äÂ§™Á¥ßÂº†„ÄÇ[0] Áé∞Âú®ÊØîËæÉÂπ≥Èùô‰∫Ü\n [I] was too nervous a while ago. ... [0] am now calmer.\n Zero anaphors complicate the task of mention detection in these languages.\n Names: Names (such as of people, locations, or organizations) can be used to refer\n to both new and old entities in the discourse:\n (23.17) a. Miss Woodhouse certainly had not done him justice.\n b. International Business Machines sought patent compensation\n from Amazon; IBM had previously sued other companies.\n\n 23.1.2 Information Status\n The way referring expressions are used to evoke new referents into the discourse\n (introducing new information), or access old entities from the model (old informainformation tion), is called their information status or information structure. Entities can be\n status\ndiscourse-new discourse-new or discourse-old, and indeed it is common to distinguish at least\ndiscourse-old three kinds of entities informationally (Prince, 1981):\n new NPs:\n brand new NPs: these introduce entities that are discourse-new and hearernew like a fruit or some walnuts.\n unused NPs: these introduce entities that are discourse-new but hearer-old\n (like Hong Kong, Marie Curie, or the New York Times.\n old NPs: also called evoked NPs, these introduce entities that already in the discourse model, hence are both discourse-old and hearer-old, like it in ‚ÄúI went\n to a new restaurant. It was...‚Äù.\n6 C HAPTER 23 ‚Ä¢ C OREFERENCE R ESOLUTION AND E NTITY L INKING\n\n inferrables: these introduce entities that are neither hearer-old nor discourse-old,\n but the hearer can infer their existence by reasoning based on other entities\n that are in the discourse. Consider the following examples:\n (23.18) I went to a superb restaurant yesterday. The chef had just opened it.\n (23.19) Mix flour, butter and water. Knead the dough until shiny.\n Neither the chef nor the dough were in the discourse model based on the first\n bridging sentence of either example, but the reader can make a bridging inference\n inference\n that these entities should be added to the discourse model and associated with\n the restaurant and the ingredients, based on world knowledge that restaurants\n have chefs and dough is the result of mixing flour and liquid (Haviland and\n Clark 1974, Webber and Baldwin 1992, Nissim et al. 2004, Hou et al. 2018).\n The form of an NP gives strong clues to its information status. We often talk\n given-new about an entity‚Äôs position on the given-new dimension, the extent to which the referent is given (salient in the discourse, easier for the hearer to call to mind, predictable\n by the hearer), versus new (non-salient in the discourse, unpredictable) (Chafe 1976,\n accessible Prince 1981, Gundel et al. 1993). A referent that is very accessible (Ariel, 2001)\n i.e., very salient in the hearer‚Äôs mind or easy to call to mind, can be referred to with\n less linguistic material. For example pronouns are used only when the referent has\n salience a high degree of activation or salience in the discourse model.4 By contrast, less\n salient entities, like a new referent being introduced to the discourse, will need to be\n introduced with a longer and more explicit referring expression to help the hearer\n recover the referent.\n Thus when an entity is first introduced into a discourse its mentions are likely\n to have full names, titles or roles, or appositive or restrictive relative clauses, as in\n the introduction of our protagonist in (23.1): Victoria Chen, CFO of Megabucks\n Banking. As an entity is discussed over a discourse, it becomes more salient to the\n hearer and its mentions on average typically becomes shorter and less informative,\n for example with a shortened name (for example Ms. Chen), a definite description\n (the 38-year-old), or a pronoun (she or her) (Hawkins 1978). However, this change\n in length is not monotonic, and is sensitive to discourse structure (Grosz 1977, Reichman 1985, Fox 1993).\n\n 23.1.3 Complications: Non-Referring Expressions\n Many noun phrases or other nominals are not referring expressions, although they\n may bear a confusing superficial resemblance. For example in some of the earliest\n computational work on reference resolution, Karttunen (1969) pointed out that the\n NP a car in the following example does not create a discourse referent:\n (23.20) Janet doesn‚Äôt have a car.\n and cannot be referred back to by anaphoric it or the car:\n (23.21) *It is a Toyota.\n (23.22) *The car is red.\n We summarize here four common types of structures that are not counted as mentions in coreference tasks and hence complicate the task of mention-detection:\n 4 Pronouns also usually (but not always) refer to entities that were introduced no further than one or two\n sentences back in the ongoing discourse, whereas definite noun phrases can often refer further back.\n 23.1 ‚Ä¢ C OREFERENCE P HENOMENA : L INGUISTIC BACKGROUND 7\n\n Appositives: An appositional structure is a noun phrase that appears next to a\n head noun phrase, describing the head. In English they often appear in commas, like\n ‚Äúa unit of UAL‚Äù appearing in apposition to the NP United, or CFO of Megabucks\n Banking in apposition to Victoria Chen.\n (23.23) Victoria Chen, CFO of Megabucks Banking, saw ...\n (23.24) United, a unit of UAL, matched the fares.\n Appositional NPs are not referring expressions, instead functioning as a kind of\n supplementary parenthetical description of the head NP. Nonetheless, sometimes it\n is useful to link these phrases to an entity they describe, and so some datasets like\n OntoNotes mark appositional relationships.\n Predicative and Prenominal NPs: Predicative or attributive NPs describe properties of the head noun. In United is a unit of UAL, the NP a unit of UAL describes\n a property of United, rather than referring to a distinct entity. Thus they are not\n marked as mentions in coreference tasks; in our example the NPs $2.3 million and\n the company‚Äôs president, are attributive, describing properties of her pay and the\n 38-year-old; Example (23.27) shows a Chinese example in which the predicate NP\n (‰∏≠ÂõΩÊúÄÂ§ßÁöÑÂüéÂ∏Ç; China‚Äôs biggest city) is not a mention.\n (23.25) her pay jumped to $2.3 million\n (23.26) the 38-year-old became the company‚Äôs president\n (23.27) ‰∏äÊµ∑ÊòØ[‰∏≠ÂõΩÊúÄÂ§ßÁöÑÂüéÂ∏Ç] [Shanghai is China‚Äôs biggest city]\n Expletives: Many uses of pronouns like it in English and corresponding pronouns\nexpletive in other languages are not referential. Such expletive or pleonastic cases include\n clefts it is raining, in idioms like hit it off, or in particular syntactic situations like clefts\n (23.28a) or extraposition (23.28b):\n (23.28) a. It was Emma Goldman who founded Mother Earth\n b. It surprised me that there was a herring hanging on her wall.\n Generics: Another kind of expression that does not refer back to an entity explicitly evoked in the text is generic reference. Consider (23.29).\n (23.29) I love mangos. They are very tasty.\n Here, they refers, not to a particular mango or set of mangos, but instead to the class\n of mangos in general. The pronoun you can also be used generically:\n (23.30) In July in San Francisco you have to wear a jacket.\n\n 23.1.4 Linguistic Properties of the Coreference Relation\n Now that we have seen the linguistic properties of individual referring expressions\n we turn to properties of the antecedent/anaphor pair. Understanding these properties\n is helpful both in designing novel features and performing error analyses.\n Number Agreement: Referring expressions and their referents must generally\n agree in number; English she/her/he/him/his/it are singular, we/us/they/them are plural, and you is unspecified for number. So a plural antecedent like the chefs cannot\n generally corefer with a singular anaphor like she. However, algorithms cannot\n enforce number agreement too strictly. First, semantically plural entities can be referred to by either it or they:\n (23.31) IBM announced a new machine translation product yesterday. They have\n been working on it for 20 years.\n8 C HAPTER 23 ‚Ä¢ C OREFERENCE R ESOLUTION AND E NTITY L INKING\n\n singular they Second, singular they has become much more common, in which they is used to\n describe singular individuals, often useful because they is gender neutral. Although\n recently increasing, singular they is quite old, part of English for many centuries.5\n\n Person Agreement: English distinguishes between first, second, and third person,\n and a pronoun‚Äôs antecedent must agree with the pronoun in person. Thus a third\n person pronoun (he, she, they, him, her, them, his, her, their) must have a third person\n antecedent (one of the above or any other noun phrase). However, phenomena like\n quotation can cause exceptions; in this example I, my, and she are coreferent:\n (23.32) ‚ÄúI voted for Nader because he was most aligned with my values,‚Äù she said.\n\n Gender or Noun Class Agreement: In many languages, all nouns have grammatical gender or noun class6 and pronouns generally agree with the grammatical gender\n of their antecedent. In English this occurs only with third-person singular pronouns,\n which distinguish between male (he, him, his), female (she, her), and nonpersonal\n (it) grammatical genders. Non-binary pronouns like ze or hir may also occur in more\n recent texts. Knowing which gender to associate with a name in text can be complex,\n and may require world knowledge about the individual. Some examples:\n (23.33) Maryam has a theorem. She is exciting. (she=Maryam, not the theorem)\n (23.34) Maryam has a theorem. It is exciting. (it=the theorem, not Maryam)\n\n Binding Theory Constraints: The binding theory is a name for syntactic constraints on the relations between a mention and an antecedent in the same sentence\n reflexive (Chomsky, 1981). Oversimplifying a bit, reflexive pronouns like himself and herself corefer with the subject of the most immediate clause that contains them (23.35),\n whereas nonreflexives cannot corefer with this subject (23.36).\n (23.35) Janet bought herself a bottle of fish sauce. [herself=Janet]\n (23.36) Janet bought her a bottle of fish sauce. [her6=Janet]\n\n Recency: Entities introduced in recent utterances tend to be more salient than\n those introduced from utterances further back. Thus, in (23.37), the pronoun it is\n more likely to refer to Jim‚Äôs map than the doctor‚Äôs map.\n (23.37) The doctor found an old map in the captain‚Äôs chest. Jim found an even\n older map hidden on the shelf. It described an island.\n\n Grammatical Role: Entities mentioned in subject position are more salient than\n those in object position, which are in turn more salient than those mentioned in\n oblique positions. Thus although the first sentence in (23.38) and (23.39) expresses\n roughly the same propositional content, the preferred referent for the pronoun he\n varies with the subject‚ÄîBilly Bones in (23.38) and Jim Hawkins in (23.39).\n (23.38) Billy Bones went to the bar with Jim Hawkins. He called for a glass of\n rum. [ he = Billy ]\n (23.39) Jim Hawkins went to the bar with Billy Bones. He called for a glass of\n rum. [ he = Jim ]\n 5 Here‚Äôs a bound pronoun example from Shakespeare‚Äôs Comedy of Errors: There‚Äôs not a man I meet but\n doth salute me As if I were their well-acquainted friend\n 6 The word ‚Äúgender‚Äù is generally only used for languages with 2 or 3 noun classes, like most Indo-\nEuropean languages; many languages, like the Bantu languages or Chinese, have a much larger number\n of noun classes.\n 23.2 ‚Ä¢ C OREFERENCE TASKS AND DATASETS 9\n\n Verb Semantics: Some verbs semantically emphasize one of their arguments, biasing the interpretation of subsequent pronouns. Compare (23.40) and (23.41).\n (23.40) John telephoned Bill. He lost the laptop.\n (23.41) John criticized Bill. He lost the laptop.\n These examples differ only in the verb used in the first sentence, yet ‚Äúhe‚Äù in (23.40)\n is typically resolved to John, whereas ‚Äúhe‚Äù in (23.41) is resolved to Bill. This may\n be partly due to the link between implicit causality and saliency: the implicit cause\n of a ‚Äúcriticizing‚Äù event is its object, whereas the implicit cause of a ‚Äútelephoning‚Äù\n event is its subject. In such verbs, the entity which is the implicit cause may be more\n salient.\n Selectional Restrictions: Many other kinds of semantic knowledge can play a role\n in referent preference. For example, the selectional restrictions that a verb places on\n its arguments (Chapter 21) can help eliminate referents, as in (23.42).\n (23.42) I ate the soup in my new bowl after cooking it for hours\n There are two possible referents for it, the soup and the bowl. The verb eat, however,\n requires that its direct object denote something edible, and this constraint can rule\n out bowl as a possible referent.\n\n We can formulate the task of coreference resolution as follows: Given a text T , find\n all entities and the coreference links between them. We evaluate our task by comparing the links our system creates with those in human-created gold coreference\n annotations on T .\n Let‚Äôs return to our coreference example, now using superscript numbers for each\n coreference chain (cluster), and subscript letters for individual mentions in the cluster:\n (23.43) [Victoria Chen]1a , CFO of [Megabucks Banking]2a , saw [[her]1b pay]3a jump\n to $2.3 million, as [the 38-year-old]1c also became [[the company]2b ‚Äôs\n president. It is widely known that [she]1d came to [Megabucks]2c from rival\n [Lotsabucks]4a .\n Assuming example (23.43) was the entirety of the article, the chains for her pay and\n Lotsabucks are singleton mentions:\n 1. {Victoria Chen, her, the 38-year-old, She}\n 2. {Megabucks Banking, the company, Megabucks}\n 3. { her pay}\n 4. { Lotsabucks}\n For most coreference evaluation campaigns, the input to the system is the raw\n text of articles, and systems must detect mentions and then link them into clusters.\n Solving this task requires dealing with pronominal anaphora (figuring out that her\n refers to Victoria Chen), filtering out non-referential pronouns like the pleonastic It\n in It has been ten years), dealing with definite noun phrases to figure out that the\n 38-year-old is coreferent with Victoria Chen, and that the company is the same as\n Megabucks. And we need to deal with names, to realize that Megabucks is the same\n as Megabucks Banking.\n10 C HAPTER 23 ‚Ä¢ C OREFERENCE R ESOLUTION AND E NTITY L INKING\n\n Exactly what counts as a mention and what links are annotated differs from task\n to task and dataset to dataset. For example some coreference datasets do not label\n singletons, making the task much simpler. Resolvers can achieve much higher scores\n on corpora without singletons, since singletons constitute the majority of mentions in\n running text, and they are often hard to distinguish from non-referential NPs. Some\n tasks use gold mention-detection (i.e. the system is given human-labeled mention\n boundaries and the task is just to cluster these gold mentions), which eliminates the\n need to detect and segment mentions from running text.\n Coreference is usually evaluated by the CoNLL F1 score, which combines three\n metrics: MUC, B3 , and CEAFe ; Section 23.8 gives the details.\n Let‚Äôs mention a few characteristics of one popular coreference dataset, OntoNotes\n (Pradhan et al. 2007b, Pradhan et al. 2007a), and the CoNLL 2012 Shared Task\n based on it (Pradhan et al., 2012a). OntoNotes contains hand-annotated Chinese\n and English coreference datasets of roughly one million words each, consisting of\n newswire, magazine articles, broadcast news, broadcast conversations, web data and\n conversational speech data, as well as about 300,000 words of annotated Arabic\n newswire. The most important distinguishing characteristic of OntoNotes is that\n it does not label singletons, simplifying the coreference task, since singletons represent 60%-70% of all entities. In other ways, it is similar to other coreference\n datasets. Referring expression NPs that are coreferent are marked as mentions, but\n generics and pleonastic pronouns are not marked. Appositive clauses are not marked\n as separate mentions, but they are included in the mention. Thus in the NP, ‚ÄúRichard\n Godown, president of the Industrial Biotechnology Association‚Äù the mention is the\n entire phrase. Prenominal modifiers are annotated as separate entities only if they\n are proper nouns. Thus wheat is not an entity in wheat fields, but UN is an entity in\n UN policy (but not adjectives like American in American policy).\n A number of corpora mark richer discourse phenomena. The ISNotes corpus\n annotates a portion of OntoNotes for information status, include bridging examples\n (Hou et al., 2018). The LitBank coreference corpus (Bamman et al., 2020) contains\n coreference annotations for 210,532 tokens from 100 different literary novels, including singletons and quantified and negated noun phrases. The AnCora-CO coreference corpus (Recasens and Martƒ±ÃÅ, 2010) contains 400,000 words each of Spanish\n (AnCora-CO-Es) and Catalan (AnCora-CO-Ca) news data, and includes labels for\n complex phenomena like discourse deixis in both languages. The ARRAU corpus\n (Uryupina et al., 2020) contains 350,000 words of English marking all NPs, which\n means singleton clusters are available. ARRAU includes diverse genres like dialog\n (the TRAINS data) and fiction (the Pear Stories), and has labels for bridging references, discourse deixis, generics, and ambiguous anaphoric relations.\n\n mention The first stage of coreference is mention detection: finding the spans of text that\n detection\n constitute each mention. Mention detection algorithms are usually very liberal in\n proposing candidate mentions (i.e., emphasizing recall), and only filtering later. For\n example many systems run parsers and named entity taggers on the text and extract\n every span that is either an NP, a possessive pronoun, or a named entity.\n Doing so from our sample text repeated in (23.44):\n (23.44) Victoria Chen, CFO of Megabucks Banking, saw her pay jump to $2.3\n 23.3 ‚Ä¢ M ENTION D ETECTION 11\n\n million, as the 38-year-old also became the company‚Äôs president. It is\n widely known that she came to Megabucks from rival Lotsabucks.\n might result in the following list of 13 potential mentions:\n Victoria Chen $2.3 million she\n CFO of Megabucks Banking the 38-year-old Megabucks\n Megabucks Banking the company Lotsabucks\n her the company‚Äôs president\n her pay It\n More recent mention detection systems are even more generous; the span-based\n algorithm we will describe in Section 23.6 first extracts literally all n-gram spans\n of words up to N=10. Of course recall from Section 23.1.3 that many NPs‚Äîand\n the overwhelming majority of random n-gram spans‚Äîare not referring expressions.\n Therefore all such mention detection systems need to eventually filter out pleonastic/expletive pronouns like It above, appositives like CFO of Megabucks Banking\n Inc, or predicate nominals like the company‚Äôs president or $2.3 million.\n Some of this filtering can be done by rules. Early rule-based systems designed\n regular expressions to deal with pleonastic it, like the following rules from Lappin\n and Leass (1994) that use dictionaries of cognitive verbs (e.g., believe, know, anticipate) to capture pleonastic it in ‚ÄúIt is thought that ketchup...‚Äù, or modal adjectives\n (e.g., necessary, possible, certain, important), for, e.g., ‚ÄúIt is likely that I...‚Äù. Such\n rules are sometimes used as part of modern systems:\n\n It is Modaladjective that S\n It is Modaladjective (for NP) to VP\n It is Cogv-ed that S\n It seems/appears/means/follows (that) S\n\n Mention-detection rules are sometimes designed specifically for particular evaluation campaigns. For OntoNotes, for example, mentions are not embedded within\n larger mentions, and while numeric quantities are annotated, they are rarely coreferential. Thus for OntoNotes tasks like CoNLL 2012 (Pradhan et al., 2012a), a\n common first pass rule-based mention detection algorithm (Lee et al., 2013) is:\n\n 1. Take all NPs, possessive pronouns, and named entities.\n 2. Remove numeric quantities (100 dollars, 8%), mentions embedded in\n larger mentions, adjectival forms of nations, and stop words (like there).\n 3. Remove pleonastic it based on regular expression patterns.\n\n Rule-based systems, however, are generally insufficient to deal with mentiondetection, and so modern systems incorporate some sort of learned mention detection component, such as a referentiality classifier, an anaphoricity classifier‚Äî\n detecting whether an NP is an anaphor‚Äîor a discourse-new classifier‚Äî detecting\n whether a mention is discourse-new and a potential antecedent for a future anaphor.\nanaphoricity An anaphoricity detector, for example, can draw its positive training examples\n detector\n from any span that is labeled as an anaphoric referring expression in hand-labeled\n datasets like OntoNotes, ARRAU, or AnCora. Any other NP or named entity can be\n marked as a negative training example. Anaphoricity classifiers use features of the\n candidate mention such as its head word, surrounding words, definiteness, animacy,\n length, position in the sentence/discourse, many of which were first proposed in\n early work by Ng and Cardie (2002a); see Section 23.5 for more on features.\n12 C HAPTER 23 ‚Ä¢ C OREFERENCE R ESOLUTION AND E NTITY L INKING\n\n Referentiality or anaphoricity detectors can be run as filters, in which only mentions that are classified as anaphoric or referential are passed on to the coreference\n system. The end result of such a filtering mention detection system on our example\n above might be the following filtered set of 9 potential mentions:\n Victoria Chen her pay she\n Megabucks Bank the 38-year-old Megabucks\n her the company Lotsabucks\n It turns out, however, that hard filtering of mentions based on an anaphoricity\n or referentiality classifier leads to poor performance. If the anaphoricity classifier\n threshold is set too high, too many mentions are filtered out and recall suffers. If the\n classifier threshold is set too low, too many pleonastic or non-referential mentions\n are included and precision suffers.\n The modern approach is instead to perform mention detection, anaphoricity, and\n coreference jointly in a single end-to-end model (Ng 2005b, Denis and Baldridge\n 2007, Rahman and Ng 2009). For example mention detection in the Lee et al.\n (2017b),2018 system is based on a single end-to-end neural network that computes\n a score for each mention being referential, a score for two mentions being coreference, and combines them to make a decision, training all these scores with a single\n end-to-end loss. We‚Äôll describe this method in detail in Section 23.6. 7\n Despite these advances, correctly detecting referential mentions seems to still be\n an unsolved problem, since systems incorrectly marking pleonastic pronouns like\n it and other non-referential NPs as coreferent is a large source of errors of modern\n coreference resolution systems (Kummerfeld and Klein 2013, Martschat and Strube\n 2014, Martschat and Strube 2015, Wiseman et al. 2015, Lee et al. 2017a).\n Mention, referentiality, or anaphoricity detection is thus an important open area\n of investigation. Other sources of knowledge may turn out to be helpful, especially\n in combination with unsupervised and semisupervised algorithms, which also mitigate the expense of labeled datasets. In early work, for example Bean and Riloff\n (1999) learned patterns for characterizing anaphoric or non-anaphoric NPs; (by extracting and generalizing over the first NPs in a text, which are guaranteed to be\n non-anaphoric). Chang et al. (2012) look for head nouns that appear frequently in\n the training data but never appear as gold mentions to help find non-referential NPs.\n Bergsma et al. (2008) use web counts as a semisupervised way to augment standard\n features for anaphoricity detection for English it, an important task because it is both\n common and ambiguous; between a quarter and half it examples are non-anaphoric.\n Consider the following two examples:\n (23.45) You can make [it] in advance. [anaphoric]\n (23.46) You can make [it] in Hollywood. [non-anaphoric]\n The it in make it is non-anaphoric, part of the idiom make it. Bergsma et al. (2008)\n turn the context around each example into patterns, like ‚Äúmake * in advance‚Äù from\n (23.45), and ‚Äúmake * in Hollywood‚Äù from (23.46). They then use Google n-grams to\n enumerate all the words that can replace it in the patterns. Non-anaphoric contexts\n tend to only have it in the wildcard positions, while anaphoric contexts occur with\n many other NPs (for example make them in advance is just as frequent in their data\n 7 Some systems try to avoid mention detection or anaphoricity detection altogether. For datasets like\n OntoNotes which don‚Äôt label singletons, an alternative to filtering out non-referential mentions is to run\n coreference resolution, and then simply delete any candidate mentions which were not corefered with\n another mention. This likely doesn‚Äôt work as well as explicitly modeling referentiality, and cannot solve\n the problem of detecting singletons, which is important for tasks like entity linking.\n 23.4 ‚Ä¢ A RCHITECTURES FOR C OREFERENCE A LGORITHMS 13\n\n as make it in advance, but make them in Hollywood did not occur at all). These\n n-gram contexts can be used as features in a supervised anaphoricity classifier.\n\n Modern systems for coreference are based on supervised neural machine learning,\n supervised from hand-labeled datasets like OntoNotes. In this section we overview\n the various architecture of modern systems, using the categorization of Ng (2010),\n which distinguishes algorithms based on whether they make each coreference decision in a way that is entity-based‚Äîrepresenting each entity in the discourse model‚Äî\n or only mention-based‚Äîconsidering each mention independently, and whether they\n use ranking models to directly compare potential antecedents. Afterwards, we go\n into more detail on one state-of-the-art algorithm in Section 23.6.\n\n 23.4.1 The Mention-Pair Architecture\n mention-pair We begin with the mention-pair architecture, the simplest and most influential\n coreference architecture, which introduces many of the features of more complex\n mention-pair algorithms, even though other architectures perform better. The mention-pair architecture is based around a classifier that‚Äî as its name suggests‚Äîis given a pair\n of mentions, a candidate anaphor and a candidate antecedent, and makes a binary\n classification decision: coreferring or not.\n Let‚Äôs consider the task of this classifier for the pronoun she in our example, and\n assume the slightly simplified set of potential antecedents in Fig. 23.2.\n\n p(coref|‚ÄùVictoria Chen‚Äù,‚Äùshe‚Äù)\n\n Victoria Chen Megabucks Banking her her pay the 37-year-old she\n\n p(coref|‚ÄùMegabucks Banking‚Äù,‚Äùshe‚Äù)\n\n Victoria Chen or her), the mention-pair classifier assigns a probability of a coreference link.\n\n For each prior mention (Victoria Chen, Megabucks Banking, her, etc.), the binary\n classifier computes a probability: whether or not the mention is the antecedent of\n she. We want this probability to be high for actual antecedents (Victoria Chen, her,\n the 38-year-old) and low for non-antecedents (Megabucks Banking, her pay).\n Early classifiers used hand-built features (Section 23.5); more recent classifiers\n use neural representation learning (Section 23.6)\n For training, we need a heuristic for selecting training samples; since most pairs\n of mentions in a document are not coreferent, selecting every pair would lead to\n a massive overabundance of negative samples. The most common heuristic, from\n (Soon et al., 2001), is to choose the closest antecedent as a positive example, and all\n pairs in between as the negative examples. More formally, for each anaphor mention\n mi we create\n ‚Ä¢ one positive instance (mi , m j ) where m j is the closest antecedent to mi , and\n14 C HAPTER 23 ‚Ä¢ C OREFERENCE R ESOLUTION AND E NTITY L INKING\n\n ‚Ä¢ a negative instance (mi , mk ) for each mk between m j and mi\n Thus for the anaphor she, we would choose (she, her) as the positive example\n and no negative examples. Similarly, for the anaphor the company we would choose\n (the company, Megabucks) as the positive example and (the company, she) (the company, the 38-year-old) (the company, her pay) and (the company, her) as negative\n examples.\n Once the classifier is trained, it is applied to each test sentence in a clustering\n step. For each mention i in a document, the classifier considers each of the prior i ‚àí 1\n mentions. In closest-first clustering (Soon et al., 2001), the classifier is run right to\n left (from mention i ‚àí 1 down to mention 1) and the first antecedent with probability\n > .5 is linked to i. If no antecedent has probably > 0.5, no antecedent is selected for\n i. In best-first clustering, the classifier is run on all i ‚àí 1 antecedents and the most\n probable preceding mention is chosen as the antecedent for i. The transitive closure\n of the pairwise relation is taken as the cluster.\n While the mention-pair model has the advantage of simplicity, it has two main\n problems. First, the classifier doesn‚Äôt directly compare candidate antecedents to\n each other, so it‚Äôs not trained to decide, between two likely antecedents, which one\n is in fact better. Second, it ignores the discourse model, looking only at mentions,\n not entities. Each classifier decision is made completely locally to the pair, without\n being able to take into account other mentions of the same entity. The next two\n models each address one of these two flaws.\n\n 23.4.2 The Mention-Rank Architecture\n The mention ranking model directly compares candidate antecedents to each other,\n choosing the highest-scoring antecedent for each anaphor.\n In early formulations, for mention i, the classifier decides which of the {1, ..., i ‚àí\n 1} prior mentions is the antecedent (Denis and Baldridge, 2008). But suppose i is\n in fact not anaphoric, and none of the antecedents should be chosen? Such a model\n would need to run a separate anaphoricity classifier on i. Instead, it turns out to be\n better to jointly learn anaphoricity detection and coreference together with a single\n loss (Rahman and Ng, 2009).\n So in modern mention-ranking systems, for the ith mention (anaphor), we have\n an associated random variable yi ranging over the values Y (i) = {1, ..., i ‚àí 1, \u000f}. The\n value \u000f is a special dummy mention meaning that i does not have an antecedent (i.e.,\n is either discourse-new and starts a new coref chain, or is non-anaphoric).\n\n p(‚ÄùVictoria Chen‚Äù|‚Äùshe‚Äù) p(‚Äùher‚Äù|she‚Äù) p(‚Äùthe 37-year-old‚Äù|she‚Äù)\n\n } One or more\n of these\n should be high\n\n }\n œµ Victoria Chen Megabucks Banking her her pay the 37-year-old she\n All of these\n should be low\n p(œµ|‚Äùshe‚Äù) p(‚ÄùMegabucks Banking‚Äù|she‚Äù) p(‚Äùher pay‚Äù|she‚Äù)\n\n At test time, for a given mention i the model computes one softmax over all the\n antecedents (plus \u000f) giving a probability for each candidate antecedent (or none).\n 23.5 ‚Ä¢ C LASSIFIERS USING HAND - BUILT FEATURES 15\n\n Fig. 23.3 shows an example of the computation for the single candidate anaphor\n she.\n Once the antecedent is classified for each anaphor, transitive closure can be run\n over the pairwise decisions to get a complete clustering.\n Training is trickier in the mention-ranking model than the mention-pair model,\n because for each anaphor we don‚Äôt know which of all the possible gold antecedents\n to use for training. Instead, the best antecedent for each mention is latent; that\n is, for each mention we have a whole cluster of legal gold antecedents to choose\n from. Early work used heuristics to choose an antecedent, for example choosing the\n closest antecedent as the gold antecedent and all non-antecedents in a window of\n two sentences as the negative examples (Denis and Baldridge, 2008). Various kinds\n of ways to model latent antecedents exist (Fernandes et al. 2012, Chang et al. 2013,\n Durrett and Klein 2013). The simplest way is to give credit to any legal antecedent\n by summing over all of them, with a loss function that optimizes the likelihood of\n all correct antecedents from the gold clustering (Lee et al., 2017b). We‚Äôll see the\n details in Section 23.6.\n Mention-ranking models can be implemented with hand-build features or with\n neural representation learning (which might also incorporate some hand-built features). we‚Äôll explore both directions in Section 23.5 and Section 23.6.\n\n 23.4.3 Entity-based Models\n Both the mention-pair and mention-ranking models make their decisions about mentions. By contrast, entity-based models link each mention not to a previous mention\n but to a previous discourse entity (cluster of mentions).\n A mention-ranking model can be turned into an entity-ranking model simply\n by having the classifier make its decisions over clusters of mentions rather than\n individual mentions (Rahman and Ng, 2009).\n For traditional feature-based models, this can be done by extracting features over\n clusters. The size of a cluster is a useful feature, as is its ‚Äòshape‚Äô, which is the\n list of types of the mentions in the cluster i.e., sequences of the tokens (P)roper,\n (D)efinite, (I)ndefinite, (Pr)onoun, so that a cluster composed of {Victoria, her, the\n 38-year-old} would have the shape P-Pr-D (BjoÃàrkelund and Kuhn, 2014). An entitybased model that includes a mention-pair classifier can use as features aggregates of\n mention-pair probabilities, for example computing the average probability of coreference over all mention-pairs in the two clusters (Clark and Manning 2015).\n Neural models can learn representations of clusters automatically, for example\n by using an RNN over the sequence of cluster mentions to encode a state corresponding to a cluster representation (Wiseman et al., 2016), or by learning distributed representations for pairs of clusters by pooling over learned representations of mention\n pairs (Clark and Manning, 2016b).\n However, although entity-based models are more expressive, the use of clusterlevel information in practice has not led to large gains in performance, so mentionranking models are still more commonly used.\n\n Feature-based classifiers, use hand-designed features in logistic regression, SVM,\n or random forest classifiers for coreference resolution. These classifiers don‚Äôt per-\n16 C HAPTER 23 ‚Ä¢ C OREFERENCE R ESOLUTION AND E NTITY L INKING\n\n form as well as neural ones. Nonetheless, they are still sometimes useful to build\n lightweight systems when compute or data are sparse, and the features themselves\n are useful for error analysis even in neural systems.\n Given an anaphor mention and a potential antecedent mention, feature based\n classifiers make use of three types of features: (i) features of the anaphor, (ii) features\n of the candidate antecedent, and (iii) features of the relationship between the pair.\n Entity-based models can make additional use of two additional classes: (iv) feature\n of all mentions from the antecedent‚Äôs entity cluster, and (v) features of the relation\n between the anaphor and the mentions in the antecedent entity cluster.\n\n Features of the Anaphor or Antecedent Mention\n First (last) word Victoria/she First or last word (or embedding) of antecedent/anaphor\n Head word Victoria/she Head word (or head embedding) of antecedent/anaphor\n Attributes Sg-F-A-3-PER/ The number, gender, animacy, person, named entity type\n Sg-F-A-3-PER attributes of (antecedent/anaphor)\n Length 2/1 length in words of (antecedent/anaphor)\n Mention type P/Pr Type: (P)roper, (D)efinite, (I)ndefinite, (Pr)onoun) of antecedent/anaphor\n Features of the Antecedent Entity\n Entity shape P-Pr-D The ‚Äòshape‚Äô or list of types of the mentions in the\n antecedent entity (cluster), i.e., sequences of (P)roper,\n (D)efinite, (I)ndefinite, (Pr)onoun.\n Entity attributes Sg-F-A-3-PER The number, gender, animacy, person, named entity type\n attributes of the antecedent entity\n Ant. cluster size 3 Number of mentions in the antecedent cluster\n Features of the Pair of Mentions\n Sentence distance 1 The number of sentences between antecedent and anaphor\n Mention distance 4 The number of mentions between antecedent and anaphor\n i-within-i F Anaphor has i-within-i relation with antecedent\n Cosine Cosine between antecedent and anaphor embeddings\n Features of the Pair of Entities\n Exact String Match F True if the strings of any two mentions from the antecedent\n and anaphor clusters are identical.\n Head Word Match F True if any mentions from antecedent cluster has same\n headword as any mention in anaphor cluster\n Word Inclusion F All words in anaphor cluster included in antecedent cluster\n‚ÄúVictoria Chen‚Äù.\n\n that would be computed for the potential anaphor ‚Äúshe‚Äù and potential antecedent\n ‚ÄúVictoria Chen‚Äù in our example sentence, repeated below:\n (23.47) Victoria Chen, CFO of Megabucks Banking, saw her pay jump to $2.3\n million, as the 38-year-old also became the company‚Äôs president. It is\n widely known that she came to Megabucks from rival Lotsabucks.\n Features that prior work has found to be particularly useful are exact string\n match, entity headword agreement, mention distance, as well as (for pronouns) exact\n attribute match and i-within-i, and (for nominals and proper names) word inclusion\n and cosine. For lexical features (like head words) it is common to only use words\n that appear enough times (>20 times).\n 23.6 ‚Ä¢ A NEURAL MENTION - RANKING ALGORITHM 17\n\n It is crucial in feature-based systems to use conjunctions of features; one experiment suggested that moving from individual features in a classifier to conjunctions\n of multiple features increased F1 by 4 points (Lee et al., 2017a). Specific conjunctions can be designed by hand (Durrett and Klein, 2013), all pairs of features can be\n conjoined (Bengtson and Roth, 2008), or feature conjunctions can be learned using\n decision tree or random forest classifiers (Ng and Cardie 2002a, Lee et al. 2017a).\n Features can also be used in neural models as well. Neural systems use contextual word embeddings so don‚Äôt benefit from shallow features like string match or or\n mention types. However features like mention length, distance between mentions,\n or genre can complement neural contextual embedding models.\n\n In this section we describe the neural e2e-coref algorithms of Lee et al. (2017b)\n (simplified and extended a bit, drawing on Joshi et al. (2019) and others). This is\n a mention-ranking algorithm that considers all possible spans of text in the document, assigns a mention-score to each span, prunes the mentions based on this score,\n then assigns coreference links to the remaining mentions.\n More formally, given a document D with T words, the model considers all of\n the T (T2+1) text spans in D (unigrams, bigrams, trigrams, 4-grams, etc; in practice\n we only consider spans up a maximum length around 10). The task is to assign\n to each span i an antecedent yi , a random variable ranging over the values Y (i) =\n {1, ..., i ‚àí 1, \u000f}; each previous span and a special dummy token \u000f. Choosing the\n dummy token means that i does not have an antecedent, either because i is discoursenew and starts a new coreference chain, or because i is non-anaphoric.\n For each pair of spans i and j, the system assigns a score s(i, j) for the coreference link between span i and span j. The system then learns a distribution P(yi )\n over the antecedents for span i:\n exp(s(i, yi ))\n P(yi ) = P 0\n (23.48)\n y0 ‚ààY (i) exp(s(i, y ))\n\n This score s(i, j) includes three factors that we‚Äôll define below: m(i); whether span\n i is a mention; m( j); whether span j is a mention; and c(i, j); whether j is the\n antecedent of i:\n\n s(i, j) = m(i) + m( j) + c(i, j) (23.49)\n\n For the dummy antecedent \u000f, the score s(i, \u000f) is fixed to 0. This way if any nondummy scores are positive, the model predicts the highest-scoring antecedent, but if\n all the scores are negative it abstains.\n\n 23.6.1 Computing span representations\n To compute the two functions m(i) and c(i, j) which score a span i or a pair of spans\n (i, j), we‚Äôll need a way to represent a span. The e2e-coref family of algorithms\n represents each span by trying to capture 3 words/tokens: the first word, the last\n word, and the most important word. We first run each paragraph or subdocument\n through an encoder (like BERT) to generate embeddings hi for each token i. The\n span i is then represented by a vector gi that is a concatenation of the encoder output\n18 C HAPTER 23 ‚Ä¢ C OREFERENCE R ESOLUTION AND E NTITY L INKING\n\n embedding for the first (start) token of the span, the encoder output for the last (end)\n token of the span, and a third vector which is an attention-based representation:\n\n gi = [hSTART(i) , hEND(i) , hATT(i) ] (23.50)\n\n The goal of the attention vector is to represent which word/token is the likely\n syntactic head-word of the span; we saw in the prior section that head-words are\n a useful feature; a matching head-word is a good indicator of coreference. The\n attention representation is computed as usual; the system learns a weight vector wŒ± ,\n and computes its dot product with the hidden state ht transformed by a FFN:\n\n Œ±t = wŒ± ¬∑ FFNŒ± (ht ) (23.51)\n\n The attention score is normalized into a distribution via a softmax:\n exp(Œ±t )\n ai,t = PEND(i) (23.52)\n k= START (i)\n exp(Œ±k )\n\n And then the attention distribution is used to create a vector hATT(i) which is an\n attention-weighted sum of the embeddings et of each of the words in span i:\n END\n X(i)\n hATT(i) = ai,t ¬∑ et (23.53)\n t= START (i)\n\n Fig. 23.5 shows the computation of the span representation and the mention\n score.\n\n General Electric Electric said the the Postal Service Service contacted the the company\n Mention score (m)\n\n Span representation (g)\n\n Span head (hATT) + + + + +\n\n Encodings (h)\n\n Encoder\n\n ‚Ä¶ General Electric said the Postal Service contacted the company\n\ne2e-coref model (Lee et al. 2017b, Joshi et al. 2019). The model considers all spans up to a maximum width of\nsay 10; the figure shows a small subset of the bigram and trigram spans.\n\n 23.6.2 Computing the mention and antecedent scores m and c\n Now that we know how to compute the vector gi for representing span i, we can\n see the details of the two scoring functions m(i) and c(i, j). Both are computed by\n feedforward networks:\n\n m(i) = wm ¬∑ FFNm (gi ) (23.54)\n c(i, j) = wc ¬∑ FFNc ([gi , g j , gi ‚ó¶ g j , ]) (23.55)\n\n At inference time, this mention score m is used as a filter to keep only the best few\n mentions.\n 23.6 ‚Ä¢ A NEURAL MENTION - RANKING ALGORITHM 19\n\n We then compute the antecedent score for high-scoring mentions. The antecedent\nscore c(i, j) takes as input a representation of the spans i and j, but also the elementwise similarity of the two spans to each other gi ‚ó¶ g j (here ‚ó¶ is element-wise multiplication). Fig. 23.6 shows the computation of the score s for the three possible\nantecedents of the company in the example sentence from Fig. 23.5.\n\n Given the set of mentions, the joint distribution of antecedents for each document is computed in a forward pass, and we can then do transitive closure on the\nantecedents to create a final clustering for the document.\n Fig. 23.7 shows example predictions from the model, showing the attention\nweights, which Lee et al. (2017b) find correlate with traditional semantic heads.\nNote that the model gets the second example wrong, presumably because attendants\nand pilot likely have nearby word embeddings.\n\nexample, showing one correct example and one mistake. Bold, parenthesized spans are mentions in the predicted cluster. The amount of red color on a word indicates the head-finding\nattention weight ai,t in Eq. 23.52. Figure adapted from Lee et al. (2017b).\n\n23.6.3 Learning\nFor training, we don‚Äôt have a single gold antecedent for each mention; instead the\ncoreference labeling only gives us each entire cluster of coreferent mentions; so a\nmention only has a latent antecedent. We therefore use a loss function that maximizes the sum of the coreference probability of any of the legal antecedents. For a\ngiven mention i with possible antecedents Y (i), let GOLD(i) be the set of mentions\nin the gold cluster containing i. Since the set of mentions occurring before i is Y (i),\nthe set of mentions in that gold cluster that also occur before i is Y (i) ‚à© GOLD(i). We\n20 C HAPTER 23 ‚Ä¢ C OREFERENCE R ESOLUTION AND E NTITY L INKING\n\n therefore want to maximize:\n X\n P(yÃÇ) (23.56)\n yÃÇ‚ààY (i)‚à© GOLD (i)\n\n If a mention i is not in a gold cluster GOLD(i) = \u000f.\n To turn this probability into a loss function, we‚Äôll use the cross-entropy loss\n function we defined in Eq. ?? in Chapter 4, by taking the ‚àí log of the probability. If\n we then sum over all mentions, we get the final loss function for training:\n N\n X X\n L= ‚àí log P(yÃÇ) (23.57)\n i=2 yÃÇ‚ààY (i)‚à© GOLD (i)\n\n entity linking Entity linking is the task of associating a mention in text with the representation of\n some real-world entity in an ontology or knowledge base (Ji and Grishman, 2011). It\n is the natural follow-on to coreference resolution; coreference resolution is the task\n of associating textual mentions that corefer to the same entity. Entity linking takes\n the further step of identifying who that entity is. It is especially important for any\n NLP task that links to a knowledge base.\n While there are all sorts of potential knowledge-bases, we‚Äôll focus in this section\n on Wikipedia, since it‚Äôs widely used as an ontology for NLP tasks. In this usage,\n each unique Wikipedia page acts as the unique id for a particular entity. This task of\n deciding which Wikipedia page corresponding to an individual is being referred to\n wikification by a text mention has its own name: wikification (Mihalcea and Csomai, 2007).\n Since the earliest systems (Mihalcea and Csomai 2007, Cucerzan 2007, Milne\n and Witten 2008), entity linking is done in (roughly) two stages: mention detection and mention disambiguation. We‚Äôll give two algorithms, one simple classic\n baseline that uses anchor dictionaries and information from the Wikipedia graph\n structure (Ferragina and Scaiella, 2011) and one modern neural algorithm (Li et al.,\n 2020). We‚Äôll focus here mainly on the application of entity linking to questions,\n since a lot of the literature has been in that context.\n\n 23.7.1 Linking based on Anchor Dictionaries and Web Graph\n As a simple baseline we introduce the TAGME linker (Ferragina and Scaiella, 2011)\n for Wikipedia, which itself draws on earlier algorithms (Mihalcea and Csomai 2007,\n Cucerzan 2007, Milne and Witten 2008). Wikification algorithms define the set of\n entities as the set of Wikipedia pages, so we‚Äôll refer to each Wikipedia page as a\n unique entity e. TAGME first creates a catalog of all entities (i.e. all Wikipedia\n pages, removing some disambiguation and other meta-pages) and indexes them in a\n standard IR engine like Lucene. For each page e, the algorithm computes an in-link\n count in(e): the total number of in-links from other Wikipedia pages that point to e.\n These counts can be derived from Wikipedia dumps.\n Finally, the algorithm requires an anchor dictionary. An anchor dictionary\n anchor texts lists for each Wikipedia page, its anchor texts: the hyperlinked spans of text on\n other pages that point to it. For example, the web page for Stanford University,\n http://www.stanford.edu, might be pointed to from another page using anchor\n texts like Stanford or Stanford University:\n 23.7 ‚Ä¢ E NTITY L INKING 21\n\n <a href=\"http://www.stanford.edu\">Stanford University</a>\n We compute a Wikipedia anchor dictionary by including, for each Wikipedia\npage e, e‚Äôs title as well as all the anchor texts from all Wikipedia pages that point to e.\nFor each anchor string a we‚Äôll also compute its total frequency freq(a) in Wikipedia\n(including non-anchor uses), the number of times a occurs as a link (which we‚Äôll call\nlink(a)), and its link probability linkprob(a) = link(a)/freq(a). Some cleanup of the\nfinal anchor dictionary is required, for example removing anchor strings composed\nonly of numbers or single characters, that are very rare, or that are very unlikely to\nbe useful entities because they have a very low linkprob.\nMention Detection Given a question (or other text we are trying to link), TAGME\ndetects mentions by querying the anchor dictionary for each token sequence up to\n6 words. This large set of sequences is pruned with some simple heuristics (for\nexample pruning substrings if they have small linkprobs). The question:\n When was Ada Lovelace born?\nmight give rise to the anchor Ada Lovelace and possibly Ada, but substrings spans\nlike Lovelace might be pruned as having too low a linkprob, and but spans like born\nhave such a low linkprob that they would not be in the anchor dictionary at all.\nMention Disambiguation If a mention span is unambiguous (points to only one\nentity/Wikipedia page), we are done with entity linking! However, many spans are\nambiguous, matching anchors for multiple Wikipedia entities/pages. The TAGME\nalgorithm uses two factors for disambiguating ambiguous spans, which have been\nreferred to as prior probability and relatedness/coherence. The first factor is p(e|a),\nthe probability with which the span refers to a particular entity. For each page e ‚àà\nE (a), the probability p(e|a) that anchor a points to e, is the ratio of the number of\nlinks into e with anchor text a to the total number of occurrences of a as an anchor:\n count(a ‚Üí e)\n prior(a ‚Üí e) = p(e|a) = (23.58)\n link(a)\nLet‚Äôs see how that factor works in linking entities in the following question:\n What Chinese Dynasty came before the Yuan?\nThe most common association for the span Yuan in the anchor dictionary is the name\nof the Chinese currency, i.e., the probability p(Yuan currency| yuan) is very high.\nRarer Wikipedia associations for Yuan include the common Chinese last name, a\nlanguage spoken in Thailand, and the correct entity in this case, the name of the\nChinese dynasty. So if we chose based only on p(e|a) , we would make the wrong\ndisambiguation and miss the correct link, Yuan dynasty.\n To help in just this sort of case, TAGME uses a second factor, the relatedness of\nthis entity to other entities in the input question. In our example, the fact that the\nquestion also contains the span Chinese Dynasty, which has a high probability link to\nthe page Dynasties in Chinese history, ought to help match Yuan dynasty.\n Let‚Äôs see how this works. Given a question q, for each candidate anchors span\na detected in q, we assign a relatedness score to each possible entity e ‚àà E (a) of a.\nThe relatedness score of the link a ‚Üí e is the weighted average relatedness between\ne and all other entities in q. Two entities are considered related to the extent their\nWikipedia pages share many in-links. More formally, the relatedness between two\nentities A and B is computed as\n log(max(|in(A)|, |in(B)|)) ‚àí log(|in(A) ‚à© in(B)|)\n rel(A, B) = (23.59)\n log(|W |) ‚àí log(min(|in(A)|, |in(B)|))\n22 C HAPTER 23 ‚Ä¢ C OREFERENCE R ESOLUTION AND E NTITY L INKING\n\n where in(x) is the set of Wikipedia pages pointing to x and W is the set of all Wikipedia pages in the collection.\n The vote given by anchor b to the candidate annotation a ‚Üí X is the average,\n over all the possible entities of b, of their relatedness to X, weighted by their prior\n probability:\n\n 1 X\n vote(b, X) = rel(X,Y )p(Y |b) (23.60)\n |E (b)|\n Y ‚ààE (b)\n\n The total relatedness score for a ‚Üí X is the sum of the votes of all the other anchors\n detected in q:\n X\n relatedness(a ‚Üí X) = vote(b, X) (23.61)\n b‚ààXq \\a\n\n To score a ‚Üí X, we combine relatedness and prior by choosing the entity X\n that has the highest relatedness(a ‚Üí X), finding other entities within a small \u000f of\n this value, and from this set, choosing the entity with the highest prior P(X|a). The\n result of this step is a single entity assigned to each span in q.\n The TAGME algorithm has one further step of pruning spurious anchor/entity\n pairs, assigning a score averaging link probability with the coherence.\n\n 1 X\n coherence(a ‚Üí X) = rel(B, X)\n |S| ‚àí 1\n B‚ààS \\X\n coherence(a ‚Üí X) + linkprob(a)\n score(a ‚Üí X) = (23.62)\n Finally, pairs are pruned if score(a ‚Üí X) < Œª , where the threshold Œª is set on a\n held-out set.\n\n 23.7.2 Neural Graph-based linking\n More recent entity linking models are based on bi-encoders, encoding a candidate\n mention span, encoding an entity, and computing the dot product between the encodings. This allows embeddings for all the entities in the knowledge base to be\n precomputed and cached (Wu et al., 2020). Let‚Äôs sketch the ELQ linking algorithm\n of Li et al. (2020), which is given a question q and a set of candidate entities from\n Wikipedia with associated Wikipedia text, and outputs tuples (e, ms , me ) of entity id,\n mention start, and mention end. As Fig. 23.8 shows, it does this by encoding each\n Wikipedia entity using text from Wikipedia, encoding each mention span using text\n from the question, and computing their similarity, as we describe below.\n Entity Mention Detection To get an h-dimensional embedding for each question\n token, the algorithm runs the question through BERT in the normal way:\n\n [q1 ¬∑ ¬∑ ¬∑ qn ] = BERT([CLS]q1 ¬∑ ¬∑ ¬∑ qn [SEP]) (23.63)\n\n It then computes the likelihood of each span [i, j] in q being an entity mention, in\n a way similar to the span-based algorithm we saw for the reader above. First we\n compute the score for i/ j being the start/end of a mention:\n\n sstart (i) = wstart ¬∑ qi , send ( j) = wend ¬∑ q j , (23.64)\n 23.7 ‚Ä¢ E NTITY L INKING 23\n\nquestions (Li et al., 2020). Each candidate question mention span and candidate entity are\nseparately encoded, and then scored by the entity/span dot product.\n\nwhere wstart and wend are vectors learned during training. Next, another trainable\nembedding, wmention is used to compute a score for each token being part of a mention:\n\n smention (t) = wmention ¬∑ qt (23.65)\n\nMention probabilities are then computed by combining these three scores:\n j\n !\n X\n p([i, j]) = œÉ sstart (i) + send ( j) + smention (t) (23.66)\n t=i\n\nEntity Linking To link mentions to entities, we next compute embeddings for\neach entity in the set E = e1 , ¬∑ ¬∑ ¬∑ , ei , ¬∑ ¬∑ ¬∑ , ew of all Wikipedia entities. For each entity ei we‚Äôll get text from the entity‚Äôs Wikipedia page, the title t(ei ) and the first\n128 tokens of the Wikipedia page which we‚Äôll call the description d(ei ). This is\nagain run through BERT, taking the output of the CLS token BERT[CLS] as the entity\nrepresentation:\n\n xei = BERT[CLS] ([CLS]t(ei )[ENT]d(ei )[SEP]) (23.67)\n\nMention spans can be linked to entities by computing, for each entity e and span\n[i, j], the dot product similarity between the span encoding (the average of the token\nembeddings) and the entity encoding.\n j\n 1 X\n yi, j = qt\n ( j ‚àí i + 1)\n t=i\n s(e, [i, j]) = x¬∑e yi, j (23.68)\n\nFinally, we take a softmax to get a distribution over entities for each span:\n\n exp(s(e, [i, j]))\n p(e|[i, j]) = P 0\n (23.69)\n e0 ‚ààE exp(s(e , [i, j]))\n\nTraining The ELQ mention detection and entity linking algorithm is fully supervised. This means, unlike the anchor dictionary algorithms from Section 23.7.1,\n24 C HAPTER 23 ‚Ä¢ C OREFERENCE R ESOLUTION AND E NTITY L INKING\n\n it requires datasets with entity boundaries marked and linked. Two such labeled\n datasets are WebQuestionsSP (Yih et al., 2016), an extension of the WebQuestions\n (Berant et al., 2013) dataset derived from Google search questions, and GraphQuestions (Su et al., 2016). Both have had entity spans in the questions marked and\n linked (Sorokin and Gurevych 2018, Li et al. 2020) resulting in entity-labeled versions WebQSPEL and GraphQEL (Li et al., 2020).\n Given a training set, the ELQ mention detection and entity linking phases are\n trained jointly, optimizing the sum of their losses. The mention detection loss is\n a binary cross-entropy loss, with L the length of the passage and N the number of\n candidates:\n 1 X\n LMD = ‚àí\n \u0001\n y[i, j] log p([i, j]) + (1 ‚àí y[i, j] ) log(1 ‚àí p([i, j])) (23.70)\n N\n 1‚â§i‚â§ j‚â§min(i+L‚àí1,n)\n\n with y[i, j] = 1 if [i, j] is a gold mention span, else 0. The entity linking loss is:\n LED = ‚àílogp(eg |[i, j]) (23.71)\n where eg is the gold entity for mention [i, j].\n\n We evaluate coreference algorithms model-theoretically, comparing a set of hypothesis chains or clusters H produced by the system against a set of gold or reference\n chains or clusters R from a human labeling, and reporting precision and recall.\n However, there are a wide variety of methods for doing this comparison. In fact,\n there are 5 common metrics used to evaluate coreference algorithms: the link based\n MUC (Vilain et al., 1995) and BLANC (Recasens and Hovy 2011, Luo et al. 2014)\n metrics, the mention based B3 metric (Bagga and Baldwin, 1998), the entity based\n CEAF metric (Luo, 2005), and the link based entity aware LEA metric (Moosavi and\n Strube, 2016).\n MUC Let‚Äôs just explore two of the metrics. The MUC F-measure (Vilain et al., 1995)\n F-measure\n is based on the number of coreference links (pairs of mentions) common to H and\n R. Precision is the number of common links divided by the number of links in H.\n Recall is the number of common links divided by the number of links in R; This\n makes MUC biased toward systems that produce large chains (and fewer entities),\n and it ignores singletons, since they don‚Äôt involve links.\n B\n 3 B3 is mention-based rather than link-based. For each mention in the reference\n chain, we compute a precision and recall, and then we take a weighted sum over all\n N mentions in the document to compute a precision and recall for the entire task. For\n a given mention i, let R be the reference chain that includes i, and H the hypothesis\n chain that has i. The set of correct mentions in H is H ‚à© R. Precision for mention i\n is thus |H‚à©R| |H‚à©R|\n |H| , and recall for mention i thus |R| . The total precision is the weighted\n sum of the precision for mention i, weighted by a weight wi . The total recall is the\n weighted sum of the recall for mention i, weighted by a weight wi . Equivalently:\n N\n X # of correct mentions in hypothesis chain containing entityi\n Precision = wi\n # of mentions in hypothesis chain containing entityi\n i=1\n N\n X # of correct mentions in hypothesis chain containing entityi\n Recall = wi\n # of mentions in reference chain containing entityi\n i=1\n 23.9 ‚Ä¢ W INOGRAD S CHEMA PROBLEMS 25\n\n The weight wi for each entity can be set to different values to produce different\n versions of the algorithm.\n Following a proposal from Denis and Baldridge (2009), the CoNLL coreference\n competitions were scored based on the average of MUC, CEAF-e, and B3 (Pradhan\n et al. 2011, Pradhan et al. 2012b), and so it is common in many evaluation campaigns\n to report an average of these 3 metrics. See Luo and Pradhan (2016) for a detailed\n description of the entire set of metrics; reference implementations of these should\n be used rather than attempting to reimplement from scratch (Pradhan et al., 2014).\n Alternative metrics have been proposed that deal with particular coreference domains or tasks. For example, consider the task of resolving mentions to named\n entities (persons, organizations, geopolitical entities), which might be useful for information extraction or knowledge base completion. A hypothesis chain that correctly contains all the pronouns referring to an entity, but has no version of the name\n itself, or is linked with a wrong name, is not useful for this task. We might instead\n want a metric that weights each mention by how informative it is (with names being\n most informative) (Chen and Ng, 2013) or a metric that considers a hypothesis to\n match a gold chain only if it contains at least one variant of a name (the NEC F1\n metric of Agarwal et al. (2019)).\n\n From early on in the field, researchers have noted that some cases of coreference\n are quite difficult, seeming to require world knowledge or sophisticated reasoning\n to solve. The problem was most famously pointed out by Winograd (1972) with the\n following example:\n (23.72) The city council denied the demonstrators a permit because\n a. they feared violence.\n b. they advocated violence.\n Winograd noticed that the antecedent that most readers preferred for the pronoun they in continuation (a) was the city council, but in (b) was the demonstrators.\n He suggested that this requires understanding that the second clause is intended\n as an explanation of the first clause, and also that our cultural frames suggest that\n city councils are perhaps more likely than demonstrators to fear violence and that\n demonstrators might be more likely to advocate violence.\n In an attempt to get the field of NLP to focus more on methods involving world\n knowledge and common-sense reasoning, Levesque (2011) proposed a challenge\n Winograd\n schema task called the Winograd Schema Challenge.8 The problems in the challenge task\n are coreference problems designed to be easily disambiguated by the human reader,\n but hopefully not solvable by simple techniques such as selectional restrictions, or\n other basic word association methods.\n The problems are framed as a pair of statements that differ in a single word or\n phrase, and a coreference question:\n (23.73) The trophy didn‚Äôt fit into the suitcase because it was too large.\n Question: What was too large? Answer: The trophy\n 8 Levesque‚Äôs call was quickly followed up by Levesque et al. (2012) and Rahman and Ng (2012), a\n competition at the IJCAI conference (Davis et al., 2017), and a natural language inference version of the\n problem called WNLI (Wang et al., 2018).\n26 C HAPTER 23 ‚Ä¢ C OREFERENCE R ESOLUTION AND E NTITY L INKING\n\n (23.74) The trophy didn‚Äôt fit into the suitcase because it was too small.\n Question: What was too small? Answer: The suitcase\n The problems have the following characteristics:\n 1. The problems each have two parties\n 2. A pronoun preferentially refers to one of the parties, but could grammatically\n also refer to the other\n 3. A question asks which party the pronoun refers to\n 4. If one word in the question is changed, the human-preferred answer changes\n to the other party\n The kind of world knowledge that might be needed to solve the problems can\n vary. In the trophy/suitcase example, it is knowledge about the physical world; that\n a bigger object cannot fit into a smaller object. In the original Winograd sentence,\n it is stereotypes about social actors like politicians and protesters. In examples like\n the following, it is knowledge about human actions like turn-taking or thanking.\n (23.75) Bill passed the gameboy to John because his turn was [over/next]. Whose\n turn was [over/next]? Answers: Bill/John\n (23.76) Joan made sure to thank Susan for all the help she had [given/received].\n Who had [given/received] help? Answers: Susan/Joan.\n Although the Winograd Schema was designed to require common-sense reasoning, a large percentage of the original set of problems can be solved by pretrained language models, fine-tuned on Winograd Schema sentences (Kocijan et al.,\n 2019). Large pretrained language models encode an enormous amount of world or\n common-sense knowledge! The current trend is therefore to propose new datasets\n with increasingly difficult Winograd-like coreference resolution problems like K NOW R EF\n (Emami et al., 2019), with examples like:\n (23.77) Marcus is undoubtedly faster than Jarrett right now but in [his] prime the\n gap wasn‚Äôt all that big.\n In the end, it seems likely that some combination of language modeling and knowledge will prove fruitful; indeed, it seems that knowledge-based models overfit less\n to lexical idiosyncracies in Winograd Schema training sets (Trichelair et al., 2018),\n\n As with other aspects of language processing, coreference models exhibit gender and\n other biases (Zhao et al. 2018, Rudinger et al. 2018, Webster et al. 2018). For example the WinoBias dataset (Zhao et al., 2018) uses a variant of the Winograd Schema\n paradigm to test the extent to which coreference algorithms are biased toward linking gendered pronouns with antecedents consistent with cultural stereotypes. As we\n summarized in Chapter 5, embeddings replicate societal biases in their training test,\n such as associating men with historically sterotypical male occupations like doctors,\n and women with stereotypical female occupations like secretaries (Caliskan et al.\n 2017, Garg et al. 2018).\n A WinoBias sentence contain two mentions corresponding to stereotypicallymale and stereotypically-female occupations and a gendered pronoun that must be\n linked to one of them. The sentence cannot be disambiguated by the gender of the\n pronoun, but a biased model might be distracted by this cue. Here is an example\n sentence:\n 23.11 ‚Ä¢ S UMMARY 27\n\n (23.78) The secretary called the physiciani and told himi about a new patient\n [pro-stereotypical]\n (23.79) The secretary called the physiciani and told heri about a new patient\n [anti-stereotypical]\n Zhao et al. (2018) consider a coreference system to be biased if it is more accurate at linking pronouns consistent with gender stereotypical occupations (e.g., him\n with physician in (23.78)) than linking pronouns inconsistent with gender-stereotypical\n occupations (e.g., her with physician in (23.79)). They show that coreference systems of all architectures (rule-based, feature-based machine learned, and end-toend-neural) all show significant bias, performing on average 21 F1 points worse in\n the anti-stereotypical cases.\n One possible source of this bias is that female entities are significantly underrepresented in the OntoNotes dataset, used to train most coreference systems.\n Zhao et al. (2018) propose a way to overcome this bias: they generate a second\n gender-swapped dataset in which all male entities in OntoNotes are replaced with\n female ones and vice versa, and retrain coreference systems on the combined original and swapped OntoNotes data, also using debiased GloVE embeddings (Bolukbasi et al., 2016). The resulting coreference systems no longer exhibit bias on the\n WinoBias dataset, without significantly impacting OntoNotes coreference accuracy.\n In a follow-up paper, Zhao et al. (2019) show that the same biases exist in ELMo\n contextualized word vector representations and coref systems that use them. They\n showed that retraining ELMo with data augmentation again reduces or removes bias\n in coreference systems on WinoBias.\n Webster et al. (2018) introduces another dataset, GAP, and the task of Gendered\n Pronoun Resolution as a tool for developing improved coreference algorithms for\n gendered pronouns. GAP is a gender-balanced labeled corpus of 4,454 sentences\n with gendered ambiguous pronouns (by contrast, only 20% of the gendered pronouns in the English OntoNotes training data are feminine). The examples were\n created by drawing on naturally occurring sentences from Wikipedia pages to create\n hard to resolve cases with two named entities of the same gender and an ambiguous\n pronoun that may refer to either person (or neither), like the following:\n (23.80) In May, Fujisawa joined Mari Motohashi‚Äôs rink as the team‚Äôs skip, moving\n back from Karuizawa to Kitami where she had spent her junior days.\n Webster et al. (2018) show that modern coreference algorithms perform significantly worse on resolving feminine pronouns than masculine pronouns in GAP.\n Kurita et al. (2019) shows that a system based on BERT contextualized word representations shows similar bias.\n\n This chapter introduced the task of coreference resolution.\n ‚Ä¢ This is the task of linking together mentions in text which corefer, i.e. refer\n to the same discourse entity in the discourse model, resulting in a set of\n coreference chains (also called clusters or entities).\n ‚Ä¢ Mentions can be definite NPs or indefinite NPs, pronouns (including zero\n pronouns) or names.\n28 C HAPTER 23 ‚Ä¢ C OREFERENCE R ESOLUTION AND E NTITY L INKING\n\n ‚Ä¢ The surface form of an entity mention is linked to its information status\n (new, old, or inferrable), and how accessible or salient the entity is.\n ‚Ä¢ Some NPs are not referring expressions, such as pleonastic it in It is raining.\n ‚Ä¢ Many corpora have human-labeled coreference annotations that can be used\n for supervised learning, including OntoNotes for English, Chinese, and Arabic, ARRAU for English, and AnCora for Spanish and Catalan.\n ‚Ä¢ Mention detection can start with all nouns and named entities and then use\n anaphoricity classifiers or referentiality classifiers to filter out non-mentions.\n ‚Ä¢ Three common architectures for coreference are mention-pair, mention-rank,\n and entity-based, each of which can make use of feature-based or neural classifiers.\n ‚Ä¢ Modern coreference systems tend to be end-to-end, performing mention detection and coreference in a single end-to-end architecture.\n ‚Ä¢ Algorithms learn representations for text spans and heads, and learn to compare anaphor spans with candidate antecedent spans.\n ‚Ä¢ Entity linking is the task of associating a mention in text with the representation of some real-world entity in an ontology .\n ‚Ä¢ Coreference systems are evaluated by comparing with gold entity labels using\n precision/recall metrics like MUC, B3 , CEAF, BLANC, or LEA.\n ‚Ä¢ The Winograd Schema Challenge problems are difficult coreference problems that seem to require world knowledge or sophisticated reasoning to solve.\n ‚Ä¢ Coreference systems exhibit gender bias which can be evaluated using datasets\n like Winobias and GAP.\n\nHistorical Notes\n Coreference has been part of natural language processing since the 1970s (Woods\n et al. 1972, Winograd 1972). The discourse model and the entity-centric foundation\n of coreference was formulated by Karttunen (1969) (at the 3rd COLING conference), playing a role also in linguistic semantics (Heim 1982, Kamp 1981). But\n it was Bonnie Webber‚Äôs 1978 dissertation and following work (Webber 1983) that\n explored the model‚Äôs computational aspects, providing fundamental insights into\n how entities are represented in the discourse model and the ways in which they can\n license subsequent reference. Many of the examples she provided continue to challenge theories of reference to this day.\n Hobbs\n algorithm The Hobbs algorithm9 is a tree-search algorithm that was the first in a long\n series of syntax-based methods for identifying reference robustly in naturally occurring text. The input to the Hobbs algorithm is a pronoun to be resolved, together\n with a syntactic (constituency) parse of the sentences up to and including the current sentence. The details of the algorithm depend on the grammar used, but can be\n understood from a simplified version due to Kehler et al. (2004) that just searches\n through the list of NPs in the current and prior sentences. This simplified Hobbs\n algorithm searches NPs in the following order: ‚Äú(i) in the current sentence from\n right-to-left, starting with the first NP to the left of the pronoun, (ii) in the previous\n sentence from left-to-right, (iii) in two sentences prior from left-to-right, and (iv) in\n 9 The simpler of two algorithms presented originally in Hobbs (1978).\n H ISTORICAL N OTES 29\n\nthe current sentence from left-to-right, starting with the first noun group to the right\nof the pronoun (for cataphora). The first noun group that agrees with the pronoun\nwith respect to number, gender, and person is chosen as the antecedent‚Äù (Kehler\net al., 2004).\n Lappin and Leass (1994) was an influential entity-based system that used weights\nto combine syntactic and other features, extended soon after by Kennedy and Boguraev (1996) whose system avoids the need for full syntactic parses.\n Approximately contemporaneously centering (Grosz et al., 1995) was applied\nto pronominal anaphora resolution by Brennan et al. (1987), and a wide variety of\nwork followed focused on centering‚Äôs use in coreference (Kameyama 1986, Di Eugenio 1990, Walker et al. 1994, Di Eugenio 1996, Strube and Hahn 1996, Kehler\n1997, Tetreault 2001, Iida et al. 2003). Kehler and Rohde (2013) show how centering can be integrated with coherence-driven theories of pronoun interpretation. See\nChapter 24 for the use of centering in measuring discourse coherence.\n Coreference competitions as part of the US DARPA-sponsored MUC conferences provided early labeled coreference datasets (the 1995 MUC-6 and 1998 MUC-\n7 corpora), and set the tone for much later work, choosing to focus exclusively\non the simplest cases of identity coreference (ignoring difficult cases like bridging,\nmetonymy, and part-whole) and drawing the community toward supervised machine\nlearning and metrics like the MUC metric (Vilain et al., 1995). The later ACE evaluations produced labeled coreference corpora in English, Chinese, and Arabic that\nwere widely used for model training and evaluation.\n This DARPA work influenced the community toward supervised learning beginning in the mid-90s (Connolly et al. 1994, Aone and Bennett 1995, McCarthy and\nLehnert 1995). Soon et al. (2001) laid out a set of basic features, extended by Ng and\nCardie (2002b), and a series of machine learning models followed over the next 15\nyears. These often focused separately on pronominal anaphora resolution (Kehler\net al. 2004, Bergsma and Lin 2006), full NP coreference (Cardie and Wagstaff 1999,\nNg and Cardie 2002b, Ng 2005a) and definite NP reference (Poesio and Vieira 1998,\nVieira and Poesio 2000), as well as separate anaphoricity detection (Bean and Riloff\n1999, Bean and Riloff 2004, Ng and Cardie 2002a, Ng 2004), or singleton detection\n(de Marneffe et al., 2015).\n The move from mention-pair to mention-ranking approaches was pioneered by\nYang et al. (2003) and Iida et al. (2003) who proposed pairwise ranking methods,\nthen extended by Denis and Baldridge (2008) who proposed to do ranking via a softmax over all prior mentions. The idea of doing mention detection, anaphoricity, and\ncoreference jointly in a single end-to-end model grew out of the early proposal of Ng\n(2005b) to use a dummy antecedent for mention-ranking, allowing ‚Äònon-referential‚Äô\nto be a choice for coreference classifiers, Denis and Baldridge‚Äôs 2007 joint system\ncombining anaphoricity classifier probabilities with coreference probabilities, the\nDenis and Baldridge (2008) ranking model, and the Rahman and Ng (2009) proposal to train the two models jointly with a single objective.\n Simple rule-based systems for coreference returned to prominence in the 2010s,\npartly because of their ability to encode entity-based features in a high-precision\nway (Zhou et al. 2004, Haghighi and Klein 2009, Raghunathan et al. 2010, Lee et al.\n2011, Lee et al. 2013, Hajishirzi et al. 2013) but in the end they suffered from an\ninability to deal with the semantics necessary to correctly handle cases of common\nnoun coreference.\n A return to supervised learning led to a number of advances in mention-ranking\nmodels which were also extended into neural architectures, for example using re-\n30 C HAPTER 23 ‚Ä¢ C OREFERENCE R ESOLUTION AND E NTITY L INKING\n\n inforcement learning to directly optimize coreference evaluation models Clark and\n Manning (2016a), doing end-to-end coreference all the way from span extraction\n (Lee et al. 2017b, Zhang et al. 2018). Neural models also were designed to take\n advantage of global entity-level information (Clark and Manning 2016b, Wiseman\n et al. 2016, Lee et al. 2018).\n Coreference is also related to the task of entity linking discussed in Chapter 11.\n Coreference can help entity linking by giving more possible surface forms to help\n link to the right Wikipedia page, and conversely entity linking can help improve\n coreference resolution. Consider this example from Hajishirzi et al. (2013):\n (23.81) [Michael Eisner]1 and [Donald Tsang]2 announced the grand opening of\n [[Hong Kong]3 Disneyland]4 yesterday. [Eisner]1 thanked [the President]2\n and welcomed [fans]5 to [the park]4 .\n Integrating entity linking into coreference can help draw encyclopedic knowledge (like the fact that Donald Tsang is a president) to help disambiguate the mention the President. Ponzetto and Strube (2006) 2007 and Ratinov and Roth (2012)\n showed that such attributes extracted from Wikipedia pages could be used to build\n richer models of entity mentions in coreference. More recent research shows how to\n do linking and coreference jointly (Hajishirzi et al. 2013, Zheng et al. 2013) or even\n jointly with named entity tagging as well (Durrett and Klein 2014).\n The coreference task as we introduced it involves a simplifying assumption that\n the relationship between an anaphor and its antecedent is one of identity: the two\n coreferring mentions refer to the identical discourse referent. In real texts, the relationship can be more complex, where different aspects of a discourse referent can\n be neutralized or refocused. For example (23.82) (Recasens et al., 2011) shows an\n metonymy example of metonymy, in which the capital city Washington is used metonymically\n to refer to the US. (23.83-23.84) show other examples (Recasens et al., 2011):\n (23.82) a strict interpretation of a policy requires The U.S. to notify foreign\n dictators of certain coup plots ... Washington rejected the bid ...\n (23.83) I once crossed that border into Ashgh-Abad on Nowruz, the Persian New\n Year. In the South, everyone was celebrating New Year; to the North, it\n was a regular day.\n (23.84) In France, the president is elected for a term of seven years, while in the\n United States he is elected for a term of four years.\n For further linguistic discussions of these complications of coreference see Pustejovsky (1991), van Deemter and Kibble (2000), Poesio et al. (2006), Fauconnier and\n Turner (2008), Versley (2008), and Barker (2010).\n Ng (2017) offers a useful compact history of machine learning models in coreference resolution. There are three excellent book-length surveys of anaphora/coreference\n resolution, covering different time periods: Hirst (1981) (early work until about\n 1981), Mitkov (2002) (1986-2001), and Poesio et al. (2016) (2001-2015).\n Andy Kehler wrote the Discourse chapter for the 2000 first edition of this textbook, which we used as the starting point for the second-edition chapter, and there\n are some remnants of Andy‚Äôs lovely prose still in this third-edition coreference chapter.\n\nExercises\n Exercises 31\n\nAgarwal, O., S. Subramanian, A. Nenkova, and D. Roth. Chomsky, N. 1981. Lectures on Government and Binding.\n 2019. Evaluation of named entity coreference. Work- Foris.\n shop on Computational Models of Reference, Anaphora Clark, K. and C. D. Manning. 2015. Entity-centric coreferand Coreference. ence resolution with model stacking. ACL.\nAone, C. and S. W. Bennett. 1995. Evaluating automated Clark, K. and C. D. Manning. 2016a. Deep reinforceand manual acquisition of anaphora resolution strategies. ment learning for mention-ranking coreference models.\n ACL. EMNLP.\nAriel, M. 2001. Accessibility theory: An overview. In Clark, K. and C. D. Manning. 2016b. Improving coreference\n T. Sanders, J. Schilperoord, and W. Spooren, eds, Text resolution by learning entity-level distributed representa-\nRepresentation: Linguistic and Psycholinguistic Aspects, tions. ACL.\n 29‚Äì87. Benjamins. Connolly, D., J. D. Burger, and D. S. Day. 1994. A machine\nBagga, A. and B. Baldwin. 1998. Algorithms for scoring learning approach to anaphoric reference. Proceedings\n coreference chains. LREC Workshop on Linguistic Coref- of the International Conference on New Methods in Lanerence. guage Processing (NeMLaP).\nBamman, D., O. Lewke, and A. Mansoor. 2020. An anno- Cucerzan, S. 2007. Large-scale named entity disambiguation\n tated dataset of coreference in English literature. LREC. based on Wikipedia data. EMNLP/CoNLL.\nBarker, C. 2010. Nominals don‚Äôt provide criteria of iden- Davis, E., L. Morgenstern, and C. L. Ortiz. 2017. The first\n tity. In M. Rathert and A. Alexiadou, eds, The Semantics Winograd schema challenge at IJCAI-16. AI Magazine,\n of Nominalizations across Languages and Frameworks, 38(3):97‚Äì98.\n 9‚Äì24. Mouton. Denis, P. and J. Baldridge. 2007. Joint determination\n of anaphoricity and coreference resolution using integer\nBean, D. and E. Riloff. 1999. Corpus-based identification of\n programming. NAACL-HLT.\n non-anaphoric noun phrases. ACL.\n Denis, P. and J. Baldridge. 2008. Specialized models and\nBean, D. and E. Riloff. 2004. Unsupervised learning of conranking for coreference resolution. EMNLP.\n textual role knowledge for coreference resolution. HLT-\nNAACL. Denis, P. and J. Baldridge. 2009. Global joint models for\n coreference resolution and named entity classification.\nBengtson, E. and D. Roth. 2008. Understanding the value of Procesamiento del Lenguaje Natural, 42.\n features for coreference resolution. EMNLP.\n Di Eugenio, B. 1990. Centering theory and the Italian\nBerant, J., A. Chou, R. Frostig, and P. Liang. 2013. Semantic pronominal system. COLING.\n parsing on freebase from question-answer pairs. EMNLP.\n Di Eugenio, B. 1996. The discourse functions of Italian sub-\nBergsma, S. and D. Lin. 2006. Bootstrapping path-based jects: A centering approach. COLING.\n pronoun resolution. COLING/ACL. Durrett, G. and D. Klein. 2013. Easy victories and uphill\nBergsma, S., D. Lin, and R. Goebel. 2008. Distributional battles in coreference resolution. EMNLP.\n identification of non-referential pronouns. ACL. Durrett, G. and D. Klein. 2014. A joint model for entity anal-\nBjoÃàrkelund, A. and J. Kuhn. 2014. Learning structured ysis: Coreference, typing, and linking. TACL, 2:477‚Äì490.\n perceptrons for coreference resolution with latent an- Emami, A., P. Trichelair, A. Trischler, K. Suleman,\n tecedents and non-local features. ACL. H. Schulz, and J. C. K. Cheung. 2019. The KNOWREF\nBolukbasi, T., K.-W. Chang, J. Zou, V. Saligrama, and A. T. coreference corpus: Removing gender and number cues\n Kalai. 2016. Man is to computer programmer as woman for difficult pronominal anaphora resolution. ACL.\n is to homemaker? Debiasing word embeddings. NeurIPS. Fauconnier, G. and M. Turner. 2008. The way we think: Con-\nBrennan, S. E., M. W. Friedman, and C. Pollard. 1987. A ceptual blending and the mind‚Äôs hidden complexities. Bacentering approach to pronouns. ACL. sic Books.\nCaliskan, A., J. J. Bryson, and A. Narayanan. 2017. Seman- Fernandes, E. R., C. N. dos Santos, and R. L. MilidiuÃÅ. 2012.\n tics derived automatically from language corpora contain Latent structure perceptron with feature induction for unhuman-like biases. Science, 356(6334):183‚Äì186. restricted coreference resolution. CoNLL.\n Ferragina, P. and U. Scaiella. 2011. Fast and accurate anno-\nCardie, C. and K. Wagstaff. 1999. Noun phrase coreference\n tation of short texts with wikipedia pages. IEEE Software,\n as clustering. EMNLP/VLC.\n 29(1):70‚Äì75.\nChafe, W. L. 1976. Givenness, contrastiveness, definiteness, Fox, B. A. 1993. Discourse Structure and Anaphora: Writsubjects, topics, and point of view. In C. N. Li, ed., Sub- ten and Conversational English. Cambridge.\n ject and Topic, 25‚Äì55. Academic Press.\n Garg, N., L. Schiebinger, D. Jurafsky, and J. Zou. 2018.\nChang, K.-W., R. Samdani, and D. Roth. 2013. A con- Word embeddings quantify 100 years of gender and ethstrained latent variable model for coreference resolution. nic stereotypes. Proceedings of the National Academy of\n EMNLP. Sciences, 115(16):E3635‚ÄìE3644.\nChang, K.-W., R. Samdani, A. Rozovskaya, M. Sammons, Grosz, B. J. 1977. The Representation and Use of Focus\n and D. Roth. 2012. Illinois-Coref: The UI system in the in Dialogue Understanding. Ph.D. thesis, University of\n CoNLL-2012 shared task. CoNLL. California, Berkeley.\nChen, C. and V. Ng. 2013. Linguistically aware coreference Grosz, B. J., A. K. Joshi, and S. Weinstein. 1995. Centerevaluation metrics. IJCNLP. ing: A framework for modeling the local coherence of\n discourse. Computational Linguistics, 21(2):203‚Äì225.\n32 Chapter 23 ‚Ä¢ Coreference Resolution and Entity Linking\n\nGundel, J. K., N. Hedberg, and R. Zacharski. 1993. Cog- Kolhatkar, V., A. Roussel, S. Dipper, and H. Zinsmeister.\n nitive status and the form of referring expressions in dis- 2018. Anaphora with non-nominal antecedents in compucourse. Language, 69(2):274‚Äì307. tational linguistics: A survey. Computational Linguistics,\nHaghighi, A. and D. Klein. 2009. Simple coreference reso- 44(3):547‚Äì612.\n lution with rich syntactic and semantic features. EMNLP. Kummerfeld, J. K. and D. Klein. 2013. Error-driven analysis\nHajishirzi, H., L. Zilles, D. S. Weld, and L. Zettlemoyer. of challenges in coreference resolution. EMNLP.\n 2013. Joint coreference resolution and named-entity link- Kurita, K., N. Vyas, A. Pareek, A. W. Black, and\n ing with multi-pass sieves. EMNLP. Y. Tsvetkov. 2019. Quantifying social biases in contex-\nHaviland, S. E. and H. H. Clark. 1974. What‚Äôs new? Acquir- tual word representations. 1st ACL Workshop on Gender\n ing new information as a process in comprehension. Jour- Bias for Natural Language Processing.\n nal of Verbal Learning and Verbal Behaviour, 13:512‚Äì Lappin, S. and H. Leass. 1994. An algorithm for pronom-\n521. inal anaphora resolution. Computational Linguistics,\nHawkins, J. A. 1978. Definiteness and indefiniteness: a study 20(4):535‚Äì561.\n in reference and grammaticality prediction. Croom Helm Lee, H., A. Chang, Y. Peirsman, N. Chambers, M. Surdeanu,\n Ltd. and D. Jurafsky. 2013. Deterministic coreference resolu-\nHeim, I. 1982. The semantics of definite and indefinite noun tion based on entity-centric, precision-ranked rules. Comphrases. Ph.D. thesis, University of Massachusetts at putational Linguistics, 39(4):885‚Äì916.\n Amherst. Lee, H., Y. Peirsman, A. Chang, N. Chambers, M. Surdeanu,\nHirst, G. 1981. Anaphora in Natural Language Understand- and D. Jurafsky. 2011. Stanford‚Äôs multi-pass sieve corefing: A survey. Number 119 in Lecture notes in computer erence resolution system at the CoNLL-2011 shared task.\n science. Springer-Verlag. CoNLL.\nHobbs, J. R. 1978. Resolving pronoun references. Lingua, Lee, H., M. Surdeanu, and D. Jurafsky. 2017a. A scaffolding\n 44:311‚Äì338. approach to coreference resolution integrating statistical\n and rule-based models. Natural Language Engineering,\nHou, Y., K. Markert, and M. Strube. 2018. Unre- 23(5):733‚Äì762.\n stricted bridging resolution. Computational Linguistics,\n 44(2):237‚Äì284. Lee, K., L. He, M. Lewis, and L. Zettlemoyer. 2017b. Endto-end neural coreference resolution. EMNLP.\nIida, R., K. Inui, H. Takamura, and Y. Matsumoto. 2003. Incorporating contextual cues in trainable models for coref- Lee, K., L. He, and L. Zettlemoyer. 2018. Highererence resolution. EACL Workshop on The Computa- order coreference resolution with coarse-to-fine infertional Treatment of Anaphora. ence. NAACL HLT.\nJi, H. and R. Grishman. 2011. Knowledge base population: Levesque, H. 2011. The Winograd Schema Challenge. Logi-\nSuccessful approaches and challenges. ACL. cal Formalizations of Commonsense Reasoning ‚Äî Papers\n from the AAAI 2011 Spring Symposium (SS-11-06).\nJoshi, M., O. Levy, D. S. Weld, and L. Zettlemoyer. 2019.\n BERT for coreference resolution: Baselines and analysis. Levesque, H., E. Davis, and L. Morgenstern. 2012. The\n EMNLP. Winograd Schema Challenge. KR-12.\nKameyama, M. 1986. A property-sharing constraint in cen- Li, B. Z., S. Min, S. Iyer, Y. Mehdad, and W.-t. Yih. 2020.\n tering. ACL. Efficient one-pass end-to-end entity linking for questions.\n EMNLP.\nKamp, H. 1981. A theory of truth and semantic representation. In J. Groenendijk, T. Janssen, and M. Stokhof, Luo, X. 2005. On coreference resolution performance meteds, Formal Methods in the Study of Language, 189‚Äì222. rics. EMNLP.\n Mathematical Centre, Amsterdam. Luo, X. and S. Pradhan. 2016. Evaluation metrics. In\nKarttunen, L. 1969. Discourse referents. COLING. Preprint M. Poesio, R. Stuckardt, and Y. Versley, eds, Anaphora\n No. 70. resolution: Algorithms, resources, and applications,\n 141‚Äì163. Springer.\nKehler, A. 1997. Current theories of centering for pronoun\n Luo, X., S. Pradhan, M. Recasens, and E. H. Hovy. 2014. An\n interpretation: A critical evaluation. Computational Linextension of BLANC to system mentions. ACL.\n guistics, 23(3):467‚Äì475.\n de Marneffe, M.-C., M. Recasens, and C. Potts. 2015. Mod-\nKehler, A., D. E. Appelt, L. Taylor, and A. Simma. 2004. The\n eling the lifespan of discourse entities with application to\n (non)utility of predicate-argument frequencies for procoreference resolution. JAIR, 52:445‚Äì475.\n noun interpretation. HLT-NAACL.\n Martschat, S. and M. Strube. 2014. Recall error analysis for\nKehler, A. and H. Rohde. 2013. A probabilistic reconcilcoreference resolution. EMNLP.\n iation of coherence-driven and centering-driven theories\n of pronoun interpretation. Theoretical Linguistics, 39(1- Martschat, S. and M. Strube. 2015. Latent structures for\n 2):1‚Äì37. coreference resolution. TACL, 3:405‚Äì418.\nKennedy, C. and B. K. Boguraev. 1996. Anaphora for every- McCarthy, J. F. and W. G. Lehnert. 1995. Using decision\n one: Pronominal anaphora resolution without a parser. trees for coreference resolution. IJCAI-95.\n COLING. Mihalcea, R. and A. Csomai. 2007. Wikify!: Linking docu-\nKocijan, V., A.-M. Cretu, O.-M. Camburu, Y. Yordanov, and ments to encyclopedic knowledge. CIKM 2007.\n T. Lukasiewicz. 2019. A surprisingly robust trick for the Milne, D. and I. H. Witten. 2008. Learning to link with wiki-\nWinograd Schema Challenge. ACL. pedia. CIKM 2008.\n Exercises 33\n\nMitkov, R. 2002. Anaphora Resolution. Longman. Pradhan, S., L. Ramshaw, R. Weischedel, J. MacBride, and\nMoosavi, N. S. and M. Strube. 2016. Which coreference L. Micciulla. 2007b. Unrestricted coreference: Identievaluation metric do you trust? A proposal for a link- fying entities and events in OntoNotes. Proceedings of\n based entity aware metric. ACL. ICSC 2007.\nNg, V. 2004. Learning noun phrase anaphoricity to improve Prince, E. 1981. Toward a taxonomy of given-new inforcoreference resolution: Issues in representation and opti- mation. In P. Cole, ed., Radical Pragmatics, 223‚Äì255.\n mization. ACL. Academic Press.\nNg, V. 2005a. Machine learning for coreference resolution: Pustejovsky, J. 1991. The generative lexicon. Computational\n From local classification to global ranking. ACL. Linguistics, 17(4).\n Raghunathan, K., H. Lee, S. Rangarajan, N. Chambers,\nNg, V. 2005b. Supervised ranking for pronoun resolution:\n M. Surdeanu, D. Jurafsky, and C. D. Manning. 2010. A\n Some recent improvements. AAAI.\n multi-pass sieve for coreference resolution. EMNLP.\nNg, V. 2010. Supervised noun phrase coreference research:\n Rahman, A. and V. Ng. 2009. Supervised models for coref-\nThe first fifteen years. ACL.\n erence resolution. EMNLP.\nNg, V. 2017. Machine learning for entity coreference reso-\nRahman, A. and V. Ng. 2012. Resolving complex cases\n lution: A retrospective look at two decades of research.\n of definite pronouns: the Winograd Schema challenge.\n AAAI.\n EMNLP.\nNg, V. and C. Cardie. 2002a. Identifying anaphoric and non- Ratinov, L. and D. Roth. 2012. Learning-based multi-sieve\n anaphoric noun phrases to improve coreference resolu- co-reference resolution with knowledge. EMNLP.\n tion. COLING.\n Recasens, M. and E. H. Hovy. 2011. BLANC: Implement-\nNg, V. and C. Cardie. 2002b. Improving machine learning ing the Rand index for coreference evaluation. Natural\n approaches to coreference resolution. ACL. Language Engineering, 17(4):485‚Äì510.\nNissim, M., S. Dingare, J. Carletta, and M. Steedman. 2004. Recasens, M., E. H. Hovy, and M. A. Martƒ±ÃÅ. 2011. Identity,\n An annotation scheme for information status in dialogue. non-identity, and near-identity: Addressing the complex-\nLREC. ity of coreference. Lingua, 121(6):1138‚Äì1152.\nPoesio, M., R. Stuckardt, and Y. Versley. 2016. Anaphora Recasens, M. and M. A. Martƒ±ÃÅ. 2010. AnCora-CO: Coreferresolution: Algorithms, resources, and applications. entially annotated corpora for Spanish and Catalan. Lan-\nSpringer. guage Resources and Evaluation, 44(4):315‚Äì345.\nPoesio, M., P. Sturt, R. Artstein, and R. Filik. 2006. Under- Reichman, R. 1985. Getting Computers to Talk Like You and\n specification and anaphora: Theoretical issues and pre- Me. MIT Press.\n liminary evidence. Discourse processes, 42(2):157‚Äì175.\n Rudinger, R., J. Naradowsky, B. Leonard, and\nPoesio, M. and R. Vieira. 1998. A corpus-based investiga- B. Van Durme. 2018. Gender bias in coreference restion of definite description use. Computational Linguis- olution. NAACL HLT.\n tics, 24(2):183‚Äì216.\n Schiebinger, L. 2013. Machine translation: Analyzing gen-\nPonzetto, S. P. and M. Strube. 2006. Exploiting semantic role der. http://genderedinnovations.stanford.edu/\n labeling, WordNet and Wikipedia for coreference resolu- case-studies/nlp.html#tabs-2.\n tion. HLT-NAACL. Soon, W. M., H. T. Ng, and D. C. Y. Lim. 2001. A ma-\nPonzetto, S. P. and M. Strube. 2007. Knowledge de- chine learning approach to coreference resolution of noun\n rived from Wikipedia for computing semantic related- phrases. Computational Linguistics, 27(4):521‚Äì544.\n ness. JAIR, 30:181‚Äì212. Sorokin, D. and I. Gurevych. 2018. Mixing context granu-\nPradhan, S., E. H. Hovy, M. P. Marcus, M. Palmer, larities for improved entity linking on question answering\n L. Ramshaw, and R. Weischedel. 2007a. OntoNotes: A data across entity categories. *SEM.\n unified relational semantic representation. Proceedings of Strube, M. and U. Hahn. 1996. Functional centering. ACL.\n ICSC.\n Su, Y., H. Sun, B. Sadler, M. Srivatsa, I. GuÃàr, Z. Yan, and\nPradhan, S., X. Luo, M. Recasens, E. H. Hovy, V. Ng, and X. Yan. 2016. On generating characteristic-rich question\n M. Strube. 2014. Scoring coreference partitions of pre- sets for QA evaluation. EMNLP.\n dicted mentions: A reference implementation. ACL.\n Tetreault, J. R. 2001. A corpus-based evaluation of center-\nPradhan, S., A. Moschitti, N. Xue, O. Uryupina, and ing and pronoun resolution. Computational Linguistics,\n Y. Zhang. 2012a. CoNLL-2012 shared task: Model- 27(4):507‚Äì520.\n ing multilingual unrestricted coreference in OntoNotes.\n Trichelair, P., A. Emami, J. C. K. Cheung, A. Trischler,\n CoNLL.\n K. Suleman, and F. Diaz. 2018. On the evaluation of\nPradhan, S., A. Moschitti, N. Xue, O. Uryupina, and common-sense reasoning in natural language understand-\nY. Zhang. 2012b. Conll-2012 shared task: Modeling mul- ing. NeurIPS 2018 Workshop on Critiquing and Correcttilingual unrestricted coreference in OntoNotes. CoNLL. ing Trends in Machine Learning.\nPradhan, S., L. Ramshaw, M. P. Marcus, M. Palmer, Uryupina, O., R. Artstein, A. Bristot, F. Cavicchio, F. Del-\nR. Weischedel, and N. Xue. 2011. CoNLL-2011 shared ogu, K. J. Rodriguez, and M. Poesio. 2020. Annotating\n task: Modeling unrestricted coreference in OntoNotes. a broad range of anaphoric phenomena, in a variety of\n CoNLL. genres: The ARRAU corpus. Natural Language Engineering, 26(1):1‚Äì34.\n34 Chapter 23 ‚Ä¢ Coreference Resolution and Entity Linking\n\nvan Deemter, K. and R. Kibble. 2000. On coreferring: coref- Zhao, J., T. Wang, M. Yatskar, V. Ordonez, and K.-W.\n erence in MUC and related annotation schemes. Compu- Chang. 2018. Gender bias in coreference resolution:\n tational Linguistics, 26(4):629‚Äì637. Evaluation and debiasing methods. NAACL HLT.\nVersley, Y. 2008. Vagueness and referential ambiguity in a Zheng, J., L. Vilnis, S. Singh, J. D. Choi, and A. McCallum.\n large-scale annotated corpus. Research on Language and 2013. Dynamic knowledge-base alignment for corefer-\nComputation, 6(3-4):333‚Äì353. ence resolution. CoNLL.\nVieira, R. and M. Poesio. 2000. An empirically based sys- Zhou, L., M. Ticrea, and E. H. Hovy. 2004. Multi-document\n tem for processing definite descriptions. Computational biography summarization. EMNLP.\n Linguistics, 26(4):539‚Äì593.\nVilain, M., J. D. Burger, J. Aberdeen, D. Connolly, and\n L. Hirschman. 1995. A model-theoretic coreference scoring scheme. MUC-6.\nWalker, M. A., M. Iida, and S. Cote. 1994. Japanese discourse and the process of centering. Computational Linguistics, 20(2):193‚Äì232.\nWang, A., A. Singh, J. Michael, F. Hill, O. Levy, and S. R.\n Bowman. 2018. Glue: A multi-task benchmark and analysis platform for natural language understanding. ICLR.\nWebber, B. L. 1978. A Formal Approach to Discourse\n Anaphora. Ph.D. thesis, Harvard University.\nWebber, B. L. 1983. So what can we talk about now? In\n M. Brady and R. C. Berwick, eds, Computational Models\n of Discourse, 331‚Äì371. The MIT Press.\nWebber, B. L. 1991. Structure and ostension in the interpretation of discourse deixis. Language and Cognitive\n Processes, 6(2):107‚Äì135.\nWebber, B. L. and B. Baldwin. 1992. Accommodating context change. ACL.\nWebber, B. L. 1988. Discourse deixis: Reference to discourse segments. ACL.\nWebster, K., M. Recasens, V. Axelrod, and J. Baldridge.\n 2018. Mind the GAP: A balanced corpus of gendered\n ambiguous pronouns. TACL, 6:605‚Äì617.\nWinograd, T. 1972. Understanding Natural Language. Academic Press.\nWiseman, S., A. M. Rush, and S. M. Shieber. 2016. Learning\n global features for coreference resolution. NAACL HLT.\nWiseman, S., A. M. Rush, S. M. Shieber, and J. Weston.\n 2015. Learning anaphoricity and antecedent ranking features for coreference resolution. ACL.\nWoods, W. A., R. M. Kaplan, and B. L. Nash-Webber. 1972.\n The lunar sciences natural language information system:\n Final report. Technical Report 2378, BBN.\nWu, L., F. Petroni, M. Josifoski, S. Riedel, and L. Zettlemoyer. 2020. Scalable zero-shot entity linking with dense\n entity retrieval. EMNLP.\nYang, X., G. Zhou, J. Su, and C. L. Tan. 2003. Coreference\n resolution using competition learning approach. ACL.\nYih, W.-t., M. Richardson, C. Meek, M.-W. Chang, and\n J. Suh. 2016. The value of semantic parse labeling for\n knowledge base question answering. ACL.\nZhang, R., C. N. dos Santos, M. Yasunaga, B. Xiang, and\n D. Radev. 2018. Neural coreference resolution with deep\n biaffine attention by joint mention detection and mention\n clustering. ACL.\nZhao, J., T. Wang, M. Yatskar, R. Cotterell, V. Ordonez, and\n K.-W. Chang. 2019. Gender bias in contextualized word\n embeddings. NAACL HLT.\n",
    "file_path": "/Users/colinsidberry/nlp-textbook-rag/data/extracted_text/23.Coreference Resolution and Entity Linking.txt",
    "file_size_kb": 110.1
  },
  {
    "id": "8523157b632ae17a",
    "source": "nlp_textbook",
    "chapter": "24 Discourse Coherence",
    "filename": "24.Discourse Coherence.txt",
    "content": "rights reserved. Draft of August 24, 2025.\n\nCHAPTER\n\n24 Discourse Coherence\n\n And even in our wildest and most wandering reveries, nay in our very dreams,\n we shall find, if we reflect, that the imagination ran not altogether at adventures, but that there was still a connection upheld among the different ideas,\n which succeeded each other. Were the loosest and freest conversation to be\n transcribed, there would immediately be transcribed, there would immediately\n be observed something which connected it in all its transitions.\n David Hume, An enquiry concerning human understanding, 1748\n\n Orson Welles‚Äô movie Citizen Kane was groundbreaking in many ways, perhaps most\n notably in its structure. The story of the life of fictional media magnate Charles\n Foster Kane, the movie does not proceed in chronological order through Kane‚Äôs\n life. Instead, the film begins with Kane‚Äôs death (famously murmuring ‚ÄúRosebud‚Äù)\n and is structured around flashbacks to his life inserted among scenes of a reporter\n investigating his death. The novel idea that the structure of a movie does not have\n to linearly follow the structure of the real timeline made apparent for 20th century\n cinematography the infinite possibilities and impact of different kinds of coherent\n narrative structures.\n But coherent structure is not just a fact about movies or works of art. Like\n movies, language does not normally consist of isolated, unrelated sentences, but\n instead of collocated, structured, coherent groups of sentences. We refer to such\n discourse a coherent structured group of sentences as a discourse, and we use the word cocoherence herence to refer to the relationship between sentences that makes real discourses\n different than just random assemblages of sentences. The chapter you are now reading is an example of a discourse, as is a news article, a conversation, a thread on\n social media, a Wikipedia page, and your favorite novel.\n What makes a discourse coherent? If you created a text by taking random sentences each from many different sources and pasted them together, would that be a\n local coherent discourse? Almost certainly not. Real discourses exhibit both local coherglobal ence and global coherence. Let‚Äôs consider three ways in which real discourses are\n locally coherent;\n First, sentences or clauses in real discourses are related to nearby sentences in\n systematic ways. Consider this example from Hobbs (1979):\n (24.1) John took a train from Paris to Istanbul. He likes spinach.\n This sequence is incoherent because it is unclear to a reader why the second\n sentence follows the first; what does liking spinach have to do with train trips? In\n fact, a reader might go to some effort to try to figure out how the discourse could be\n coherent; perhaps there is a French spinach shortage? The very fact that hearers try\n to identify such connections suggests that human discourse comprehension involves\n the need to establish this kind of coherence.\n By contrast, in the following coherent example:\n (24.2) Jane took a train from Paris to Istanbul. She had to attend a conference.\n2 C HAPTER 24 ‚Ä¢ D ISCOURSE C OHERENCE\n\n the second sentence gives a REASON for Jane‚Äôs action in the first sentence. Structured relationships like REASON that hold between text units are called coherence\n coherence relations, and coherent discourses are structured by many such coherence relations.\n relations\n Coherence relations are introduced in Section 24.1.\n A second way a discourse can be locally coherent is by virtue of being ‚Äúabout‚Äù\n someone or something. In a coherent discourse some entities are salient, and the\n discourse focuses on them and doesn‚Äôt go back and forth between multiple entities.\n This is called entity-based coherence. Consider the following incoherent passage,\n in which the salient entity seems to wildly swing from John to Jenny to the piano\n store to the living room, back to Jenny, then the piano again:\n (24.3) John wanted to buy a piano for his living room.\n Jenny also wanted to buy a piano.\n He went to the piano store.\n It was nearby.\n The living room was on the second floor.\n She didn‚Äôt find anything she liked.\n The piano he bought was hard to get up to that floor.\n Entity-based coherence models measure this kind of coherence by tracking salient\n Centering\n Theory entities across a discourse. For example Centering Theory (Grosz et al., 1995), the\n most influential theory of entity-based coherence, keeps track of which entities in\n the discourse model are salient at any point (salient entities are more likely to be\n pronominalized or to appear in prominent syntactic positions like subject or object).\n In Centering Theory, transitions between sentences that maintain the same salient\n entity are considered more coherent than ones that repeatedly shift between entities.\n entity grid The entity grid model of coherence (Barzilay and Lapata, 2008) is a commonly\n used model that realizes some of the intuitions of the Centering Theory framework.\n Entity-based coherence is introduced in Section 24.3.\n topically Finally, discourses can be locally coherent by being topically coherent: nearby\n coherent\n sentences are generally about the same topic and use the same or similar vocabulary to discuss these topics. Because topically coherent discourses draw from a\n single semantic field or topic, they tend to exhibit the surface property known as\nlexical cohesion lexical cohesion (Halliday and Hasan, 1976): the sharing of identical or semantically related words in nearby sentences. For example, the fact that the words house,\n chimney, garret, closet, and window‚Äî all of which belong to the same semantic\n field‚Äî appear in the two sentences in (24.4), or that they share the identical word\n shingled, is a cue that the two are tied together as a discourse:\n (24.4) Before winter I built a chimney, and shingled the sides of my house...\n I have thus a tight shingled and plastered house... with a garret and a\n closet, a large window on each side....\n In addition to the local coherence between adjacent or nearby sentences, discourses also exhibit global coherence. Many genres of text are associated with\n particular conventional discourse structures. Academic articles might have sections\n describing the Methodology or Results. Stories might follow conventional plotlines\n or motifs. Persuasive essays have a particular claim they are trying to argue for,\n and an essay might express this claim together with a structured set of premises that\n support the argument and demolish potential counterarguments. We‚Äôll introduce\n versions of each of these kinds of global coherence.\n Why do we care about the local or global coherence of a discourse? Since coherence is a property of a well-written text, coherence detection plays a part in any\n 24.1 ‚Ä¢ C OHERENCE R ELATIONS 3\n\n task that requires measuring the quality of a text. For example coherence can help\n in pedagogical tasks like essay grading or essay quality measurement that are trying\n to grade how well-written a human essay is (Somasundaran et al. 2014, Feng et al.\n 2014, Lai and Tetreault 2018). Coherence can also help for summarization; knowing\n the coherence relationship between sentences can help know how to select information from them. Finally, detecting incoherent text may even play a role in mental\n health tasks like measuring symptoms of schizophrenia or other kinds of disordered\n language (Ditman and Kuperberg 2010, ElvevaÃäg et al. 2007, Bedi et al. 2015, Iter\n et al. 2018).\n\n Recall from the introduction the difference between passages (24.5) and (24.6).\n (24.5) Jane took a train from Paris to Istanbul. She likes spinach.\n (24.6) Jane took a train from Paris to Istanbul. She had to attend a conference.\n The reason (24.6) is more coherent is that the reader can form a connection between the two sentences, in which the second sentence provides a potential REASON\n for the first sentences. This link is harder to form for (24.5). These connections\n coherence between text spans in a discourse can be specified as a set of coherence relations.\n relation\n The next two sections describe two commonly used models of coherence relations\n and associated corpora: Rhetorical Structure Theory (RST), and the Penn Discourse\n TreeBank (PDTB).\n\n 24.1.1 Rhetorical Structure Theory\n The most commonly used model of discourse organization is Rhetorical Structure\n RST Theory (RST) (Mann and Thompson, 1987). In RST relations are defined between\n nucleus two spans of text, generally a nucleus and a satellite. The nucleus is the unit that\n satellite is more central to the writer‚Äôs purpose and that is interpretable independently; the\n satellite is less central and generally is only interpretable with respect to the nucleus.\n Some symmetric relations, however, hold between two nuclei.\n Below are a few examples of RST coherence relations, with definitions adapted\n from the RST Treebank Manual (Carlson and Marcu, 2001).\n Reason: The nucleus is an action carried out by an animate agent and the satellite\n is the reason for the nucleus.\n (24.7) [NUC Jane took a train from Paris to Istanbul.] [SAT She had to attend a\n conference.]\n\n Elaboration: The satellite gives additional information or detail about the situation\n presented in the nucleus.\n (24.8) [NUC Dorothy was from Kansas.] [SAT She lived in the midst of the great\n Kansas prairies.]\n\n Evidence: The satellite gives additional information or detail about the situation\n presented in the nucleus. The information is presented with the goal of convince the\n reader to accept the information presented in the nucleus.\n (24.9) [NUC Kevin must be here.] [SAT His car is parked outside.]\n4 C HAPTER 24 ‚Ä¢ D ISCOURSE C OHERENCE\n\n Attribution: The satellite gives the source of attribution for an instance of reported\n speech in the nucleus.\n (24.10) [SAT Analysts estimated] [NUC that sales at U.S. stores declined in the\n quarter, too]\n\n List: In this multinuclear relation, a series of nuclei is given, without contrast or\n explicit comparison:\n (24.11) [NUC Billy Bones was the mate; ] [NUC Long John, he was quartermaster]\n\n RST relations are traditionally represented graphically; the asymmetric Nucleus-\nSatellite relation is represented with an arrow from the satellite to the nucleus:\n\n evidence\n\n Kevin must be here. His car is parked outside\n\n We can also talk about the coherence of a larger text by considering the hierarchical structure between coherence relations. Figure 24.1 shows the rhetorical structure of a paragraph from Marcu (2000a) for the text in (24.12) from the Scientific\n American magazine.\n (24.12) With its distant orbit‚Äì50 percent farther from the sun than Earth‚Äìand slim\n atmospheric blanket, Mars experiences frigid weather conditions. Surface\n temperatures typically average about -60 degrees Celsius (-76 degrees\n Fahrenheit) at the equator and can dip to -123 degrees C near the poles. Only\n the midday sun at tropical latitudes is warm enough to thaw ice on occasion,\n but any liquid water formed in this way would evaporate almost instantly\n because of the low atmospheric pressure.\n\n Title 2-9\n (1)\n evidence\n Mars\n\n 2-3 4-9\n background elaboration-additional\n\n (2) (3) 4-5 6-9\n WIth its Mars\n distant orbit experiences\n <p> -- 50 frigid weather List Contrast\n percent conditions.\n farther from (4) (5) 6-7 8-9\n the sun than Surface and can dip\n Earth -- </p> temperatures to -123 purpose explanation-argumentative\n and slim typically average degrees C\n atmospheric about -60 near the (6) (7) (8) (9)\n blanket, degrees Celsius poles. Only the to thaw ice but any liquid water because of\n <p> (-76 degrees midday sun at on occasion, formed in this way the low\n Fahrenheit)</p> tropical latitudes would evaporate atmospheric\n at the equator is warm enough almost instantly pressure.\n\nasymmetric relations are represented with a curved arrow from the satellite to the nucleus.\n\n The leaves in the Fig. 24.1 tree correspond to text spans of a sentence, clause or\n EDU phrase that are called elementary discourse units or EDUs in RST; these units can\n also be referred to as discourse segments. Because these units may correspond to\n arbitrary spans of text, determining the boundaries of an EDU is an important task\n for extracting coherence relations. Roughly speaking, one can think of discourse\n 24.1 ‚Ä¢ C OHERENCE R ELATIONS 5\n\n segments as being analogous to constituents in sentence syntax, and indeed as we‚Äôll\n see in Section 24.2 we generally draw on parsing algorithms to infer discourse structure.\n There are corpora for many discourse coherence models; the RST Discourse\n TreeBank (Carlson et al., 2001) is the largest available discourse corpus. It consists of 385 English language documents selected from the Penn Treebank, with full\n RST parses for each one, using a large set of 78 distinct relations, grouped into 16\n classes. RST treebanks exist also for Spanish, German, Basque, Dutch and Brazilian\n Portuguese (Braud et al., 2017).\n Now that we‚Äôve seen examples of coherence, we can see more clearly how a\n coherence relation can play a role in summarization or information extraction. For\n example, the nuclei of a text presumably express more important information than\n the satellites, which might be dropped in a summary.\n\n 24.1.2 Penn Discourse TreeBank (PDTB)\n PDTB The Penn Discourse TreeBank (PDTB) is a second commonly used dataset that\n embodies another model of coherence relations (Miltsakaki et al. 2004, Prasad et al.\n 2008, Prasad et al. 2014). PDTB labeling is lexically grounded. Instead of asking\n annotators to directly tag the coherence relation between text spans, they were given\n discourse a list of discourse connectives, words that signal discourse relations, like because,\nconnectives\n although, when, since, or as a result. In a part of a text where these words marked a\n coherence relation between two text spans, the connective and the spans were then\n annotated, as in Fig. 24.13, where the phrase as a result signals a causal relationship\n between what PDTB calls Arg1 (the first two sentences, here in italics) and Arg2\n (the third sentence, here in bold).\n (24.13) Jewelry displays in department stores were often cluttered and uninspired.\n And the merchandise was, well, fake. As a result, marketers of faux gems\n steadily lost space in department stores to more fashionable\n rivals‚Äîcosmetics makers.\n (24.14) In July, the Environmental Protection Agency imposed a gradual ban on\n virtually all uses of asbestos. (implicit=as a result) By 1997, almost all\n remaining uses of cancer-causing asbestos will be outlawed.\n Not all coherence relations are marked by an explicit discourse connective, and\n so the PDTB also annotates pairs of neighboring sentences with no explicit signal,\n like (24.14). The annotator first chooses the word or phrase that could have been its\n signal (in this case as a result), and then labels its sense. For example for the ambiguous discourse connective since annotators marked whether it is using a C AUSAL\n or a T EMPORAL sense.\n The final dataset contains roughly 18,000 explicit relations and 16,000 implicit\n relations. Fig. 24.2 shows examples from each of the 4 major semantic classes, while\n Fig. 24.3 shows the full tagset.\n Unlike the RST Discourse Treebank, which integrates these pairwise coherence\n relations into a global tree structure spanning an entire discourse, the PDTB does not\n annotate anything above the span-pair level, making no commitment with respect to\n higher-level discourse structure.\n There are also treebanks using similar methods for other languages; (24.15)\n shows an example from the Chinese Discourse TreeBank (Zhou and Xue, 2015).\n Because Chinese has a smaller percentage of explicit discourse connectives than\n English (only 22% of all discourse relations are marked with explicit connectives,\n6 C HAPTER 24 ‚Ä¢ D ISCOURSE C OHERENCE\n\nClass Type\n Example\nTEMPORAL The parishioners of St. Michael and All Angels stop to chat at\n SYNCHRONOUS\n the church door, as members here always have. (Implicit while)\n In the tower, five men and women pull rhythmically on ropes\n attached to the same five bells that first sounded here in 1614.\nCONTINGENCY REASON Also unlike Mr. Ruder, Mr. Breeden appears to be in a position\n to get somewhere with his agenda. (implicit=because) As a former White House aide who worked closely with Congress,\n he is savvy in the ways of Washington.\nCOMPARISON CONTRAST The U.S. wants the removal of what it perceives as barriers to\n investment; Japan denies there are real barriers.\nEXPANSION CONJUNCTION Not only do the actors stand outside their characters and make\n it clear they are at odds with them, but they often literally stand\n on their heads.\n\nTemporal Comparison\n ‚Ä¢ Asynchronous ‚Ä¢ Contrast (Juxtaposition, Opposition)\n ‚Ä¢ Synchronous (Precedence, Succession) ‚Ä¢Pragmatic Contrast (Juxtaposition, Opposition)\n ‚Ä¢ Concession (Expectation, Contra-expectation)\n ‚Ä¢ Pragmatic Concession\n\n Contingency Expansion\n ‚Ä¢ Cause (Reason, Result) ‚Ä¢ Exception\n ‚Ä¢ Pragmatic Cause (Justification) ‚Ä¢ Instantiation\n ‚Ä¢ Condition (Hypothetical, General, Unreal ‚Ä¢ Restatement (Specification, Equivalence, Generalization)\n Present/Past, Factual Present/Past)\n ‚Ä¢ Pragmatic Condition (Relevance, Implicit As- ‚Ä¢ Alternative (Conjunction, Disjunction, Chosen Alternasertion) tive)\n ‚Ä¢ List\ntypes have subtypes). 11 of the 16 types are commonly used for implicit ¬Ø argument classification; the 5 types in\nitalics are too rare in implicit labeling to be used.\n\n compared to 47% in English), annotators labeled this corpus by directly mapping\n pairs of sentences to 11 sense tags, without starting with a lexical discourse connector.\n (24.15) [Conn ‰∏∫] [Arg2 Êé®Âä®Âõæ‰ª¨Ê±üÂú∞Âå∫ÂºÄÂèë] Ôºå[Arg1 Èü©ÂõΩÊçêÊ¨æ‰∏ÄÁôæ‰∏áÁæéÂÖÉ\n ËÆæÁ´ã‰∫ÜÂõæ‰ª¨Ê±üÂèëÂ±ïÂü∫Èáë]\n ‚Äú[In order to] [Arg2 promote the development of the Tumen River region],\n [Arg1 South Korea donated one million dollars to establish the Tumen\n River Development Fund].‚Äù\n These discourse treebanks have been used for shared tasks on multilingual discourse parsing (Xue et al., 2016).\n\n Given a sequence of sentences, how can we automatically determine the coherence\n discourse\n parsing relations between them? This task is often called discourse parsing (even though\n for PDTB we are only assigning labels to leaf spans and not building a full parse\n 24.2 ‚Ä¢ D ISCOURSE S TRUCTURE PARSING 7\n\ntree as we do for RST).\n\n24.2.1 EDU segmentation for RST parsing\nRST parsing is generally done in two stages. The first stage, EDU segmentation,\nextracts the start and end of each EDU. The output of this stage would be a labeling\nlike the following:\n(24.16) [Mr. Rambo says]e1 [that a 3.2-acre property]e2 [overlooking the San\n Fernando Valley]e3 [is priced at $4 million]e4 [because the late actor Erroll\n Flynn once lived there.]e5\n Since EDUs roughly correspond to clauses, early models of EDU segmentation\nfirst ran a syntactic parser, and then post-processed the output. Modern systems\ngenerally use neural sequence models supervised by the gold EDU segmentation in\ndatasets like the RST Discourse Treebank. Fig. 24.4 shows an example architecture\nsimplified from the algorithm of Lukasik et al. (2020) that predicts for each token\nwhether or not it is a break. Here the input sentence is passed through an encoder\nand then passed through a linear layer and a softmax to produce a sequence of 0s\nand 1, where 1 indicates the start of an EDU.\n\n EDU break 0 0 0 1\n\n softmax\n\n linear layer\n\n ENCODER\n Mr. Rambo says that ‚Ä¶\n\n24.2.2 RST parsing\nTools for building RST coherence structure for a discourse have long been based on\nsyntactic parsing algorithms like shift-reduce parsing (Marcu, 1999). Many modern\nRST parsers since Ji and Eisenstein (2014) draw on the neural syntactic parsers we\nsaw in Chapter 19, using representation learning to build representations for each\nspan, and training a parser to choose the correct shift and reduce actions based on\nthe gold parses in the training set.\n We‚Äôll describe the shift-reduce parser of Yu et al. (2018). The parser state consists of a stack and a queue, and produces this structure by taking a series of actions\non the states. Actions include:\n ‚Ä¢ shift: pushes the first EDU in the queue onto the stack creating a single-node\n subtree.\n ‚Ä¢ reduce(l,d): merges the top two subtrees on the stack, where l is the coherence\n relation label, and d is the nuclearity direction, d ‚àà {NN, NS, SN}.\nAs well as the pop root operation, to remove the final tree from the stack.\n Fig. 24.6 shows the actions the parser takes to build the structure in Fig. 24.5.\n 8 C HAPTER 24 ‚Ä¢ D ISCOURSE C OHERENCE\n\n elab e1 : American Telephone & Telegraph Co. said it\n e2 : will lay off 75 to 85 technicians here , effective Nov. 1.\n e3 : The workers install , maintain and repair its private branch exchanges,\n attr elab e4 : which are large intracompany telephone networks.\n e1 e2 e3 e4\n\n Figure\nFigure 24.5example\n 1: An ExampleofRST\n RSTdiscourse tree,tree,\n discourse showing four{e\n where EDUs. Figure from Yu et al. (2018).\n 1 , e2 , e3 , e4 } are EDUs, attr and elab are\ndiscourse relation labels, and arrows indicate the nuclearities of discourse relations.\n Step Stack Queue Action Relation\n\nRST discourse parsing. Other 1 studies? still adopt e1 , ediscrete\n 2 , e3 , e4\n syntax features SH proposed by statistical ? models,\n 2 e1 e2 , e3 , e4 SH ?\nfeeding them into neural 3network emodels (Braud et al., 2016; Braud et al., 2017).\n 1 , e2 e3 , e4 RD(attr,SN) ?\n The above approaches 4model syntax e1:2 trees in ane3explicit , e4 way, requiring SH discrete syntaxed parsing outputs\n 1 e2\nas inputs for RST parsing.5 These eapproaches 1:2 , e 3 may suffer\n e 4 from the error SH propagation problem.ed 1 e2 Syntax trees\nproduced by a supervised6 syntax , e3 , e4model could\n e1:2parsing ? have errors, RD(elab,NS)which may propagate ed 1 einto\n 2 discourse\nparsing models. The problem 7 e1:2 be\n could , 3:4extremely serious ? when RD(elab,SN)\n inputs of discourse parsing ed1 e2 , ed 3 e4 different\n have\ndistributions with the training 8 data eof1:4the supervised ? syntax parser. Recently, PR Zhang ed1 e2et\n , ed , e\\\n 3 e4(2017)\n al. 1:2 e3:4\n suggest\nan alternative method, which extracts syntax features from a Bi-Affine dependency parser (Dozat and\nManning, 2016), Table\n and 1: An\n the(2018). example\n method givesofcompetitive\n the transition-based\n performancessystem on relationfor RST discourse\n extraction. parsing.\n It actually\n et al.\nrepresents syntax trees implicitly, thus it can reduce the error propagation problem.\n In this work, we investigate\n The Yu the implicit\n et al. (2018)syntaxuses anfeature extraction approach\n encoder-decoder architecture, for RST\n whereparsing.\n the encoder In ad-\nThe initial\ndition, state ais\n we propose an empty\n represents\n transition-based state,\n the input andmodel\n span\n neural ofthe final\n words forand state\n this EDUs represents\n task, a full\n using aishierarchical\n which able to result. biLSTM.\n incorporate There Theare three kinds of\n various\n actionsflexibly.\nfeatures in our transition\n Wefirst\n exploitbiLSTM system:layer represents\n hierarchical bi-directionalthe words LSTMs inside an EDU, and\n (Bi-LSTMs) the second\n to encode texts,represents\n and further\n the EDU sequence. Given an input\nenhance the transition-based model with dynamic oracle. Based 1on 2the proposed sentence w , w , ..., w m , the words\n model, canwebestudy\n repre-the\n ‚Ä¢ Shift (SH),\neffectiveness sented\n of our which\n proposed as usual (by\n implicit the\n removes static embeddings,\n firstfeatures.\n syntax EDU inWe combinations\n theconduct\n queueexperiments with\n onto e stack,character embeddings\n formingRST\n on a standard or\n a ingle-node\n dis- subtree.\ncourse TreeBank (Carlsontags, or etcontextual\n al., 2003). embeddings)\n First, we resulting\n evaluate thein an input\n performance word of representation\n our proposed sequence\n transition-\n ‚Ä¢ Reduce (RD) xw1 ,(l,d),\n w , ...,which\n xthat Themerges\n xwm .model result of the\n the top two subtrees\n word-level biLSTM onthen\n is thea stack,\n sequence where is a discourse relation\n of hw lvalues:\nbased baseline, finding 2 the is able to achieve strong performances after applying dynamic\n label, and d 2 {NN, NS,\noracle. Then we evaluate the effectiveness SN} indicates the relation nuclearity (nuclear (N) or satellite\n depen- (S)).\n hw1 of\n , hw2implicit\n , ..., hwm syntax\n = biLSTM(x featuresw1 extracted\n , w2 , ..., xwmfrom\n ) a Bi-Affine (24.17)\ndency‚Ä¢ Pop\n parser.Root (PR),\n Results showwhich that thepops\n implicit outsyntax\n the top tree on\n features the stack,\n are effective, marking\n giving better the decodingthan\n performances being completed,\n An EDU of span ws , ws+1 , ..., wt then has biLSTM output representation hws , hws+1 , ..., htw ,\nexplicitwhen\n Tree-LSTM\n the stack(Li et\n holdsal., 2015b).\n only one Our codes will\n subtreepooling: be released\n and the queue is empty. for public under the Apache License\n and is represented by average\n InGiven\n summary,\n the weRSTmainly\n tree makeas shown the following\n in Figure two1, contributions\n e it can 1 be X int this\n generated work:by (1)thewe following\n propose a transition-action sequence: {SH,\nbased neural RST discourse parsing model with x\n dynamic= oracle, (2) hwew\n k compare three different syntactic (24.18)\n SH, RD(attr,SN), SH, SH, RD(elab,NS), RD(elab,SN), t ‚àí s + 1 PR}. Table 1 shows the decoding\nintegration approaches proposed by us. The rest of the paper is organized k=s\n as follows. Section 2 describes\n process in detail.\nour proposed modelsThe\n By this\n including\n way,\n second layer we naturally\n uses this input toneural\n the transition-based\n convert\n compute RST discourse\n a final\n model, therepresentation\n parsing\n dynamic oracle of the intosequence\n strategy\n predicting\n andofthe\n a sequence of\n transition\nimplicit actions,\n syntax EDU\n feature where eachapproach.\n representations\n extraction line includes\n h : Sectiona 3state\n e\n presentsand thenext step action\n experiments referringourtomodels.\n to evaluate the tree.\nSection 4 shows the related work. Finally,hsection , h , ...,5hdraws\n e e e conclusions.\n = biLSTM(x , x , ..., xe )\n e e\n (24.19)\n\n2Previous\n Transition-based\n The Discourse\n RSTParsing\n decoder is then\n transition-based a feedforward network W that outputs an action o based on a\n discourse parsing studies exploit statistical models, using manuallyconcatenation of the top three subtrees on the stack (so , s1 , s2 ) plus the first EDU in\n designed\nWe follow Jidiscrete features\n and Eisenstein\n the (q(Sagae,\n queue(2014),0 ):\n 2009;aHeilman\n exploiting and Sagae,\n transition-based framework2015; forWang et al., 2017).\n RST discourse parsing.In this work, we\nThe framework\n propose is conceptually simple\n a transition-based neuraland flexible\n model fortoRST\n support arbitraryparsing,\n discourse features, which\n which has been widely\n follows an encoder-decoder\n o =DyerW(h t , ht , ht , he ) (24.20) a\nused in a number\n framework. of NLP\n Given tasks (Zhu\n an input sequenceet al., of\n 2013;\n EDUs {eet1s0al., 2015;\n , ...,s2enZhang\n , e2s1 q0 theetencoder\n }, al., 2016). In addition,\n computes the input representransition-based model formalizes a certain task into predicting a sequence e of actions, which is essential\n tations {h1 , h2 , ...,\n e e e\n hn },\n where theand the decoder\n representation of predicts\n the EDU next on thestepqueue actions\n h comes conditioned\n directly on\n fromthetheencoder outputs.\nsimilar to sequence-to-sequence models proposed recently (Bahdanau et q0 al., 2014). In the following,\nwe first describe encoder, and the three hidden vectors representing partial trees\n the transition system for RST discourse parsing, and then introduce our neural network are computed by\n 2.2.1 Encoder average pooling over the encoder output for the EDUs in those trees:\nmodel by its encoder and decoder parts, respectively. Thirdly, we present our proposed dynamic oracle\n We follow\nstrategy Li toet enhance\n aiming al. (2016), using hierarchical\n the transition-based model.Bi-LSTMs\n Then j to encode the the source method\n EDU inputs, where the\n t 1 weX introduce integration of\n first-layer is used to represent sequencial\nimplicit syntax features. Finally we describe the training words inside\n hs = method of our of EDUs, and the second layer\n hkneural network models.(24.21)\n e is used to represent\n j‚àíi+1\n sequencial EDUs. Given an input sentence {w1 , w2 , ...,k=i wm }, first we represent each word by its form\n (e.g., wi ) and POS tag (e.g. ti ), concatenating their neural embeddings. By this way, the input vectors\nThe transition-based\n of the framework converts\n first-layer Bi-LSTM are {xw a structural learning problem\n w into a sequence ofemb(t\n action predic-\n1 , x2 , ..., xm }, where xi = emb(wi ) i ), and then we apply\n w w\ntions, whose key point is a transition system. A transition system consists of two parts: states and actions.\n Bi-LSTM directly, obtaining:\nThe states are used to store partially-parsed results and the actions are used to control state transitions.\n w w w w w w\n 24.2 ‚Ä¢ D ISCOURSE S TRUCTURE PARSING 9\n\n Training first maps each RST gold parse tree into a sequence of oracle actions, and\n then uses the standard cross-entropy loss (with l2 regularization) to train the system\n to take such actions. Give a state S and oracle action a, we first compute the decoder\n output using Eq. 24.20, apply a softmax to get probabilities:\n\n exp(oa )\n pa = P (24.22)\n exp(oa0 )\n a0 ‚ààA\n\n and then computing the cross-entropy loss:\n\n Œª\n LCE () = ‚àí log(pa ) + ||Œò||2 (24.23)\n RST discourse parsers are evaluated on the test section of the RST Discourse Treebank, either with gold EDUs or end-to-end, using the RST-Pareval metrics (Marcu,\n 2000b). It is standard to first transform the gold RST trees into right-branching binary trees, and to report four metrics: trees with no labels (S for Span), labeled\n with nuclei (N), with relations (R), or both (F for Full), for each metric computing\n micro-averaged F1 over all spans from all documents (Marcu 2000b, Morey et al.\n 2017).\n\n 24.2.3 PDTB discourse parsing\n shallow\n PDTB discourse parsing, the task of detecting PDTB coherence relations between\ndiscourse spans, is sometimes called shallow discourse parsing because the task just involves\n parsing\n flat relationships between text spans, rather than the full trees of RST parsing.\n The set of four subtasks for PDTB discourse parsing was laid out by Lin et al.\n (2014) in the first complete system, with separate tasks for explicit (tasks 1-3) and\n implicit (task 4) connectives:\n 1. Find the discourse connectives (disambiguating them from non-discourse uses)\n 2. Find the two spans for each connective\n 3. Label the relationship between these spans\n 4. Assign a relation between every adjacent pair of sentences\n Many systems have been proposed for Task 4: taking a pair of adjacent sentences\n as input and assign a coherence relation sense label as output. The setup often follows Lin et al. (2009) in assuming gold sentence span boundaries and assigning each\n adjacent span one of the 11 second-level PDTB tags or none (removing the 5 very\n rare tags of the 16 shown in italics in Fig. 24.3).\n A simple but very strong algorithm for Task 4 is to represent each of the two\n spans by BERT embeddings and take the last layer hidden state corresponding to\n the position of the [CLS] token, pass this through a single layer tanh feedforward\n network and then a softmax for sense classification (Nie et al., 2019).\n Each of the other tasks also have been addressed. Task 1 is to disambiguating discourse connectives from their non-discourse use. For example as Pitler and\n Nenkova (2009) point out, the word and is a discourse connective linking the two\n clauses by an elaboration/expansion relation in (24.24) while it‚Äôs a non-discourse\n NP conjunction in (24.25):\n (24.24) Selling picked up as previous buyers bailed out of their positions and\n aggressive short sellers‚Äîanticipating further declines‚Äîmoved in.\n (24.25) My favorite colors are blue and green.\n10 C HAPTER 24 ‚Ä¢ D ISCOURSE C OHERENCE\n\n Similarly, once is a discourse connective indicating a temporal relation in (24.26),\n but simply a non-discourse adverb meaning ‚Äòformerly‚Äô and modifying used in (24.27):\n (24.26) The asbestos fiber, crocidolite, is unusually resilient once it enters the\n lungs, with even brief exposures to it causing symptoms that show up\n decades later, researchers said.\n (24.27) A form of asbestos once used to make Kent cigarette filters has caused a\n high percentage of cancer deaths among a group of workers exposed to it\n more than 30 years ago, researchers reported.\n Determining whether a word is a discourse connective is thus a special case\n of word sense disambiguation. Early work on disambiguation showed that the 4\n PDTB high-level sense classes could be disambiguated with high (94%) accuracy\n used syntactic features from gold parse trees (Pitler and Nenkova, 2009). Recent\n work performs the task end-to-end from word inputs using a biLSTM-CRF with\n BIO outputs (B - CONN, I - CONN, O) (Yu et al., 2019).\n For task 2, PDTB spans can be identified with the same sequence models used to\n find RST EDUs: a biLSTM sequence model with pretrained contextual embedding\n (BERT) inputs (Muller et al., 2019). Simple heuristics also do pretty well as a baseline at finding spans, since 93% of relations are either completely within a single\n sentence or span two adjacent sentences, with one argument in each sentence (Biran\n and McKeown, 2015).\n\n A second way a discourse can be coherent is by virtue of being ‚Äúabout‚Äù some entity.\n This idea that at each point in the discourse some entity is salient, and a discourse\n is coherent by continuing to discuss the same entity, appears early in functional linguistics and the psychology of discourse (Chafe 1976, Kintsch and Van Dijk 1978),\n and soon made its way to computational models. In this section we introduce two\n entity-based models of this kind of entity-based coherence: Centering Theory (Grosz et al.,\n 1995), and the entity grid model of Barzilay and Lapata (2008).\n\n 24.3.1 Centering\n Centering\n Theory Centering Theory (Grosz et al., 1995) is a theory of both discourse salience and\n discourse coherence. As a model of discourse salience, Centering proposes that at\n any given point in the discourse one of the entities in the discourse model is salient:\n it is being ‚Äúcentered‚Äù on. As a model of discourse coherence, Centering proposes\n that discourses in which adjacent sentences CONTINUE to maintain the same salient\n entity are more coherent than those which SHIFT back and forth between multiple\n entities (we will see that CONTINUE and SHIFT are technical terms in the theory).\n The following two texts from Grosz et al. (1995) which have exactly the same\n propositional content but different saliences, can help in understanding the main\n Centering intuition.\n (24.28) a. John went to his favorite music store to buy a piano.\n b. He had frequented the store for many years.\n c. He was excited that he could finally buy a piano.\n d. He arrived just as the store was closing for the day.\n 24.3 ‚Ä¢ C ENTERING AND E NTITY-BASED C OHERENCE 11\n\n (24.29) a. John went to his favorite music store to buy a piano.\n b. It was a store John had frequented for many years.\n c. He was excited that he could finally buy a piano.\n d. It was closing just as John arrived.\n While these two texts differ only in how the two entities (John and the store) are\n realized in the sentences, the discourse in (24.28) is intuitively more coherent than\n the one in (24.29). As Grosz et al. (1995) point out, this is because the discourse\n in (24.28) is clearly about one individual, John, describing his actions and feelings.\n The discourse in (24.29), by contrast, focuses first on John, then the store, then back\n to John, then to the store again. It lacks the ‚Äúaboutness‚Äù of the first discourse.\n Centering Theory realizes this intuition by maintaining two representations for\n backwardlooking each utterance Un . The backward-looking center of Un , denoted as Cb (Un ), repcenter\n resents the current salient entity, the one being focused on in the discourse after Un\nforward-looking\n center\n is interpreted. The forward-looking centers of Un , denoted as C f (Un ), are a set\n of potential future salient entities, the discourse entities evoked by Un any of which\n could serve as Cb (the salient entity) of the following utterance, i.e. Cb (Un+1 ).\n The set of forward-looking centers C f (Un ) are ranked according to factors like\n discourse salience and grammatical role (for example subjects are higher ranked\n than objects, which are higher ranked than all other grammatical roles). We call the\n highest-ranked forward-looking center C p (for ‚Äúpreferred center‚Äù). C p is a kind of\n prediction about what entity will be talked about next. Sometimes the next utterance\n indeed talks about this entity, but sometimes another entity becomes salient instead.\n We‚Äôll use here the algorithm for centering presented in Brennan et al. (1987),\n which defines four intersentential relationships between a pair of utterances Un and\n Un+1 that depend on the relationship between Cb (Un+1 ), Cb (Un ), and C p (Un+1 );\n these are shown in Fig. 24.7.\n\n Cb (Un+1 ) = Cb (Un ) Cb (Un+1 ) 6= Cb (Un )\n or undefined Cb (Un )\n Cb (Un+1 ) = C p (Un+1 ) Continue Smooth-Shift\n Cb (Un+1 ) 6= C p (Un+1 ) Retain Rough-Shift\n\n The following rules are used by the algorithm:\n\n Rule 1: If any element of C f (Un ) is realized by a pronoun in utterance\n Un+1 , then Cb (Un+1 ) must be realized as a pronoun also.\n Rule 2: Transition states are ordered. Continue is preferred to Retain is\n preferred to Smooth-Shift is preferred to Rough-Shift.\n\n Rule 1 captures the intuition that pronominalization (including zero-anaphora)\n is a common way to mark discourse salience. If there are multiple pronouns in an\n utterance realizing entities from the previous utterance, one of these pronouns must\n realize the backward center Cb ; if there is only one pronoun, it must be Cb .\n Rule 2 captures the intuition that discourses that continue to center the same entity are more coherent than ones that repeatedly shift to other centers. The transition\n table is based on two factors: whether the backward-looking center Cb is the same\n from Un to Un+1 and whether this discourse entity is the one that is preferred (C p )\n in the new utterance Un+1 . If both of these hold, a CONTINUE relation, the speaker\n has been talking about the same entity and is going to continue talking about that\n12 C HAPTER 24 ‚Ä¢ D ISCOURSE C OHERENCE\n\n entity. In a RETAIN relation, the speaker intends to SHIFT to a new entity in a future\n utterance and meanwhile places the current entity in a lower rank C f . In a SHIFT\n relation, the speaker is shifting to a new salient entity.\n Let‚Äôs walk though the start of (24.28) again, repeated as (24.30), showing the\n representations after each utterance is processed.\n (24.30) John went to his favorite music store to buy a piano. (U1 )\n He was excited that he could finally buy a piano. (U2 )\n He arrived just as the store was closing for the day. (U3 )\n It was closing just as John arrived (U4 )\n Using the grammatical role hierarchy to order the C f , for sentence U1 we get:\n C f (U1 ): {John, music store, piano}\n C p (U1 ): John\n Cb (U1 ): undefined\n and then for sentence U2 :\n C f (U2 ): {John, piano}\n C p (U2 ): John\n Cb (U2 ): John\n Result: Continue (C p (U2 )=Cb (U2 ); Cb (U1 ) undefined)\n The transition from U1 to U2 is thus a CONTINUE. Completing this example is left\n as exercise (1) for the reader\n\n 24.3.2 Entity Grid model\n Centering embodies a particular theory of how entity mentioning leads to coherence: that salient entities appear in subject position or are pronominalized, and that\n discourses are salient by means of continuing to mention the same entity in such\n ways.\n entity grid The entity grid model of Barzilay and Lapata (2008) is an alternative way to\n capture entity-based coherence: instead of having a top-down theory, the entity-grid\n model using machine learning to induce the patterns of entity mentioning that make\n a discourse more coherent.\n The model is based around an entity grid, a two-dimensional array that represents the distribution of entity mentions across sentences. The rows represent sentences, and the columns represent discourse entities (most versions of the entity grid\n model focus just on nominal mentions). Each cell represents the possible appearance\n of an entity in a sentence, and the values represent whether the entity appears and its\n grammatical role. Grammatical roles are subject (S), object (O), neither (X), or absent (‚Äì); in the implementation of Barzilay and Lapata (2008), subjects of passives\n are represented with O, leading to a representation with some of the characteristics\n of thematic roles.\n Fig. 24.8 from Barzilay and Lapata (2008) shows a grid for the text shown in\n Fig. 24.9. There is one row for each of the six sentences. The second column, for\n the entity ‚Äòtrial‚Äô, is O ‚Äì ‚Äì ‚Äì X, showing that the trial appears in the first sentence as\n direct object, in the last sentence as an oblique, and does not appear in the middle\n sentences. The third column, for the entity Microsoft, shows that it appears as subject in sentence 1 (it also appears as the object of the preposition against, but entities\n that appear multiple times are recorded with their highest-ranked grammatical function). Computing the entity grids requires extracting entities and doing coreference\n present in sentences 1 and 6 (as O and X, respectively) but is absent from the rest of the\n sentences. Also note that the grid in Table 1 takes coreference resolution into account.\n Even though the same entity appears in different linguistic forms, for example, Microsoft\n Corp., Microsoft, and the company , it is mapped to a single entry in the grid (see the\n column introduced by Microsoft in Table 1).\n Computational Linguistics Volume 34, Number 1\n\n a feature space with\n 24.3transitions\n ‚Ä¢ C\n Table\n of length two is illustrated in Table 3. The second13row\n 1 ENTERING AND E NTITY-BASED C OHERENCE\n (introduced by d1 ) is the\n A feature\n fragment vector representation\n of the entity of thearegrid\n grid. Noun phrases in Table\n represented by1.\n their head nouns. Grid cells\n correspond to grammatical roles: subjects (S), objects (O), or neither (X).\n\n Government\n Competitors\n Department\n\n Microsoft\n\n Netscape\n Evidence\n\n Earnings\n Products\n\n Software\n Markets\n\n Brands\n\n Tactics\n Case\n Trial\n\n Suit\n One of the central research issues in developing entity-based models of coherence is\n determining what sources 1 S ofO linguistic\n S X O ‚Äì ‚Äìknowledge\n ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì are ‚Äì ‚Äìessential\n ‚Äì 1 for accurate prediction,\n and how to encode them 2 succinctly\n ‚Äì ‚Äì O ‚Äì ‚Äì in X a ‚Äì ‚Äì ‚Äì 2\n O ‚Äì ‚Äì ‚Äì ‚Äì representation.\n S discourse Previous approaches\n 3 ‚Äì ‚Äì S O ‚Äì ‚Äì ‚Äì ‚Äì S O O ‚Äì ‚Äì ‚Äì ‚Äì 3\n tend to agree on the 4features ‚Äì ‚Äì S of ‚Äì ‚Äìentity\n ‚Äì ‚Äì ‚Äì distribution\n ‚Äì ‚Äì ‚Äì S ‚Äì ‚Äì related\n ‚Äì 4 to local coherence‚Äîthe\n disagreement\n Barzilay lies in the5 way\n and Lapata ‚Äì ‚Äì these\n ‚Äì ‚Äì ‚Äìfeatures\n ‚Äì ‚Äì ‚Äì ‚Äìare ‚Äì modeled.\n ‚Äì ‚Äì S O ‚Äì 5 Modeling Local Coherence\n Our study of alternative 6 ‚Äì X S encodings\n ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì is ‚Äì ‚Äìnot ‚Äì O 6duplication of previous ef-\n ‚Äì ‚Äìa ‚Äìmere\n forts (Poesio\nFigure 24.8 Part et al.\n of2004) that grid\n the entity focusforonthe\n linguistic aspects\n text in Fig. 24.9. of parameterization.\n Entities Because\n are listed by their head we\n are interested in an automatically\nnoun; each cell represents whether constructed model, we have to take into account\n an entity appears as subject (S), object (O), neither (X), or computational and learning issues when\nis absent (‚Äì). Figure from Barzilay and Lapata (2008).\n Table 2 considering alternative representations. Therefore,\n our exploration\n Summary augmented of thewithparameter space is guided\n syntactic annotations bycomputation.\n for grid three considerations: the linguistic\n importance of a parameter, the accuracy of its automatic computation, and the size of the\n 1resulting\n [The Justice Department]\n feature space. From S is conducting an [anti-trust trial] O against [Microsoft Corp.] X\n the linguistic side, we focus on properties of entity distriwith [evidence]X that [the company]S is increasingly attempting to crush [competitors]O .\n bution that are tightly linked\n 2 [Microsoft]O is accused of trying to to\n local coherence,\n forcefully and[markets]\n buy into at the same time allow for multiple\n X where [its own\n interpretations\n products]S are notduring the encoding\n competitive enoughprocess.\n to unseatComputational considerations\n [established brands] O.\n prevent us\n 3from\n [Theconsidering\n case]S revolves discourse representations\n around [evidence] that cannot\n O of [Microsoft] be computed\n S aggressively reliably by existpressuring\n [Netscape]\n ing tools. For O into merging\n instance, we[browser\n could not software] O.\n experiment with the granularity of an utterance‚Äî\n 4sentence\n [Microsoft] S claims [its tactics] S are commonplace and good economically.\n versus clause‚Äîbecause available clause separators introduce substantial noise\n 5 [The government]S may file [a civil suit]O ruling that [conspiracy]S to curb [competition]O\n into a grid[collusion]\n through construction. Finally, we exclude representations that will explode the size of\n X is [a violation of the Sherman Act] O .\n 6the feature space,\n [Microsoft] S\n thereby\n continues to increasing\n show the earnings]\n [increased amount Oofdespite\n data required\n [the trial]for\n X.\n training the model.\n\nFigure\n Entity24.9 A discourse\n Ex traction. with thecomputation\n The accurate entities marked and annotated\n of entity classes iswithkey grammatical\n to computing funcmean-\ntions.\n ingfulFigure\n When entity from\n a noun Barzilay\n grids. is and Lapata\n Inattested\n previous more (2008).\n than once with\n implementations a different grammatical\n of entity-based models, classes roleof in the\n corefsame\n erent sentence,\n nouns have we default\n been extractedto the role with the (Miltsakaki\n manually highest grammatical\n and Kukich ranking:\n 2000; subjects\n Karamanis are\n ranked\nresolution\n et al. 2004;higher\n to clusterthan\n Poesio them objects,\n et al. into which\n 2004), in\n discourse turn are\n but thisentities ranked\n is not (Chapter higher\n an option23) than\n forasour the\n well rest. For\n as parsing\n model. example,\n the\n An obvious\n the entityto\n solution\nsentences forMicrosoft\n identifying\n get is mentioned\n grammatical entity twiceisintoSentence\n classes\n roles. employ an 1 with the grammatical\n automatic coreferenceroles x (for\n resolution\n Microsoft\n tool that Corp.) and s\n determines (for the\n which noun company\n phrases ), but\n refer istorepresented\n the same only in\n entity bya sdocument.\n in the grid (see\n In the1 and\n Tables\n resulting grid, columns that are dense (like the column for Microsoft) in-\nCurrent 2). approaches recast coreference resolution as a classification task. A pair\ndicate entities that are mentioned often in the texts; sparse columns (like the column\n of NPs is classified as coreferring or not based on constraints that are learned from\nforanearnings)\n annotated indicate\n corpus. entities\n A separate that are mentioned rarely.\n In the entity pairwise\n contradictory grid model, coherence is\n classifications andmeasured\n constructs by apatterns\n partition of local\n on theentity\n set oftran-NPs. In\nsition. For example, Department Ng is\n and a subject\n Cardie‚Äôs in sentence\n (2002)\n A fundamental assumption underlying our approach is that the distribution of system.\n our experiments, we employ 1,\n coreference and then not\n resolution menentities\ntioned\n The\n in in sentence\n system\n coherent decides\n texts 2; this\n whether\n exhibits thetwo\n iscertain ‚Äì].\n NPs are[ Scoreferent\n transition\n regularities The transitions\n reflected by\n in exploiting are athus\n grid topology. sequences\n wealth\n Some ofoflexical,\n these\n grammatical,\n , O X , ‚Äì}n which\n{Sregularities aresemantic,\n can be\n formalized and positional\n extracted\n in as features.\n Centering continuous\n Theory It is trained\n ascells on the\n from\n constraints each MUC\n on (6‚Äì7) data\n column.\n transitions Eachof sets\n the\n and yields\ntransition\n local focus hasstate-of-the-art\n ina adjacent\n probability; performance\n sentences. Grids (70.4\n the probability of of F-measure\n [ S ‚Äì] intexts\n coherent fromand\n MUC-6\n theongrid\n are likely Fig.\n to 63.4\n 24.8\n have onisMUC-7).\n some 0.08dense\n(itcolumns\n occurs 6(i.e., times columns\n out of the with 75just\n totala transitions\n few gaps, such as Microsoft\n of length two). Fig. in 24.10\n Table shows\n 1) and the many\ndistribution over transitions of length 2 for the text of Fig. 24.9 (shown as the first 1).\n sparse columns which will consist mostly of gaps (see markets and earnings in Table\n One\nrow d1would\n Table ),3 and 2further expect that entities corresponding to dense columns are more often\n other documents.\n subjects\n Example or of aobjects. These characteristics\n feature-vector document representationwill be less pronounced\n using all transitionsin low-coherence\n of length two given texts.\n syntactic\n Inspiredcategories S , O , X , and\n by Centering ‚Äì.\n Theory, our analysis revolves around patterns of local entity\n transitions. A local entity transition is a sequence {S, O, X, ‚Äì}n that represents entity\n SS SO SX S‚Äì OS OO OX O‚Äì XS XO XX X‚Äì ‚ÄìS ‚ÄìO ‚ÄìX ‚Äì‚Äì\n occurrences and their syntactic roles in n adjacent sentences. Local transitions can be\n d1 .01\n easily .01 0 from.08\n obtained a grid .01as0continuous\n 0 .09subsequences\n 0 0 0 of each\n .03 column.\n .05 .07 Each .03 transition\n .59\n d2 have\n will .02 .01 .01 .02\n a certain 0\n probability .07 in0 a given\n .02 grid.\n .14 For .14 instance,\n .06 .04 .03 .07 0.1 .36of the\n the probability\n d3 .02 0 0 .03 .09 0 .09 .06 0 0 0 .05 .03 .07 .17 .39\n transition [ S ‚Äì] in the grid from Table 1 is 0.08 (computed as a ratio of its frequency\n [i.e., six]\nFigure 24.10 divided by the\n A feature totalfor\n vector number of transitions\n representing documentsofusing length all two [i.e., 75]).\n transitions Each2.text\n of length\n can thusdbe\nDocument 1 isviewed\n the textas inaFig.\n distribution\n 24.9. Figure defined over transition\n from Barzilay and Lapatatypes.(2008).\n We can now go one step further and represent each text by a fixed set of transition\n sequences using a standard feature vector notation. Each grid rendering j of a document\n The transitions and their probabilities can then be used as features for a machine\n di corresponds to a feature vector Œ¶(x ij ) = (p1 (x ij ), p2 (x ij ), . . . , pm (x ij )), where m is the\nlearning model. This model can be a text classifier trained to produce human-labeled\n number of all predefined entity transitions, and pt (x ij ) the probability of transition t\ncoherence\n in grid x ijscores (for example\n . This feature vector from humans labeling\n representation is usefully each text as coherent\n amenable to machine or learning\n incoherent).\n algorithms But such (see data is expensive in\n our experiments to gather.\n SectionsBarzilay and Lapata (2005)\n 4‚Äì6). Furthermore, it allows introduced\n the consida eration\n simplifying of large innovation:\n numbers of coherence\n transitions models\n whichcan couldbe potentially\n trained by uncoverself-supervision:\n novel entity\n distribution\ntrained patterns relevant\n to distinguish the natural for coherence\n original order assessment or other in\n of sentences coherence-related\n a discourse from tasks.\n Note that considerable latitude is available when specifying the transition types to\n be included in a feature vector. These can be all transitions of a given length (e.g., two\n or three) or the most frequent transitions within a document collection. An example of\n\n14 C HAPTER 24 ‚Ä¢ D ISCOURSE C OHERENCE\n\n a modified order (such as a randomized order). We turn to these evaluations in the\n next section.\n\n 24.3.3 Evaluating Neural and Entity-based coherence\n Entity-based coherence models, as well as the neural models we introduce in the\n next section, are generally evaluated in one of two ways.\n First, we can have humans rate the coherence of a document and train a classifier\n to predict these human ratings, which can be categorial (high/low, or high/mid/low)\n or continuous. This is the best evaluation to use if we have some end task in mind,\n like essay grading, where human raters are the correct definition of the final label.\n Alternatively, since it‚Äôs very expensive to get human labels, and we might not\n yet have an end-task in mind, we can use natural texts to do self-supervision. In\n self-supervision we pair up a natural discourse with a pseudo-document created by\n changing the ordering. Since naturally-ordered discourses are more coherent than\n random permutation (Lin et al., 2011), a successful coherence algorithm should prefer the original ordering.\n Self-supervision has been implemented in 3 ways. In the sentence order discrimination task (Barzilay and Lapata, 2005), we compare a document to a random\n permutation of its sentences. A model is considered correct for an (original, permuted) test pair if it ranks the original document higher. Given k documents, we can\n compute n permutations, resulting in kn pairs each with one original document and\n one permutation, to use in training and testing.\n In the sentence insertion task (Chen et al., 2007) we take a document, remove\n one of the n sentences s, and create n ‚àí 1 copies of the document with s inserted into\n each position. The task is to decide which of the n documents is the one with the\n original ordering, distinguishing the original position for s from all other positions.\n Insertion is harder than discrimination since we are comparing documents that differ\n by only one sentence.\n Finally, in the sentence order reconstruction task (Lapata, 2003), we take a\n document, randomize the sentences, and train the model to put them back in the\n correct order. Again given k documents, we can compute n permutations, resulting\n in kn pairs each with one original document and one permutation, to use in training\n and testing. Reordering is of course a much harder task than simple classification.\n\n The third kind of local coherence is topical or semantic field coherence. Discourses\n cohere by talking about the same topics and subtopics, and drawing on the same\n semantic fields in doing so.\n The field was pioneered by a series of unsupervised models in the 1990s of this\nlexical cohesion kind of coherence that made use of lexical cohesion (Halliday and Hasan, 1976):\n the sharing of identical or semantically related words in nearby sentences. Morris\n and Hirst (1991) computed lexical chains of words (like pine, bush trees, trunk) that\n occurred through a discourse and that were related in Roget‚Äôs Thesaurus (by being in\n the same category, or linked categories). They showed that the number and density\n TextTiling of chain correlated with the topic structure. The TextTiling algorithm of Hearst\n (1997) computed the cosine between neighboring text spans (the normalized dot\n product of vectors of raw word counts), again showing that sentences or paragraph in\n24.4 ‚Ä¢ R EPRESENTATION LEARNING MODELS FOR LOCAL COHERENCE 15\n\na subtopic have high cosine with each other, but not with sentences in a neighboring\nsubtopic.\n A third early model, the LSA Coherence method of Foltz et al. (1998) was the\nfirst to use embeddings, modeling the coherence between two sentences as the cosine between their LSA sentence embedding vectors1 , computing embeddings for a\nsentence s by summing the embeddings of its words w:\n sim(s,t) = cos(s, t)\n X X\n = cos( w, w) (24.31)\n w‚ààs w‚ààt\n\nand defining the overall coherence of a text as the average similarity over all pairs of\nadjacent sentences si and si+1 :\n n‚àí1\n 1 X\n coherence(T ) = cos(si , si+1 ) (24.32)\n n‚àí1\n i=1\n\n Modern neural representation-learning coherence models, beginning with Li et al.\n(2014), draw on the intuitions of these early unsupervised models for learning sentence representations and measuring how they change between neighboring sentences. But the new models also draw on the idea pioneered by Barzilay and Lapata\n(2005) of self-supervision. That is, unlike say coherence relation models, which\ntrain on hand-labeled representations for RST or PDTB, these models are trained to\ndistinguish natural discourses from unnatural discourses formed by scrambling the\norder of sentences, thus using representation learning to discover the features that\nmatter for at least the ordering aspect of coherence.\n Here we present one such model, the local coherence discriminator (LCD) (Xu\net al., 2019). Like early models, LCD computes the coherence of a text as the average of coherence scores between consecutive pairs of sentences. But unlike the\nearly unsupervised models, LCD is a self-supervised model trained to discriminate\nconsecutive sentence pairs (si , si+1 ) in the training documents (assumed to be coherent) from (constructed) incoherent pairs (si , s0 ). All consecutive pairs are positive\nexamples, and the negative (incoherent) partner for a sentence si is another sentence\nuniformly sampled from the same document as si .\n Fig. 24.11 describes the architecture of the model fŒ∏ , which takes a sentence\npair and returns a score, higher scores for more coherent pairs. Given an input\nsentence pair s and t, the model computes sentence embeddings s and t (using any\nsentence embeddings algorithm), and then concatenates four features of the pair: (1)\nthe concatenation of the two vectors (2) their difference s ‚àí t; (3) the absolute value\nof their difference |s ‚àí t|; (4) their element-wise product s t. These are passed\nthrough a one-layer feedforward network to output the coherence score.\n The model is trained to make this coherence score higher for real pairs than for\nnegative pairs. More formally, the training objective for a corpus C of documents d,\neach of which consists of a list of sentences si , is:\n XX\n LŒ∏ = E [L( fŒ∏ (si , si+1 ), fŒ∏ (si , s ))] (24.33)\n p(s0 |si )\n d‚ààC si ‚ààd\n\nE p(s0 |si ) is the expectation with respect to the negative sampling distribution conditioned on si : given a sentence si the algorithms samples a negative sentence s0\n1 See Chapter 5 for more on LSA embeddings; they are computed by applying SVD to the termdocument matrix (each cell weighted by log frequency and normalized by entropy), and then the first\n300 dimensions are used as the embedding.\n16 C HAPTER 24 ‚Ä¢ D ISCOURSE C OHERENCE\n\n computation of the score for a pair of sentences s and t. Figure from Xu et al. (2019).\n\n uniformly over the other sentences in the same document. L is a loss function that\n takes two scores, one for a positive pair and one for a negative pair, with the goal of\n encouraging f + = fŒ∏ (si , si+1 ) to be high and f ‚àí = fŒ∏ (si , s0 )) to be low. Fig. 24.11\n use the margin loss l( f + , f ‚àí ) = max(0, Œ∑ ‚àí f + + f ‚àí ) where Œ∑ is the margin hyperparameter.\n Xu et al. (2019) also give a useful baseline algorithm that itself has quite high\n performance in measuring perplexity: train an RNN language model on the data,\n and compute the log likelihood of sentence si in two ways, once given the preceding\n context (conditional log likelihood) and once with no context (marginal log likelihood). The difference between these values tells us how much the preceding context\n improved the predictability of si , a predictability measure of coherence.\n Training models to predict longer contexts than just consecutive pairs of sentences can result in even stronger discourse representations. For example a Transformer language model trained with a contrastive sentence objective to predict text\n up to a distance of ¬±2 sentences improves performance on various discourse coherence tasks (Iter et al., 2020).\n Language-model style models are generally evaluated by the methods of Section 24.3.3, although they can also be evaluated on the RST and PDTB coherence\n relation tasks.\n\n A discourse must also cohere globally rather than just at the level of pairs of sentences. Consider stories, for example. The narrative structure of stories is one of\n the oldest kinds of global coherence to be studied. In his influential Morphology of\n the Folktale, Propp (1968) models the discourse structure of Russian folktales via\n a kind of plot grammar. His model includes a set of character categories he called\n dramatis personae, like Hero, Villain, Donor, or Helper, and a set of events he\n called functions (like ‚ÄúVillain commits kidnapping‚Äù, ‚ÄúDonor tests Hero‚Äù, or ‚ÄúHero\n 24.5 ‚Ä¢ G LOBAL C OHERENCE 17\n\n is pursued‚Äù) that have to occur in particular order, along with other components.\n Propp shows that the plots of each of the fairy tales he studies can be represented as\n a sequence of these functions, different tales choosing different subsets of functions,\n but always in the same order. Indeed Lakoff (1972) showed that Propp‚Äôs model\n amounted to a discourse grammar of stories, and in recent computational work Finlayson (2016) demonstrates that some of these Proppian functions could be induced\n from corpora of folktale texts by detecting events that have similar actions across\n stories. Bamman et al. (2013) showed that generalizations over dramatis personae\n could be induced from movie plot summaries on Wikipedia. Their model induced\n latent personae from features like the actions the character takes (e.g., Villains strangle), the actions done to them (e.g., Villains are foiled and arrested) or the descriptive\n words used of them (Villains are evil).\n In this section we introduce two kinds of such global discourse structure that\n have been widely studied computationally. The first is the structure of arguments:\n the way people attempt to convince each other in persuasive essays by offering\n claims and supporting premises. The second is somewhat related: the structure of\n scientific papers, and the way authors present their goals, results, and relationship to\n prior work in their papers.\n\n 24.5.1 Argumentation Structure\n The first type of global discourse structure is the structure of arguments. Analyzing\nargumentation\n mining people‚Äôs argumentation computationally is often called argumentation mining.\n The study of arguments dates back to Aristotle, who in his Rhetorics described\n pathos three components of a good argument: pathos (appealing to the emotions of the\n ethos listener), ethos (appealing to the speaker‚Äôs personal character), and logos (the logical\n logos structure of the argument).\n Most of the discourse structure studies of argumentation have focused on logos,\n particularly via building and training on annotated datasets of persuasive essays or\n other arguments (Reed et al. 2008, Stab and Gurevych 2014a, Peldszus and Stede\n 2016, Habernal and Gurevych 2017, Musi et al. 2018). Such corpora, for examclaims ple, often include annotations of argumentative components like claims (the central\n premises component of the argument that is controversial and needs support) and premises\n (the reasons given by the author to persuade the reader by supporting or attacking\nargumentative the claim or other premises), as well as the argumentative relations between them\n relations\n like SUPPORT and ATTACK.\n Consider the following example of a persuasive essay from Stab and Gurevych\n (2014b). The first sentence (1) presents a claim (in bold). (2) and (3) present two\n premises supporting the claim. (4) gives a premise supporting premise (3).\n ‚Äú(1) Museums and art galleries provide a better understanding\n about arts than Internet. (2) In most museums and art galleries, detailed descriptions in terms of the background, history and author are\n provided. (3) Seeing an artwork online is not the same as watching it\n with our own eyes, as (4) the picture online does not show the texture\n or three-dimensional structure of the art, which is important to study.‚Äù\n Thus this example has three argumentative relations: SUPPORT(2,1), SUPPORT(3,1)\n and SUPPORT(4,3). Fig. 24.12 shows the structure of a much more complex argument.\n While argumentation mining is clearly related to rhetorical structure and other\n kinds of coherence relations, arguments tend to be much less local; often a persuamilitary purposes]Claim6 , I strongly believe that [this technology is beneficial to\n humanity]MajorClaim2 . It is likely that [this technology bears some important cures which\n will significantly improve life conditions]Claim7 .\n The conclusion of the essay starts with an attacking claim followed by the restatement of\n the major claim. The last sentence includes another claim that summarizes the most important points of the author‚Äôs argumentation. Figure 2 shows the entire argumentation\n structure of the example essay.\n18 C HAPTER 24 ‚Ä¢ D ISCOURSE C OHERENCE\n\n Figure\nFigure 2\n Argumentation structure of the example essay. Arrows indicate argumentative relations.\nther of SUPPORT (with arrowheads) or ATTACK (with circleheads); P denotes premises. Figure from Stab and\n Arrowheads denote argumentative support relations and circleheads attack relations. Dashed\nGurevych (2017).relations that are encoded in the stance attributes of claims. ‚ÄúP‚Äù denotes premises.\n lines indicate\n\n sive essay will have only a single main claim, with premises spread throughout 629the\n text, without the local coherence we see in coherence relations.\n Algorithms for detecting argumentation structure often include classifiers for\n distinguishing claims, premises, or non-argumentation, together with relation classifiers for deciding if two spans have the SUPPORT, ATTACK, or neither relation\n (Peldszus and Stede, 2013). While these are the main focus of much computational\n work, there is also preliminary efforts on annotating and detecting richer semantic\n relationships (Park and Cardie 2014, Hidey et al. 2017) such as detecting argumenargumentation tation schemes, larger-scale structures for argument like argument from example,\n schemes\n or argument from cause to effect, or argument from consequences (Feng and\n Hirst, 2011).\n Another important line of research is studying how these argument structure (or\n other features) are associated with the success or persuasiveness of an argument\n (Habernal and Gurevych 2016, Tan et al. 2016, Hidey et al. 2017. Indeed, while it\n is Aristotle‚Äôs logos that is most related to discourse structure, Aristotle‚Äôs ethos and\n pathos techniques are particularly relevant in the detection of mechanisms of this\n persuasion sort of persuasion. For example scholars have investigated the linguistic realization\n of features studied by social scientists like reciprocity (people return favors), social\n proof (people follow others‚Äô choices), authority (people are influenced by those\n with power), and scarcity (people value things that are scarce), all of which can\n be brought up in a persuasive argument (Cialdini, 1984). Rosenthal and McKeown\n (2017) showed that these features could be combined with argumentation structure\n to predict who influences whom on social media, Althoff et al. (2014) found that\n linguistic models of reciprocity and authority predicted success in online requests,\n while the semisupervised model of Yang et al. (2019) detected mentions of scarcity,\n commitment, and social identity to predict the success of peer-to-peer lending platforms.\n See Stede and Schneider (2018) for a comprehensive survey of argument mining.\n 24.6 ‚Ä¢ S UMMARY 19\n\n 24.5.2 The structure of scientific discourse\n Scientific papers have a very specific global structure: somewhere in the course of\n the paper the authors must indicate a scientific goal, develop a method for a solution, provide evidence for the solution, and compare to prior work. One popular\n annotation scheme for modeling these rhetorical goals is the argumentative zonargumentative\n zoning ing model of Teufel et al. (1999) and Teufel et al. (2009), which is informed by the\n idea that each scientific paper tries to make a knowledge claim about a new piece\n of knowledge being added to the repository of the field (Myers, 1992). Sentences\n in a scientific paper can be assigned one of 15 tags; Fig. 24.13 shows 7 (shortened)\n examples of labeled sentences.\n\nCategory Description Example\nA IM Statement of specific research goal, or ‚ÄúThe aim of this process is to examine the role that\n hypothesis of current paper training plays in the tagging process‚Äù\nOWN M ETHOD New Knowledge claim, own work: ‚ÄúIn order for it to be useful for our purposes, the\n methods following extensions must be made:‚Äù\nOWN R ESULTS Measurable/objective outcome of own ‚ÄúAll the curves have a generally upward trend but\n work always lie far below backoff (51% error rate)‚Äù\nU SE Other work is used in own work ‚ÄúWe use the framework for the allocation and\n transfer of control of Whittaker....‚Äù\nG AP W EAK Lack of solution in field, problem with ‚ÄúHere, we will produce experimental evidence\n other solutions suggesting that this simple model leads to serious\n overestimates‚Äù\nS UPPORT Other work supports current work or is ‚ÄúWork similar to that described here has been carsupported by current work ried out by Merialdo (1994), with broadly similar\n conclusions.‚Äù\nA NTISUPPORT Clash with other‚Äôs results or theory; su- ‚ÄúThis result challenges the claims of...‚Äù\n periority of own work\n\n Teufel et al. (1999) and Teufel et al. (2009) develop labeled corpora of scientific\n articles from computational linguistics and chemistry, which can be used as supervision for training standard sentence-classification architecture to assign the 15 labels.\n\n In this chapter we introduced local and global models for discourse coherence.\n ‚Ä¢ Discourses are not arbitrary collections of sentences; they must be coherent.\n Among the factors that make a discourse coherent are coherence relations\n between the sentences, entity-based coherence, and topical coherence.\n ‚Ä¢ Various sets of coherence relations and rhetorical relations have been proposed. The relations in Rhetorical Structure Theory (RST) hold between\n spans of text and are structured into a tree. Because of this, shift-reduce\n and other parsing algorithms are generally used to assign these structures.\n The Penn Discourse Treebank (PDTB) labels only relations between pairs of\n spans, and the labels are generally assigned by sequence models.\n ‚Ä¢ Entity-based coherence captures the intuition that discourses are about an\n entity, and continue mentioning the entity from sentence to sentence. Centering Theory is a family of models describing how salience is modeled for\n20 C HAPTER 24 ‚Ä¢ D ISCOURSE C OHERENCE\n\n discourse entities, and hence how coherence is achieved by virtue of keeping\n the same discourse entities salient over the discourse. The entity grid model\n gives a more bottom-up way to compute which entity realization transitions\n lead to coherence.\n ‚Ä¢ Many different genres have different types of global coherence. Persuasive\n essays have claims and premises that are extracted in the field of argument\n mining, scientific articles have structure related to aims, methods, results, and\n comparisons.\n\nHistorical Notes\n Coherence relations arose from the independent development of a number of scholars, including Hobbs (1979) idea that coherence relations play an inferential role for\n the hearer, and the investigations by Mann and Thompson (1987) of the discourse\n structure of large texts. Other approaches to coherence relations and their extrac-\nSDRT tion include Segmented Discourse Representation Theory (SDRT) (Asher and Lascarides 2003, Baldridge et al. 2007) and the Linguistic Discourse Model (Polanyi\n 1988, Scha and Polanyi 1988, Polanyi et al. 2004). Wolf and Gibson (2005) argue\n that coherence structure includes crossed bracketings, which make it impossible to\n represent as a tree, and propose a graph representation instead. A compendium of\n over 350 relations that have been proposed in the literature can be found in Hovy\n (1990).\n RST parsing was first proposed by Marcu (1997), and early work was rule-based,\n focused on discourse markers (Marcu, 2000a). The creation of the RST Discourse\n TreeBank (Carlson et al. 2001, Carlson and Marcu 2001) enabled a wide variety\n of machine learning algorithms, beginning with the shift-reduce parser of Marcu\n (1999) that used decision trees to choose actions, and continuing with a wide variety\n of machine learned parsing methods (Soricut and Marcu 2003, Sagae 2009, Hernault\n et al. 2010, Feng and Hirst 2014, Surdeanu et al. 2015, Joty et al. 2015) and chunkers\n (Sporleder and Lapata, 2005). Subba and Di Eugenio (2009) integrated sophisticated\n semantic information into RST parsing. Ji and Eisenstein (2014) first applied neural\n models to RST parsing neural models, leading to the modern set of neural RST\n models (Li et al. 2014, Li et al. 2016, Braud et al. 2017, Yu et al. 2018, inter alia) as\n well as neural segmenters (Wang et al. 2018). and neural PDTB parsing models (Ji\n and Eisenstein 2015, Qin et al. 2016, Qin et al. 2017).\n Barzilay and Lapata (2005) pioneered the idea of self-supervision for coherence: training a coherence model to distinguish true orderings of sentences from\n random permutations. Li et al. (2014) first applied this paradigm to neural sentencerepresentation, and many neural self-supervised models followed (Li and Jurafsky\n 2017, Logeswaran et al. 2018, Lai and Tetreault 2018, Xu et al. 2019, Iter et al.\n 2020)\n Another aspect of global coherence is the global topic structure of a text, the way\n the topics shift over the course of the document. Barzilay and Lee (2004) introduced\n an HMM model for capturing topics for coherence, and later work expanded this\n intuition (Soricut and Marcu 2006, Elsner et al. 2007, Louis and Nenkova 2012, Li\n and Jurafsky 2017).\n The relationship between explicit and implicit discourse connectives has been\n a fruitful one for research. Marcu and Echihabi (2002) first proposed to use sen-\nH ISTORICAL N OTES 21\n\ntences with explicit relations to help provide training data for implicit relations, by\nremoving the explicit relations and trying to re-predict them as a way of improving performance on implicit connectives; this idea was refined by Sporleder and\nLascarides (2005), (Pitler et al., 2009), and Rutherford and Xue (2015). This relationship can also be used as a way to create discourse-aware representations. The\nDisSent algorithm (Nie et al., 2019) creates the task of predicting explicit discourse\nmarkers between two sentences. They show that representations learned to be good\nat this task also function as powerful sentence representations for other discourse\ntasks.\n The idea of entity-based coherence seems to have arisen in multiple fields in the\nmid-1970s, in functional linguistics (Chafe, 1976), in the psychology of discourse\nprocessing (Kintsch and Van Dijk, 1978), and in the roughly contemporaneous work\nof Grosz, Sidner, Joshi, and their colleagues. Grosz (1977) addressed the focus of\nattention that conversational participants maintain as the discourse unfolds. She defined two levels of focus; entities relevant to the entire discourse were said to be in\nglobal focus, whereas entities that are locally in focus (i.e., most central to a particular utterance) were said to be in immediate focus. Sidner 1979; 1983 described a\nmethod for tracking (immediate) discourse foci and their use in resolving pronouns\nand demonstrative noun phrases. She made a distinction between the current discourse focus and potential foci, which are the predecessors to the backward- and\nforward-looking centers of Centering theory, respectively. The name and further\nroots of the centering approach lie in papers by Joshi and Kuhn (1979) and Joshi\nand Weinstein (1981), who addressed the relationship between immediate focus and\nthe inferences required to integrate the current utterance into the discourse model.\nGrosz et al. (1983) integrated this work with the prior work of Sidner and Grosz.\nThis led to a manuscript on centering which, while widely circulated since 1986,\nremained unpublished until Grosz et al. (1995). A collection of centering papers appears in Walker et al. (1998). See Karamanis et al. (2004) and Poesio et al. (2004) for\na deeper exploration of centering and its parameterizations, and the History section\nof Chapter 23 for more on the use of centering on coreference.\n The grid model of entity-based coherence was first proposed by Barzilay and\nLapata (2005) drawing on earlier work by Lapata (2003) and Barzilay, and then\nextended by them Barzilay and Lapata (2008) and others with additional features\n(Elsner and Charniak 2008, 2011, Feng et al. 2014, Lin et al. 2011) a model that\nprojects entities into a global graph for the discourse (Guinaudeau and Strube 2013,\nMesgar and Strube 2016), and a convolutional model to capture longer-range entity\ndependencies (Nguyen and Joty, 2017).\n Theories of discourse coherence have also been used in algorithms for interpreting discourse-level linguistic phenomena, including verb phrase ellipsis and gapping (Asher 1993, Kehler 1993), and tense interpretation (Lascarides and Asher\n1993, Kehler 1994, Kehler 2000). An extensive investigation into the relationship\nbetween coherence relations and discourse connectives can be found in Knott and\nDale (1994).\n Useful surveys of discourse processing and structure include Stede (2011) and\nWebber et al. (2012).\n Andy Kehler wrote the Discourse chapter for the 2000 first edition of this textbook, which we used as the starting point for the second-edition chapter, and there\nare some remnants of Andy‚Äôs lovely prose still in this third-edition coherence chapter.\n22 C HAPTER 24 ‚Ä¢ D ISCOURSE C OHERENCE\n\nExercises\n and show how (24.29) would be processed. Does the algorithm indeed mark\n (24.29) as less coherent?\n discourse structure for a 10‚Äì20 sentence portion. What problems did you\n encounter? Were you helped by superficial cues the speaker included (e.g.,\n discourse connectives) in any places?\n Exercises 23\n\nAlthoff, T., C. Danescu-Niculescu-Mizil, and D. Jurafsky. ElvevaÃäg, B., P. W. Foltz, D. R. Weinberger, and T. E. Gold-\n2014. How to ask for a favor: A case study on the suc- berg. 2007. Quantifying incoherence in speech: an autocess of altruistic requests. ICWSM 2014. mated methodology and novel application to schizophre-\nAsher, N. 1993. Reference to Abstract Objects in Dis- nia. Schizophrenia research, 93(1-3):304‚Äì316.\n course. Studies in Linguistics and Philosophy (SLAP) Feng, V. W. and G. Hirst. 2011. Classifying arguments by\n 50, Kluwer. scheme. ACL.\nAsher, N. and A. Lascarides. 2003. Logics of Conversation. Feng, V. W. and G. Hirst. 2014. A linear-time bottom-up\n Cambridge University Press. discourse parser with constraints and post-editing. ACL.\nBaldridge, J., N. Asher, and J. Hunter. 2007. Annotation for Feng, V. W., Z. Lin, and G. Hirst. 2014. The impact of deep\n and robust parsing of discourse structure on unrestricted hierarchical discourse structures in the evaluation of text\n texts. Zeitschrift fuÃàr Sprachwissenschaft, 26:213‚Äì239. coherence. COLING.\nBamman, D., B. O‚ÄôConnor, and N. A. Smith. 2013. Learning Finlayson, M. A. 2016. Inferring Propp‚Äôs functions from selatent personas of film characters. ACL. mantically annotated text. The Journal of American Folklore, 129(511):55‚Äì77.\nBarzilay, R. and M. Lapata. 2005. Modeling local coherence:\n An entity-based approach. ACL. Foltz, P. W., W. Kintsch, and T. K. Landauer. 1998. The\n measurement of textual coherence with latent semantic\nBarzilay, R. and M. Lapata. 2008. Modeling local coheranalysis. Discourse processes, 25(2-3):285‚Äì307.\n ence: An entity-based approach. Computational Linguistics, 34(1):1‚Äì34. Grosz, B. J. 1977. The representation and use of focus in\n a system for understanding dialogs. IJCAI-77. Morgan\nBarzilay, R. and L. Lee. 2004. Catching the drift: Prob- Kaufmann.\n abilistic content models, with applications to generation\n and summarization. HLT-NAACL. Grosz, B. J., A. K. Joshi, and S. Weinstein. 1983. Providing a unified account of definite noun phrases in English.\nBedi, G., F. Carrillo, G. A. Cecchi, D. F. Slezak, M. Sig- ACL.\n man, N. B. Mota, S. Ribeiro, D. C. Javitt, M. Copelli,\n and C. M. Corcoran. 2015. Automated analysis of free Grosz, B. J., A. K. Joshi, and S. Weinstein. 1995. Centerspeech predicts psychosis onset in high-risk youths. npj ing: A framework for modeling the local coherence of\n Schizophrenia, 1. discourse. Computational Linguistics, 21(2):203‚Äì225.\nBiran, O. and K. McKeown. 2015. PDTB discourse parsing Guinaudeau, C. and M. Strube. 2013. Graph-based local coas a tagging task: The two taggers approach. SIGDIAL. herence modeling. ACL.\n Habernal, I. and I. Gurevych. 2016. Which argument is more\nBraud, C., M. Coavoux, and A. S√∏gaard. 2017. Cross-lingual\n convincing? Analyzing and predicting convincingness of\n RST discourse parsing. EACL.\n Web arguments using bidirectional LSTM. ACL.\nBrennan, S. E., M. W. Friedman, and C. Pollard. 1987. A\n Habernal, I. and I. Gurevych. 2017. Argumentation mining\n centering approach to pronouns. ACL.\n in user-generated web discourse. Computational Linguis-\nCarlson, L. and D. Marcu. 2001. Discourse tagging manual. tics, 43(1):125‚Äì179.\n Technical Report ISI-TR-545, ISI.\n Halliday, M. A. K. and R. Hasan. 1976. Cohesion in English.\nCarlson, L., D. Marcu, and M. E. Okurowski. 2001. Building Longman. English Language Series, Title No. 9.\n a discourse-tagged corpus in the framework of rhetorical Hearst, M. A. 1997. Texttiling: Segmenting text into multistructure theory. SIGDIAL. paragraph subtopic passages. Computational Linguistics,\nChafe, W. L. 1976. Givenness, contrastiveness, definiteness, 23:33‚Äì64.\n subjects, topics, and point of view. In C. N. Li, ed., Sub- Hernault, H., H. Prendinger, D. A. duVerle, and M. Ishizuka.\n ject and Topic, 25‚Äì55. Academic Press. 2010. Hilda: A discourse parser using support vector ma-\nChen, E., B. Snyder, and R. Barzilay. 2007. Incre- chine classification. Dialogue & Discourse, 1(3).\n mental text structuring with online hierarchical ranking. Hidey, C., E. Musi, A. Hwang, S. Muresan, and K. McKe-\nEMNLP/CoNLL. own. 2017. Analyzing the semantic types of claims and\nCialdini, R. B. 1984. Influence: The psychology of persua- premises in an online persuasive forum. 4th Workshop on\n sion. Morrow. Argument Mining.\nDitman, T. and G. R. Kuperberg. 2010. Building coherence: Hobbs, J. R. 1979. Coherence and coreference. Cognitive\n A framework for exploring the breakdown of links across Science, 3:67‚Äì90.\n clause boundaries in schizophrenia. Journal of neurolin- Hovy, E. H. 1990. Parsimonious and profligate approaches to\n guistics, 23(3):254‚Äì269. the question of discourse structure relations. Proceedings\nElsner, M., J. Austerweil, and E. Charniak. 2007. A unified of the 5th International Workshop on Natural Language\n local and global model for discourse coherence. NAACL- Generation.\n HLT. Iter, D., K. Guu, L. Lansing, and D. Jurafsky. 2020. Pretrain-\nElsner, M. and E. Charniak. 2008. Coreference-inspired co- ing with contrastive sentence objectives improves disherence modeling. ACL. course performance of language models. ACL.\nElsner, M. and E. Charniak. 2011. Extending the entity grid Iter, D., J. Yoon, and D. Jurafsky. 2018. Automatic detecwith entity-specific features. ACL. tion of incoherent speech for diagnosing schizophrenia.\n Fifth Workshop on Computational Linguistics and Clinical Psychology.\n24 Chapter 24 ‚Ä¢ Discourse Coherence\n\nJi, Y. and J. Eisenstein. 2014. Representation learning for Louis, A. and A. Nenkova. 2012. A coherence model based\n text-level discourse parsing. ACL. on syntactic patterns. EMNLP.\nJi, Y. and J. Eisenstein. 2015. One vector is not enough: Lukasik, M., B. Dadachev, K. Papineni, and G. SimoÃÉes.\n Entity-augmented distributed semantics for discourse re- 2020. Text segmentation by cross segment attention.\n lations. TACL, 3:329‚Äì344. EMNLP.\nJoshi, A. K. and S. Kuhn. 1979. Centered logic: The role Mann, W. C. and S. A. Thompson. 1987. Rhetorical structure\n of entity centered sentence representation in natural lan- theory: A theory of text organization. Technical Report\n guage inferencing. IJCAI-79. RS-87-190, Information Sciences Institute.\nJoshi, A. K. and S. Weinstein. 1981. Control of inference: Marcu, D. 1997. The rhetorical parsing of natural language\n Role of some aspects of discourse structure ‚Äì centering. texts. ACL.\n IJCAI-81. Marcu, D. 1999. A decision-based approach to rhetorical\nJoty, S., G. Carenini, and R. T. Ng. 2015. CODRA: A novel parsing. ACL.\n discriminative framework for rhetorical analysis. Compu- Marcu, D. 2000a. The rhetorical parsing of unrestricted\n tational Linguistics, 41(3):385‚Äì435. texts: A surface-based approach. Computational Linguistics, 26(3):395‚Äì448.\nKaramanis, N., M. Poesio, C. Mellish, and J. Oberlander.\n 2004. Evaluating centering-based metrics of coherence Marcu, D., ed. 2000b. The Theory and Practice of Discourse\n for text structuring using a reliably annotated corpus. Parsing and Summarization. MIT Press.\n ACL. Marcu, D. and A. Echihabi. 2002. An unsupervised approach\nKehler, A. 1993. The effect of establishing coherence in el- to recognizing discourse relations. ACL.\n lipsis and anaphora resolution. ACL. Mesgar, M. and M. Strube. 2016. Lexical coherence graph\n modeling using word embeddings. ACL.\nKehler, A. 1994. Temporal relations: Reference or discourse\n coherence? ACL. Miltsakaki, E., R. Prasad, A. K. Joshi, and B. L. Webber.\n 2004. The Penn Discourse Treebank. LREC.\nKehler, A. 2000. Coherence, Reference, and the Theory of\n Grammar. CSLI Publications. Morey, M., P. Muller, and N. Asher. 2017. How much\n progress have we made on RST discourse parsing? a\nKintsch, W. and T. A. Van Dijk. 1978. Toward a model of replication study of recent results on the rst-dt. EMNLP.\n text comprehension and production. Psychological re-\nMorris, J. and G. Hirst. 1991. Lexical cohesion computed by\n view, 85(5):363‚Äì394.\n thesaural relations as an indicator of the structure of text.\nKnott, A. and R. Dale. 1994. Using linguistic phenomena Computational Linguistics, 17(1):21‚Äì48.\n to motivate a set of coherence relations. Discourse Pro- Muller, P., C. Braud, and M. Morey. 2019. ToNy: Contextual\n cesses, 18(1):35‚Äì62. embeddings for accurate multilingual discourse segmen-\nLai, A. and J. Tetreault. 2018. Discourse coherence in the tation of full documents. Workshop on Discourse Relation\n wild: A dataset, evaluation and methods. SIGDIAL. Parsing and Treebanking.\nLakoff, G. 1972. Structural complexity in fairy tales. In The Musi, E., M. Stede, L. Kriese, S. Muresan, and A. Rocci.\n Study of Man, 128‚Äì50. School of Social Sciences, Uni- 2018. A multi-layer annotated corpus of argumentaversity of California, Irvine, CA. tive text: From argument schemes to discourse relations.\nLapata, M. 2003. Probabilistic text structuring: Experiments LREC.\n with sentence ordering. ACL. Myers, G. 1992. ‚ÄúIn this paper we report...‚Äù: Speech acts and\nLascarides, A. and N. Asher. 1993. Temporal interpretation, scientific facts. Journal of Pragmatics, 17(4):295‚Äì313.\n discourse relations, and common sense entailment. Lin- Nguyen, D. T. and S. Joty. 2017. A neural local coherence\n guistics and Philosophy, 16(5):437‚Äì493. model. ACL.\nLi, J. and D. Jurafsky. 2017. Neural net models of open- Nie, A., E. Bennett, and N. Goodman. 2019. DisSent: Learndomain discourse coherence. EMNLP. ing sentence representations from explicit discourse relations. ACL.\nLi, J., R. Li, and E. H. Hovy. 2014. Recursive deep models\n for discourse parsing. EMNLP. Park, J. and C. Cardie. 2014. Identifying appropriate support\n for propositions in online user comments. First workshop\nLi, Q., T. Li, and B. Chang. 2016. Discourse parsing with on argumentation mining.\n attention-based hierarchical neural networks. EMNLP.\n Peldszus, A. and M. Stede. 2013. From argument diagrams\nLin, Z., M.-Y. Kan, and H. T. Ng. 2009. Recognizing im- to argumentation mining in texts: A survey. International\n plicit discourse relations in the Penn Discourse Treebank. Journal of Cognitive Informatics and Natural Intelligence\n EMNLP. (IJCINI), 7(1):1‚Äì31.\nLin, Z., H. T. Ng, and M.-Y. Kan. 2011. Automatically eval- Peldszus, A. and M. Stede. 2016. An annotated corpus of\n uating text coherence using discourse relations. ACL. argumentative microtexts. 1st European Conference on\nLin, Z., H. T. Ng, and M.-Y. Kan. 2014. A pdtb-styled end- Argumentation.\n to-end discourse parser. Natural Language Engineering, Pitler, E., A. Louis, and A. Nenkova. 2009. Automatic sense\n 20(2):151‚Äì184. prediction for implicit discourse relations in text. ACL\nLogeswaran, L., H. Lee, and D. Radev. 2018. Sentence IJCNLP.\n ordering and coherence modeling using recurrent neural Pitler, E. and A. Nenkova. 2009. Using syntax to disamnetworks. AAAI. biguate explicit discourse connectives in text. ACL IJC-\nNLP.\n Exercises 25\n\nPoesio, M., R. Stevenson, B. Di Eugenio, and J. Hitzeman. Stab, C. and I. Gurevych. 2014b. Identifying argumentative\n 2004. Centering: A parametric theory and its instantia- discourse structures in persuasive essays. EMNLP.\n tions. Computational Linguistics, 30(3):309‚Äì363. Stab, C. and I. Gurevych. 2017. Parsing argumentation struc-\nPolanyi, L. 1988. A formal model of the structure of dis- tures in persuasive essays. Computational Linguistics,\n course. Journal of Pragmatics, 12. 43(3):619‚Äì659.\nPolanyi, L., C. Culy, M. van den Berg, G. L. Thione, and Stede, M. 2011. Discourse processing. Morgan & Claypool.\n D. Ahn. 2004. A rule based approach to discourse pars- Stede, M. and J. Schneider. 2018. Argumentation Mining.\n ing. Proceedings of SIGDIAL. Morgan & Claypool.\nPrasad, R., N. Dinesh, A. Lee, E. Miltsakaki, L. Robaldo, Subba, R. and B. Di Eugenio. 2009. An effective discourse\n A. K. Joshi, and B. L. Webber. 2008. The Penn Discourse parser that uses rich linguistic information. NAACL HLT.\n TreeBank 2.0. LREC.\n Surdeanu, M., T. Hicks, and M. A. Valenzuela-Escarcega.\nPrasad, R., B. L. Webber, and A. Joshi. 2014. Reflections on 2015. Two practical rhetorical structure theory parsers.\n the Penn Discourse Treebank, comparable corpora, and NAACL HLT.\n complementary annotation. Computational Linguistics,\n 40(4):921‚Äì950. Tan, C., V. Niculae, C. Danescu-Niculescu-Mizil, and\n L. Lee. 2016. Winning arguments: Interaction dynam-\nPropp, V. 1968. Morphology of the Folktale, 2nd edition. ics and persuasion strategies in good-faith online discus-\nUniversity of Texas Press. Original Russian 1928. Trans- sions. WWW-16.\n lated by Laurence Scott.\n Teufel, S., J. Carletta, and M. Moens. 1999. An annotation\nQin, L., Z. Zhang, and H. Zhao. 2016. A stacking gated scheme for discourse-level argumentation in research arneural architecture for implicit discourse relation classifi- ticles. EACL.\n cation. EMNLP.\n Teufel, S., A. Siddharthan, and C. Batchelor. 2009. To-\nQin, L., Z. Zhang, H. Zhao, Z. Hu, and E. Xing. 2017. Adwards domain-independent argumentative zoning: Evversarial connective-exploiting networks for implicit disidence from chemistry and computational linguistics.\n course relation classification. ACL.\n EMNLP.\nReed, C., R. Mochales Palau, G. Rowe, and M.-F. Moens.\n Walker, M. A., A. K. Joshi, and E. Prince, eds. 1998. Cen-\n2008. Language resources for studying argument. LREC.\n tering in Discourse. Oxford University Press.\nRosenthal, S. and K. McKeown. 2017. Detecting influencers\n Wang, Y., S. Li, and J. Yang. 2018. Toward fast and accurate\n in multiple online genres. ACM Transactions on Internet\n neural discourse segmentation. EMNLP.\n Technology (TOIT), 17(2).\n Webber, B. L., M. Egg, and V. Kordoni. 2012. Discourse\nRutherford, A. and N. Xue. 2015. Improving the inference\n structure and language technology. Natural Language\n of implicit discourse relations via classifying explicit dis-\nEngineering, 18(4):437‚Äì490.\n course connectives. NAACL HLT.\n Wolf, F. and E. Gibson. 2005. Representing discourse coher-\nSagae, K. 2009. Analysis of discourse structure with synence: A corpus-based analysis. Computational Linguistactic dependencies and data-driven shift-reduce parsing.\n tics, 31(2):249‚Äì287.\n IWPT-09.\n Xu, P., H. Saghir, J. S. Kang, T. Long, A. J. Bose, Y. Cao,\nScha, R. and L. Polanyi. 1988. An augmented context free\n and J. C. K. Cheung. 2019. A cross-domain transferable\n grammar for discourse. COLING.\n neural coherence model. ACL.\nSidner, C. L. 1979. Towards a computational theory of defi-\nXue, N., H. T. Ng, S. Pradhan, A. Rutherford, B. L. Webnite anaphora comprehension in English discourse. Techber, C. Wang, and H. Wang. 2016. CoNLL 2016 shared\n nical Report 537, MIT Artificial Intelligence Laboratory,\n task on multilingual shallow discourse parsing. CoNLL-\nCambridge, MA.\n 16 shared task.\nSidner, C. L. 1983. Focusing in the comprehension of defi-\nYang, D., J. Chen, Z. Yang, D. Jurafsky, and E. H. Hovy.\n nite anaphora. In M. Brady and R. C. Berwick, eds, Com-\n2019. Let‚Äôs make your request more persuasive: Modelputational Models of Discourse, 267‚Äì330. MIT Press.\n ing persuasive strategies via semi-supervised neural nets\nSomasundaran, S., J. Burstein, and M. Chodorow. 2014. on crowdfunding platforms. NAACL HLT.\n Lexical chaining for measuring discourse coherence qual-\nYu, N., M. Zhang, and G. Fu. 2018. Transition-based neural\n ity in test-taker essays. COLING.\n RST parsing with implicit syntax features. COLING.\nSoricut, R. and D. Marcu. 2003. Sentence level discourse\n Yu, Y., Y. Zhu, Y. Liu, Y. Liu, S. Peng, M. Gong, and\n parsing using syntactic and lexical information. HLT-\nA. Zeldes. 2019. GumDrop at the DISRPT2019 shared\n NAACL.\n task: A model stacking approach to discourse unit seg-\nSoricut, R. and D. Marcu. 2006. Discourse generation using mentation and connective detection. Workshop on Disutility-trained coherence models. COLING/ACL. course Relation Parsing and Treebanking 2019.\nSporleder, C. and A. Lascarides. 2005. Exploiting linguistic Zhou, Y. and N. Xue. 2015. The Chinese Discourse Treecues to classify rhetorical relations. RANLP-05. Bank: a Chinese corpus annotated with discourse rela-\nSporleder, C. and M. Lapata. 2005. Discourse chunking and tions. Language Resources and Evaluation, 49(2):397‚Äì\n its application to sentence compression. EMNLP. 431.\nStab, C. and I. Gurevych. 2014a. Annotating argument components and relations in persuasive essays. COLING.\n",
    "file_path": "/Users/colinsidberry/nlp-textbook-rag/data/extracted_text/24.Discourse Coherence.txt",
    "file_size_kb": 92.73
  },
  {
    "id": "1f3505343373fec7",
    "source": "nlp_textbook",
    "chapter": "Conversation and its Structure",
    "filename": "25.Conversation and its Structure.txt",
    "content": "rights reserved. Draft of August 24, 2025.\n\nCHAPTER\n\n Conversation and its Structure\n25 Conversation is an intricate and complex joint activity, and conversations have structure. This is true of all conversations, whether they are conversations between people\n or conversations between people and language models. Understanding the structure\n of human conversations is an important social science and linguistic task. The concepts we introduce in studying human conversation can also be a useful tool for\n analyzing human-LLM conversations.\n [This draft is the initial stub of a chapter that will introduce different kinds of\n conversational structure and how to annotate them computationally.]\n\n What are the conversational phenomena that take place when humans converse with\n each other? Are conversations between humans and machines different? Consider\n what goes on in the conversation between a human travel agent and a human client\n excerpted in Fig. 25.1.\n\n C1 : . . . I need to travel in May.\n A2 : And, what day in May did you want to travel?\n C3 : OK uh I need to be there for a meeting that‚Äôs from the 12th to the 15th.\n A4 : And you‚Äôre flying into what city?\n C5 : Seattle.\n A6 : And what time would you like to leave Pittsburgh?\n C7 : Uh hmm I don‚Äôt think there‚Äôs many options for non-stop.\n A8 : Right. There‚Äôs three non-stops today.\n C9 : What are they?\n A10 : The first one departs PGH at 10:00am arrives Seattle at 12:05 their time.\n The second flight departs PGH at 5:55pm, arrives Seattle at 8pm. And the\n last flight departs PGH at 8:15pm arrives Seattle at 10:28pm.\n C11 : OK I‚Äôll take the 5ish flight on the night before on the 11th.\n A12 : On the 11th? OK. Departing at 5:55pm arrives Seattle at 8pm, U.S. Air\n flight 115.\n C13 : OK.\n A14 : And you said returning on May 15th?\n C15 : Uh, yeah, at the end of the day.\n A16 : OK. There‚Äôs #two non-stops . . . #\n C17 : #Act. . . actually #, what day of the week is the 15th?\n A18 : It‚Äôs a Friday.\n C19 : Uh hmm. I would consider staying there an extra day til Sunday.\n A20 : OK. . . OK. On Sunday I have . . .\n client (C). The passages framed by # in A16 and C17 indicate overlaps in speech.\n2 C HAPTER 25 ‚Ä¢ C ONVERSATION AND ITS S TRUCTURE\n\n 25.1.1 Turns\n turn A dialogue is a sequence of turns (C1 , A2 , C3 , and so on), each a single contribution\n from one speaker to the dialogue (as if in a game: I take a turn, then you take a turn,\n then me, and so on). There are 20 turns in Fig. 25.1. A turn can consist of a sentence\n (like C1 ), although it might be as short as a single word (C13 ) or as long as multiple\n sentences (A10 ).\n Turn structure has important implications for spoken dialogue. A human has\n to know when to stop talking; the client interrupts (in A16 and C17 ), so a system\n that was performing this role must know to stop talking (and that the user might be\n making a correction).\n The same issues come up for LLMs; a system also has to know when to start\n talking. For example, most of the time in conversation, speakers start their turns\n almost immediately after the other speaker finishes, without a long pause, because\n people are can usually predict when the other person is about to finish talking.\n Spoken language models must also detect whether a user is done speaking, so\n endpointing they can process the utterance and respond. This task‚Äîcalled endpointing or endpoint detection‚Äî can be quite challenging because of noise and because people\n often pause in the middle of turns.\n\n 25.1.2 Speech Acts\n A key insight into conversation‚Äîdue originally to the philosopher Wittgenstein\n (1953) but worked out more fully by Austin (1962)‚Äîis that each utterance in a\n dialogue is a kind of action being performed by the speaker. These actions are comspeech acts monly called speech acts or dialogue acts: here‚Äôs one taxonomy consisting of 4\n major classes (Bach and Harnish, 1979):\n Constatives: committing the speaker to something‚Äôs being the case (answering, claiming,\n confirming, denying, disagreeing, stating)\n Directives: attempts by the speaker to get the addressee to do something (advising, asking, forbidding, inviting, ordering, requesting)\n Commissives: committing the speaker to some future course of action (promising, planning,\n vowing, betting, opposing)\n Acknowledgments: express the speaker‚Äôs attitude regarding the hearer with respect to some social action (apologizing, greeting, thanking, accepting an acknowledgment)\n\n A user asking a person or a dialogue system to do something (‚ÄòTurn up the music‚Äô) is issuing a D IRECTIVE. Asking a question that requires an answer is also\n a way of issuing a D IRECTIVE: in a sense when the system says (A2 ) ‚Äúwhat day\n in May did you want to travel?‚Äù it‚Äôs as if the system is (very politely) commanding the user to answer. By contrast, a user stating a constraint (like C1 ‚ÄòI need to\n travel in May‚Äô) is issuing a C ONSTATIVE. A user thanking the system is issuing\n an ACKNOWLEDGMENT. The speech act expresses an important component of the\n intention of the speaker (or writer) in saying what they said.\n\n 25.1.3 Grounding\n A dialogue is not just a series of independent speech acts, but rather a collective act\n performed by the speaker and the hearer. Like all collective acts, it‚Äôs important for\n common\n ground the participants to establish what they both agree on, called the common ground\n grounding (Stalnaker, 1978). Speakers do this by grounding each other‚Äôs utterances. Ground-\n25.1 ‚Ä¢ P ROPERTIES OF H UMAN C ONVERSATION 3\n\n ing means acknowledging that the hearer has understood the speaker (Clark, 1996).\n (People need grounding for non-linguistic actions as well; the reason an elevator button lights up when it‚Äôs pressed is to acknowledge that the elevator has indeed been\n called, essentially grounding your action of pushing the button (Norman, 1988).)\n Grounding is also important when the hearer needs to indicate that the speaker\n has not succeeded in performing an action. If the hearer has problems in understanding, she must indicate these problems to the speaker, again so that mutual understanding can eventually be achieved.\n How is closure achieved? Clark and Schaefer (1989) introduce the idea that each\n contribution joint linguistic act or contribution has two phases, called presentation and acceptance. In the first phase, a speaker presents the hearer with an utterance, performing\n a sort of speech act. In the acceptance phase, the hearer has to ground the utterance,\n indicating to the speaker whether understanding was achieved.\n What methods can the hearer B use to ground the speaker A‚Äôs utterance? Clark\n and Schaefer (1989) discuss a continuum of methods ordered from weakest to strongest:\nContinued attention: B shows she is continuing to attend and therefore remains satisfied with\n A‚Äôs presentation.\nNext contribution: B starts in on the next relevant contribution.\nAcknowledgment: B nods or says a continuer like uh-huh, yeah, or the like, or an assessment like that‚Äôs great.\nDemonstration: B demonstrates all or part of what she has understood A to mean, for\n example, by reformulating (paraphrasing) A‚Äôs utterance or by collaborative completion of A‚Äôs utterance.\nDisplay: B displays verbatim all or part of A‚Äôs presentation.\n\n Examples of these kind of grounding occur in the travel agent conversation. We\n can ground by explicitly saying ‚ÄúOK‚Äù, as the agent does in A8 or A10 . Or we can\n ground by repeating what the other person says; in utterance A2 the agent repeats\n ‚Äúin May‚Äù, demonstrating her understanding to the client. Or notice that when the\n client answers a question, the agent begins the next question with ‚ÄúAnd‚Äù. The ‚ÄúAnd‚Äù\n implies that the new question is ‚Äòin addition‚Äô to the old question, again indicating to\n the client that the agent has successfully understood the answer to the last question.\n This particular fragment doesn‚Äôt have an example of an acknowledgment, but\n there‚Äôs an example in another fragment:\n\n C: He wants to fly from Boston to Baltimore\n A: Uh huh\n\n continuer The word uh-huh here is a continuer, also often called an acknowledgment tobackchannel ken or a backchannel. A continuer is a (short) optional utterance that acknowledges\n the content of the utterance of the other and that doesn‚Äôt require an acknowledgment\n by the other (Yngve, 1970; Jefferson, 1984; Schegloff, 1982; Ward and Tsukahara,\n 2000).\n\n 25.1.4 Subdialogues and Dialogue Structure\n Conversations have structure. Consider, for example, the local structure between\n conversation\n analysis speech acts discussed in the field of conversation analysis (Sacks et al., 1974).\n Q UESTIONS set up an expectation for an ANSWER. P ROPOSALS are followed by\n ACCEPTANCE (or REJECTION ). C OMPLIMENTS (‚ÄúNice jacket!‚Äù) often give rise to\nadjacency pair DOWNPLAYERS (‚ÄúOh, this old thing?‚Äù). These pairs, called adjacency pairs, are\n4 C HAPTER 25 ‚Ä¢ C ONVERSATION AND ITS S TRUCTURE\n\n composed of a first pair part and a second pair part (Schegloff, 1968), and these\n expectations can help systems decide what actions to take.\n However, dialogue acts aren‚Äôt always followed immediately by their second pair\n side sequence part. The two parts can be separated by a side sequence (Jefferson 1972) or subsubdialogue dialogue. For example utterances C17 to A20 constitute a correction subdialogue\n (Litman 1985, Litman and Allen 1987, Chu-Carroll and Carberry 1998):\n C17 : #Act. . . actually#, what day of the week is the 15th?\n A18 : It‚Äôs a Friday.\n C19 : Uh hmm. I would consider staying there an extra day til Sunday.\n A20 : OK. . . OK. On Sunday I have . . .\n The question in C17 interrupts the prior discourse, in which the agent was looking\n for a May 15 return flight. The agent must answer the question and also realize that\n ‚Äò‚ÄôI would consider staying...til Sunday‚Äù means that the client would probably like to\n change their plan, and now go back to finding return flights, but for the 17th.\n Another side sequence is the clarification question, which can form a subdialogue between a REQUEST and a RESPONSE. This is especially common in dialogue\n systems where speech recognition errors causes the system to have to ask for clarifications or repetitions like the following:\n User: What do you have going to UNKNOWN WORD on the 5th?\n System: Let‚Äôs see, going where on the 5th?\n User: Going to Hong Kong.\n System: OK, here are some flights...\n\n presequence In addition to side-sequences, questions often have presequences, like the following example where a user starts with a question about the system‚Äôs capabilities\n (‚ÄúCan you make train reservations‚Äù) before making a request.\n User: Can you make train reservations?\n System: Yes I can.\n User: Great, I‚Äôd like to reserve a seat on the 4pm train to New York.\n\n 25.1.5 Initiative\n Sometimes a conversation is completely controlled by one participant. For example a reporter interviewing a chef might ask questions, and the chef responds. We\n initiative say that the reporter in this case has the conversational initiative (Carbonell, 1970;\n Nickerson, 1976). In normal human-human dialogue, however, it‚Äôs more common\n for initiative to shift back and forth between the participants, as they sometimes\n answer questions, sometimes ask them, sometimes take the conversations in new directions, sometimes not. You may ask me a question, and then I respond asking you\n to clarify something you said, which leads the conversation in all sorts of ways. We\n call such interactions mixed initiative (Carbonell, 1970).\n Full mixed initiative, while the norm for human-human conversations, can be\n difficult for dialogue systems. The most primitive dialogue systems tend to use\n system-initiative, where the system asks a question and the user can‚Äôt do anything\n until they answer it, or user-initiative like simple search engines, where the user\n specifies a query and the system passively responds. Even modern large language\n model-based dialogue systems, which come much closer to using full mixed initiative, often don‚Äôt have completely natural initiative switching. Getting this right is an\n important goal for modern systems.\n 25.2 ‚Ä¢ D IALOG ACTS AND C ORPORA 5\n\n 25.1.6 Inference and Implicature\n Inference is also important in dialogue understanding. Consider the client‚Äôs response\n C2 , repeated here:\n A2 : And, what day in May did you want to travel?\n C3 : OK uh I need to be there for a meeting that‚Äôs from the 12th to the 15th.\n Notice that the client does not in fact answer the agent‚Äôs question. The client\n merely mentions a meeting at a certain time. What is it that licenses the agent to\n infer that the client is mentioning this meeting so as to inform the agent of the travel\n dates?\n The speaker seems to expect the hearer to draw certain inferences; in other\n words, the speaker is communicating more information than seems to be present\n in the uttered words. This kind of example was pointed out by Grice (1975, 1978)\n implicature as part of his theory of conversational implicature. Implicature means a particular class of licensed inferences. Grice proposed that what enables hearers to draw\n these inferences is that conversation is guided by a set of maxims, general heuristics\n that play a guiding role in the interpretation of conversational utterances. One such\n relevance maxim is the maxim of relevance which says that speakers attempt to be relevant,\n they don‚Äôt just utter random speech acts. When the client mentions a meeting on the\n 12th, the agent reasons ‚ÄòThere must be some relevance for mentioning this meeting.\n What could it be?‚Äô. The agent knows that one precondition for having a meeting\n (at least before Web conferencing) is being at the place where the meeting is held,\n and therefore that maybe the meeting is a reason for the travel, and if so, then since\n people like to arrive the day before a meeting, the agent should infer that the flight\n should be on the 11th.\n These subtle characteristics of human conversations (turns, speech acts, grounding, dialogue structure, initiative, and implicature) are among the reasons it is difficult to build dialogue systems that can carry on natural conversations with humans.\n Many of these challenges are active areas of dialogue systems research.\n\n The ideas of speech acts and grounding are combined in a single kind of action\n dialogue act called a dialogue act, a tag which represents the interactive function of the sentence\n being tagged.\n Dialog acts can be used to analyze human-human conversation or human-LLM\n conversation. Both the nature of the participants and the type of dialogue (task-based\n or not task-based) influence the development of dialogue act tagsets.\n meetings. It has tags specific to the domain of scheduling, such as S UGGEST, used\n for the proposal of a particular date to meet, and ACCEPT and R EJECT, used for\n acceptance or rejection of a proposal for a date, but also tags that have a more general\n function, like C LARIFY, used to request a user to clarify an ambiguous proposal.\n shows these tags labeling a sample dialogue from the HIS system (Young et al.,\n 2010). This example also shows the content of each dialogue acts, which are the slot\n fillers being communicated.\n There are a number of more general and domain-independent dialogue act tagsets.\n In the DAMSL (Dialogue Act Markup in Several Layers) architecture inspired by\n6 C HAPTER 25 ‚Ä¢ C ONVERSATION AND ITS S TRUCTURE\n\n Tag Example\n T HANK Thanks\n G REET Hello Dan\n I NTRODUCE It‚Äôs me again\n B YE Alright bye\n R EQUEST-C OMMENT How does that look?\n S UGGEST from thirteenth through seventeenth June\n R EJECT No Friday I‚Äôm booked all day\n ACCEPT Saturday sounds fine\n R EQUEST-S UGGEST What is a good day of the week for you?\n I NIT I wanted to make an appointment with you\n G IVE R EASON Because I have meetings all afternoon\n F EEDBACK Okay\n D ELIBERATE Let me check my calendar here\n C ONFIRM Okay, that would be wonderful\n C LARIFY Okay, do you mean Tuesday the 23rd?\n D IGRESS [we could meet for lunch] and eat lots of ice cream\n M OTIVATE We should go to visit our subsidiary in Munich\n G ARBAGE Oops, I-\n Verbmobil-1 system (Jekat et al., 1995).\n\n Tag Sys User Description\n HELLO (a = x, b = y, ...) X X Open a dialogue and give info a = x, b = y, ...\n INFORM (a = x, b = y, ...) X X Give info a = x, b = y, ...\n REQUEST(a, b = x, ...) X X Request value for a given b = x, ...\n REQALTS (a = x, ...) œá X Request alternative with a = x, ...\n CONFIRM (a = x, b = y, ...) X X Explicitly confirm a = x, b = y, ...\n CONFREQ (a = x, ..., d) X œá Implicitly confirm a = x, ... and request value of d\n SELECT(a = x, a = y) X œá Implicitly confirm a = x, ... and request value of d\n AFFIRM (a = x, b = y, ...) X X Affirm and give further info a = x, b = y, ...\n NEGATE (a = x) œá X Negate and give corrected value a = x\n DENY (a = x) œá X Deny that a = x\n BYE() X X Close a dialogue\n et al. (2010). The Sys and User columns indicate which acts are valid as system outputs and\n user inputs, respectively.\n\n the work of Clark and Schaefer (1989), Allwood et al. (1992), and (Allwood, 1995),\n each utterance is tagged for two types of functions, forward-looking functions like\n speech act functions, and backward-looking functions, like grounding and answering, which ‚Äúlook back‚Äù to the interlocutor‚Äôs previous utterance (Allen and Core,\n 1997; Walker et al., 1996; Carletta et al., 1997; Core et al., 1999).\n 25.2 ‚Ä¢ D IALOG ACTS AND C ORPORA 7\n\n Utterance Dialogue act\nU: Hi, I am looking for somewhere to eat. hello(task = find,type=restaurant)\nS: You are looking for a restaurant. What confreq(type = restaurant, food)\n type of food do you like?\nU: I‚Äôd like an Italian somewhere near the inform(food = Italian, near=museum)\n museum.\nS: Roma is a nice Italian restaurant near inform(name = \"Roma\", type = restaurant,\n the museum. food = Italian, near = museum)\nU: Is it reasonably priced? confirm(pricerange = moderate)\nS: Yes, Roma is in the moderate price affirm(name = \"Roma\", pricerange =\n range. moderate)\nU: What is the phone number? request(phone)\nS: The number of Roma is 385456. inform(name = \"Roma\", phone = \"385456\")\nU: Ok, thank you goodbye. bye()\nFig. 25.3.\n\n Forward Looking Function\n STATEMENT a claim made by the speaker\n INFO-REQUEST a question by the speaker\n CHECK a question for confirming information\n INFLUENCE-ON-ADDRESSEE (=Bach‚Äôs directives)\n OPEN-OPTION a weak suggestion or listing of options\n ACTION-DIRECTIVE an actual command\n INFLUENCE-ON-SPEAKER (=Austin‚Äôs commissives)\n OFFER speaker offers to do something,\n (subject to confirmation)\n COMMIT speaker is committed to doing something\n CONVENTIONAL other\n OPENING greetings\n CLOSING farewells\n THANKING thanking and responding to thanks\n The backward looking function of DAMSL focuses on the relationship of an utterance to previous utterances by the other speaker. These include accepting and rejecting proposals (since DAMSL is focused on task-oriented dialogue), and grounding and repair acts:\n Backward Looking Function\n AGREEMENT speaker‚Äôs response to previous proposal\n ACCEPT accepting the proposal\n ACCEPT-PART accepting some part of the proposal\n MAYBE neither accepting nor rejecting the proposal\n REJECT-PART rejecting some part of the proposal\n REJECT rejecting the proposal\n HOLD putting off response, usually via subdialogue\n ANSWER answering a question\n UNDERSTANDING whether speaker understood previous\n SIGNAL-NON-UNDER. speaker didn‚Äôt understand\n SIGNAL-UNDER. speaker did understand\n ACK demonstrated via continuer or assessment\n REPEAT-REPHRASE demonstrated via repetition or reformulation\n COMPLETION demonstrated via collaborative completion\n Fig. 25.5 shows a labeling of parts of our sample conversation using versions of\n8 C HAPTER 25 ‚Ä¢ C ONVERSATION AND ITS S TRUCTURE\n\n the DAMSL Forward and Backward tags.\n\n [assert] C1 : . . . I need to travel in May.\n [info-req,ack] A2 : And, what day in May did you want to travel?\n [assert, answer] C3 : OK uh I need to be there for a meeting that‚Äôs from the 12th to the\n 15th.\n [info-req,ack] A4 : And you‚Äôre flying into what city?\n [assert,answer] C5 : Seattle.\n [info-req,ack] A6 : And what time would you like to leave Pittsburgh?\n [check,hold] C7 : Uh hmm I don‚Äôt think there‚Äôs many options for non-stop.\n [accept,ack] A7 : Right.\n [assert] There‚Äôs three non-stops today.\n [info-req] C8 : What are they?\n [assert, open-option] A9 : The first one departs PGH at 10:00am arrives Seattle at 12:05 their\n time. The second flight departs PGH at 5:55pm, arrives Seattle at\n 8pm. And the last flight departs PGH at 8:15pm arrives Seattle at\n 10:28pm.\n [accept,ack] C10 : OK I‚Äôll take the 5ish flight on the night before on the 11th.\n [check,ack] A11 : On the 11th?\n [assert,ack] OK. Departing at 5:55pm arrives Seattle at 8pm, U.S. Air flight\n 115.\n [ack] C12 : OK.\n 25.2 ‚Ä¢ Dialog Acts and Corpora 9\n\nAllen, J. and M. Core. 1997. Draft of DAMSL: Dialog act Sacks, H., E. A. Schegloff, and G. Jefferson. 1974. A simmarkup in several layers. Unpublished manuscript. plest systematics for the organization of turn-taking for\nAllwood, J. 1995. An activity-based approach to pragmatics. conversation. Language, 50(4):696‚Äì735.\n Gothenburg Papers in Theoretical Linguistics, 76:1‚Äì38. Schegloff, E. A. 1968. Sequencing in conversational open-\nAllwood, J., J. Nivre, and E. AhlseÃÅn. 1992. On the semantics ings. American Anthropologist, 70:1075‚Äì1095.\n and pragmatics of linguistic feedback. Journal of Seman- Schegloff, E. A. 1982. Discourse as an interactional achievetics, 9:1‚Äì26. ment: Some uses of ‚Äòuh huh‚Äô and other things that come\nAustin, J. L. 1962. How to Do Things with Words. Harvard between sentences. In D. Tannen, ed., Analyzing Dis-\nUniversity Press. course: Text and Talk, 71‚Äì93. Georgetown University\n Press, Washington, D.C.\nBach, K. and R. Harnish. 1979. Linguistic communication\n and speech acts. MIT Press. Stalnaker, R. C. 1978. Assertion. In P. Cole, ed., Pragmatics: Syntax and Semantics Volume 9, 315‚Äì332. Academic\nCarbonell, J. R. 1970. AI in CAI: An artificial-intelligence Press.\n approach to computer-assisted instruction. IEEE transactions on man-machine systems, 11(4):190‚Äì202. Walker, M. A., E. Maier, J. Allen, J. Carletta, S. Condon, G. Flammia, J. Hirschberg, S. Isard, M. Ishizaki,\nCarletta, J., N. DahlbaÃàck, N. Reithinger, and M. A. Walker. L. Levin, S. Luperfoy, D. R. Traum, and S. Whittaker.\n 1997. Standards for dialogue coding in natural language 1996. Penn multiparty standard coding scheme: Draft\n processing. Technical Report 167, Dagstuhl Seminars. annotation manual. www.cis.upenn.edu/Àúircs/dis\n Report from Dagstuhl seminar number 9706. course-tagging/newcoding.html.\nChu-Carroll, J. and S. Carberry. 1998. Collaborative re- Ward, N. and W. Tsukahara. 2000. Prosodic features which\n sponse generation in planning dialogues. Computational cue back-channel feedback in English and Japanese.\n Linguistics, 24(3):355‚Äì400. Journal of Pragmatics, 32:1177‚Äì1207.\nClark, H. H. 1996. Using Language. Cambridge University Wittgenstein, L. 1953. Philosophical Investigations. (Trans-\nPress. lated by Anscombe, G.E.M.). Blackwell.\nClark, H. H. and E. F. Schaefer. 1989. Contributing to dis- Yngve, V. H. 1970. On getting a word in edgewise. CLS-70.\n course. Cognitive Science, 13:259‚Äì294. University of Chicago.\nCore, M., M. Ishizaki, J. D. Moore, C. Nakatani, N. Rei- Young, S. J., M. GasÃåicÃÅ, S. Keizer, F. Mairesse, J. Schatzthinger, D. R. Traum, and S. Tutiya. 1999. The Report mann, B. Thomson, and K. Yu. 2010. The Hidden Inforof the 3rd workshop of the Discourse Resource Initia- mation State model: A practical framework for POMDPtive. Technical Report No.3 CC-TR-99-1, Chiba Corpus based spoken dialogue management. Computer Speech &\n Project, Chiba, Japan. Language, 24(2):150‚Äì174.\nGrice, H. P. 1975. Logic and conversation. In P. Cole and\n J. L. Morgan, eds, Speech Acts: Syntax and Semantics\n Volume 3, 41‚Äì58. Academic Press.\nGrice, H. P. 1978. Further notes on logic and conversation. In\n P. Cole, ed., Pragmatics: Syntax and Semantics Volume 9,\n 113‚Äì127. Academic Press.\nJefferson, G. 1972. Side sequences. In D. Sudnow, ed., Studies in social interaction, 294‚Äì333. Free Press, New York.\nJefferson, G. 1984. Notes on a systematic deployment of the\n acknowledgement tokens ‚Äòyeah‚Äô and ‚Äòmm hm‚Äô. Papers in\n Linguistics, 17(2):197‚Äì216.\nJekat, S., A. Klein, E. Maier, I. Maleck, M. Mast, and\n J. Quantz. 1995. Dialogue acts in verbmobil. Verbmobil‚Äì\n Report‚Äì65‚Äì95.\nLitman, D. J. 1985. Plan Recognition and Discourse Analysis: An Integrated Approach for Understanding Dialogues. Ph.D. thesis, University of Rochester, Rochester,\n NY.\nLitman, D. J. and J. Allen. 1987. A plan recognition model\n for subdialogues in conversation. Cognitive Science,\n 11:163‚Äì200.\nNickerson, R. S. 1976. On conversational interaction with\n computers. Proceedings of the ACM/SIGGRAPH workshop on User-oriented design of interactive graphics systems.\nNorman, D. A. 1988. The Design of Everyday Things. Basic\n Books.\n",
    "file_path": "/Users/colinsidberry/nlp-textbook-rag/data/extracted_text/25.Conversation and its Structure.txt",
    "file_size_kb": 24.56
  },
  {
    "id": "78f4a4160277cec3",
    "source": "nlp_textbook",
    "chapter": "Hidden Markov Models",
    "filename": "A.Hidden Markov Models.txt",
    "content": "rights reserved. Draft of August 24, 2025.\n\nCHAPTER\n\n Hidden Markov Models\nA Chapter 17 introduced the Hidden Markov Model and applied it to part of speech\n tagging. Part of speech tagging is a fully-supervised learning task, because we have\n a corpus of words labeled with the correct part-of-speech tag. But many applications\n don‚Äôt have labeled data. So in this chapter, we introduce the full set of algorithms for\n HMMs, including the key unsupervised learning algorithm for HMM, the Forward-\nBackward algorithm. We‚Äôll repeat some of the text from Chapter 17 for readers who\n want the whole story laid out in a single chapter.\n\nA.1 Markov Chains\n Markov chain The HMM is based on augmenting the Markov chain. A Markov chain is a model\n that tells us something about the probabilities of sequences of random variables,\n states, each of which can take on values from some set. These sets can be words, or\n tags, or symbols representing anything, like the weather. A Markov chain makes a\n very strong assumption that if we want to predict the future in the sequence, all that\n matters is the current state. The states before the current state have no impact on the\n future except via the current state. It‚Äôs as if to predict tomorrow‚Äôs weather you could\n examine today‚Äôs weather but you weren‚Äôt allowed to look at yesterday‚Äôs weather.\n\n .8\n are .2\n .1 COLD2 .1 .4 .5\n .1 .5\n .1\n .3 uniformly charming\n HOT1 WARM3 .5\n\n .6 .3 .6 .1 .2\n .6\n (a) (b)\n Figure A.1 A Markov chain for weather (a) and one for words (b), showing states and\n transitions. A start distribution œÄ is required; setting œÄ = [0.1, 0.7, 0.2] for (a) would mean a\n probability 0.7 of starting in state 2 (cold), probability 0.1 of starting in state 1 (hot), etc.\n\n More formally, consider a sequence of state variables q1 , q2 , ..., qi . A Markov\n Markov\n assumption model embodies the Markov assumption on the probabilities of this sequence: that\n when predicting the future, the past doesn‚Äôt matter, only the present.\n\n Markov Assumption: P(qi = a|q1 ...qi‚àí1 ) = P(qi = a|qi‚àí1 ) (A.1)\n\n Figure A.1a shows a Markov chain for assigning a probability to a sequence of\n weather events, for which the vocabulary consists of HOT, COLD, and WARM. The\n states are represented as nodes in the graph, and the transitions, with their probabilities, as edges. The transitions are probabilities: the values of arcs leaving a given\n2 A PPENDIX A ‚Ä¢ H IDDEN M ARKOV M ODELS\n\n state must sum to 1. Figure A.1b shows a Markov chain for assigning a probability to a sequence of words w1 ...wn . This Markov chain should be familiar; in fact,\n it represents a bigram language model, with each edge expressing the probability\n p(wi |w j )! Given the two models in Fig. A.1, we can assign a probability to any\n sequence from our vocabulary.\n Formally, a Markov chain is specified by the following components:\n Q = q1 q2 . . . qN a set of N states\n A = a11 a12 . . . aN1 . . . aNN a transition probability matrix A, each ai j representing the probability of moving from state i to state j, s.t.\n P n\n j=1 ai j = 1 ‚àÄi\n œÄ = œÄ1 , œÄ2 , ..., œÄN an initial probability distribution over states. œÄi is the\n probability that the Markov chain will start in state i.\n Some states j may have œÄ j = 0, meaning that they cannot\n be initial states. Also, Ni=1 œÄi = 1\n P\n\n Before you go on, use the sample probabilities in Fig. A.1a (with œÄ = [.1, .7., 2])\n to compute the probability of each of the following sequences:\n (A.2) hot hot hot hot\n (A.3) cold hot cold hot\n What does the difference in these probabilities tell you about a real-world weather\n fact encoded in Fig. A.1a?\n\nA.2 The Hidden Markov Model\n A Markov chain is useful when we need to compute a probability for a sequence\n of observable events. In many cases, however, the events we are interested in are\n hidden hidden: we don‚Äôt observe them directly. For example we don‚Äôt normally observe\n part-of-speech tags in a text. Rather, we see words, and must infer the tags from the\n word sequence. We call the tags hidden because they are not observed.\n Hidden A hidden Markov model (HMM) allows us to talk about both observed events\n Markov model\n (like words that we see in the input) and hidden events (like part-of-speech tags) that\n we think of as causal factors in our probabilistic model. An HMM is specified by\n the following components:\n Q = q1 q2 . . . qN a set of N states\n A = a11 . . . ai j . . . aNN a transition probability matrix A, each ai j representing the probability\n of moving from state i to state j, s.t. Nj=1 ai j = 1 ‚àÄi\n P\n\n B = bi (ot ) a sequence of observation likelihoods, also called emission probabilities, each expressing the probability of an observation ot (drawn from a\n vocabulary V = v1 , v2 , ..., vV ) being generated from a state qi\n œÄ = œÄ1 , œÄ2 , ..., œÄN an initial probability distribution over states. œÄi is the probability that\n the Markov chain will start in state i. Some states P j may have œÄ j = 0,\n meaning that they cannot be initial states. Also, ni=1 œÄi = 1\n\n The HMM is given as input O = o1 o2 . . . oT : a sequence of T observations, each\n one drawn from the vocabulary V .\n A first-order hidden Markov model instantiates two simplifying assumptions.\n First, as with a first-order Markov chain, the probability of a particular state depends\n A.2 ‚Ä¢ T HE H IDDEN M ARKOV M ODEL 3\n\nonly on the previous state:\n\n Markov Assumption: P(qi |q1 ...qi‚àí1 ) = P(qi |qi‚àí1 ) (A.4)\n Second, the probability of an output observation oi depends only on the state that\nproduced the observation qi and not on any other states or any other observations:\n\n Output Independence: P(oi |q1 . . . qi , . . . , qT , o1 , . . . , oi , . . . , oT ) = P(oi |qi ) (A.5)\n\n To exemplify these models, we‚Äôll use a task invented by Jason Eisner (2002).\nImagine that you are a climatologist in the year 2799 studying the history of global\nwarming. You cannot find any records of the weather in Baltimore, Maryland, for\nthe summer of 2020, but you do find Jason Eisner‚Äôs diary, which lists how many ice\ncreams Jason ate every day that summer. Our goal is to use these observations to\nestimate the temperature every day. We‚Äôll simplify this weather task by assuming\nthere are only two kinds of days: cold (C) and hot (H). So the Eisner task is as\nfollows:\n Given a sequence of observations O (each an integer representing the\n number of ice creams eaten on a given day) find the ‚Äòhidden‚Äô sequence\n Q of weather states (H or C) which caused Jason to eat the ice cream.\n Figure A.2 shows a sample HMM for the ice cream task. The two hidden states\n(H and C) correspond to hot and cold weather, and the observations (drawn from the\nalphabet O = {1, 2, 3}) correspond to the number of ice creams eaten by Jason on a\ngiven day.\n\n .5 .6\n\n .5\n COLD1 HOT2\n .4\n B1 B2\n P(1 | COLD) .5 P(1 | HOT) .2\n P(2 | COLD) = .4 P(2 | HOT) = .4\n P(3 | COLD) .1 P(3 | HOT) .4\n œÄ = [.2,.8]\n\nFigure A.2 A hidden Markov model for relating numbers of ice creams eaten by Jason (the\nobservations) to the weather (H or C, the hidden variables).\n\n An influential tutorial by Rabiner (1989), based on tutorials by Jack Ferguson in\nthe 1960s, introduced the idea that hidden Markov models should be characterized\nby three fundamental problems:\n\n Problem 1 (Likelihood): Given an HMM Œª = (A, B) and an observation sequence O, determine the likelihood P(O|Œª ).\n Problem 2 (Decoding): Given an observation sequence O and an HMM Œª =\n (A, B), discover the best hidden state sequence Q.\n Problem 3 (Learning): Given an observation sequence O and the set of states\n in the HMM, learn the HMM parameters A and B.\n\n We already saw an example of Problem 2 in Chapter 17. In the next two sections\nwe introduce the Forward and Forward-Backward algorithms to solve Problems 1\nand 3 and give more information on Problem 2\n4 A PPENDIX A ‚Ä¢ H IDDEN M ARKOV M ODELS\n\nA.3 Likelihood Computation: The Forward Algorithm\n Our first problem is to compute the likelihood of a particular observation sequence.\n For example, given the ice-cream eating HMM in Fig. A.2, what is the probability\n of the sequence 3 1 3? More formally:\n Computing Likelihood: Given an HMM Œª = (A, B) and an observation sequence O, determine the likelihood P(O|Œª ).\n For a Markov chain, where the surface observations are the same as the hidden\n events, we could compute the probability of 3 1 3 just by following the states labeled\n 3 1 3 and multiplying the probabilities along the arcs. For a hidden Markov model,\n things are not so simple. We want to determine the probability of an ice-cream\n observation sequence like 3 1 3, but we don‚Äôt know what the hidden state sequence\n is!\n Let‚Äôs start with a slightly simpler situation. Suppose we already knew the weather\n and wanted to predict how much ice cream Jason would eat. This is a useful part\n of many HMM tasks. For a given hidden state sequence (e.g., hot hot cold), we can\n easily compute the output likelihood of 3 1 3.\n Let‚Äôs see how. First, recall that for hidden Markov models, each hidden state\n produces only a single observation. Thus, the sequence of hidden states and the\n sequence of observations have the same length. 1\n Given this one-to-one mapping and the Markov assumptions expressed in Eq. A.4,\n for a particular hidden state sequence Q = q1 , q2 , ..., qT and an observation sequence\n O = o1 , o2 , ..., oT , the likelihood of the observation sequence is\n T\n Y\n P(O|Q) = P(oi |qi ) (A.6)\n i=1\n\n The computation of the forward probability for our ice-cream observation 3 1 3 from\n one possible hidden state sequence hot hot cold is shown in Eq. A.7. Figure A.3\n shows a graphic representation of this computation.\n\n P(3 1 3|hot hot cold) = P(3|hot) √ó P(1|hot) √ó P(3|cold) (A.7)\n\n hot hot cold\n .4 .2 .1\n\n 3 1 3\n Figure A.3 The computation of the observation likelihood for the ice-cream events 3 1 3\n given the hidden state sequence hot hot cold.\n\n But of course, we don‚Äôt actually know what the hidden state (weather) sequence\n was. We‚Äôll need to compute the probability of ice-cream events 3 1 3 instead by\n 1 In a variant of HMMs called segmental HMMs (in speech recognition) or semi-HMMs (in text processing) this one-to-one mapping between the length of the hidden state sequence and the length of the\n observation sequence does not hold.\n A.3 ‚Ä¢ L IKELIHOOD C OMPUTATION : T HE F ORWARD A LGORITHM 5\n\n summing over all possible weather sequences, weighted by their probability. First,\n let‚Äôs compute the joint probability of being in a particular weather sequence Q and\n generating a particular sequence O of ice-cream events. In general, this is\n T\n Y T\n Y\n P(O, Q) = P(O|Q) √ó P(Q) = P(oi |qi ) √ó P(qi |qi‚àí1 ) (A.8)\n i=1 i=1\n\n The computation of the joint probability of our ice-cream observation 3 1 3 and one\n possible hidden state sequence hot hot cold is shown in Eq. A.9. Figure A.4 shows\n a graphic representation of this computation.\n\n P(3 1 3, hot hot cold) = P(hot|start) √ó P(hot|hot) √ó P(cold|hot)\n √óP(3|hot) √ó P(1|hot) √ó P(3|cold) (A.9)\n\n .6 .4\n hot hot cold\n .4 .2 .1\n\n 3 1 3\n Figure A.4 The computation of the joint probability of the ice-cream events 3 1 3 and the\n hidden state sequence hot hot cold.\n\n Now that we know how to compute the joint probability of the observations\n with a particular hidden state sequence, we can compute the total probability of the\n observations just by summing over all possible hidden state sequences:\n X X\n P(O) = P(O, Q) = P(O|Q)P(Q) (A.10)\n Q Q\n\n For our particular case, we would sum over the eight 3-event sequences cold cold\n cold, cold cold hot, that is,\n\n P(3 1 3) = P(3 1 3, cold cold cold) + P(3 1 3, cold cold hot) + P(3 1 3, hot hot cold) + ...\n\n For an HMM with N hidden states and an observation sequence of T observations, there are N T possible hidden sequences. For real tasks, where N and T are\n both large, N T is a very large number, so we cannot compute the total observation\n likelihood by computing a separate observation likelihood for each hidden state sequence and then summing them.\n Instead of using such an extremely exponential algorithm, we use an efficient\n forward\nalgorithm O(N 2 T ) algorithm called the forward algorithm. The forward algorithm is a kind\n of dynamic programming algorithm, that is, an algorithm that uses a table to store\n intermediate values as it builds up the probability of the observation sequence. The\n forward algorithm computes the observation probability by summing over the probabilities of all possible hidden state paths that could generate the observation sequence, but it does so efficiently by implicitly folding each of these paths into a\n single forward trellis.\n Figure A.5 shows an example of the forward trellis for computing the likelihood\n of 3 1 3 given the hidden state sequence hot hot cold.\n6 A PPENDIX A ‚Ä¢ H IDDEN M ARKOV M ODELS\n\n Œ±1(2)=.32 Œ±2(2)= .32*.12 + .02*.1 = .0404\n\n P(H|H) * P(1|H)\n q2 H H P(C .6 * .2 H H\n |H)\n .4 * P(1|C\n H) .5 )\n\n |H) Œ±2(1) = .32*.2 + .02*.25 = .069\n * . (3|\n\n Œ±1(1) = .02 P(1\n )* 2\n .8 t)*P\n\n |C .\n\n P(H .5 *\n r\n sta\n\n q1 P(C|C) * P(1|C)\n C C C C\n H|\n\n ) .5 * .5\n P(\n\n 3 |C\n *P(\n rt) .1\n |sta .2 *\n C\n P( 1 3\n œÄ\n o1 o2 o3\n\n t\nFigure A.5 The forward trellis for computing the total observation likelihood for the ice-cream events 3 1 3.\nHidden states are in circles, observations in squares. The figure shows the P computation of Œ±t ( j) for two states at\ntwo time steps. The computation in each cell follows Eq. A.12: Œ±t ( j) = N i=1 Œ±t‚àí1 (i)ai j b j (ot ). The resulting\nprobability expressed in each cell is Eq. A.11: Œ±t ( j) = P(o1 , o2 . . . ot , qt = j|Œª ).\n\n Each cell of the forward algorithm trellis Œ±t ( j) represents the probability of being in state j after seeing the first t observations, given the automaton Œª . The value\n of each cell Œ±t ( j) is computed by summing over the probabilities of every path that\n could lead us to this cell. Formally, each cell expresses the following probability:\n\n Œ±t ( j) = P(o1 , o2 . . . ot , qt = j|Œª ) (A.11)\n\n Here, qt = j means ‚Äúthe t th state in the sequence of states is state j‚Äù. We compute\n this probability Œ±t ( j) by summing over the extensions of all the paths that lead to\n the current cell. For a given state q j at time t, the value Œ±t ( j) is computed as\n N\n X\n Œ±t ( j) = Œ±t‚àí1 (i)ai j b j (ot ) (A.12)\n i=1\n\n The three factors that are multiplied in Eq. A.12 in extending the previous paths\n to compute the forward probability at time t are\n\n Œ±t‚àí1 (i) the previous forward path probability from the previous time step\n ai j the transition probability from previous state qi to current state q j\n b j (ot ) the state observation likelihood of the observation symbol ot given\n the current state j\n\n Consider the computation in Fig. A.5 of Œ±2 (2), the forward probability of being\n at time step 2 in state 2 having generated the partial observation 3 1. We compute by\n extending the Œ± probabilities from time step 1, via two paths, each extension consisting of the three factors above: Œ±1 (1) √ó P(H|C) √ó P(1|H) and Œ±1 (2) √ó P(H|H) √ó\n P(1|H).\n Figure A.6 shows another visualization of this induction step for computing the\n value in one new cell of the trellis.\n We give two formal definitions of the forward algorithm: the pseudocode in\n Fig. A.7 and a statement of the definitional recursion here.\n A.3 ‚Ä¢ L IKELIHOOD C OMPUTATION : T HE F ORWARD A LGORITHM 7\n\n Œ±t-2(N) Œ±t-1(N)\n\n qN qN qN\n aNj Œ±t(j)= Œ£i Œ±t-1(i) aij bj(ot)\n\n qj\n\n Œ±t-2(3) Œ±t-1(3) a3j\n q3 q3 q3\n a2j\n Œ±t-2(2) Œ±t-1(2)\n bj(ot)\n q2 q2 a1j q2 q2\n\n Œ±t-2(1) Œ±t-1(1)\n\n q1 q1 q1 q1\n\n ot-2 ot-1 ot ot+1\n\nFigure A.6 Visualizing the computation of a single element Œ±t (i) in the trellis by summing\nall the previous values Œ±t‚àí1 , weighted by their transition probabilities a, and multiplying by\nthe observation probability bi (ot ). For many applications of HMMs, many of the transition\nprobabilities are 0, so not all previous states will contribute to the forward probability of the\ncurrent state. Hidden states are in circles, observations in squares. Shaded nodes are included\nin the probability computation for Œ±t (i).\n\n function F ORWARD(observations of len T, state-graph of len N) returns forward-prob\n\n create a probability matrix forward[N,T]\n for each state s from 1 to N do ; initialization step\n forward[s,1] ‚Üê œÄs ‚àó bs (o1 )\n for each time step t from 2 to T do ; recursion step\n for each state s from 1 to N do\n N\n forward[s0 ,t ‚àí 1] ‚àó as0 ,s ‚àó bs (ot )\n X\n forward[s,t] ‚Üê\n s0 =1\n N\n X\n forwardprob ‚Üê forward[s, T ] ; termination step\n s=1\n return forwardprob\n\nFigure A.7 The forward algorithm, where forward[s,t] represents Œ±t (s).\n\n 1. Initialization:\n\n Œ±1 ( j) = œÄ j b j (o1 ) 1 ‚â§ j ‚â§ N\n\n 2. Recursion:\n N\n X\n Œ±t ( j) = Œ±t‚àí1 (i)ai j b j (ot ); 1 ‚â§ j ‚â§ N, 1 < t ‚â§ T\n i=1\n\n 3. Termination:\n N\n X\n P(O|Œª ) = Œ±T (i)\n i=1\n8 A PPENDIX A ‚Ä¢ H IDDEN M ARKOV M ODELS\n\nA.4 Decoding: The Viterbi Algorithm\n For any model, such as an HMM, that contains hidden variables, the task of determining which sequence of variables is the underlying source of some sequence of\n decoding observations is called the decoding task. In the ice-cream domain, given a sequence\n of ice-cream observations 3 1 3 and an HMM, the task of the decoder is to find the\n best hidden weather sequence (H H H). More formally,\n Decoding: Given as input an HMM Œª = (A, B) and a sequence of observations O = o1 , o2 , ..., oT , find the most probable sequence of states\n Q = q1 q2 q3 . . . qT .\n We might propose to find the best sequence as follows: For each possible hidden state sequence (HHH, HHC, HCH, etc.), we could run the forward algorithm\n and compute the likelihood of the observation sequence given that hidden state sequence. Then we could choose the hidden state sequence with the maximum observation likelihood. It should be clear from the previous section that we cannot do this\n because there are an exponentially large number of state sequences.\n Instead, the most common decoding algorithms for HMMs is the Viterbi algo-\nViterbi\n algorithm rithm. Like the forward algorithm, Viterbi is a kind of dynamic programming\n that makes uses of a dynamic programming trellis. Viterbi also strongly resembles\n another dynamic programming variant, the minimum edit distance algorithm of\n Chapter 2.\n\n v1(2)=.32 v2(2)= max(.32*.12, .02*.10) = .038\n\n P(H|H) * P(1|H)\n q2 H H P(C .6 * .2 H H\n |H)\n .4 * P(1|C\n .5 )\n H)\n\n |H) v2(1) = max(.32*.20, .02*.25) = .064\n * . (3|\n\n v1(1) = .02 P(1\n )* 2\n .8 t)*P\n\n |C .\n\n P(H .5 *\n r\n sta\n\n q1 P(C|C) * P(1|C)\n C C C C\n H|\n\n ) .5 * .5\n P(\n\n 3 |C\n P(*\n t)\n ar .1\n C |st .2 *\n P (\n\n œÄ 3 1 3\n\n o1 o2 o3\n\n t\nFigure A.8 The Viterbi trellis for computing the best path through the hidden state space for the ice-cream\neating events 3 1 3. Hidden states are in circles, observations in squares. White (unfilled) circles indicate illegal\ntransitions. The figure shows the computation of vt ( j) for two states at two time steps. The computation in each\ncell follows Eq. A.14: vt ( j) = max1‚â§i‚â§N‚àí1 vt‚àí1 (i) ai j b j (ot ). The resulting probability expressed in each cell\nis Eq. A.13: vt ( j) = P(q0 , q1 , . . . , qt‚àí1 , o1 , o2 , . . . , ot , qt = j|Œª ).\n\n Figure A.8 shows an example of the Viterbi trellis for computing the best hidden\n state sequence for the observation sequence 3 1 3. The idea is to process the observation sequence left to right, filling out the trellis. Each cell of the trellis, vt ( j),\n represents the probability that the HMM is in state j after seeing the first t observations and passing through the most probable state sequence q1 , ..., qt‚àí1 , given the\n A.4 ‚Ä¢ D ECODING : T HE V ITERBI A LGORITHM 9\n\n automaton Œª . The value of each cell vt ( j) is computed by recursively taking the\n most probable path that could lead us to this cell. Formally, each cell expresses the\n probability\n\n vt ( j) = max P(q1 ...qt‚àí1 , o1 , o2 . . . ot , qt = j|Œª ) (A.13)\n q1 ,...,qt‚àí1\n\n Note that we represent the most probable path by taking the maximum over all\n possible previous state sequences max . Like other dynamic programming algoq1 ,...,qt‚àí1\n rithms, Viterbi fills each cell recursively. Given that we had already computed the\n probability of being in every state at time t ‚àí 1, we compute the Viterbi probability\n by taking the most probable of the extensions of the paths that lead to the current\n cell. For a given state q j at time t, the value vt ( j) is computed as\n N\n vt ( j) = max vt‚àí1 (i) ai j b j (ot ) (A.14)\n i=1\n\n The three factors that are multiplied in Eq. A.14 for extending the previous paths to\n compute the Viterbi probability at time t are\n\n vt‚àí1 (i) the previous Viterbi path probability from the previous time step\n ai j the transition probability from previous state qi to current state q j\n b j (ot ) the state observation likelihood of the observation symbol ot given\n the current state j\n\n function V ITERBI(observations of len T,state-graph of len N) returns best-path, path-prob\n\n create a path probability matrix viterbi[N,T]\n for each state s from 1 to N do ; initialization step\n viterbi[s,1] ‚Üê œÄs ‚àó bs (o1 )\n backpointer[s,1] ‚Üê 0\n for each time step t from 2 to T do ; recursion step\n for each state s from 1 to N do\n N\n viterbi[s,t] ‚Üê max\n viterbi[s0 ,t ‚àí 1] ‚àó as0 ,s ‚àó bs (ot )\n s =1\n N\n backpointer[s,t] ‚Üê argmax viterbi[s0 ,t ‚àí 1] ‚àó as0 ,s ‚àó bs (ot )\n s0 =1\n N\n bestpathprob ‚Üê max viterbi[s, T ] ; termination step\n s=1\n N\n bestpathpointer ‚Üê argmax viterbi[s, T ] ; termination step\n s=1\n bestpath ‚Üê the path starting at state bestpathpointer, that follows backpointer[] to states back in time\n return bestpath, bestpathprob\n\nFigure A.9 Viterbi algorithm for finding optimal sequence of hidden states. Given an observation sequence\nand an HMM Œª = (A, B), the algorithm returns the state path through the HMM that assigns maximum likelihood\nto the observation sequence.\n\n Figure A.9 shows pseudocode for the Viterbi algorithm. Note that the Viterbi\n algorithm is identical to the forward algorithm except that it takes the max over the\n previous path probabilities whereas the forward algorithm takes the sum. Note also\n that the Viterbi algorithm has one component that the forward algorithm doesn‚Äôt\n10 A PPENDIX A ‚Ä¢ H IDDEN M ARKOV M ODELS\n\n have: backpointers. The reason is that while the forward algorithm needs to produce an observation likelihood, the Viterbi algorithm must produce a probability and\n also the most likely state sequence. We compute this best state sequence by keeping\n track of the path of hidden states that led to each state, as suggested in Fig. A.10, and\n Viterbi then at the end backtracing the best path to the beginning (the Viterbi backtrace).\n backtrace\n\n v1(2)=.32 v2(2)= max(.32*.12, .02*.10) = .038\n\n P(H|H) * P(1|H)\n q2 H H P(C .6 * .2\n H H\n |H)\n .4 * P(1|C\n .5 )\n )\n )\n\n (1|H\n * .4 3|H\n\n *P v2(1) = max(.32*.20, .02*.25) = .064\n )\n .8 t)*P(\n\n v1(1) = .02 |C .2\n P(H .5 *\n r\n sta\n\n q1 P(C|C) * P(1|C)\n C C C C\n H|\n\n ) .5 * .5\n P(\n\n 3 |C\n * P(\n rt) .1\n |sta .2 *\n C\n P(\n 3 1 3\n œÄ\n o1 o2 o3\n\n t\nFigure A.10 The Viterbi backtrace. As we extend each path to a new state account for the next observation,\nwe keep a backpointer (shown with broken lines) to the best path that led us to this state.\n\n Finally, we can give a formal definition of the Viterbi recursion as follows:\n\n 1. Initialization:\n\n v1 ( j) = œÄ j b j (o1 ) 1‚â§ j‚â§N\n bt1 ( j) = 0 1‚â§ j‚â§N\n\n 2. Recursion\n N\n vt ( j) = max vt‚àí1 (i) ai j b j (ot ); 1 ‚â§ j ‚â§ N, 1 < t ‚â§ T\n i=1\n N\n btt ( j) = argmax vt‚àí1 (i) ai j b j (ot ); 1 ‚â§ j ‚â§ N, 1 < t ‚â§ T\n i=1\n\n 3. Termination:\n N\n The best score: P‚àó = max vT (i)\n i=1\n N\n The start of backtrace: qT ‚àó = argmax vT (i)\n i=1\n\nA.5 HMM Training: The Forward-Backward Algorithm\n We turn to the third problem for HMMs: learning the parameters of an HMM, that\n is, the A and B matrices. Formally,\n Learning: Given an observation sequence O and the set of possible\n states in the HMM, learn the HMM parameters A and B.\n A.5 ‚Ä¢ HMM T RAINING : T HE F ORWARD -BACKWARD A LGORITHM 11\n\n The input to such a learning algorithm would be an unlabeled sequence of observations O and a vocabulary of potential hidden states Q. Thus, for the ice cream\n task, we would start with a sequence of observations O = {1, 3, 2, ..., } and the set of\n hidden states H and C.\n Forward- The standard algorithm for HMM training is the forward-backward, or Baumbackward\nBaum-Welch Welch algorithm (Baum, 1972), a special case of the Expectation-Maximization\n EM or EM algorithm (Dempster et al., 1977). The algorithm will let us train both the\n transition probabilities A and the emission probabilities B of the HMM. EM is an\n iterative algorithm, computing an initial estimate for the probabilities, then using\n those estimates to compute a better estimate, and so on, iteratively improving the\n probabilities that it learns.\n Let us begin by considering the much simpler case of training a fully visible\n Markov model, where we know both the temperature and the ice cream count for\n every day. That is, imagine we see the following set of input observations and magically knew the aligned hidden state sequences:\n 3 3 2 1 1 2 1 2 3\n hot hot cold cold cold cold cold hot hot\n This would easily allow us to compute the HMM parameters just by maximum\n likelihood estimation from the training data. First, we can compute œÄ from the count\n of the 3 initial hidden states:\n\n œÄh = 1/3 œÄc = 2/3\n\n Next we can directly compute the A matrix from the transitions, ignoring the final\n hidden states:\n\n p(hot|hot) = 2/3 p(cold|hot) = 1/3\n p(cold|cold) = 2/3 p(hot|cold) = 1/3\n\n and the B matrix:\n\n P(1|hot) = 0/4 = 0 p(1|cold) = 3/5 = .6\n P(2|hot) = 1/4 = .25 p(2|cold = 2/5 = .4\n P(3|hot) = 3/4 = .75 p(3|cold) = 0\n\n For a real HMM, we cannot compute these counts directly from an observation\n sequence since we don‚Äôt know which path of states was taken through the machine\n for a given input. For example, suppose I didn‚Äôt tell you the temperature on day 2,\n and you had to guess it, but you (magically) had the above probabilities, and the\n temperatures on the other days. You could do some Bayesian arithmetic with all the\n other probabilities to get estimates of the likely temperature on that missing day, and\n use those to get expected counts for the temperatures for day 2.\n But the real problem is even harder: we don‚Äôt know the counts of being in any\n of the hidden states!! The Baum-Welch algorithm solves this by iteratively estimating the counts. We will start with an estimate for the transition and observation\n probabilities and then use these estimated probabilities to derive better and better\n probabilities. And we‚Äôre going to do this by computing the forward probability for\n an observation and then dividing that probability mass among all the different paths\n that contributed to this forward probability.\n To understand the algorithm, we need to define a useful probability related to the\n backward\n probability forward probability and called the backward probability. The backward probabil-\n12 A PPENDIX A ‚Ä¢ H IDDEN M ARKOV M ODELS\n\n ity Œ≤ is the probability of seeing the observations from time t + 1 to the end, given\n that we are in state i at time t (and given the automaton Œª ):\n\n Œ≤t (i) = P(ot+1 , ot+2 . . . oT |qt = i, Œª ) (A.15)\n It is computed inductively in a similar manner to the forward algorithm.\n\n 1. Initialization:\n\n Œ≤T (i) = 1, 1 ‚â§ i ‚â§ N\n\n 2. Recursion\n N\n X\n Œ≤t (i) = ai j b j (ot+1 ) Œ≤t+1 ( j), 1 ‚â§ i ‚â§ N, 1 ‚â§ t < T\n j=1\n\n 3. Termination:\n N\n X\n P(O|Œª ) = œÄ j b j (o1 ) Œ≤1 ( j)\n j=1\n\n Figure A.11 illustrates the backward induction step.\n\n Œ≤t+1(N)\n qN Œ≤t(i)= Œ£j Œ≤t+1(j) aij bj(ot+1) qN\n aiN\n\n qi\n\n ai3 Œ≤t+1(3)\n q3 q3\n ai2 bN(ot+1)\n Œ≤t+1(2)\n b3(ot+1)\n q2 q2 ai1 q2 b2(ot+1)\n Œ≤t+1(1)\n b1(ot+1)\n q1 q1 q1\n\n ot-1 ot ot+1\n\n Figure A.11 The computation of Œ≤t (i) by summing all the successive values Œ≤t+1 ( j)\n weighted by their transition probabilities ai j and their observation probabilities b j (ot+1 ).\n\n We are now ready to see how the forward and backward probabilities can help\n compute the transition probability ai j and observation probability bi (ot ) from an observation sequence, even though the actual path taken through the model is hidden.\n Let‚Äôs begin by seeing how to estimate aÃÇi j by a variant of simple maximum likelihood estimation:\n expected number of transitions from state i to state j\n aÃÇi j = (A.16)\n expected number of transitions from state i\n How do we compute the numerator? Here‚Äôs the intuition. Assume we had some\n estimate of the probability that a given transition i ‚Üí j was taken at a particular\n point in time t in the observation sequence. If we knew this probability for each\n particular time t, we could sum over all times t to estimate the total count for the\n transition i ‚Üí j.\n A.5 ‚Ä¢ HMM T RAINING : T HE F ORWARD -BACKWARD A LGORITHM 13\n\n More formally, let‚Äôs define the probability Œæt as the probability of being in state\ni at time t and state j at time t + 1, given the observation sequence and of course the\nmodel:\n\n Œæt (i, j) = P(qt = i, qt+1 = j|O, Œª ) (A.17)\n\nTo compute Œæt , we first compute a probability which is similar to Œæt , but differs in\nincluding the probability of the observation; note the different conditioning of O\nfrom Eq. A.17:\n\n not-quite-Œæt (i, j) = P(qt = i, qt+1 = j, O|Œª ) (A.18)\n\n si sj\n\n aijbj(ot+1)\n\n Œ±t(i) Œ≤t+1(j)\n\n ot-1 ot ot+1 ot+2\n\nFigure A.12 Computation of the joint probability of being in state i at time t and state j at\ntime t + 1. The figure shows the various probabilities that need to be combined to produce\nP(qt = i, qt+1 = j, O|Œª ): the Œ± and Œ≤ probabilities, the transition probability ai j and the\nobservation probability b j (ot+1 ). After Rabiner (1989) which is ¬©1989 IEEE.\n\n Figure A.12 shows the various probabilities that go into computing not-quite-Œæt :\nthe transition probability for the arc in question, the Œ± probability before the arc, the\nŒ≤ probability after the arc, and the observation probability for the symbol just after\nthe arc. These four are multiplied together to produce not-quite-Œæt as follows:\n\n not-quite-Œæt (i, j) = Œ±t (i) ai j b j (ot+1 )Œ≤t+1 ( j) (A.19)\n\nTo compute Œæt from not-quite-Œæt , we follow the laws of probability and divide by\nP(O|Œª ), since\n\n P(X,Y |Z)\n P(X|Y, Z) = (A.20)\n P(Y |Z)\n The probability of the observation given the model is simply the forward probability of the whole utterance (or alternatively, the backward probability of the whole\nutterance):\n X N\n P(O|Œª ) = Œ±t ( j)Œ≤t ( j) (A.21)\n j=1\n14 A PPENDIX A ‚Ä¢ H IDDEN M ARKOV M ODELS\n\n So, the final equation for Œæt is\n Œ±t (i) ai j b j (ot+1 )Œ≤t+1 ( j)\n Œæt (i, j) = PN (A.22)\n j=1 Œ±t ( j)Œ≤t ( j)\n\n The expected number of transitions from state i to state j is then the sum over all\n t of Œæ . For our estimate of ai j in Eq. A.16, we just need one more thing: the total\n expected number of transitions from state i. We can get this by summing over all\n transitions out of state i. Here‚Äôs the final formula for aÃÇi j :\n PT ‚àí1\n Œæt (i, j)\n aÃÇi j = PT ‚àí1t=1\n PN (A.23)\n t=1 k=1 Œæt (i, k)\n\n We also need a formula for recomputing the observation probability. This is the\n probability of a given symbol vk from the observation vocabulary V , given a state j:\n bÃÇ j (vk ). We will do this by trying to compute\n\n expected number of times in state j and observing symbol vk\n bÃÇ j (vk ) = (A.24)\n expected number of times in state j\n For this, we will need to know the probability of being in state j at time t, which\n we will call Œ≥t ( j):\n\n Œ≥t ( j) = P(qt = j|O, Œª ) (A.25)\n Once again, we will compute this by including the observation sequence in the\n probability:\n P(qt = j, O|Œª )\n Œ≥t ( j) = (A.26)\n P(O|Œª )\n\n sj\n\n Œ±t(j) Œ≤t(j)\n\n ot-1 ot ot+1\n\n Figure A.13 The computation of Œ≥t ( j), the probability of being in state j at time t. Note\n that Œ≥ is really a degenerate case of Œæ and hence this figure is like a version of Fig. A.12 with\n state i collapsed with state j. After Rabiner (1989) which is ¬©1989 IEEE.\n\n As Fig. A.13 shows, the numerator of Eq. A.26 is just the product of the forward\n probability and the backward probability:\n Œ±t ( j)Œ≤t ( j)\n Œ≥t ( j) = (A.27)\n P(O|Œª )\n A.5 ‚Ä¢ HMM T RAINING : T HE F ORWARD -BACKWARD A LGORITHM 15\n\n We are ready to compute b. For the numerator, we sum Œ≥t ( j) for all time steps\n t in which the observation ot is the symbol vk that we are interested in. For the\n denominator, we sum Œ≥t ( j) over all time steps t. The result is the\n PTpercentage of the\n times that we were in state j and saw symbol vk (the notation t=1 s.t.Ot =vk means\n ‚Äúsum over all t for which the observation at time t was vk ‚Äù):\n PT\n t=1 s.t.Ot =vk Œ≥t ( j)\n bÃÇ j (vk ) = PT (A.28)\n t=1 Œ≥t ( j)\n\n We now have ways in Eq. A.23 and Eq. A.28 to re-estimate the transition A and observation B probabilities from an observation sequence O, assuming that we already\n have a previous estimate of A and B.\n These re-estimations form the core of the iterative forward-backward algorithm.\n The forward-backward algorithm (Fig. A.14) starts with some initial estimate of the\n HMM parameters Œª = (A, B). We then iteratively run two steps. Like other cases of\n the EM (expectation-maximization) algorithm, the forward-backward algorithm has\nE-step two steps: the expectation step, or E-step, and the maximization step, or M-step.\nM-step In the E-step, we compute the expected state occupancy count Œ≥ and the expected\n state transition count Œæ from the earlier A and B probabilities. In the M-step, we use\n Œ≥ and Œæ to recompute new A and B probabilities.\n\n function F ORWARD -BACKWARD(observations of len T, output vocabulary V, hidden\n state set Q) returns HMM=(A,B)\n\n initialize A and B\n iterate until convergence\n E-step\n Œ± ( j)Œ≤t ( j)\n Œ≥t ( j) = t ‚àÄ t and j\n Œ±T (qF )\n Œ± (i) ai j b j (ot+1 )Œ≤t+1 ( j)\n Œæt (i, j) = t ‚àÄ t, i, and j\n Œ±T (qF )\n M-step\n T\n X ‚àí1\n Œæt (i, j)\n t=1\n aÃÇi j = T ‚àí1 X N\n X\n Œæt (i, k)\n t=1 k=1\n XT\n Œ≥t ( j)\n t=1s.t. Ot =vk\n bÃÇ j (vk ) = T\n X\n Œ≥t ( j)\n t=1\n return A, B\n\n Figure A.14 The forward-backward algorithm.\n\n Although in principle the forward-backward algorithm can do completely unsupervised learning of the A and B parameters, in practice the initial conditions are\n very important. For this reason the algorithm is often given extra information. For\n example, for HMM-based speech recognition, the HMM structure is often set by\n hand, and only the emission (B) and (non-zero) A transition probabilities are trained\n from a set of observation sequences O.\n16 A PPENDIX A ‚Ä¢ H IDDEN M ARKOV M ODELS\n\nA.6 Summary\n This chapter introduced the hidden Markov model for probabilistic sequence classification.\n ‚Ä¢ Hidden Markov models (HMMs) are a way of relating a sequence of observations to a sequence of hidden classes or hidden states that explain the\n observations.\n ‚Ä¢ The process of discovering the sequence of hidden states, given the sequence\n of observations, is known as decoding or inference. The Viterbi algorithm is\n commonly used for decoding.\n ‚Ä¢ The parameters of an HMM are the A transition probability matrix and the B\n observation likelihood matrix. Both can be trained with the Baum-Welch or\n forward-backward algorithm.\n\nHistorical Notes\n As we discussed in Chapter 17, Markov chains were first used by Markov (1913)\n (translation Markov 2006), to predict whether an upcoming letter in Pushkin‚Äôs Eugene Onegin would be a vowel or a consonant. The hidden Markov model was developed by Baum and colleagues at the Institute for Defense Analyses in Princeton\n (Baum and Petrie 1966, Baum and Eagon 1967).\n The Viterbi algorithm was first applied to speech and language processing in the\n context of speech recognition by Vintsyuk (1968) but has what Kruskal (1983) calls\n a ‚Äúremarkable history of multiple independent discovery and publication‚Äù. Kruskal\n and others give at least the following independently-discovered variants of the algorithm published in four separate fields:\n\n Citation Field\n Viterbi (1967) information theory\n Vintsyuk (1968) speech processing\n Needleman and Wunsch (1970) molecular biology\n Sakoe and Chiba (1971) speech processing\n Sankoff (1972) molecular biology\n Reichert et al. (1973) molecular biology\n Wagner and Fischer (1974) computer science\n\n The use of the term Viterbi is now standard for the application of dynamic programming to any kind of probabilistic maximization problem in speech and language\n processing. For non-probabilistic problems (such as for minimum edit distance), the\n plain term dynamic programming is often used. Forney, Jr. (1973) wrote an early\n survey paper that explores the origin of the Viterbi algorithm in the context of information and communications theory.\n Our presentation of the idea that hidden Markov models should be characterized\n by three fundamental problems was modeled after an influential tutorial by Rabiner\n (1989), which was itself based on tutorials by Jack Ferguson of IDA in the 1960s.\n Jelinek (1997) and Rabiner and Juang (1993) give very complete descriptions of the\n forward-backward algorithm as applied to the speech recognition problem. Jelinek\n (1997) also shows the relationship between forward-backward and EM.\n Historical Notes 17\n\nBaum, L. E. 1972. An inequality and associated maximiza- Vintsyuk, T. K. 1968. Speech discrimination by dynamic\n tion technique in statistical estimation for probabilistic programming. Cybernetics, 4(1):52‚Äì57. Original Rusfunctions of Markov processes. Inequalities III: Pro- sian: Kibernetika 4(1):81-88. 1968.\n ceedings of the 3rd Symposium on Inequalities. Academic Viterbi, A. J. 1967. Error bounds for convolutional codes and\n Press. an asymptotically optimum decoding algorithm. IEEE\nBaum, L. E. and J. A. Eagon. 1967. An inequality with appli- Transactions on Information Theory, IT-13(2):260‚Äì269.\n cations to statistical estimation for probabilistic functions Wagner, R. A. and M. J. Fischer. 1974. The string-to-string\n of Markov processes and to a model for ecology. Bulletin correction problem. Journal of the ACM, 21:168‚Äì173.\n of the American Mathematical Society, 73(3):360‚Äì363.\nBaum, L. E. and T. Petrie. 1966. Statistical inference for\n probabilistic functions of finite-state Markov chains. Annals of Mathematical Statistics, 37(6):1554‚Äì1563.\nDempster, A. P., N. M. Laird, and D. B. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, 39(1):1‚Äì\n 21.\nEisner, J. 2002. An interactive spreadsheet for teaching\n the forward-backward algorithm. Proceedings of the\n ACL Workshop on Effective Tools and Methodologies for\n Teaching NLP and CL.\nForney, Jr., G. D. 1973. The Viterbi algorithm. Proceedings\n of the IEEE, 61(3):268‚Äì278.\nJelinek, F. 1997. Statistical Methods for Speech Recognition.\n MIT Press.\nKruskal, J. B. 1983. An overview of sequence comparison.\n In D. Sankoff and J. B. Kruskal, eds, Time Warps, String\n Edits, and Macromolecules: The Theory and Practice of\n Sequence Comparison, 1‚Äì44. Addison-Wesley.\nMarkov, A. A. 1913. Essai d‚Äôune recherche statistique sur\n le texte du roman ‚ÄúEugene Onegin‚Äù illustrant la liaison\n des epreuve en chain (‚ÄòExample of a statistical investigation of the text of ‚ÄúEugene Onegin‚Äù illustrating the dependence between samples in chain‚Äô). Izvistia Imperatorskoi Akademii Nauk (Bulletin de l‚ÄôAcadeÃÅmie ImpeÃÅriale\n des Sciences de St.-PeÃÅtersbourg), 7:153‚Äì162.\nMarkov, A. A. 2006. Classical text in translation: A. A.\n Markov, an example of statistical investigation of the text\n Eugene Onegin concerning the connection of samples in\n chains. Science in Context, 19(4):591‚Äì600. Translated by\n David Link.\nNeedleman, S. B. and C. D. Wunsch. 1970. A general\n method applicable to the search for similarities in the\n amino-acid sequence of two proteins. Journal of Molecular Biology, 48:443‚Äì453.\nRabiner, L. R. 1989. A tutorial on hidden Markov models\n and selected applications in speech recognition. Proceedings of the IEEE, 77(2):257‚Äì286.\nRabiner, L. R. and B. H. Juang. 1993. Fundamentals of\n Speech Recognition. Prentice Hall.\nReichert, T. A., D. N. Cohen, and A. K. C. Wong. 1973.\n An application of information theory to genetic mutations\n and the matching of polypeptide sequences. Journal of\n Theoretical Biology, 42:245‚Äì261.\nSakoe, H. and S. Chiba. 1971. A dynamic programming\n approach to continuous speech recognition. Proceedings\n of the Seventh International Congress on Acoustics, volume 3. AkadeÃÅmiai KiadoÃÅ.\nSankoff, D. 1972. Matching sequences under deletioninsertion constraints. Proceedings of the National\n Academy of Sciences, 69:4‚Äì6.\n",
    "file_path": "/Users/colinsidberry/nlp-textbook-rag/data/extracted_text/A.Hidden Markov Models.txt",
    "file_size_kb": 39.87
  },
  {
    "id": "5f78218fa07c0bcf",
    "source": "nlp_textbook",
    "chapter": "Naive Bayes, Text Classification, B and Sentiment",
    "filename": "B.Naive Bayes, Text Classification, and Sentiment.txt",
    "content": "rights reserved. Draft of August 24, 2025.\n\nCHAPTER\n\n Naive Bayes, Text Classification,\nB and Sentiment\n Classification lies at the heart of both human and machine intelligence. Deciding\n what letter, word, or image has been presented to our senses, recognizing faces\n or voices, sorting mail, assigning grades to homeworks; these are all examples of\n assigning a category to an input. The potential challenges of this task are highlighted\n by the fabulist Jorge Luis Borges (1964), who imagined classifying animals into:\n (a) those that belong to the Emperor, (b) embalmed ones, (c) those that\n are trained, (d) suckling pigs, (e) mermaids, (f) fabulous ones, (g) stray\n dogs, (h) those that are included in this classification, (i) those that\n tremble as if they were mad, (j) innumerable ones, (k) those drawn with\n a very fine camel‚Äôs hair brush, (l) others, (m) those that have just broken\n a flower vase, (n) those that resemble flies from a distance.\n Many language processing tasks involve classification, although luckily our classes\n are much easier to define than those of Borges. In this chapter we introduce the naive\n text\n categorization Bayes algorithm and apply it to text categorization, the task of assigning a label or\n category to an entire text or document.\n sentiment\n analysis We focus on one common text categorization task, sentiment analysis, the extraction of sentiment, the positive or negative orientation that a writer expresses\n toward some object. A review of a movie, book, or product on the web expresses the\n author‚Äôs sentiment toward the product, while an editorial or political text expresses\n sentiment toward a candidate or political action. Extracting consumer or public sentiment is thus relevant for fields from marketing to politics.\n The simplest version of sentiment analysis is a binary classification task, and\n the words of the review provide excellent cues. Consider, for example, the following phrases extracted from positive and negative reviews of movies and restaurants.\n Words like great, richly, awesome, and pathetic, and awful and ridiculously are very\n informative cues:\n + ...zany characters and richly applied satire, and some great plot twists\n ‚àí It was pathetic. The worst part about it was the boxing scenes...\n + ...awesome caramel sauce and sweet toasty almonds. I love this place!\n ‚àí ...awful pizza and ridiculously overpriced...\nspam detection Spam detection is another important commercial application, the binary classification task of assigning an email to one of the two classes spam or not-spam.\n Many lexical and other features can be used to perform this classification. For example you might quite reasonably be suspicious of an email containing phrases like\n ‚Äúonline pharmaceutical‚Äù or ‚ÄúWITHOUT ANY COST‚Äù or ‚ÄúDear Winner‚Äù.\n Another thing we might want to know about a text is the language it‚Äôs written\n in. Texts on social media, for example, can be in any number of languages and\n language id we‚Äôll need to apply different processing. The task of language id is thus the first\n step in most language processing pipelines. Related text classification tasks like auauthorship thorship attribution‚Äî determining a text‚Äôs author‚Äî are also relevant to the digital\n attribution\n humanities, social sciences, and forensic linguistics.\n2 A PPENDIX B ‚Ä¢ NAIVE BAYES , T EXT C LASSIFICATION , AND S ENTIMENT\n\n Finally, one of the oldest tasks in text classification is assigning a library subject category or topic label to a text. Deciding whether a research paper concerns\n epidemiology or instead, perhaps, embryology, is an important component of information retrieval. Various sets of subject categories exist, such as the MeSH (Medical\n Subject Headings) thesaurus. In fact, as we will see, subject category classification\n is the task for which the naive Bayes algorithm was invented in 1961 (Maron, 1961).\n Classification is essential for tasks below the level of the document as well.\n We‚Äôve already seen period disambiguation (deciding if a period is the end of a sentence or part of a word), and word tokenization (deciding if a character should be\n a word boundary). Even language modeling can be viewed as classification: each\n word can be thought of as a class, and so predicting the next word is classifying the\n context-so-far into a class for each next word. A part-of-speech tagger (Chapter 17)\n classifies each occurrence of a word in a sentence as, e.g., a noun or a verb.\n The goal of classification is to take a single observation, extract some useful\n features, and thereby classify the observation into one of a set of discrete classes.\n One method for classifying text is to use rules handwritten by humans. Handwritten rule-based classifiers can be components of state-of-the-art systems in language\n processing. But rules can be fragile, as situations or data change over time, and for\n some tasks humans aren‚Äôt necessarily good at coming up with the rules.\n supervised\n The most common way of doing text classification in language processing is\n machine instead via supervised machine learning, the subject of this chapter. In supervised\n learning\n learning, we have a data set of input observations, each associated with some correct\n output (a ‚Äòsupervision signal‚Äô). The goal of the algorithm is to learn how to map\n from a new observation to a correct output.\n Formally, the task of supervised classification is to take an input x and a fixed\n set of output classes Y = {y1 , y2 , ..., yM } and return a predicted class y ‚àà Y . For\n text classification, we‚Äôll sometimes talk about c (for ‚Äúclass‚Äù) instead of y as our\n output variable, and d (for ‚Äúdocument‚Äù) instead of x as our input variable. In the\n supervised situation we have a training set of N documents that have each been handlabeled with a class: {(d1 , c1 ), ...., (dN , cN )}. Our goal is to learn a classifier that is\n capable of mapping from a new document d to its correct class c ‚àà C, where C is\n some set of useful document classes. A probabilistic classifier additionally will tell\n us the probability of the observation being in the class. This full distribution over\n the classes can be useful information for downstream decisions; avoiding making\n discrete decisions early on can be useful when combining systems.\n Many kinds of machine learning algorithms are used to build classifiers. This\n chapter introduces naive Bayes; the following one introduces logistic regression.\n These exemplify two ways of doing classification. Generative classifiers like naive\n Bayes build a model of how a class could generate some input data. Given an observation, they return the class most likely to have generated the observation. Discriminative classifiers like logistic regression instead learn what features from the\n input are most useful to discriminate between the different possible classes. While\n discriminative systems are often more accurate and hence more commonly used,\n generative classifiers still have a role.\n\nB.1 Naive Bayes Classifiers\n naive Bayes In this section we introduce the multinomial naive Bayes classifier, so called beclassifier\n cause it is a Bayesian classifier that makes a simplifying (naive) assumption about\n B.1 ‚Ä¢ NAIVE BAYES C LASSIFIERS 3\n\n how the features interact.\n The intuition of the classifier is shown in Fig. B.1. We represent a text document\n bag of words as if it were a bag of words, that is, an unordered set of words with their position\n ignored, keeping only their frequency in the document. In the example in the figure,\n instead of representing the word order in all the phrases like ‚ÄúI love this movie‚Äù and\n ‚ÄúI would recommend it‚Äù, we simply note that the word I occurred 5 times in the\n entire excerpt, the word it 6 times, the words love, recommend, and movie once, and\n so on.\n\n it 6\n I 5\nI love this movie! It's sweet, the 4\nbut with satirical humor. The fairy always love it to 3\n it whimsical it to and 3\ndialogue is great and the I\n and seen are seen 2\nadventure scenes are fun... friend anyone\nIt manages to be whimsical happy dialogue yet 1\nand romantic while laughing adventure recommend would 1\n satirical whimsical 1\nat the conventions of the who sweet of movie it\nfairy tale genre. I would it I to\n but romantic I times 1\nrecommend it to just about several yet sweet 1\nanyone. I've seen it several again it the humor satirical 1\n the seen would\ntimes, and I'm always happy adventure 1\n to scenes I the manages\nto see it again whenever I the genre 1\n fun I times and fairy 1\nhave a friend who hasn't and\n about while humor 1\nseen it yet! whenever have\n conventions have 1\n with great 1\n ‚Ä¶ ‚Ä¶\n\nFigure B.1 Intuition of the multinomial naive Bayes classifier applied to a movie review. The position of the\nwords is ignored (the bag-of-words assumption) and we make use of the frequency of each word.\n\n Naive Bayes is a probabilistic classifier, meaning that for a document d, out of\n all classes c ‚àà C the classifier returns the class cÃÇ which has the maximum posterior\n ÀÜ probability given the document. In Eq. B.1 we use the hat notation ÀÜ to mean ‚Äúour\n argmax estimate of the correct class‚Äù, and we use argmax to mean an operation that selects\n the argument (in this case the class c) that maximizes a function (in this case the\n probability P(c|d)).\n\n cÃÇ = argmax P(c|d) (B.1)\n c‚ààC\n\n Bayesian This idea of Bayesian inference has been known since the work of Bayes (1763),\n inference\n and was first applied to text classification by Mosteller and Wallace (1964). The\n intuition of Bayesian classification is to use Bayes‚Äô rule to transform Eq. B.1 into\n other probabilities that have some useful properties. Bayes‚Äô rule is presented in\n Eq. B.2; it gives us a way to break down any conditional probability P(x|y) into\n three other probabilities:\n P(y|x)P(x)\n P(x|y) = (B.2)\n P(y)\n4 A PPENDIX B ‚Ä¢ NAIVE BAYES , T EXT C LASSIFICATION , AND S ENTIMENT\n\n We can then substitute Eq. B.2 into Eq. B.1 to get Eq. B.3:\n\n P(d|c)P(c)\n cÃÇ = argmax P(c|d) = argmax (B.3)\n c‚ààC c‚ààC P(d)\n\n We can conveniently simplify Eq. B.3 by dropping the denominator P(d). This\n is possible because we will be computing P(d|c)P(c)\n P(d) for each possible class. But P(d)\n doesn‚Äôt change for each class; we are always asking about the most likely class for\n the same document d, which must have the same probability P(d). Thus, we can\n choose the class that maximizes this simpler formula:\n\n cÃÇ = argmax P(c|d) = argmax P(d|c)P(c) (B.4)\n c‚ààC c‚ààC\n\n We call Naive Bayes a generative model because we can read Eq. B.4 as stating\n a kind of implicit assumption about how a document is generated: first a class is\n sampled from P(c), and then the words are generated by sampling from P(d|c). (In\n fact we could imagine generating artificial documents, or at least their word counts,\n by following this process). We‚Äôll say more about this intuition of generative models\n in Chapter 4.\n To return to classification: we compute the most probable class cÃÇ given some\n document d by choosing the class which has the highest product of two probabilities:\n prior\n probability the prior probability of the class P(c) and the likelihood of the document P(d|c):\n likelihood\n likelihood prior\n z }| { z}|{\n cÃÇ = argmax P(d|c) P(c) (B.5)\n c‚ààC\n\n Without loss of generality, we can represent a document d as a set of features\n f1 , f2 , ..., fn :\n\n likelihood prior\n z }| { z}|{\n cÃÇ = argmax P( f1 , f2 , ...., fn |c) P(c) (B.6)\n c‚ààC\n\n Unfortunately, Eq. B.6 is still too hard to compute directly: without some simplifying assumptions, estimating the probability of every possible combination of\n features (for example, every possible set of words and positions) would require huge\n numbers of parameters and impossibly large training sets. Naive Bayes classifiers\n therefore make two simplifying assumptions.\n The first is the bag-of-words assumption discussed intuitively above: we assume\n position doesn‚Äôt matter, and that the word ‚Äúlove‚Äù has the same effect on classification\n whether it occurs as the 1st, 20th, or last word in the document. Thus we assume\n that the features f1 , f2 , ..., fn only encode word identity and not position.\n naive Bayes\n assumption The second is commonly called the naive Bayes assumption: this is the conditional independence assumption that the probabilities P( fi |c) are independent given\n the class c and hence can be ‚Äònaively‚Äô multiplied as follows:\n\n P( f1 , f2 , ...., fn |c) = P( f1 |c) ¬∑ P( f2 |c) ¬∑ ... ¬∑ P( fn |c) (B.7)\n\n The final equation for the class chosen by a naive Bayes classifier is thus:\n Y\n cNB = argmax P(c) P( f |c) (B.8)\n c‚ààC f ‚ààF\n B.2 ‚Ä¢ T RAINING THE NAIVE BAYES C LASSIFIER 5\n\n To apply the naive Bayes classifier to text, we will use each word in the documents\n as a feature, as suggested above, and we consider each of the words in the document\n by walking an index through every word position in the document:\n positions ‚Üê all word positions in test document\n Y\n cNB = argmax P(c) P(wi |c) (B.9)\n c‚ààC i‚ààpositions\n\n Naive Bayes calculations, like calculations for language modeling, are done in log\n space, to avoid underflow and increase speed. Thus Eq. B.9 is generally instead\n expressed1 as\n X\n cNB = argmax log P(c) + log P(wi |c) (B.10)\n c‚ààC i‚ààpositions\n\n By considering features in log space, Eq. B.10 computes the predicted class as a linear function of input features. Classifiers that use a linear combination of the inputs\n to make a classification decision ‚Äîlike naive Bayes and also logistic regression‚Äî\n linear are called linear classifiers.\n classifiers\n\nB.2 Training the Naive Bayes Classifier\n How can we learn the probabilities P(c) and P( fi |c)? Let‚Äôs first consider the maximum likelihood estimate. We‚Äôll simply use the frequencies in the data. For the class\n prior P(c) we ask what percentage of the documents in our training set are in each\n class c. Let Nc be the number of documents in our training data with class c and\n Ndoc be the total number of documents. Then:\n Nc\n PÃÇ(c) = (B.11)\n Ndoc\n To learn the probability P( fi |c), we‚Äôll assume a feature is just the existence of a word\n in the document‚Äôs bag of words, and so we‚Äôll want P(wi |c), which we compute as\n the fraction of times the word wi appears among all words in all documents of topic\n c. We first concatenate all documents with category c into one big ‚Äúcategory c‚Äù text.\n Then we use the frequency of wi in this concatenated document to give a maximum\n likelihood estimate of the probability:\n count(wi , c)\n PÃÇ(wi |c) = P (B.12)\n w‚ààV count(w, c)\n Here the vocabulary V consists of the union of all the word types in all classes, not\n just the words in one class c.\n There is a problem, however, with maximum likelihood training. Imagine we\n are trying to estimate the likelihood of the word ‚Äúfantastic‚Äù given class positive, but\n suppose there are no training documents that both contain the word ‚Äúfantastic‚Äù and\n are classified as positive. Perhaps the word ‚Äúfantastic‚Äù happens to occur (sarcastically?) in the class negative. In such a case the probability for this feature will be\n zero:\n count(‚Äúfantastic‚Äù, positive)\n PÃÇ(‚Äúfantastic‚Äù|positive) = P =0 (B.13)\n w‚ààV count(w, positive)\n\n 1 In practice throughout this book, we‚Äôll use log to mean natural log (ln) when the base is not specified.\n6 A PPENDIX B ‚Ä¢ NAIVE BAYES , T EXT C LASSIFICATION , AND S ENTIMENT\n\n But since naive Bayes naively multiplies all the feature likelihoods together, zero\n probabilities in the likelihood term for any class will cause the probability of the\n class to be zero, no matter the other evidence!\n The simplest solution is the add-one (Laplace) smoothing introduced in Chapter 3. While Laplace smoothing is usually replaced by more sophisticated smoothing\n algorithms in language modeling, it is commonly used in naive Bayes text categorization:\n count(wi , c) + 1 count(wi , c) + 1\n PÃÇ(wi |c) = P = P \u0001 (B.14)\n w‚ààV (count(w, c) + 1) w‚ààV count(w, c) + |V |\n\n Note once again that it is crucial that the vocabulary V consists of the union of all the\n word types in all classes, not just the words in one class c (try to convince yourself\n why this must be true; see the exercise at the end of the chapter).\n What do we do about words that occur in our test data but are not in our vocabulary at all because they did not occur in any training document in any class? The\nunknown word solution for such unknown words is to ignore them‚Äîremove them from the test\n document and not include any probability for them at all.\n Finally, some systems choose to completely ignore another class of words: stop\n stop words words, very frequent words like the and a. This can be done by sorting the vocabulary by frequency in the training set, and defining the top 10‚Äì100 vocabulary entries\n as stop words, or alternatively by using one of the many predefined stop word lists\n available online. Then each instance of these stop words is simply removed from\n both training and test documents as if it had never occurred. In most text classification applications, however, using a stop word list doesn‚Äôt improve performance, and\n so it is more common to make use of the entire vocabulary and not use a stop word\n list.\n Fig. B.2 shows the final algorithm.\n\nB.3 Worked example\n Let‚Äôs walk through an example of training and testing naive Bayes with add-one\n smoothing. We‚Äôll use a sentiment analysis domain with the two classes positive\n (+) and negative (-), and take the following miniature training and test documents\n simplified from actual movie reviews.\n Cat Documents\n Training - just plain boring\n - entirely predictable and lacks energy\n - no surprises and very few laughs\n + very powerful\n + the most fun film of the summer\n Test ? predictable with no fun\n The prior P(c) for the two classes is computed via Eq. B.11 as NNc :\n doc\n\n 3 2\n P(‚àí) = P(+) =\n 5 5\n The word with doesn‚Äôt occur in the training set, so we drop it completely (as\n mentioned above, we don‚Äôt use unknown word models for naive Bayes). The likelihoods from the training set for the remaining three words ‚Äúpredictable‚Äù, ‚Äúno‚Äù, and\n B.4 ‚Ä¢ O PTIMIZING FOR S ENTIMENT A NALYSIS 7\n\n function T RAIN NAIVE BAYES(D, C) returns V, log P(c), log P(w|c)\n\n for each class c ‚àà C # Calculate P(c) terms\n Ndoc = number of documents in D\n Nc = number of documents from D in class c\n Nc\n logprior[c] ‚Üê log\n Ndoc\n V ‚Üê vocabulary of D\n bigdoc[c] ‚Üê append(d) for d ‚àà D with class c\n for each word w in V # Calculate P(w|c) terms\n count(w,c) ‚Üê # of occurrences of w in bigdoc[c]\n count(w, c) + 1\n loglikelihood[w,c] ‚Üê log P 0\n w0 in V (count (w , c) + 1)\n return logprior, loglikelihood, V\n\n function T EST NAIVE BAYES(testdoc, logprior, loglikelihood, C, V) returns best c\n\n for each class c ‚àà C\n sum[c] ‚Üê logprior[c]\n for each position i in testdoc\n word ‚Üê testdoc[i]\n if word ‚àà V\n sum[c] ‚Üê sum[c]+ loglikelihood[word,c]\n return argmaxc sum[c]\n\n Figure B.2 The naive Bayes algorithm, using add-1 smoothing. To use add-Œ± smoothing\n instead, change the +1 to +Œ± for loglikelihood counts in training.\n\n ‚Äúfun‚Äù, are as follows, from Eq. B.14 (computing the probabilities for the remainder\n of the words in the training set is left as an exercise for the reader):\n 1+1 0+1\n P(‚Äúpredictable‚Äù|‚àí) = P(‚Äúpredictable‚Äù|+) =\n 14 + 20 9 + 20\n 1+1 0+1\n P(‚Äúno‚Äù|‚àí) = P(‚Äúno‚Äù|+) =\n 14 + 20 9 + 20\n 0+1 1+1\n P(‚Äúfun‚Äù|‚àí) = P(‚Äúfun‚Äù|+) =\n 14 + 20 9 + 20\n For the test sentence S = ‚Äúpredictable with no fun‚Äù, after removing the word ‚Äòwith‚Äô,\n the chosen class, via Eq. B.9, is therefore computed as follows:\n 3 2√ó2√ó1\n P(‚àí)P(S|‚àí) = √ó = 6.1 √ó 10‚àí5\n 5 343\n 2 1√ó1√ó2\n P(+)P(S|+) = √ó = 3.2 √ó 10‚àí5\n 5 293\n The model thus predicts the class negative for the test sentence.\n\nB.4 Optimizing for Sentiment Analysis\n While standard naive Bayes text classification can work well for sentiment analysis,\n some small changes are generally employed that improve performance.\n8 A PPENDIX B ‚Ä¢ NAIVE BAYES , T EXT C LASSIFICATION , AND S ENTIMENT\n\n First, for sentiment classification and a number of other text classification tasks,\n whether a word occurs or not seems to matter more than its frequency. Thus it often\n improves performance to clip the word counts in each document at 1 (see the end\n of the chapter for pointers to these results). This variant is called binary multinobinary naive\n Bayes mial naive Bayes or binary naive Bayes. The variant uses the same algorithm as in\n Fig. B.2 except that for each document we remove all duplicate words before concatenating them into the single big document during training and we also remove\n duplicate words from test documents. Fig. B.3 shows an example in which a set\n of four documents (shortened and text-normalized for this example) are remapped\n to binary, with the modified counts shown in the table on the right. The example\n is worked without add-1 smoothing to make the differences clearer. Note that the\n results counts need not be 1; the word great has a count of 2 even for binary naive\n Bayes, because it appears in multiple documents.\n\n NB Binary\n Counts Counts\n Four original documents: + ‚àí + ‚àí\n ‚àí it was pathetic the worst part was the and 2 0 1 0\n boxing scenes boxing 0 1 0 1\n film 1 0 1 0\n ‚àí no plot twists or great scenes great 3 1 2 1\n + and satire and great plot twists it 0 1 0 1\n + great scenes great film no 0 1 0 1\n or 0 1 0 1\n After per-document binarization: part 0 1 0 1\n ‚àí it was pathetic the worst part boxing pathetic 0 1 0 1\n plot 1 1 1 1\n scenes satire 1 0 1 0\n ‚àí no plot twists or great scenes scenes 1 2 1 2\n + and satire great plot twists the 0 2 0 1\n + great scenes film twists 1 1 1 1\n was 0 2 0 1\n worst 0 1 0 1\n Figure B.3 An example of binarization for the binary naive Bayes algorithm.\n\n A second important addition commonly made when doing text classification for\n sentiment is to deal with negation. Consider the difference between I really like this\n movie (positive) and I didn‚Äôt like this movie (negative). The negation expressed by\n didn‚Äôt completely alters the inferences we draw from the predicate like. Similarly,\n negation can modify a negative word to produce a positive review (don‚Äôt dismiss this\n film, doesn‚Äôt let us get bored).\n A very simple baseline that is commonly used in sentiment analysis to deal with\n negation is the following: during text normalization, prepend the prefix NOT to\n every word after a token of logical negation (n‚Äôt, not, no, never) until the next punctuation mark. Thus the phrase\n didn‚Äôt like this movie , but I\n becomes\n didn‚Äôt NOT_like NOT_this NOT_movie , but I\n Newly formed ‚Äòwords‚Äô like NOT like, NOT recommend will thus occur more\n often in negative document and act as cues for negative sentiment, while words\n like NOT bored, NOT dismiss will acquire positive associations. Syntactic parsing\n (Chapter 18) can be used to deal more accurately with the scope relationship between\n B.5 ‚Ä¢ NAIVE BAYES FOR OTHER TEXT CLASSIFICATION TASKS 9\n\n these negation words and the predicates they modify, but this simple baseline works\n quite well in practice.\n Finally, in some situations we might have insufficient labeled training data to\n train accurate naive Bayes classifiers using all words in the training set to estimate\n positive and negative sentiment. In such cases we can instead derive the positive\n sentiment and negative word features from sentiment lexicons, lists of words that are prelexicons\n annotated with positive or negative sentiment. Four popular lexicons are the General\n General\n Inquirer Inquirer (Stone et al., 1966), LIWC (Pennebaker et al., 2007), the opinion lexicon\n LIWC of Hu and Liu (2004) and the MPQA Subjectivity Lexicon (Wilson et al., 2005).\n For example the MPQA subjectivity lexicon has 6885 words each marked for\n whether it is strongly or weakly biased positive or negative. Some examples:\n + : admirable, beautiful, confident, dazzling, ecstatic, favor, glee, great\n ‚àí : awful, bad, bias, catastrophe, cheat, deny, envious, foul, harsh, hate\n A common way to use lexicons in a naive Bayes classifier is to add a feature\n that is counted whenever a word from that lexicon occurs. Thus we might add a\n feature called ‚Äòthis word occurs in the positive lexicon‚Äô, and treat all instances of\n words in the lexicon as counts for that one feature, instead of counting each word\n separately. Similarly, we might add as a second feature ‚Äòthis word occurs in the\n negative lexicon‚Äô of words in the negative lexicon. If we have lots of training data,\n and if the test data matches the training data, using just two features won‚Äôt work as\n well as using all the words. But when training data is sparse or not representative of\n the test set, using dense lexicon features instead of sparse individual-word features\n may generalize better.\n We‚Äôll return to this use of lexicons in Chapter 22, showing how these lexicons\n can be learned automatically, and how they can be applied to many other tasks beyond sentiment classification.\n\nB.5 Naive Bayes for other text classification tasks\n In the previous section we pointed out that naive Bayes doesn‚Äôt require that our\n classifier use all the words in the training data as features. In fact features in naive\n Bayes can express any property of the input text we want.\nspam detection Consider the task of spam detection, deciding if a particular piece of email is\n an example of spam (unsolicited bulk email)‚Äîone of the first applications of naive\n Bayes to text classification (Sahami et al., 1998).\n A common solution here, rather than using all the words as individual features,\n is to predefine likely sets of words or phrases as features, combined with features\n that are not purely linguistic. For example the open-source SpamAssassin tool2\n predefines features like the phrase ‚Äúone hundred percent guaranteed‚Äù, or the feature\n mentions millions of dollars, which is a regular expression that matches suspiciously\n large sums of money. But it also includes features like HTML has a low ratio of text\n to image area, that aren‚Äôt purely linguistic and might require some sophisticated\n computation, or totally non-linguistic features about, say, the path that the email\n took to arrive. More sample SpamAssassin features:\n ‚Ä¢ Email subject line is all capital letters\n ‚Ä¢ Contains phrases of urgency like ‚Äúurgent reply‚Äù\n 2 https://spamassassin.apache.org\n10 A PPENDIX B ‚Ä¢ NAIVE BAYES , T EXT C LASSIFICATION , AND S ENTIMENT\n\n ‚Ä¢ Email subject line contains ‚Äúonline pharmaceutical‚Äù\n ‚Ä¢ HTML has unbalanced ‚Äúhead‚Äù tags\n ‚Ä¢ Claims you can be removed from the list\n language id For other tasks, like language id‚Äîdetermining what language a given piece\n of text is written in‚Äîthe most effective naive Bayes features are not words at all,\n but character n-grams, 2-grams (‚Äòzw‚Äô) 3-grams (‚Äònya‚Äô, ‚Äò Vo‚Äô), or 4-grams (‚Äòie z‚Äô,\n ‚Äòthei‚Äô), or, even simpler byte n-grams, where instead of using the multibyte Unicode\n character representations called codepoints, we just pretend everything is a string of\n raw bytes. Because spaces count as a byte, byte n-grams can model statistics about\n the beginning or ending of words. A widely used naive Bayes system, langid.py\n (Lui and Baldwin, 2012) begins with all possible n-grams of lengths 1-4, using feature selection to winnow down to the most informative 7000 final features.\n Language ID systems are trained on multilingual text, such as Wikipedia (Wikipedia text in 68 different languages was used by (Lui and Baldwin, 2011)), or\n newswire. To make sure that this multilingual text correctly reflects different regions, dialects, and socioeconomic classes, systems also add Twitter text in many\n languages geotagged to many regions (important for getting world English dialects\n from countries with large Anglophone populations like Nigeria or India), Bible and\n Quran translations, slang websites like Urban Dictionary, corpora of African American Vernacular English (Blodgett et al., 2016), and so on (Jurgens et al., 2017).\n\nB.6 Naive Bayes as a Language Model\n As we saw in the previous section, naive Bayes classifiers can use any sort of feature:\n dictionaries, URLs, email addresses, network features, phrases, and so on. But if,\n as in Section B.3, we use only individual word features, and we use all of the words\n in the text (not a subset), then naive Bayes has an important similarity to language\n modeling. Specifically, a naive Bayes model can be viewed as a set of class-specific\n unigram language models, in which the model for each class instantiates a unigram\n language model.\n Since the likelihood features from the naive Bayes model assign a probability to\n each word P(word|c), the model also assigns a probability to each sentence:\n Y\n P(s|c) = P(wi |c) (B.15)\n i‚ààpositions\n\n Thus consider a naive Bayes model with the classes positive (+) and negative (-)\n and the following model parameters:\n\n w P(w|+) P(w|-)\n I 0.1 0.2\n love 0.1 0.001\n this 0.01 0.01\n fun 0.05 0.005\n film 0.1 0.1\n ... ... ...\n\n Each of the two columns above instantiates a language model that can assign a\n probability to the sentence ‚ÄúI love this fun film‚Äù:\n B.7 ‚Ä¢ E VALUATION : P RECISION , R ECALL , F- MEASURE 11\n\n P(‚ÄúI love this fun film‚Äù|+) = 0.1 √ó 0.1 √ó 0.01 √ó 0.05 √ó 0.1 = 5 √ó 10‚àí7\n P(‚ÄúI love this fun film‚Äù|‚àí) = 0.2 √ó 0.001 √ó 0.01 √ó 0.005 √ó 0.1 = 1.0 √ó 10‚àí9\n\n As it happens, the positive model assigns a higher probability to the sentence:\n P(s|pos) > P(s|neg). Note that this is just the likelihood part of the naive Bayes\n model; once we multiply in the prior a full naive Bayes model might well make a\n different classification decision.\n\nB.7 Evaluation: Precision, Recall, F-measure\n To introduce the methods for evaluating text classification, let‚Äôs first consider some\n simple binary detection tasks. For example, in spam detection, our goal is to label\n every text as being in the spam category (‚Äúpositive‚Äù) or not in the spam category\n (‚Äúnegative‚Äù). For each item (email document) we therefore need to know whether\n our system called it spam or not. We also need to know whether the email is actually\n spam or not, i.e. the human-defined labels for each document that we are trying to\n gold labels match. We will refer to these human labels as the gold labels.\n Or imagine you‚Äôre the CEO of the Delicious Pie Company and you need to know\n what people are saying about your pies on social media, so you build a system that\n detects tweets concerning Delicious Pie. Here the positive class is tweets about\n Delicious Pie and the negative class is all other tweets.\n In both cases, we need a metric for knowing how well our spam detector (or\n pie-tweet-detector) is doing. To evaluate any system for detecting things, we start\n confusion by building a confusion matrix like the one shown in Fig. B.4. A confusion matrix\n matrix\n is a table for visualizing how an algorithm performs with respect to the human gold\n labels, using two dimensions (system output and gold labels), and each cell labeling\n a set of possible outcomes. In the spam detection case, for example, true positives\n are documents that are indeed spam (indicated by human-created gold labels) that\n our system correctly said were spam. False negatives are documents that are indeed\n spam but our system incorrectly labeled as non-spam.\n To the bottom right of the table is the equation for accuracy, which asks what\n percentage of all the observations (for the spam or pie examples that means all emails\n or tweets) our system labeled correctly. Although accuracy might seem a natural\n metric, we generally don‚Äôt use it for text classification tasks. That‚Äôs because accuracy\n doesn‚Äôt work well when the classes are unbalanced (as indeed they are with spam,\n which is a large majority of email, or with tweets, which are mainly not about pie).\n To make this more explicit, imagine that we looked at a million tweets, and\n let‚Äôs say that only 100 of them are discussing their love (or hatred) for our pie,\n while the other 999,900 are tweets about something completely unrelated. Imagine a\n simple classifier that stupidly classified every tweet as ‚Äúnot about pie‚Äù. This classifier\n would have 999,900 true negatives and only 100 false negatives for an accuracy of\n 999,900/1,000,000 or 99.99%! What an amazing accuracy level! Surely we should\n be happy with this classifier? But of course this fabulous ‚Äòno pie‚Äô classifier would\n be completely useless, since it wouldn‚Äôt find a single one of the customer comments\n we are looking for. In other words, accuracy is not a good metric when the goal is\n to discover something that is rare, or at least not completely balanced in frequency,\n which is a very common situation in the world.\n12 A PPENDIX B ‚Ä¢ NAIVE BAYES , T EXT C LASSIFICATION , AND S ENTIMENT\n\n gold standard labels\n gold positive gold negative\n system system tp\n positive true positive false positive precision = tp+fp\n output\n labels system\n negative false negative true negative\n tp tp+tn\n recall = accuracy =\n tp+fn tp+fp+tn+fn\n\n Figure B.4 A confusion matrix for visualizing how well a binary classification system performs against gold standard labels.\n\n That‚Äôs why instead of accuracy we generally turn to two other metrics shown in\n precision Fig. B.4: precision and recall. Precision measures the percentage of the items that\n the system detected (i.e., the system labeled as positive) that are in fact positive (i.e.,\n are positive according to the human gold labels). Precision is defined as\n\n true positives\n Precision =\n true positives + false positives\n recall Recall measures the percentage of items actually present in the input that were\n correctly identified by the system. Recall is defined as\n\n true positives\n Recall =\n true positives + false negatives\n\n Precision and recall will help solve the problem with the useless ‚Äúnothing is\n pie‚Äù classifier. This classifier, despite having a fabulous accuracy of 99.99%, has\n a terrible recall of 0 (since there are no true positives, and 100 false negatives, the\n recall is 0/100). You should convince yourself that the precision at finding relevant\n tweets is equally problematic. Thus precision and recall, unlike accuracy, emphasize\n true positives: finding the things that we are supposed to be looking for.\n There are many ways to define a single metric that incorporates aspects of both\n F-measure precision and recall. The simplest of these combinations is the F-measure (van\n Rijsbergen, 1975) , defined as:\n (Œ≤ 2 + 1)PR\n FŒ≤ =\n Œ≤ 2P + R\n The Œ≤ parameter differentially weights the importance of recall and precision,\n based perhaps on the needs of an application. Values of Œ≤ > 1 favor recall, while\n values of Œ≤ < 1 favor precision. When Œ≤ = 1, precision and recall are equally bal-\nF1 anced; this is the most frequently used metric, and is called FŒ≤ =1 or just F1 :\n 2PR\n F1 = (B.16)\n P+R\n F-measure comes from a weighted harmonic mean of precision and recall. The\n harmonic mean of a set of numbers is the reciprocal of the arithmetic mean of reciprocals:\n n\n HarmonicMean(a1 , a2 , a3 , a4 , ..., an ) = 1 1 1 1\n (B.17)\n a1 + a2 + a3 + ... + an\n B.7 ‚Ä¢ E VALUATION : P RECISION , R ECALL , F- MEASURE 13\n\n and hence F-measure is\n\n (Œ≤ 2 + 1)PR\n \u0012 \u0013\n 1 1‚àíŒ±\n F= or with Œ≤ 2 = F= (B.18)\n Œ± P + (1 ‚àí Œ±) R1 Œ± Œ≤ 2P + R\n\n Harmonic mean is used because the harmonic mean of two values is closer to the\n minimum of the two values than the arithmetic mean is. Thus it weighs the lower of\n the two numbers more heavily, which is more conservative in this situation.\n\n B.7.1 Evaluating with more than two classes\n Up to now we have been describing text classification tasks with only two classes.\n But lots of classification tasks in language processing have more than two classes.\n For sentiment analysis we generally have 3 classes (positive, negative, neutral) and\n even more classes are common for tasks like part-of-speech tagging, word sense\n disambiguation, semantic role labeling, emotion detection, and so on. Luckily the\n naive Bayes algorithm is already a multi-class classification algorithm.\n\n gold labels\n urgent normal spam\n urgent 8 10 1 precisionu=\n 8+10+1\n system 60\n output normal 5 60 50 precisionn=\n 5+60+50\n spam 3 30 200 precisions=\n 3+30+200\n recallu = recalln = recalls =\n 8 60 200\n 8+5+3 10+60+30 1+50+200\n\n Figure B.5 Confusion matrix for a three-class categorization task, showing for each pair of\n classes (c1 , c2 ), how many documents from c1 were (in)correctly assigned to c2 .\n\n But we‚Äôll need to slightly modify our definitions of precision and recall. Consider the sample confusion matrix for a hypothetical 3-way one-of email categorization decision (urgent, normal, spam) shown in Fig. B.5. The matrix shows, for\n example, that the system mistakenly labeled one spam document as urgent, and we\n have shown how to compute a distinct precision and recall value for each class. In\n order to derive a single metric that tells us how well the system is doing, we can commacroaveraging bine these values in two ways. In macroaveraging, we compute the performance\nmicroaveraging for each class, and then average over classes. In microaveraging, we collect the decisions for all classes into a single confusion matrix, and then compute precision and\n recall from that table. Fig. B.6 shows the confusion matrix for each class separately,\n and shows the computation of microaveraged and macroaveraged precision.\n As the figure shows, a microaverage is dominated by the more frequent class (in\n this case spam), since the counts are pooled. The macroaverage better reflects the\n statistics of the smaller classes, and so is more appropriate when performance on all\n the classes is equally important.\n14 A PPENDIX B ‚Ä¢ NAIVE BAYES , T EXT C LASSIFICATION , AND S ENTIMENT\n\n Class 1: Urgent Class 2: Normal Class 3: Spam Pooled\n true true true true true true true true\n urgent not normal not spam not yes no\n system system system system\n urgent 8 11 normal 60 55 spam 200 33 yes 268 99\n system system system system\n not 8 340 not 40 212 not 51 83 no 99 635\n 8 60 200 microaverage = 268\n precision = = .42 precision = = .52 precision = = .86 = .73\n 8+11 60+55 200+33 precision 268+99\n\n macroaverage = .42+.52+.86\n = .60\n precision 3\n\nFigure B.6 Separate confusion matrices for the 3 classes from the previous figure, showing the pooled confusion matrix and the microaveraged and macroaveraged precision.\n\nB.8 Test sets and Cross-validation\n\n The training and testing procedure for text classification follows what we saw with\n language modeling (Section ??): we use the training set to train the model, then use\n development the development test set (also called a devset) to perhaps tune some parameters,\n test set\n devset and in general decide what the best model is. Once we come up with what we think\n is the best model, we run it on the (hitherto unseen) test set to report its performance.\n While the use of a devset avoids overfitting the test set, having a fixed training set, devset, and test set creates another problem: in order to save lots of data\n for training, the test set (or devset) might not be large enough to be representative.\n Wouldn‚Äôt it be better if we could somehow use all our data for training and still use\ncross-validation all our data for test? We can do this by cross-validation.\n In cross-validation, we choose a number k, and partition our data into k disjoint\n folds subsets called folds. Now we choose one of those k folds as a test set, train our\n classifier on the remaining k ‚àí 1 folds, and then compute the error rate on the test\n set. Then we repeat with another fold as the test set, again training on the other k ‚àí 1\n folds. We do this sampling process k times and average the test set error rate from\n these k runs to get an average error rate. If we choose k = 10, we would train 10\n different models (each on 90% of our data), test the model 10 times, and average\n 10-fold these 10 values. This is called 10-fold cross-validation.\ncross-validation\n The only problem with cross-validation is that because all the data is used for\n testing, we need the whole corpus to be blind; we can‚Äôt examine any of the data\n to suggest possible features and in general see what‚Äôs going on, because we‚Äôd be\n peeking at the test set, and such cheating would cause us to overestimate the performance of our system. However, looking at the corpus to understand what‚Äôs going\n on is important in designing NLP systems! What to do? For this reason, it is common to create a fixed training set and test set, then do 10-fold cross-validation inside\n the training set, but compute error rate the normal way in the test set, as shown in\n Fig. B.7.\n B.9 ‚Ä¢ S TATISTICAL S IGNIFICANCE T ESTING 15\n\n Training Iterations Testing\n 1 Dev Training\n 2 Dev Training\n 3 Dev Training\n 4 Dev Training\n Test\n 5 Training Dev Training\n Set\n 6 Training Dev\n 7 Training Dev\n 8 Training Dev\n 9 Training Dev\n 10 Training Dev\n\n Figure B.7 10-fold cross-validation\n\nB.9 Statistical Significance Testing\n In building systems we often need to compare the performance of two systems. How\n can we know if the new system we just built is better than our old one? Or better\n than some other system described in the literature? This is the domain of statistical\n hypothesis testing, and in this section we introduce tests for statistical significance\n for NLP classifiers, drawing especially on the work of Dror et al. (2020) and Berg-\nKirkpatrick et al. (2012).\n Suppose we‚Äôre comparing the performance of classifiers A and B on a metric M\n such as F1 , or accuracy. Perhaps we want to know if our logistic regression sentiment classifier A (Chapter 4) gets a higher F1 score than our naive Bayes sentiment\n classifier B on a particular test set x. Let‚Äôs call M(A, x) the score that system A gets\n on test set x, and Œ¥ (x) the performance difference between A and B on x:\n\n Œ¥ (x) = M(A, x) ‚àí M(B, x) (B.19)\n\n We would like to know if Œ¥ (x) > 0, meaning that our logistic regression classifier\n effect size has a higher F1 than our naive Bayes classifier on x. Œ¥ (x) is called the effect size; a\n bigger Œ¥ means that A seems to be way better than B; a small Œ¥ means A seems to\n be only a little better.\n Why don‚Äôt we just check if Œ¥ (x) is positive? Suppose we do, and we find that\n the F1 score of A is higher than B‚Äôs by .04. Can we be certain that A is better? We\n cannot! That‚Äôs because A might just be accidentally better than B on this particular x.\n We need something more: we want to know if A‚Äôs superiority over B is likely to hold\n again if we checked another test set x0 , or under some other set of circumstances.\n In the paradigm of statistical hypothesis testing, we test this by formalizing two\n hypotheses.\n\n H0 : Œ¥ (x) ‚â§ 0\n H1 : Œ¥ (x) > 0 (B.20)\n\nnull hypothesis The hypothesis H0 , called the null hypothesis, supposes that Œ¥ (x) is actually negative or zero, meaning that A is not better than B. We would like to know if we can\n confidently rule out this hypothesis, and instead support H1 , that A is better.\n We do this by creating a random variable X ranging over all test sets. Now we\n ask how likely is it, if the null hypothesis H0 was correct, that among these test sets\n16 A PPENDIX B ‚Ä¢ NAIVE BAYES , T EXT C LASSIFICATION , AND S ENTIMENT\n\n we would encounter the value of Œ¥ (x) that we found, if we repeated the experiment\n p-value a great many times. We formalize this likelihood as the p-value: the probability,\n assuming the null hypothesis H0 is true, of seeing the Œ¥ (x) that we saw or one even\n greater\n P(Œ¥ (X) ‚â• Œ¥ (x)|H0 is true) (B.21)\n\n So in our example, this p-value is the probability that we would see Œ¥ (x) assuming\n A is not better than B. If Œ¥ (x) is huge (let‚Äôs say A has a very respectable F1 of .9\n and B has a terrible F1 of only .2 on x), we might be surprised, since that would be\n extremely unlikely to occur if H0 were in fact true, and so the p-value would be low\n (unlikely to have such a large Œ¥ if A is in fact not better than B). But if Œ¥ (x) is very\n small, it might be less surprising to us even if H0 were true and A is not really better\n than B, and so the p-value would be higher.\n A very small p-value means that the difference we observed is very unlikely\n under the null hypothesis, and we can reject the null hypothesis. What counts as very\n small? It is common to use values like .05 or .01 as the thresholds. A value of .01\n means that if the p-value (the probability of observing the Œ¥ we saw assuming H0 is\n true) is less than .01, we reject the null hypothesis and assume that A is indeed better\n statistically\n significant than B. We say that a result (e.g., ‚ÄúA is better than B‚Äù) is statistically significant if\n the Œ¥ we saw has a probability that is below the threshold and we therefore reject\n this null hypothesis.\n How do we compute this probability we need for the p-value? In NLP we generally don‚Äôt use simple parametric tests like t-tests or ANOVAs that you might be\n familiar with. Parametric tests make assumptions about the distributions of the test\n statistic (such as normality) that don‚Äôt generally hold in our cases. So in NLP we\n usually use non-parametric tests based on sampling: we artificially create many versions of the experimental setup. For example, if we had lots of different test sets x0\n we could just measure all the Œ¥ (x0 ) for all the x0 . That gives us a distribution. Now\n we set a threshold (like .01) and if we see in this distribution that 99% or more of\n those deltas are smaller than the delta we observed, i.e., that p-value(x)‚Äîthe probability of seeing a Œ¥ (x) as big as the one we saw‚Äîis less than .01, then we can reject\n the null hypothesis and agree that Œ¥ (x) was a sufficiently surprising difference and\n A is really a better algorithm than B.\n There are two common non-parametric tests used in NLP: approximate ranapproximate domization (Noreen, 1989) and the bootstrap test. We will describe bootstrap\n randomization\n below, showing the paired version of the test, which again is most common in NLP.\n paired Paired tests are those in which we compare two sets of observations that are aligned:\n each observation in one set can be paired with an observation in another. This happens naturally when we are comparing the performance of two systems on the same\n test set; we can pair the performance of system A on an individual observation xi\n with the performance of system B on the same xi .\n\n B.9.1 The Paired Bootstrap Test\n bootstrap test The bootstrap test (Efron and Tibshirani, 1993) can apply to any metric; from precision, recall, or F1 to the BLEU metric used in machine translation. The word\n bootstrapping bootstrapping refers to repeatedly drawing large numbers of samples with replacement (called bootstrap samples) from an original set. The intuition of the bootstrap\n test is that we can create many virtual test sets from an observed test set by repeatedly sampling from it. The method only makes the assumption that the sample is\n representative of the population.\n B.9 ‚Ä¢ S TATISTICAL S IGNIFICANCE T ESTING 17\n\n Consider a tiny text classification example with a test set x of 10 documents. The\nfirst row of Fig. B.8 shows the results of two classifiers (A and B) on this test set.\nEach document is labeled by one of the four possibilities (A and B both right, both\nwrong, A right and B wrong, A wrong and B right). A slash through a letter (\u0013 B)\nmeans that that classifier got the answer wrong. On the first document both A and\nB get the correct class (AB), while on the second document A got it right but B got\nit wrong (A\u0013 B). If we assume for simplicity that our metric is accuracy, A has an\naccuracy of .70 and B of .50, so Œ¥ (x) is .20.\n Now we create a large number b (perhaps 105 ) of virtual test sets x(i) , each of size\nn = 10. Fig. B.8 shows a couple of examples. To create each virtual test set x(i) , we\nrepeatedly (n = 10 times) select a cell from row x with replacement. For example, to\ncreate the first cell of the first virtual test set x(1) , if we happened to randomly select\nthe second cell of the x row, we would copy the value A\u0013 B into our new cell, and\nmove on to create the second cell of x(1) , each time sampling (randomly choosing)\nfrom the original x with replacement.\n\n 1 2 3 4 5 6 7 8 9 10 A% B% Œ¥ ()\nx AB AB \u0013 AB AB AB\n \u0013 AB AB\n \u0013 AB AB\n \u0013 AB\n \u0013 .70 .50 .20\nx(1) AB\u0013 AB AB\u0013 AB AB AB\n \u0013 AB AB AB\n \u0013 AB .60 .60 .00\nx(2) AB\u0013 AB AB\u0013 AB AB AB AB AB\n \u0013 AB AB .60 .70 -.10\n...\nx(b)\nFigure B.8 The paired bootstrap test: Examples of b pseudo test sets x(i) being created\nfrom an initial true test set x. Each pseudo test set is created by sampling n = 10 times with\nreplacement; thus an individual sample is a single cell, a document with its gold label and\nthe correct or incorrect performance of classifiers A and B. Of course real test sets don‚Äôt have\nonly 10 examples, and b needs to be large as well.\n\n Now that we have the b test sets, providing a sampling distribution, we can do\nstatistics on how often A has an accidental advantage. There are various ways to\ncompute this advantage; here we follow the version laid out in Berg-Kirkpatrick\net al. (2012). Assuming H0 (A isn‚Äôt better than B), we would expect that Œ¥ (X),\nestimated over many test sets, would be zero or negative; a much higher value would\nbe surprising, since H0 specifically assumes A isn‚Äôt better than B. To measure exactly\nhow surprising our observed Œ¥ (x) is, we would in other circumstances compute the\np-value by counting over many test sets how often Œ¥ (x(i) ) exceeds the expected zero\nvalue by Œ¥ (x) or more:\n\n b\n 1 X \u0010 (i) \u0011\n p-value(x) = 1 Œ¥ (x ) ‚àí Œ¥ (x) ‚â• 0\n b\n i=1\n\n(We use the notation 1(x) to mean ‚Äú1 if x is true, and 0 otherwise‚Äù.) However,\nalthough it‚Äôs generally true that the expected value of Œ¥ (X) over many test sets,\n(again assuming A isn‚Äôt better than B) is 0, this isn‚Äôt true for the bootstrapped test\nsets we created. That‚Äôs because we didn‚Äôt draw these samples from a distribution\nwith 0 mean; we happened to create them from the original test set x, which happens\nto be biased (by .20) in favor of A. So to measure how surprising is our observed\nŒ¥ (x), we actually compute the p-value by counting over many test sets how often\n18 A PPENDIX B ‚Ä¢ NAIVE BAYES , T EXT C LASSIFICATION , AND S ENTIMENT\n\n Œ¥ (x(i) ) exceeds the expected value of Œ¥ (x) by Œ¥ (x) or more:\n b\n 1 X \u0010 (i) \u0011\n p-value(x) = 1 Œ¥ (x ) ‚àí Œ¥ (x) ‚â• Œ¥ (x)\n b\n i=1\n b\n 1X \u0010 \u0011\n = 1 Œ¥ (x(i) ) ‚â• 2Œ¥ (x) (B.22)\n b\n i=1\n\n So if for example we have 10,000 test sets x(i) and a threshold of .01, and in only 47\n of the test sets do we find that A is accidentally better Œ¥ (x(i) ) ‚â• 2Œ¥ (x), the resulting\n p-value of .0047 is smaller than .01, indicating that the delta we found, Œ¥ (x) is indeed\n sufficiently surprising and unlikely to have happened by accident, and we can reject\n the null hypothesis and conclude A is better than B.\n\n function B OOTSTRAP(test set x, num of samples b) returns p-value(x)\n\n Calculate Œ¥ (x) # how much better does algorithm A do than B on x\n s=0\n for i = 1 to b do\n for j = 1 to n do # Draw a bootstrap sample x(i) of size n\n Select a member of x at random and add it to x(i)\n Calculate Œ¥ (x(i) ) # how much better does algorithm A do than B on x(i)\n s ‚Üê s + 1 if Œ¥ (x(i) ) ‚â• 2Œ¥ (x)\n p-value(x) ‚âà bs # on what % of the b samples did algorithm A beat expectations?\n return p-value(x) # if very few did, our observed Œ¥ is probably not accidental\n\n Figure B.9 A version of the paired bootstrap algorithm after Berg-Kirkpatrick et al. (2012).\n\n The full algorithm for the bootstrap is shown in Fig. B.9. It is given a test set x, a\n number of samples b, and counts the percentage of the b bootstrap test sets in which\n Œ¥ (x(i) ) > 2Œ¥ (x). This percentage then acts as a one-sided empirical p-value.\n\nB.10 Avoiding Harms in Classification\n It is important to avoid harms that may result from classifiers, harms that exist both\n for naive Bayes classifiers and for the other classification algorithms we introduce\n in later chapters.\nrepresentational One class of harms is representational harms (Crawford 2017, Blodgett et al.\n harms\n 2020), harms caused by a system that demeans a social group, for example by perpetuating negative stereotypes about them. For example Kiritchenko and Mohammad (2018) examined the performance of 200 sentiment analysis systems on pairs of\n sentences that were identical except for containing either a common African American first name (like Shaniqua) or a common European American first name (like\n Stephanie), chosen from the Caliskan et al. (2017) study discussed in Chapter 5.\n They found that most systems assigned lower sentiment and more negative emotion\n to sentences with African American names, reflecting and perpetuating stereotypes\n that associate African Americans with negative emotions (Popp et al., 2003).\n In other tasks classifiers may lead to both representational harms and other\n harms, such as silencing. For example the important text classification task of tox-\nB.11 ‚Ä¢ S UMMARY 19\n\n toxicity icity detection is the task of detecting hate speech, abuse, harassment, or other\n detection\n kinds of toxic language. While the goal of such classifiers is to help reduce societal harm, toxicity classifiers can themselves cause harms. For example, researchers\n have shown that some widely used toxicity classifiers incorrectly flag as being toxic\n sentences that are non-toxic but simply mention identities like women (Park et al.,\n 2018), blind people (Hutchinson et al., 2020) or gay people (Dixon et al., 2018;\n Dias Oliva et al., 2021), or simply use linguistic features characteristic of varieties\n like African-American Vernacular English (Sap et al. 2019, Davidson et al. 2019).\n Such false positive errors could lead to the silencing of discourse by or about these\n groups.\n These model problems can be caused by biases or other problems in the training\n data; in general, machine learning systems replicate and even amplify the biases\n in their training data. But these problems can also be caused by the labels (for\n example due to biases in the human labelers), by the resources used (like lexicons,\n or model components like pretrained embeddings), or even by model architecture\n (like what the model is trained to optimize). While the mitigation of these biases\n (for example by carefully considering the training data sources) is an important area\n of research, we currently don‚Äôt have general solutions. For this reason it‚Äôs important,\n when introducing any NLP model, to study these kinds of factors and make them\n model card clear. One way to do this is by releasing a model card (Mitchell et al., 2019) for\n each version of a model. A model card documents a machine learning model with\n information like:\n ‚Ä¢ training algorithms and parameters\n ‚Ä¢ training data sources, motivation, and preprocessing\n ‚Ä¢ evaluation data sources, motivation, and preprocessing\n ‚Ä¢ intended use and users\n ‚Ä¢ model performance across different demographic or other groups and environmental situations\n\nB.11 Summary\n This chapter introduced the naive Bayes model for classification and applied it to\n the text categorization task of sentiment analysis.\n ‚Ä¢ Many language processing tasks can be viewed as tasks of classification.\n ‚Ä¢ Text categorization, in which an entire text is assigned a class from a finite set,\n includes such tasks as sentiment analysis, spam detection, language identification, and authorship attribution.\n ‚Ä¢ Sentiment analysis classifies a text as reflecting the positive or negative orientation (sentiment) that a writer expresses toward some object.\n ‚Ä¢ Naive Bayes is a generative model that makes the bag-of-words assumption\n (position doesn‚Äôt matter) and the conditional independence assumption (words\n are conditionally independent of each other given the class)\n ‚Ä¢ Naive Bayes with binarized features seems to work better for many text classification tasks.\n ‚Ä¢ Classifiers are evaluated based on precision and recall.\n ‚Ä¢ Classifiers are trained using distinct training, dev, and test sets, including the\n use of cross-validation in the training set.\n20 A PPENDIX B ‚Ä¢ NAIVE BAYES , T EXT C LASSIFICATION , AND S ENTIMENT\n\n ‚Ä¢ Statistical significance tests should be used to determine whether we can be\n confident that one version of a classifier is better than another.\n ‚Ä¢ Designers of classifiers should carefully consider harms that may be caused\n by the model, including its training data and other components, and report\n model characteristics in a model card.\n\nHistorical Notes\n Multinomial naive Bayes text classification was proposed by Maron (1961) at the\n RAND Corporation for the task of assigning subject categories to journal abstracts.\n His model introduced most of the features of the modern form presented here, approximating the classification task with one-of categorization, and implementing\n add-Œ¥ smoothing and information-based feature selection.\n The conditional independence assumptions of naive Bayes and the idea of Bayesian analysis of text seems to have arisen multiple times. The same year as Maron‚Äôs\n paper, Minsky (1961) proposed a naive Bayes classifier for vision and other artificial intelligence problems, and Bayesian techniques were also applied to the text\n classification task of authorship attribution by Mosteller and Wallace (1963). It had\n long been known that Alexander Hamilton, John Jay, and James Madison wrote\n the anonymously-published Federalist papers in 1787‚Äì1788 to persuade New York\n to ratify the United States Constitution. Yet although some of the 85 essays were\n clearly attributable to one author or another, the authorship of 12 were in dispute\n between Hamilton and Madison. Mosteller and Wallace (1963) trained a Bayesian\n probabilistic model on the writing of Hamilton and another model on the writings\n of Madison, then computed the maximum-likelihood author for each of the disputed\n essays. Naive Bayes was first applied to spam detection in Heckerman et al. (1998).\n Metsis et al. (2006), Pang et al. (2002), and Wang and Manning (2012) show\n that using boolean attributes with multinomial naive Bayes works better than full\n counts. Binary multinomial naive Bayes is sometimes confused with another variant\n of naive Bayes that also uses a binary representation of whether a term occurs in\n a document: Multivariate Bernoulli naive Bayes. The Bernoulli variant instead\n estimates P(w|c) as the fraction of documents that contain a term, and includes a\n probability for whether a term is not in a document. McCallum and Nigam (1998)\n and Wang and Manning (2012) show that the multivariate Bernoulli variant of naive\n Bayes doesn‚Äôt work as well as the multinomial algorithm for sentiment or other text\n tasks.\n There are a variety of sources covering the many kinds of text classification\n tasks. For sentiment analysis see Pang and Lee (2008), and Liu and Zhang (2012).\n Stamatatos (2009) surveys authorship attribute algorithms. On language identification see Jauhiainen et al. (2019); Jaech et al. (2016) is an important early neural\n system. The task of newswire indexing was often used as a test case for text classification algorithms, based on the Reuters-21578 collection of newswire articles.\n See Manning et al. (2008) and Aggarwal and Zhai (2012) on text classification;\n classification in general is covered in machine learning textbooks (Hastie et al. 2001,\n Witten and Frank 2005, Bishop 2006, Murphy 2012).\n Non-parametric methods for computing statistical significance were used first in\n NLP in the MUC competition (Chinchor et al., 1993), and even earlier in speech\n recognition (Gillick and Cox 1989, Bisani and Ney 2004). Our description of the\n bootstrap draws on the description in Berg-Kirkpatrick et al. (2012). Recent work\n has focused on issues including multiple test sets and multiple metrics (S√∏gaard et al.\n E XERCISES 21\n\n 2014, Dror et al. 2017).\n Feature selection is a method of removing features that are unlikely to generalize\n well. Features are generally ranked by how informative they are about the classificainformation\n gain tion decision. A very common metric, information gain, tells us how many bits of\n information the presence of the word gives us for guessing the class. Other feature\n selection metrics include œá 2 , pointwise mutual information, and GINI index; see\n Yang and Pedersen (1997) for a comparison and Guyon and Elisseeff (2003) for an\n introduction to feature selection.\n\nExercises\n B.1 Assume the following likelihoods for each word being part of a positive or\n negative movie review, and equal prior probabilities for each class.\n pos neg\n I 0.09 0.16\n always 0.07 0.06\n like 0.29 0.06\n foreign 0.04 0.15\n films 0.08 0.11\n What class will Naive bayes assign to the sentence ‚ÄúI always like foreign\n films.‚Äù?\n B.2 Given the following short movie reviews, each labeled with a genre, either\n comedy or action:\n 1. fun, couple, love, love comedy\n 2. fast, furious, shoot action\n 3. couple, fly, fast, fun, fun comedy\n 4. furious, shoot, shoot, fun action\n 5. fly, fast, shoot, love action\n and a new document D:\n fast, couple, shoot, fly\n compute the most likely class for D. Assume a naive Bayes classifier and use\n add-1 smoothing for the likelihoods.\n B.3 Train two models, multinomial naive Bayes and binarized naive Bayes, both\n with add-1 smoothing, on the following document counts for key sentiment\n words, with positive or negative class assigned as noted.\n doc ‚Äúgood‚Äù ‚Äúpoor‚Äù ‚Äúgreat‚Äù (class)\n d1. 3 0 3 pos\n d2. 0 1 2 pos\n d3. 1 3 0 neg\n d4. 1 5 2 neg\n d5. 0 2 0 neg\n Use both naive Bayes models to assign a class (pos or neg) to this sentence:\n A good, good plot and great characters, but poor acting.\n Recall from page 6 that with naive Bayes text classification, we simply ignore\n (throw out) any word that never occurred in the training document. (We don‚Äôt\n throw out words that appear in some classes but not others; that‚Äôs what addone smoothing is for.) Do the two models agree or disagree?\n22 Appendix B ‚Ä¢ Naive Bayes, Text Classification, and Sentiment\n\nAggarwal, C. C. and C. Zhai. 2012. A survey of text classi- Heckerman, D., E. Horvitz, M. Sahami, and S. T. Dumais.\n fication algorithms. In C. C. Aggarwal and C. Zhai, eds, 1998. A bayesian approach to filtering junk e-mail. AAAI-\nMining text data, 163‚Äì222. Springer. 98 Workshop on Learning for Text Categorization.\nBayes, T. 1763. An Essay Toward Solving a Problem in the Hu, M. and B. Liu. 2004. Mining and summarizing customer\n Doctrine of Chances, volume 53. Reprinted in Facsimiles reviews. KDD.\n of Two Papers by Bayes, Hafner Publishing, 1963. Hutchinson, B., V. Prabhakaran, E. Denton, K. Webster,\nBerg-Kirkpatrick, T., D. Burkett, and D. Klein. 2012. An Y. Zhong, and S. Denuyl. 2020. Social biases in NLP\n empirical investigation of statistical significance in NLP. models as barriers for persons with disabilities. ACL.\n EMNLP. Jaech, A., G. Mulcaire, S. Hathi, M. Ostendorf, and N. A.\nBisani, M. and H. Ney. 2004. Bootstrap estimates for confi- Smith. 2016. Hierarchical character-word models for landence intervals in ASR performance evaluation. ICASSP. guage identification. ACL Workshop on NLP for Social\nBishop, C. M. 2006. Pattern recognition and machine learn- Media.\n ing. Springer. Jauhiainen, T., M. Lui, M. Zampieri, T. Baldwin, and\nBlodgett, S. L., S. Barocas, H. DaumeÃÅ III, and H. Wallach. K. LindeÃÅn. 2019. Automatic language identification in\n 2020. Language (technology) is power: A critical survey texts: A survey. JAIR, 65(1):675‚Äì682.\n of ‚Äúbias‚Äù in NLP. ACL. Jurgens, D., Y. Tsvetkov, and D. Jurafsky. 2017. Incorpo-\nBlodgett, S. L., L. Green, and B. O‚ÄôConnor. 2016. Demo- rating dialectal variability for socially equitable language\n graphic dialectal variation in social media: A case study identification. ACL.\n of African-American English. EMNLP. Kiritchenko, S. and S. M. Mohammad. 2018. Examining\nBorges, J. L. 1964. The analytical language of john wilkins. gender and race bias in two hundred sentiment analysis\n In Other inquisitions 1937‚Äì1952. University of Texas systems. *SEM.\n Press. Trans. Ruth L. C. Simms. Liu, B. and L. Zhang. 2012. A survey of opinion mining and\nCaliskan, A., J. J. Bryson, and A. Narayanan. 2017. Seman- sentiment analysis. In C. C. Aggarwal and C. Zhai, eds,\n tics derived automatically from language corpora contain Mining text data, 415‚Äì464. Springer.\n human-like biases. Science, 356(6334):183‚Äì186. Lui, M. and T. Baldwin. 2011. Cross-domain feature selec-\nChinchor, N., L. Hirschman, and D. L. Lewis. 1993. Eval- tion for language identification. IJCNLP.\n uating Message Understanding systems: An analysis of Lui, M. and T. Baldwin. 2012. langid.py: An off-the-shelf\n the third Message Understanding Conference. Computa- language identification tool. ACL.\n tional Linguistics, 19(3):409‚Äì449. Manning, C. D., P. Raghavan, and H. SchuÃàtze. 2008. Intro-\nCrawford, K. 2017. The trouble with bias. Keynote at duction to Information Retrieval. Cambridge.\n NeurIPS. Maron, M. E. 1961. Automatic indexing: an experimental\nDavidson, T., D. Bhattacharya, and I. Weber. 2019. Racial inquiry. Journal of the ACM, 8(3):404‚Äì417.\n bias in hate speech and abusive language detection McCallum, A. and K. Nigam. 1998. A comparison of event\n datasets. Third Workshop on Abusive Language Online. models for naive bayes text classification. AAAI/ICML-98\nDias Oliva, T., D. Antonialli, and A. Gomes. 2021. Fighting Workshop on Learning for Text Categorization.\n hate speech, silencing drag queens? artificial intelligence Metsis, V., I. Androutsopoulos, and G. Paliouras. 2006.\n in content moderation and risks to lgbtq voices online. Spam filtering with naive bayes-which naive bayes?\n Sexuality & Culture, 25:700‚Äì732. CEAS.\nDixon, L., J. Li, J. Sorensen, N. Thain, and L. Vasserman. Minsky, M. 1961. Steps toward artificial intelligence. Pro-\n2018. Measuring and mitigating unintended bias in text ceedings of the IRE, 49(1):8‚Äì30.\n classification. 2018 AAAI/ACM Conference on AI, Ethics,\n Mitchell, M., S. Wu, A. Zaldivar, P. Barnes, L. Vasserman,\n and Society.\n B. Hutchinson, E. Spitzer, I. D. Raji, and T. Gebru. 2019.\nDror, R., G. Baumer, M. Bogomolov, and R. Reichart. 2017. Model cards for model reporting. ACM FAccT.\n Replicability analysis for natural language processing:\n Mosteller, F. and D. L. Wallace. 1963. Inference in an au-\nTesting significance with multiple datasets. TACL, 5:471‚Äì\n thorship problem: A comparative study of discrimination\n ‚Äì486.\n methods applied to the authorship of the disputed feder-\nDror, R., L. Peled-Cohen, S. Shlomov, and R. Reichart. alist papers. Journal of the American Statistical Associa-\n2020. Statistical Significance Testing for Natural Lan- tion, 58(302):275‚Äì309.\n guage Processing, volume 45 of Synthesis Lectures on\n Mosteller, F. and D. L. Wallace. 1964. Inference and Dis-\nHuman Language Technologies. Morgan & Claypool.\n puted Authorship: The Federalist. Springer-Verlag. 1984\nEfron, B. and R. J. Tibshirani. 1993. An introduction to the 2nd edition: Applied Bayesian and Classical Inference.\n bootstrap. CRC press. Murphy, K. P. 2012. Machine learning: A probabilistic per-\nGillick, L. and S. J. Cox. 1989. Some statistical issues in the spective. MIT Press.\n comparison of speech recognition algorithms. ICASSP. Noreen, E. W. 1989. Computer Intensive Methods for Testing\nGuyon, I. and A. Elisseeff. 2003. An introduction to variable Hypothesis. Wiley.\n and feature selection. JMLR, 3:1157‚Äì1182. Pang, B. and L. Lee. 2008. Opinion mining and sentiment\nHastie, T., R. J. Tibshirani, and J. H. Friedman. 2001. The analysis. Foundations and trends in information retrieval,\n Elements of Statistical Learning. Springer. 2(1-2):1‚Äì135.\n Exercises 23\n\nPang, B., L. Lee, and S. Vaithyanathan. 2002. Thumbs\n up? Sentiment classification using machine learning techniques. EMNLP.\nPark, J. H., J. Shin, and P. Fung. 2018. Reducing gender bias\n in abusive language detection. EMNLP.\nPennebaker, J. W., R. J. Booth, and M. E. Francis. 2007.\n Linguistic Inquiry and Word Count: LIWC 2007. Austin,\n TX.\nPopp, D., R. A. Donovan, M. Crawford, K. L. Marsh, and\n M. Peele. 2003. Gender, race, and speech style stereotypes. Sex Roles, 48(7-8):317‚Äì325.\nSahami, M., S. T. Dumais, D. Heckerman, and E. Horvitz.\n 1998. A Bayesian approach to filtering junk e-mail. AAAI\n Workshop on Learning for Text Categorization.\nSap, M., D. Card, S. Gabriel, Y. Choi, and N. A. Smith. 2019.\n The risk of racial bias in hate speech detection. ACL.\nS√∏gaard, A., A. Johannsen, B. Plank, D. Hovy, and H. M.\n Alonso. 2014. What‚Äôs in a p-value in NLP? CoNLL.\nStamatatos, E. 2009. A survey of modern authorship attribution methods. JASIST, 60(3):538‚Äì556.\nStone, P., D. Dunphry, M. Smith, and D. Ogilvie. 1966.\n The General Inquirer: A Computer Approach to Content\n Analysis. MIT Press.\nvan Rijsbergen, C. J. 1975. Information Retrieval. Butterworths.\nWang, S. and C. D. Manning. 2012. Baselines and bigrams:\n Simple, good sentiment and topic classification. ACL.\nWilson, T., J. Wiebe, and P. Hoffmann. 2005. Recognizing contextual polarity in phrase-level sentiment analysis.\n EMNLP.\nWitten, I. H. and E. Frank. 2005. Data Mining: Practical Machine Learning Tools and Techniques, 2nd edition.\n Morgan Kaufmann.\nYang, Y. and J. Pedersen. 1997. A comparative study on\n feature selection in text categorization. ICML.\n",
    "file_path": "/Users/colinsidberry/nlp-textbook-rag/data/extracted_text/B.Naive Bayes, Text Classification, and Sentiment.txt",
    "file_size_kb": 68.15
  },
  {
    "id": "6ac79c6b0ab6c51a",
    "source": "nlp_textbook",
    "chapter": "Kneser-Ney Smoothing",
    "filename": "C.Kneser-Ney Smoothing.txt",
    "content": "rights reserved. Draft of August 24, 2025.\n\nCHAPTER\n\n Kneser-Ney Smoothing\nB Kneser-Ney A popular advanced n-gram smoothing method is the interpolated Kneser-Ney algorithm (Kneser and Ney 1995, Chen and Goodman 1998).\n\nB.1 Absolute Discounting\n Kneser-Ney has its roots in a method called absolute discounting. Recall that discounting of the counts for frequent n-grams is necessary to save some probability\n mass for the smoothing algorithm to distribute to the unseen n-grams.\n To see this, we can use a clever idea from Church and Gale (1991). Consider\n an n-gram that has count 4. We need to discount this count by some amount. But\n how much should we discount it? Church and Gale‚Äôs clever idea was to look at a\n held-out corpus and just see what the count is for all those bigrams that had count\n 4 in the training set. They computed a bigram grammar from 22 million words of\n AP newswire and then checked the counts of each of these bigrams in another 22\n million words. On average, a bigram that occurred 4 times in the first 22 million\n words occurred 3.23 times in the next 22 million words. Fig. B.1 from Church and\n Gale (1991) shows these counts for bigrams with c from 0 to 9.\n\n Bigram count in Bigram count in\n training set heldout set\n 0 0.0000270\n 1 0.448\n 2 1.25\n 3 2.24\n 4 3.23\n 5 4.21\n 6 5.23\n 7 6.21\n 8 7.21\n 9 8.26\n Figure B.1 For all bigrams in 22 million words of AP newswire of count 0, 1, 2,...,9, the\n counts of these bigrams in a held-out corpus also of 22 million words.\n\n Notice in Fig. B.1 that except for the held-out counts for 0 and 1, all the other\n bigram counts in the held-out set could be estimated pretty well by just subtracting\n absolute\n discounting 0.75 from the count in the training set! Absolute discounting formalizes this intuition by subtracting a fixed (absolute) discount d from each count. The intuition\n is that since we have good estimates already for the very high counts, a small discount d won‚Äôt affect them much. It will mainly modify the smaller counts, for which\n we don‚Äôt necessarily trust the estimate anyway, and Fig. B.1 suggests that in practice this discount is actually a good one for bigrams with counts 2 through 9. The\n2 A PPENDIX B ‚Ä¢ K NESER -N EY S MOOTHING\n\n equation for interpolated absolute discounting applied to bigrams:\n\n C(wi‚àí1 wi ) ‚àí d\n PAbsoluteDiscounting (wi |wi‚àí1 ) = P + Œª (wi‚àí1 )P(wi ) (B.1)\n v C(wi‚àí1 v)\n\n The first term is the discounted bigram, with 0 ‚â§ d ‚â§ 1, and the second term is the\n unigram with an interpolation weight Œª . By inspection of Fig. B.1, it looks like just\n setting all the d values to .75 would work very well, or perhaps keeping a separate\n second discount value of 0.5 for the bigrams with counts of 1. There are principled\n methods for setting d. For example, Ney et al. (1994) set d as a function of n1 and\n n2 , the number of unigrams that have a count of 1 and a count of 2, respectively:\n n1\n d= (B.2)\n n1 + 2n2\n\nB.2 Kneser-Ney Discounting\n Kneser-Ney discounting (Kneser and Ney, 1995) augments absolute discounting\n with a more sophisticated way to handle the lower-order unigram distribution. Consider the job of predicting the next word in this sentence, assuming we are interpolating a bigram and a unigram model.\n I can‚Äôt see without my reading .\n The word glasses seems much more likely to follow here than, say, the word\n Kong, so we‚Äôd like our unigram model to prefer glasses. But in fact it‚Äôs Kong that is\n more common, since Hong Kong is a very frequent word. A standard unigram model\n will assign Kong a higher probability than glasses. We would like to capture the\n intuition that although Kong is frequent, it is mainly only frequent in the phrase Hong\n Kong, that is, after the word Hong. The word glasses has a much wider distribution.\n In other words, instead of P(w), which answers the question ‚ÄúHow likely is\n w?‚Äù, we‚Äôd like to create a unigram model that we might call PCONTINUATION , which\n answers the question ‚ÄúHow likely is w to appear as a novel continuation?‚Äù. How can\n we estimate this probability of seeing the word w as a novel continuation, in a new\n unseen context? The Kneser-Ney intuition is to base our estimate of PCONTINUATION\n on the number of different contexts word w has appeared in, that is, the number of\n bigram types it completes. Every bigram type was a novel continuation the first time\n it was seen. We hypothesize that words that have appeared in more contexts in the\n past are more likely to appear in some new context as well. The number of times a\n word w appears as a novel continuation can be expressed as:\n\n PCONTINUATION (w) ‚àù |{v : C(vw) > 0}| (B.3)\n\n To turn this count into a probability, we normalize by the total number of word\n bigram types. In summary:\n\n |{v : C(vw) > 0}|\n PCONTINUATION (w) = (B.4)\n |{(u0 , w0 ) : C(u0 w0 ) > 0}|\n An equivalent formulation based on a different metaphor is to use the number of\n word types seen to precede w (Eq. B.3 repeated):\n\n PCONTINUATION (w) ‚àù |{v : C(vw) > 0}| (B.5)\n B.2 ‚Ä¢ K NESER -N EY D ISCOUNTING 3\n\n normalized by the number of words preceding all words, as follows:\n\n |{v : C(vw) > 0}|\n PCONTINUATION (w) = P 0\n (B.6)\n w0 |{v : C(vw ) > 0}|\n\n A frequent word (Kong) occurring in only one context (Hong) will have a low continuation probability.\nInterpolated\n Kneser-Ney The final equation for Interpolated Kneser-Ney smoothing for bigrams is then:\n\n max(C(wi‚àí1 wi ) ‚àí d, 0)\n PKN (wi |wi‚àí1 ) = + Œª (wi‚àí1 )PCONTINUATION (wi ) (B.7)\n C(wi‚àí1 )\n\n The Œª is a normalizing constant that is used to distribute the probability mass we‚Äôve\n discounted:\n d\n Œª (wi‚àí1 ) = P |{w : C(wi‚àí1 w) > 0}| (B.8)\n v C(wi‚àí1 v)\n\n d\n The first term, P , is the normalized discount (the discount d, 0 ‚â§ d ‚â§\n v C(w i‚àí1 v)\n 1, was introduced in the absolute discounting section above). The second term,\n |{w : C(wi‚àí1 w) > 0}|, is the number of word types that can follow wi‚àí1 or, equivalently, the number of word types that we discounted; in other words, the number of\n times we applied the normalized discount.\n The general recursive formulation is as follows:\n\n max(cKN (w i‚àín+1: i ) ‚àí d, 0)\n PKN (wi |wi‚àín+1:i‚àí1 ) = P + Œª (wi‚àín+1:i‚àí1 )PKN (wi |wi‚àín+2:i‚àí1 ) (B.9)\n v cKN (wi‚àín+1:i‚àí1 v)\n\n where the definition of the count cKN depends on whether we are counting the\n highest-order n-gram being interpolated (for example trigram if we are interpolating\n trigram, bigram, and unigram) or one of the lower-order n-grams (bigram or unigram\n if we are interpolating trigram, bigram, and unigram):\n \u001a\n count(¬∑) for the highest order\n cKN (¬∑) = (B.10)\n continuationcount(¬∑) for lower orders\n\n The continuation count of a string ¬∑ is the number of unique single word contexts for\n that string ¬∑.\n At the termination of the recursion, unigrams are interpolated with the uniform\n distribution, where the parameter \u000f is the empty string:\n\n max(cKN (w) ‚àí d, 0) 1\n PKN (w) = P 0\n + Œª (\u000f) (B.11)\n w0 cKN (w ) V\n\n If we want to include an unknown word <UNK>, it‚Äôs just included as a regular vocabulary entry with count zero, and hence its probability will be a lambda-weighted\n uniform distribution ŒªV(\u000f) .\n The best performing version of Kneser-Ney smoothing is called modified Knesermodified\nKneser-Ney Ney smoothing, and is due to Chen and Goodman (1998). Rather than use a single\n fixed discount d, modified Kneser-Ney uses three different discounts d1 , d2 , and\n d3+ for n-grams with counts of 1, 2 and three or more, respectively. See Chen and\n Goodman (1998, p. 19) or Heafield et al. (2013) for the details.\n4 Appendix B ‚Ä¢ Kneser-Ney Smoothing\n\nChen, S. F. and J. Goodman. 1998. An empirical study of\n smoothing techniques for language modeling. Technical Report TR-10-98, Computer Science Group, Harvard\n University.\nChurch, K. W. and W. A. Gale. 1991. A comparison of the\n enhanced Good-Turing and deleted estimation methods\n for estimating probabilities of English bigrams. Computer Speech and Language, 5:19‚Äì54.\nHeafield, K., I. Pouzyrevsky, J. H. Clark, and P. Koehn. 2013.\n Scalable modified Kneser-Ney language model estimation. ACL.\nKneser, R. and H. Ney. 1995. Improved backing-off for Mgram language modeling. ICASSP, volume 1.\nNey, H., U. Essen, and R. Kneser. 1994. On structuring probabilistic dependencies in stochastic language modelling.\n Computer Speech and Language, 8:1‚Äì38.\n",
    "file_path": "/Users/colinsidberry/nlp-textbook-rag/data/extracted_text/C.Kneser-Ney Smoothing.txt",
    "file_size_kb": 8.2
  },
  {
    "id": "c193636f95bdb7d9",
    "source": "nlp_textbook",
    "chapter": "Spelling Correction and the Noisy D Channel",
    "filename": "D.Spelling Correction and the Noisy Channel.txt",
    "content": "rights reserved. Draft of August 24, 2025.\n\nCHAPTER\n\n Spelling Correction and the Noisy\nD Channel\n\n A LGERNON: But my own sweet Cecily, I have never written you any letters.\n C ECILY: You need hardly remind me of that, Ernest. I remember only too well\n that I was forced to write your letters for you. I wrote always three times a week,\n and sometimes oftener.\n A LGERNON: Oh, do let me read them, Cecily?\n C ECILY: Oh, I couldn‚Äôt possibly. They would make you far too conceited. The\n three you wrote me after I had broken off the engagement are so beautiful, and\n so badly spelled, that even now I can hardly read them without crying a little.\n Oscar Wilde, The Importance of Being Earnest\n\n Like Oscar Wilde‚Äôs fabulous Cecily, a lot of people were thinking about spelling\n during the last turn of the century. Gilbert and Sullivan provide many examples. The\n Gondoliers‚Äô Giuseppe, for example, worries that his private secretary is ‚Äúshaky in his\n spelling‚Äù, while Iolanthe‚Äôs Phyllis can ‚Äúspell every word that she uses‚Äù. Thorstein\n Veblen‚Äôs explanation (in his 1899 classic The Theory of the Leisure Class) was that\n a main purpose of the ‚Äúarchaic, cumbrous, and ineffective‚Äù English spelling system\n was to be difficult enough to provide a test of membership in the leisure class.\n Whatever the social role of spelling, we can certainly agree that many more of\n us are like Cecily than like Phyllis. Estimates for the frequency of spelling errors\n in human-typed text vary from 1-2% for carefully retyping already printed text to\n 10-15% for web queries.\n In this chapter we introduce the problem of detecting and correcting spelling\n errors. Fixing spelling errors is an integral part of writing in the modern world,\n whether this writing is part of texting on a phone, sending email, writing longer\n documents, or finding information on the web. Modern spell correctors aren‚Äôt perfect\n (indeed, autocorrect-gone-wrong is a popular source of amusement on the web) but\n they are ubiquitous in pretty much any software that relies on keyboard input.\n Spelling correction is often considered from two perspectives. Non-word spelling\n correction is the detection and correction of spelling errors that result in non-words\n (like graffe for giraffe). By contrast, real word spelling correction is the task of\n detecting and correcting spelling errors even if they accidentally result in an actual\n real-word word of English (real-word errors). This can happen from typographical errors\n errors\n (insertion, deletion, transposition) that accidentally produce a real word (e.g., there\n for three), or cognitive errors where the writer substituted the wrong spelling of a\n homophone or near-homophone (e.g., dessert for desert, or piece for peace).\n Non-word errors are detected by looking for any word not found in a dictionary. For example, the misspelling graffe above would not occur in a dictionary.\n The larger the dictionary the better; modern systems often use enormous dictionaries derived from the web. To correct non-word spelling errors we first generate\n candidates candidates: real words that have a similar letter sequence to the error. Candidate\n2 A PPENDIX D ‚Ä¢ S PELLING C ORRECTION AND THE N OISY C HANNEL\n\n corrections from the spelling error graffe might include giraffe, graf, gaffe, grail, or\n craft. We then rank the candidates using a distance metric between the source and\n the surface error. We‚Äôd like a metric that shares our intuition that giraffe is a more\n likely source than grail for graffe because giraffe is closer in spelling to graffe than\n grail is to graffe. The minimum edit distance algorithm from Chapter 2 will play a\n role here. But we‚Äôd also like to prefer corrections that are more frequent words, or\n more likely to occur in the context of the error. The noisy channel model introduced\n in the next section offers a way to formalize this intuition.\n Real word spelling error detection is a much more difficult task, since any word\n in the input text could be an error. Still, it is possible to use the noisy channel to find\n candidates for each word w typed by the user, and rank the correction that is most\n likely to have been the user‚Äôs original intention.\n\nD.1 The Noisy Channel Model\n In this section we introduce the noisy channel model and show how to apply it to\n the task of detecting and correcting spelling errors. The noisy channel model was\n applied to the spelling correction task at about the same time by researchers at AT&T\n Bell Laboratories (Kernighan et al. 1990, Church and Gale 1991) and IBM Watson\n Research (Mays et al., 1991).\n\n noisy channel\n\n original word\n\n noisy word\n\n decoder\n word hyp1\n guessed word word hyp2 noisy 1\n ...\n word hyp3\n noisy 2\n noisy N\n\n Figure D.1 In the noisy channel model, we imagine that the surface form we see is actually\n a ‚Äúdistorted‚Äù form of an original word passed through a noisy channel. The decoder passes\n each hypothesis through a model of this channel and picks the word that best matches the\n surface noisy word.\n\n noisy channel The intuition of the noisy channel model (see Fig. D.1) is to treat the misspelled\n word as if a correctly spelled word had been ‚Äúdistorted‚Äù by being passed through a\n noisy communication channel.\n This channel introduces ‚Äúnoise‚Äù in the form of substitutions or other changes to\n the letters, making it hard to recognize the ‚Äútrue‚Äù word. Our goal, then, is to build a\n model of the channel. Given this model, we then find the true word by passing every\n word of the language through our model of the noisy channel and seeing which one\n comes the closest to the misspelled word.\n Bayesian This noisy channel model is a kind of Bayesian inference. We see an observation x (a misspelled word) and our job is to find the word w that generated this\n D.1 ‚Ä¢ T HE N OISY C HANNEL M ODEL 3\n\n misspelled word. Out of all possible words in the vocabulary V we want to find the\n word w such that P(w|x) is highest. We use the hat notation ÀÜ to mean ‚Äúour estimate\n of the correct word‚Äù.\n\n wÃÇ = argmax P(w|x) (D.1)\n w‚ààV\n argmax The function argmaxx f (x) means ‚Äúthe x such that f (x) is maximized‚Äù. Equation D.1 thus means, that out of all words in the vocabulary, we want the particular\n word that maximizes the right-hand side P(w|x).\n The intuition of Bayesian classification is to use Bayes‚Äô rule to transform Eq. D.1\n into a set of other probabilities. Bayes‚Äô rule is presented in Eq. D.2; it gives us a way\n to break down any conditional probability P(a|b) into three other probabilities:\n\n P(b|a)P(a)\n P(a|b) = (D.2)\n P(b)\n\n We can then substitute Eq. D.2 into Eq. D.1 to get Eq. D.3:\n\n P(x|w)P(w)\n wÃÇ = argmax (D.3)\n w‚ààV P(x)\n\n We can conveniently simplify Eq. D.3 by dropping the denominator P(x). Why\n is that? Since we are choosing a potential correction word out of all words, we will\n be computing P(x|w)P(w)\n P(x) for each word. But P(x) doesn‚Äôt change for each word; we\n are always asking about the most likely word for the same observed error x, which\n must have the same probability P(x). Thus, we can choose the word that maximizes\n this simpler formula:\n\n wÃÇ = argmax P(x|w) P(w) (D.4)\n w‚ààV\n\n To summarize, the noisy channel model says that we have some true underlying\n word w, and we have a noisy channel that modifies the word into some possible\n likelihood misspelled observed surface form. The likelihood or channel model of the noisy\nchannel model channel producing any particular observation sequence x is modeled by P(x|w). The\n prior\n probability prior probability of a hidden word is modeled by P(w). We can compute the most\n probable word wÃÇ given that we‚Äôve seen some observed misspelling x by multiplying the prior P(w) and the likelihood P(x|w) and choosing the word for which this\n product is greatest.\n We apply the noisy channel approach to correcting non-word spelling errors by\n taking any word not in our spelling dictionary, generating a list of candidate words,\n ranking them according to Eq. D.4, and picking the highest-ranked one. We can\n modify Eq. D.4 to refer to this list of candidate words instead of the full vocabulary\n V as follows:\n channel model prior\n z }| { z }| {\n wÃÇ = argmax P(x|w) P(w) (D.5)\n w‚ààC\n\n The noisy channel algorithm is shown in Fig. D.2.\n To see the details of the computation of the likelihood and the prior (language\n model), let‚Äôs walk through an example, applying the algorithm to the example misspelling acress. The first stage of the algorithm proposes candidate corrections by\n finding words that have a similar spelling to the input word. Analysis of spelling\n4 A PPENDIX D ‚Ä¢ S PELLING C ORRECTION AND THE N OISY C HANNEL\n\n function N OISY C HANNEL S PELLING(word x, dict D, lm, editprob) returns correction\n\n if x ‚àà\n /D\n candidates, edits ‚Üê All strings at edit distance 1 from x that are ‚àà D, and their edit\n for each c, e in candidates, edits\n channel ‚Üê editprob(e)\n prior ‚Üê lm(x)\n score[c] = log channel + log prior\n return argmaxc score[c]\n\n Figure D.2 Noisy channel model for spelling correction for unknown words.\n\n error data has shown that the majority of spelling errors consist of a single-letter\n change and so we often make the simplifying assumption that these candidates have\n an edit distance of 1 from the error word. To find this list of candidates we‚Äôll use\n the minimum edit distance algorithm introduced in Chapter 2, but extended so that\n in addition to insertions, deletions, and substitutions, we‚Äôll add a fourth type of edit,\n transpositions, in which two letters are swapped. The version of edit distance with\n Damerau- transposition is called Damerau-Levenshtein edit distance. Applying all such sin-\nLevenshtein\n gle transformations to acress yields the list of candidate words in Fig. D.3.\n\n Transformation\n Correct Error Position\n Error Correction Letter Letter (Letter #) Type\n acress actress t ‚Äî 2 deletion\n acress cress ‚Äî a 0 insertion\n acress caress ca ac 0 transposition\n acress access c r 2 substitution\n acress across o e 3 substitution\n acress acres ‚Äî s 5 insertion\n acress acres ‚Äî s 4 insertion\n Figure D.3 Candidate corrections for the misspelling acress and the transformations that\n would have produced the error (after Kernighan et al. (1990)). ‚Äú‚Äî‚Äù represents a null letter.\n\n Once we have a set of a candidates, to score each one using Eq. D.5 requires that\n we compute the prior and the channel model.\n The prior probability of each correction P(w) is the language model probability\n of the word w in context, which can be computed using any language model, from\n unigram to trigram or 4-gram. For this example let‚Äôs start in the following table by\n assuming a unigram language model. We computed the language model from the\n 404,253,213 words in the Corpus of Contemporary English (COCA).\n w count(w) p(w)\n actress 9,321 .0000231\n cress 220 .000000544\n caress 686 .00000170\n access 37,038 .0000916\n across 120,844 .000299\n acres 12,874 .0000318\n channel model How can we estimate the likelihood P(x|w), also called the channel model or\n error model error model? A perfect model of the probability that a word will be mistyped would\n D.1 ‚Ä¢ T HE N OISY C HANNEL M ODEL 5\n\n condition on all sorts of factors: who the typist was, whether the typist was lefthanded or right-handed, and so on. Luckily, we can get a pretty reasonable estimate\n of P(x|w) just by looking at local context: the identity of the correct letter itself, the\n misspelling, and the surrounding letters. For example, the letters m and n are often\n substituted for each other; this is partly a fact about their identity (these two letters\n are pronounced similarly and they are next to each other on the keyboard) and partly\n a fact about context (because they are pronounced similarly and they occur in similar\n contexts).\n A simple model might estimate, for example, p(acress|across) just using the\n number of times that the letter e was substituted for the letter o in some large corpus\n of errors. To compute the probability for each edit in this way we‚Äôll need a confuconfusion sion matrix that contains counts of errors. In general, a confusion matrix lists the\n matrix\n number of times one thing was confused with another. Thus for example a substitution matrix will be a square matrix of size 26√ó26 (or more generally |A| √ó |A|,\n for an alphabet A) that represents the number of times one letter was incorrectly\n used instead of another. Following Kernighan et al. (1990) we‚Äôll use four confusion\n matrices.\n\n del[x, y]: count(xy typed as x)\n ins[x, y]: count(x typed as xy)\n sub[x, y]: count(x typed as y)\n trans[x, y]: count(xy typed as yx)\n\n Note that we‚Äôve conditioned the insertion and deletion probabilities on the previous character; we could instead have chosen to condition on the following character.\n Where do we get these confusion matrices? One way is to extract them from\n lists of misspellings like the following:\n\n additional: addional, additonal\n environments: enviornments, enviorments, enviroments\n preceded: preceeded\n ...\n\n There are lists available on Wikipedia and from Roger Mitton (http://www.\n dcs.bbk.ac.uk/ÀúROGER/corpora.html) and Peter Norvig (http://norvig.\n com/ngrams/). Norvig also gives the counts for each single-character edit that can\n be used to directly create the error model probabilities.\n An alternative approach used by Kernighan et al. (1990) is to compute the matrices by iteratively using this very spelling error correction algorithm itself. The\n iterative algorithm first initializes the matrices with equal values; thus, any character\n is equally likely to be deleted, equally likely to be substituted for any other character, etc. Next, the spelling error correction algorithm is run on a set of spelling\n errors. Given the set of typos paired with their predicted corrections, the confusion\n matrices can now be recomputed, the spelling algorithm run again, and so on. This\n iterative algorithm is an instance of the important EM algorithm (Dempster et al.,\n 1977), which we discuss in Appendix A.\n Once we have the confusion matrices, we can estimate P(x|w) as follows (where\n6 A PPENDIX D ‚Ä¢ S PELLING C ORRECTION AND THE N OISY C HANNEL\n\n wi is the ith character of the correct word w) and xi is the ith character of the typo x:\n\n del[xi‚àí1 , wi ]\n Ô£±\n Ô£¥\n Ô£¥ , if deletion\n Ô£¥\n Ô£¥\n Ô£¥\n Ô£¥ count[xi‚àí1 wi ]\n Ô£¥\n Ô£¥\n Ô£¥ ins[xi‚àí1 , wi ]\n Ô£≤ count[wi‚àí1 ] , if insertion\n Ô£¥\n Ô£¥\n Ô£¥\n P(x|w) = sub[xi , wi ] (D.6)\n , if substitution\n Ô£¥\n Ô£¥\n count[wi ]\n Ô£¥\n Ô£¥\n Ô£¥\n Ô£¥\n Ô£¥\n trans[w i , wi+1 ]\n Ô£¥\n Ô£¥\n Ô£≥ count[w w ] , if transposition\n Ô£¥\n Ô£¥\n Ô£¥\n i i+1\n\n Using the counts from Kernighan et al. (1990) results in the error model probabilities for acress shown in Fig. D.4.\n\n Candidate Correct Error\n Correction Letter Letter x|w P(x|w)\n actress t - c|ct .000117\n cress - a a|# .00000144\n caress ca ac ac|ca .00000164\n access c r r|c .000000209\n across o e e|o .0000093\n acres - s es|e .0000321\n acres - s ss|s .0000342\n Figure D.4 Channel model for acress; the probabilities are taken from the del[], ins[],\n sub[], and trans[] confusion matrices as shown in Kernighan et al. (1990).\n\n Figure D.5 shows the final probabilities for each of the potential corrections;\n the unigram prior is multiplied by the likelihood (computed with Eq. D.6 and the\n confusion matrices). The final column shows the product, multiplied by 109 just for\n readability.\n\n Candidate Correct Error\n Correction Letter Letter x|w P(x|w) P(w) 109 *P(x|w)P(w)\n actress t - c|ct .000117 .0000231 2.7\n cress - a a|# .00000144 .000000544 0.00078\n caress ca ac ac|ca .00000164 .00000170 0.0028\n access c r r|c .000000209 .0000916 0.019\n across o e e|o .0000093 .000299 2.8\n acres - s es|e .0000321 .0000318 1.0\n acres - s ss|s .0000342 .0000318 1.0\n Figure D.5 Computation of the ranking for each candidate correction, using the language\n model shown earlier and the error model from Fig. D.4. The final score is multiplied by 109\n for readability.\n\n The computations in Fig. D.5 show that our implementation of the noisy channel\n model chooses across as the best correction, and actress as the second most\n likely word.\n Unfortunately, the algorithm was wrong here; the writer‚Äôs intention becomes\n clear from the context: . . . was called a ‚Äústellar and versatile acress whose combination of sass and glamour has defined her. . . ‚Äù. The surrounding words make it\n clear that actress and not across was the intended word.\n D.2 ‚Ä¢ R EAL - WORD SPELLING ERRORS 7\n\n For this reason, it is important to use larger language models than unigrams.\n For example, if we use the Corpus of Contemporary American English to compute\n bigram probabilities for the words actress and across in their context using add-one\n smoothing, we get the following probabilities:\n\n P(actress|versatile) = .000021\n P(across|versatile) = .000021\n P(whose|actress) = .0010\n P(whose|across) = .000006\n\n Multiplying these out gives us the language model estimate for the two candidates in context:\n\n P(‚Äúversatile actress whose‚Äù) = .000021 ‚àó .0010 = 210 √ó 10‚àí10\n P(‚Äúversatile across whose‚Äù) = .000021 ‚àó .000006 = 1 √ó 10‚àí10\n\n Combining the language model with the error model in Fig. D.5, the bigram\n noisy channel model now chooses the correct word actress.\n Evaluating spell correction algorithms is generally done by holding out a training, development and test set from lists of errors like those on the Norvig and Mitton\n sites mentioned above.\n\nD.2 Real-word spelling errors\n The noisy channel approach can also be applied to detect and correct real-word\nreal-word error spelling errors, errors that result in an actual word of English. This can happen from\n detection\n typographical errors (insertion, deletion, transposition) that accidentally produce a\n real word (e.g., there for three) or because the writer substituted the wrong spelling\n of a homophone or near-homophone (e.g., dessert for desert, or piece for peace). A\n number of studies suggest that between 25% and 40% of spelling errors are valid\n English words as in the following examples (Kukich, 1992):\n This used to belong to thew queen. They are leaving in about fifteen minuets to go to her house.\n The design an construction of the system will take more than a year.\n Can they lave him my messages?\n The study was conducted mainly be John Black.\n\n The noisy channel can deal with real-word errors as well. Let‚Äôs begin with a\n version of the noisy channel model first proposed by Mays et al. (1991) to deal\n with these real-word spelling errors. Their algorithm takes the input sentence X =\n {x1 , x2 , . . . , xk , . . . , xn }, generates a large set of candidate correction sentences C(X),\n then picks the sentence with the highest language model probability.\n To generate the candidate correction sentences, we start by generating a set of\n candidate words for each input word xi . The candidates, C(xi ), include every English word with a small edit distance from xi . With edit distance 1, a common choice\n (Mays et al., 1991), the candidate set for the real word error thew (a rare word meaning ‚Äòmuscular strength‚Äô) might be C(thew) = {the, thaw, threw, them, thwe}. We then\n make the simplifying assumption that every sentence has only one error. Thus the\n set of candidate sentences C(X) for a sentence X = Only two of thew apples\n would be:\n8 A PPENDIX D ‚Ä¢ S PELLING C ORRECTION AND THE N OISY C HANNEL\n\n only two of thew apples\n oily two of thew apples\n only too of thew apples\n only to of thew apples\n only tao of the apples\n only two on thew apples\n only two off thew apples\n only two of the apples\n only two of threw apples\n only two of thew applies\n only two of thew dapples\n ...\n Each sentence is scored by the noisy channel:\n\n WÃÇ = argmax P(X|W ) P(W ) (D.7)\n W ‚ààC(X)\n\n For P(W ), we can use the trigram probability of the sentence.\n What about the channel model? Since these are real words, we need to consider\n the possibility that the input word is not an error. Let‚Äôs say that the channel probability of writing a word correctly, P(w|w), is Œ±; we can make different assumptions\n about exactly what the value of Œ± is in different tasks; perhaps Œ± is .95, assuming people write 1 word wrong out of 20, for some tasks, or maybe .99 for others.\n Mays et al. (1991) proposed a simple model: given a typed word x, let the channel\n model P(x|w) be Œ± when x = w, and then just distribute 1 ‚àí Œ± evenly over all other\n candidate corrections C(x):\n Ô£±\n Ô£¥\n Ô£¥ Œ± if x = w\n Ô£≤ 1‚àíŒ±\n Ô£¥\n p(x|w) = if x ‚àà C(x) (D.8)\n Ô£¥\n Ô£¥ |C(x)|\n Ô£¥\n Ô£≥\n 0 otherwise\n Now we can replace the equal distribution of 1‚àíŒ± over all corrections in Eq. D.8;\n we‚Äôll make the distribution proportional to the edit probability from the more sophisticated channel model from Eq. D.6 that used the confusion matrices.\n Let‚Äôs see an example of this integrated noisy channel model applied to a real\n word. Suppose we see the string two of thew. The author might have intended\n to type the real word thew (‚Äòmuscular strength‚Äô). But thew here could also be a\n typo for the or some other word. For the purposes of this example let‚Äôs consider\n edit distance 1, and only the following five candidates the, thaw, threw, and thwe\n (a rare name) and the string as typed, thew. We took the edit probabilities from\n Norvig‚Äôs 2009 analysis of this example. For the language model probabilities, we\n used a Stupid Backoff model (Section ??) trained on the Google n-grams:\n P(the|two of) = 0.476012\n P(thew|two of) = 9.95051 √ó10‚àí8\n P(thaw|two of) = 2.09267 √ó10‚àí7\n P(threw|two of) = 8.9064 √ó10‚àí7\n P(them|two of) = 0.00144488\n P(thwe|two of) = 5.18681 √ó10‚àí9\n Here we‚Äôve just computed probabilities for the single phrase two of thew, but\n the model applies to entire sentences; so if the example in context was two of thew\n D.3 ‚Ä¢ N OISY C HANNEL M ODEL : T HE S TATE OF THE A RT 9\n\n people, we‚Äôd need to also multiply in probabilities for P(people|of the), P(people|of\n thew), P(people|of threw), and so on.\n Following Norvig (2009), we assume that the probability of a word being a typo\n in this task is .05, meaning that Œ± = P(w|w) is .95. Fig. D.6 shows the computation.\n\n x w x|w P(x|w) P(w|wi‚àí2 , wi‚àí1 ) 108 P(x|w)P(w|wi‚àí2 , wi‚àí1 )\n thew the ew|e 0.000007 0.48 333\n thew thew Œ±=0.95 9.95 √ó10‚àí8 9.45\n thew thaw e|a 0.001 2.1 √ó10‚àí7 0.0209\n thew threw h|hr 0.000008 8.9 √ó10‚àí7 0.000713\n thew thwe ew|we 0.000003 5.2 √ó10‚àí9 0.00000156\n Figure D.6 The noisy channel model on 5 possible candidates for thew, with a Stupid\n Backoff trigram language model computed from the Google n-gram corpus and the error\n model from Norvig (2009).\n\n For the error phrase two of thew, the model correctly picks the as the correction.\n But note that a lower error rate might change things; in a task where the probability\n of an error is low enough (Œ± is very high), the model might instead decide that the\n word thew was what the writer intended.\n\nD.3 Noisy Channel Model: The State of the Art\n State of the art implementations of noisy channel spelling correction make a number\n of extensions to the simple models we presented above.\n First, rather than make the assumption that the input sentence has only a single error, modern systems go through the input one word at a time, using the noisy\n channel to make a decision for that word. But if we just run the basic noisy channel system described above on each word, it is prone to overcorrecting, replacing\n correct but rare words (for example names) with more frequent words (Whitelaw\n et al. 2009, Wilcox-O‚ÄôHearn 2014). Modern algorithms therefore need to augment\n the noisy channel with methods for detecting whether or not a real word should actually be corrected. For example state of the art systems like Google‚Äôs (Whitelaw\n et al., 2009) use a blacklist, forbidding certain tokens (like numbers, punctuation,\n and single letter words) from being changed. Such systems are also more cautious\n in deciding whether to trust a candidate correction. Instead of just choosing a candidate correction if it has a higher probability P(w|x) than the word itself, these more\n careful systems choose to suggest a correction w over keeping the non-correction x\n only if the difference in probabilities is sufficiently great. The best correction w is\n chosen only if:\n\n log P(w|x) ‚àí log P(x|x) > Œ∏\n autocorrect Depending on the specific application, spell-checkers may decide to autocorrect\n (automatically change a spelling to a hypothesized correction) or merely to flag the\n error and offer suggestions. This decision is often made by another classifier which\n decides whether the best candidate is good enough, using features such as the difference in log probabilities between the candidates (we‚Äôll introduce algorithms for\n classification in the next chapter).\n Modern systems also use much larger dictionaries than early systems. Ahmad\n and Kondrak (2005) found that a 100,000 word UNIX dictionary only contained\n10 A PPENDIX D ‚Ä¢ S PELLING C ORRECTION AND THE N OISY C HANNEL\n\n 73% of the word types in their corpus of web queries, missing words like pics, multiplayer, google, xbox, clipart, and mallorca. For this reason modern systems often\n use much larger dictionaries automatically derived from very large lists of unigrams\n like the Google n-gram corpus. Whitelaw et al. (2009), for example, used the most\n frequently occurring ten million word types in a large sample of web pages. Because\n this list will include lots of misspellings, their system requires a more sophisticated\n error model. The fact that words are generally more frequent than their misspellings\n can be used in candidate suggestion, by building a set of words and spelling variations that have similar contexts, sorting by frequency, treating the most frequent\n variant as the source, and learning an error model from the difference, whether from\n web text (Whitelaw et al., 2009) or from query logs (Cucerzan and Brill, 2004).\n Words can also be automatically added to the dictionary when a user rejects a correction, and systems running on phones can automatically add words from the user‚Äôs\n address book or calendar.\n We can also improve the performance of the noisy channel model by changing\n how the prior and the likelihood are combined. In the standard model they are just\n multiplied together. But often these probabilities are not commensurate; the language model or the channel model might have very different ranges. Alternatively\n for some task or dataset we might have reason to trust one of the two models more.\n Therefore we use a weighted combination, by raising one of the factors to a power\n Œª:\n\n wÃÇ = argmax P(x|w) P(w)Œª (D.9)\n w‚ààV\n\n or in log space:\n\n wÃÇ = argmax log P(x|w) + Œª log P(w) (D.10)\n w‚ààV\n\n We then tune the parameter Œª on a development test set.\n Finally, if our goal is to do real-word spelling correction only for specific conconfusion sets fusion sets like peace/piece, affect/effect, weather/whether, or even grammar correction examples like among/between, we can train supervised classifiers to draw on\n many features of the context and make a choice between the two candidates. Such\n classifiers can achieve very high accuracy for these specific sets, especially when\n drawing on large-scale features from web statistics (Golding and Roth 1999, Lapata\n and Keller 2004, Bergsma et al. 2009, Bergsma et al. 2010).\n\n D.3.1 Improved Edit Models: Partitions and Pronunciation\n Other recent research has focused on improving the channel model P(t|c). One\n important extension is the ability to compute probabilities for multiple-letter transformations. For example Brill and Moore (2000) propose a channel model that\n (informally) models an error as being generated by a typist first choosing a word,\n then choosing a partition of the letters of that word, and then typing each partition,\n possibly erroneously. For example, imagine a person chooses the word physical,\n then chooses the partition ph y s i c al. She would then generate each partition, possibly with errors. For example the probability that she would generate the\n string fisikle with partition f i s i k le would be p(f|ph) ‚àó p(i|y) ‚àó p(s|s) ‚àó\n p(i|i)‚àó p(k|k)‚àó p(le|al). Unlike the Damerau-Levenshtein edit distance, the Brill-\nMoore channel model can thus model edit probabilities like P(f|ph) or P(le|al), or\n D.3 ‚Ä¢ N OISY C HANNEL M ODEL : T HE S TATE OF THE A RT 11\n\n the high likelihood of P(ent|ant). Furthermore, each edit is conditioned on where\n it is in the word (beginning, middle, end) so instead of P(f|ph) the model actually\n estimates P(f|ph, beginning).\n More formally, let R be a partition of the typo string x into adjacent (possibly\n empty) substrings, and T be a partition of the candidate string. Brill and Moore\n (2000) then approximates the total likelihood P(x|w) (e.g., P(fisikle|physical))\n by the probability of the single best partition:\n X\n P(x|w) ‚âà max P(Ti |Ri , position) (D.11)\n R,T s.t.|T |=|R|\n i=1\n\n The probability of each transform P(Ti |Ri ) can be learned from a training set of\n triples of an error, the correct string, and the number of times it occurs. For example\n given a training pair akgsual/actual, standard minimum edit distance is used to\n produce an alignment:\n a c t u a l\n\n a k g s u a l\n\n This alignment corresponds to the sequence of edit operations:\n a‚Üía, c‚Üík, \u000f ‚Üíg t‚Üís, u‚Üíu, a‚Üía, l‚Üíl\n Each nonmatch substitution is then expanded to incorporate up to N additional\n edits; For N=2, we would expand c‚Üík to:\n ac‚Üíak\n c‚Üícg\n ac‚Üíakg\n ct‚Üíkgs\n\n Each of these multiple edits then gets a fractional count, and the probability for\n each edit Œ± ‚Üí Œ≤ is then estimated from counts in the training corpus of triples as\n count(Œ±‚ÜíŒ≤ ) .\n count(Œ±)\n Another research direction in channel models is the use of pronunciation in addition to spelling. Pronunciation is an important feature in some non-noisy-channel\naspell algorithms for spell correction like the GNU aspell algorithm (Atkinson, 2011),\n which makes use of the metaphone pronunciation of a word (Philips, 1990). Metaphone is a series of rules that map a word to a normalized representation of its\n pronunciation. Some example rules:\n ‚Ä¢ ‚ÄúDrop duplicate adjacent letters, except for C.‚Äù\n ‚Ä¢ ‚ÄúIf the word begins with ‚ÄòKN‚Äô, ‚ÄòGN‚Äô, ‚ÄòPN‚Äô, ‚ÄòAE‚Äô, ‚ÄòWR‚Äô, drop the first letter.‚Äù\n ‚Ä¢ ‚ÄúDrop ‚ÄòB‚Äô if after ‚ÄòM‚Äô and if it is at the end of the word‚Äù\n Aspell works similarly to the channel component of the noisy channel model, finding\n all words in the dictionary whose pronunciation string is a short edit distance (1 or\n 2 pronunciation letters) from the typo, and then scoring this list of candidates by\n a metric that combines two edit distances: the pronunciation edit distance and the\n weighted letter edit distance.\n Pronunciation can also be incorporated directly the noisy channel model. For example the Toutanova and Moore (2002) model, like aspell, interpolates two channel\n12 A PPENDIX D ‚Ä¢ S PELLING C ORRECTION AND THE N OISY C HANNEL\n\n function S OUNDEX(name) returns soundex form\n\n 1. Keep the first letter of name\n 2. Drop all occurrences of non-initial a, e, h, i, o, u, w, y.\n 3. Replace the remaining letters with the following numbers:\n b, f, p, v ‚Üí 1\n c, g, j, k, q, s, x, z ‚Üí 2\n d, t ‚Üí 3\n l‚Üí4\n m, n ‚Üí 5\n r‚Üí6\n 4. Replace any sequences of identical numbers, only if they derive from two or more\n letters that were adjacent in the original name, with a single number (e.g., 666 ‚Üí 6).\n 5. Convert to the form Letter Digit Digit Digit by dropping digits past the third\n (if necessary) or padding with trailing zeros (if necessary).\n\n Figure D.7 The Soundex Algorithm\n\n models, one based on spelling and one based on pronunciation. The pronunciation\nletter-to-sound model is based on using letter-to-sound models to translate each input word and\n phones each dictionary word into a sequences of phones representing the pronunciation of\n the word. For example actress and aktress would both map to the phone string\n ae k t r ix s. See Chapter 16 on the task of letter-to-sound or grapheme-tophoneme.\n Some additional string distance functions have been proposed for dealing specifdeduplication ically with names. These are mainly used for the task of deduplication (deciding if\n two names in a census list or other namelist are the same) rather than spell-checking.\n The Soundex algorithm (Knuth 1973, Odell and Russell 1918/1922) is an older\n method used originally for census records for representing people‚Äôs names. It has the\n advantage that versions of the names that are slightly misspelled will still have the\n same representation as correctly spelled names. (e.g., Jurafsky, Jarofsky, Jarovsky,\n and Jarovski all map to J612). The algorithm is shown in Fig. D.7.\n Jaro-Winkler Instead of Soundex, more recent work uses Jaro-Winkler distance, which is\n an edit distance algorithm designed for names that allows characters to be moved\n longer distances in longer names, and also gives a higher similarity to strings that\n have identical initial characters (Winkler, 2006).\n\nHistorical Notes\n Algorithms for spelling error detection and correction have existed since at least\n Blair (1960). Most early algorithms were based on similarity keys like the Soundex\n algorithm (Odell and Russell 1918/1922, Knuth 1973). Damerau (1964) gave a\n dictionary-based algorithm for error detection; most error-detection algorithms since\n then have been based on dictionaries. Early research (Peterson, 1986) had suggested\n that spelling dictionaries might need to be kept small because large dictionaries contain very rare words (wont, veery) that resemble misspellings of other words, but\n Damerau and Mays (1989) found that in practice larger dictionaries proved more\n helpful. Damerau (1964) also gave a correction algorithm that worked for single\n errors.\n The idea of modeling language transmission as a Markov source passed through\n E XERCISES 13\n\n a noisy channel model was developed very early on by Claude Shannon (1948).\n The idea of combining a prior and a likelihood to deal with the noisy channel was\n developed at IBM Research by Raviv (1967), for the similar task of optical character recognition (OCR). While earlier spell-checkers like Kashyap and Oommen\n (1983) had used likelihood-based models of edit distance, the idea of combining a\n prior and a likelihood seems not to have been applied to the spelling correction task\n until researchers at AT&T Bell Laboratories (Kernighan et al. 1990, Church and\n Gale 1991) and IBM Watson Research (Mays et al., 1991) roughly simultaneously\n proposed noisy channel spelling correction. Much later, the Mays et al. (1991) algorithm was reimplemented and tested on standard datasets by Wilcox-O‚ÄôHearn et al.\n (2008), who showed its high performance.\n Most algorithms since Wagner and Fischer (1974) have relied on dynamic programming.\n Recent focus has been on using the web both for language models and for training the error model, and on incorporating additional features in spelling, like the\n pronunciation models described earlier, or other information like parses or semantic\n relatedness (Jones and Martin 1997, Hirst and Budanitsky 2005).\n See Mitton (1987) for a survey of human spelling errors, and Kukich (1992)\n for an early survey of spelling error detection and correction. Norvig (2007) gives\n a nice explanation and a Python implementation of the noisy channel model, with\n more details and an efficient algorithm presented in Norvig (2009).\n\nExercises\n D.1 Suppose we want to apply add-one smoothing to the likelihood term (channel\n model) P(x|w) of a noisy channel model of spelling. For simplicity, pretend\n that the only possible operation is deletion. The MLE estimate for deletion is\n given in Eq. D.6, which is P(x|w) = countdel[xi ‚àí1,wi ] . What is the estimate for\n (xi‚àí1 wi )\n P(x|w) if we use add-one smoothing on the deletion edit model? Assume the\n only characters we use are lower case a-z, that there are V word types in our\n corpus, and N total characters, not counting spaces.\n14 Appendix D ‚Ä¢ Spelling Correction and the Noisy Channel\n\nAhmad, F. and G. Kondrak. 2005. Learning a spelling error Odell, M. K. and R. C. Russell. 1918/1922. U.S. Patents\n model from search query logs. EMNLP. 1261167 (1918), 1435663 (1922). Cited in Knuth (1973).\nAtkinson, K. 2011. Gnu aspell. Peterson, J. L. 1986. A note on undetected typing errors.\nBergsma, S., D. Lin, and R. Goebel. 2009. Web-scale n-gram CACM, 29(7):633‚Äì637.\n models for lexical disambiguation. IJCAI. Philips, L. 1990. Hanging on the metaphone. Computer\nBergsma, S., E. Pitler, and D. Lin. 2010. Creating robust Language, 7(12).\n supervised classifiers via web-scale n-gram data. ACL. Raviv, J. 1967. Decision making in Markov chains applied\nBlair, C. R. 1960. A program for correcting spelling errors. to the problem of pattern recognition. IEEE Transactions\n Information and Control, 3:60‚Äì67. on Information Theory, 13(4):536‚Äì551.\nBrill, E. and R. C. Moore. 2000. An improved error model Shannon, C. E. 1948. A mathematical theory of commufor noisy channel spelling correction. ACL. nication. Bell System Technical Journal, 27(3):379‚Äì423.\nChurch, K. W. and W. A. Gale. 1991. Probability scoring for Continued in the following volume.\n spelling correction. Statistics and Computing, 1(2):93‚Äì Toutanova, K. and R. C. Moore. 2002. Pronunciation mod-\n103. eling for improved spelling correction. ACL.\nCucerzan, S. and E. Brill. 2004. Spelling correction as an Veblen, T. 1899. Theory of the Leisure Class. Macmillan,\n iterative process that exploits the collective knowledge of New York.\n web users. EMNLP, volume 4. Wagner, R. A. and M. J. Fischer. 1974. The string-to-string\nDamerau, F. J. 1964. A technique for computer detection and correction problem. Journal of the ACM, 21:168‚Äì173.\n correction of spelling errors. CACM, 7(3):171‚Äì176. Whitelaw, C., B. Hutchinson, G. Y. Chung, and G. El-\nDamerau, F. J. and E. Mays. 1989. An examination of un- lis. 2009. Using the web for language independent\n detected typing errors. Information Processing and Man- spellchecking and autocorrection. EMNLP.\n agement, 25(6):659‚Äì664. Wilcox-O‚ÄôHearn, L. A. 2014. Detection is the central prob-\nDempster, A. P., N. M. Laird, and D. B. Rubin. 1977. Max- lem in real-word spelling correction. http://arxiv.\n imum likelihood from incomplete data via the EM algo- org/abs/1408.3153.\n rithm. Journal of the Royal Statistical Society, 39(1):1‚Äì Wilcox-O‚ÄôHearn, L. A., G. Hirst, and A. Budanitsky. 2008.\n 21. Real-word spelling correction with trigrams: A recon-\nGolding, A. R. and D. Roth. 1999. A Winnow based ap- sideration of the Mays, Damerau, and Mercer model.\n proach to context-sensitive spelling correction. Machine CICLing-2008.\n Learning, 34(1-3):107‚Äì130.\n Winkler, W. E. 2006. Overview of record linkage and current\nHirst, G. and A. Budanitsky. 2005. Correcting real-word research directions. Technical report, Statistical Research\n spelling errors by restoring lexical cohesion. Natural Division, U.S. Census Bureau.\n Language Engineering, 11:87‚Äì111.\nJones, M. P. and J. H. Martin. 1997. Contextual spelling correction using latent semantic analysis. ANLP.\nKashyap, R. L. and B. J. Oommen. 1983. Spelling correction\n using probabilistic methods. Pattern Recognition Letters,\n 2:147‚Äì154.\nKernighan, M. D., K. W. Church, and W. A. Gale. 1990.\n A spelling correction program base on a noisy channel\n model. COLING, volume II.\nKnuth, D. E. 1973. Sorting and Searching: The Art of Computer Programming Volume 3. Addison-Wesley.\nKukich, K. 1992. Techniques for automatically correcting\n words in text. ACM Computing Surveys, 24(4):377‚Äì439.\nLapata, M. and F. Keller. 2004. The web as a baseline: Evaluating the performance of unsupervised web-based models for a range of NLP tasks. HLT-NAACL.\nMays, E., F. J. Damerau, and R. L. Mercer. 1991. Context\n based spelling correction. Information Processing and\n Management, 27(5):517‚Äì522.\nMitton, R. 1987. Spelling checkers, spelling correctors and\n the misspellings of poor spellers. Information processing\n & management, 23(5):495‚Äì505.\nNorvig, P. 2007. How to write a spelling corrector. http:\n //www.norvig.com/spell-correct.html.\nNorvig, P. 2009. Natural language corpus data. In T. Segaran\n and J. Hammerbacher, eds, Beautiful data: the stories behind elegant data solutions. O‚ÄôReilly.\n",
    "file_path": "/Users/colinsidberry/nlp-textbook-rag/data/extracted_text/D.Spelling Correction and the Noisy Channel.txt",
    "file_size_kb": 39.6
  },
  {
    "id": "fc4e72ed1a0fe0a1",
    "source": "nlp_textbook",
    "chapter": "Statistical Constituency Parsing E",
    "filename": "E.Statistical Constituency Parsing.txt",
    "content": "2025. All rights reserved. Draft of August 24, 2025.\n\nCHAPTER\n\n Statistical Constituency Parsing\nE\n The characters in Damon Runyon‚Äôs short stories are willing to bet ‚Äúon any proposition whatever‚Äù, as Runyon says about Sky Masterson in The Idyll of Miss Sarah\n Brown, from getting aces back-to-back to a man being able to throw a peanut from\n second base to home plate. There is a moral here for language processing: with\n enough knowledge we can estimate the probability of just about anything. Chapter 18 introduced constituency structure and the task of parsing it. Here, we show\n how to build probabilistic models of syntactic knowledge and efficient probabilistic\n parsers.\n One use of probabilistic parsing is to solve the problem of disambiguation. Recall from Chapter 18 that sentences on average tend to be syntactically ambiguous\n because of phenomena like coordination ambiguity and attachment ambiguity.\n The CKY parsing algorithm can represent these ambiguities in an efficient way but\n is not equipped to resolve them. There we introduced a neural algorithm for disambiguation. Here we introduce probabilistic parsers, which offer an alternative solution to the problem: compute the probability of each interpretation and choose the\n most probable interpretation. The most commonly used probabilistic constituency\n grammar formalism is the probabilistic context-free grammar (PCFG), a probabilistic augmentation of context-free grammars in which each rule is associated\n with a probability. We introduce PCFGs in the next section, showing how they can\n be trained on Treebank grammars and how they can be parsed with a probabilistic\n version of the CKY algorithm of Chapter 18.\n We then show a number of ways that we can improve on this basic probability model (PCFGs trained on Treebank grammars), such as by modifying the set of\n non-terminals (making them either more specific or more general), or adding more\n sophisticated conditioning factors like subcategorization or dependencies. Heavily lexicalized grammar formalisms such as Lexical-Functional Grammar (LFG)\n (Bresnan, 1982), Head-Driven Phrase Structure Grammar (HPSG) (Pollard and Sag,\n 1994), Tree-Adjoining Grammar (TAG) (Joshi, 1985), and Combinatory Categorial\n Grammar (CCG) pose additional problems for probabilistic parsers. Section ?? introduces the task of supertagging and the use of heuristic search methods based on\n the A* algorithm in the context of CCG parsing.\n\nE.1 Probabilistic Context-Free Grammars\n The simplest augmentation of the context-free grammar is the Probabilistic Context-\nPCFG Free Grammar (PCFG), also known as the Stochastic Context-Free Grammar\n SCFG (SCFG), first proposed by Booth (1969). Recall that a context-free grammar G is\n defined by four parameters (N, Œ£, R, S); a probabilistic context-free grammar is also\n defined by four parameters, with a slight augmentation to each of the rules in R:\n2 A PPENDIX E ‚Ä¢ S TATISTICAL C ONSTITUENCY PARSING\n\n N a set of non-terminal symbols (or variables)\n Œ£ a set of terminal symbols (disjoint from N)\n R a set of rules or productions, each of the form A ‚Üí Œ≤ [p],\n where A is a non-terminal,\n Œ≤ is a string of symbols from the infinite set of strings (Œ£ ‚à™ N)‚àó,\n and p is a number between 0 and 1 expressing P(Œ≤ |A)\n S a designated start symbol\n\n That is, a PCFG differs from a standard CFG by augmenting each rule in R with\n a conditional probability:\n\n A ‚Üí Œ≤ [p] (E.1)\n Here p expresses the probability that the given non-terminal A will be expanded\n to the sequence Œ≤ . That is, p is the conditional probability of a given expansion Œ≤\n given the left-hand-side (LHS) non-terminal A. We can represent this probability as\n\n P(A ‚Üí Œ≤ )\n\n or as\n P(A ‚Üí Œ≤ |A)\n or as\n P(RHS|LHS)\n Thus, if we consider all the possible expansions of a non-terminal, the sum of their\n probabilities must be 1: X\n P(A ‚Üí Œ≤ ) = 1\n Œ≤\n\n Figure E.1 shows a PCFG: a probabilistic augmentation of the L1 miniature English CFG grammar and lexicon. Note that the probabilities of all of the expansions\n of each non-terminal sum to 1. Also note that these probabilities were made up\n for pedagogical purposes. A real grammar has a great many more rules for each\n non-terminal; hence, the probabilities of any particular rule would tend to be much\n smaller.\n consistent A PCFG is said to be consistent if the sum of the probabilities of all sentences\n in the language equals 1. Certain kinds of recursive rules cause a grammar to be\n inconsistent by causing infinitely looping derivations for some sentences. For example, a rule S ‚Üí S with probability 1 would lead to lost probability mass due to\n derivations that never terminate. See Booth and Thompson (1973) for more details\n on consistent and inconsistent grammars.\n How are PCFGs used? A PCFG can be used to estimate a number of useful\n probabilities concerning a sentence and its parse tree(s), including the probability of\n a particular parse tree (useful in disambiguation) and the probability of a sentence\n or a piece of a sentence (useful in language modeling). Let‚Äôs see how this works.\n\n E.1.1 PCFGs for Disambiguation\n A PCFG assigns a probability to each parse tree T (i.e., each derivation) of a sentence S. This attribute is useful in disambiguation. For example, consider the two\n parses of the sentence ‚ÄúBook the dinner flight‚Äù shown in Fig. E.2. The sensible parse\n E.1 ‚Ä¢ P ROBABILISTIC C ONTEXT-F REE G RAMMARS 3\n\n Grammar Lexicon\n S ‚Üí NP VP [.80] Det ‚Üí that [.10] | a [.30] | the [.60]\n S ‚Üí Aux NP VP [.15] Noun ‚Üí book [.10] | trip [.30]\n S ‚Üí VP [.05] | meal [.05] | money [.05]\n NP ‚Üí Pronoun [.35] | flight [.40] | dinner [.10]\n NP ‚Üí Proper-Noun [.30] Verb ‚Üí book [.30] | include [.30]\n NP ‚Üí Det Nominal [.20] | prefer [.40]\n NP ‚Üí Nominal [.15] Pronoun ‚Üí I [.40] | she [.05]\n Nominal ‚Üí Noun [.75] | me [.15] | you [.40]\n Nominal ‚Üí Nominal Noun [.20] Proper-Noun ‚Üí Houston [.60]\n Nominal ‚Üí Nominal PP [.05] | NWA [.40]\n VP ‚Üí Verb [.35] Aux ‚Üí does [.60] | can [.40]\n VP ‚Üí Verb NP [.20] Preposition ‚Üí from [.30] | to [.30]\n VP ‚Üí Verb NP PP [.10] | on [.20] | near [.15]\n VP ‚Üí Verb PP [.15] | through [.05]\n VP ‚Üí Verb NP NP [.05]\n VP ‚Üí VP PP [.15]\n PP ‚Üí Preposition NP [1.0]\nFigure E.1 A PCFG that is a probabilistic augmentation of the L1 miniature English CFG\ngrammar and lexicon of Fig. ??. These probabilities were made up for pedagogical purposes\nand are not based on a corpus (any real corpus would have many more rules, so the true\nprobabilities of each rule would be much smaller).\n\non the left means ‚ÄúBook a flight that serves dinner‚Äù. The nonsensical parse on the\nright, however, would have to mean something like ‚ÄúBook a flight on behalf of ‚Äòthe\ndinner‚Äù‚Äô just as a structurally similar sentence like ‚ÄúCan you book John a flight?‚Äù\nmeans something like ‚ÄúCan you book a flight on behalf of John?‚Äù\n The probability of a particular parse T is defined as the product of the probabilities of all the n rules used to expand each of the n non-terminal nodes in the parse\ntree T, where each rule i can be expressed as LHSi ‚Üí RHSi :\n n\n Y\n P(T, S) = P(RHSi |LHSi ) (E.2)\n i=1\n\n The resulting probability P(T, S) is both the joint probability of the parse and the\nsentence and also the probability of the parse P(T ). How can this be true? First, by\nthe definition of joint probability:\n\n P(T, S) = P(T )P(S|T ) (E.3)\n But since a parse tree includes all the words of the sentence, P(S|T ) is 1. Thus,\n\n P(T, S) = P(T )P(S|T ) = P(T ) (E.4)\n\n We can compute the probability of each of the trees in Fig. E.2 by multiplying the\nprobabilities of each of the rules used in the derivation. For example, the probability\nof the left tree in Fig. E.2a (call it Tle f t ) and the right tree (Fig. E.2b or Tright ) can be\ncomputed as follows:\n\n P(Tle f t ) = .05 ‚àó .20 ‚àó .20 ‚àó .20 ‚àó .75 ‚àó .30 ‚àó .60 ‚àó .10 ‚àó .40 = 2.2 √ó 10‚àí6\n P(Tright ) = .05 ‚àó .10 ‚àó .20 ‚àó .15 ‚àó .75 ‚àó .75 ‚àó .30 ‚àó .60 ‚àó .10 ‚àó .40 = 6.1 √ó 10‚àí7\n4 A PPENDIX E ‚Ä¢ S TATISTICAL C ONSTITUENCY PARSING\n\n S S\n\n VP VP\n\n Verb NP Verb NP NP\n\n Book Det Nominal Book Det Nominal Nominal\n\n the Nominal Noun the Noun Noun\n\n Noun flight dinner flight\n\n dinner\n\n Rules P Rules P\n S ‚Üí VP .05 S ‚Üí VP .05\n VP ‚Üí Verb NP .20 VP ‚Üí Verb NP NP .10\n NP ‚Üí Det Nominal .20 NP ‚Üí Det Nominal .20\n Nominal ‚Üí Nominal Noun .20 NP ‚Üí Nominal .15\n Nominal ‚Üí Noun .75 Nominal ‚Üí Noun .75\n Nominal ‚Üí Noun .75\n Verb ‚Üí book .30 Verb ‚Üí book .30\n Det ‚Üí the .60 Det ‚Üí the .60\n Noun ‚Üí dinner .10 Noun ‚Üí dinner .10\n Noun ‚Üí flight .40 Noun ‚Üí flight .40\n\n Figure E.2 Two parse trees for an ambiguous sentence. The parse on the left corresponds\n to the sensible meaning ‚ÄúBook a flight that serves dinner‚Äù, while the parse on the right corresponds to the nonsensical meaning ‚ÄúBook a flight on behalf of ‚Äòthe dinner‚Äô ‚Äù.\n\n We can see that the left tree in Fig. E.2 has a much higher probability than the\n tree on the right. Thus, this parse would correctly be chosen by a disambiguation\n algorithm that selects the parse with the highest PCFG probability.\n Let‚Äôs formalize this intuition that picking the parse with the highest probability\n is the correct way to do disambiguation. Consider all the possible parse trees for a\n yield given sentence S. The string of words S is called the yield of any parse tree over S.\n Thus, out of all parse trees with a yield of S, the disambiguation algorithm picks the\n parse tree that is most probable given S:\n\n TÃÇ (S) = argmax P(T |S) (E.5)\n T s.t.S=yield(T )\n\n By definition, the probability P(T |S) can be rewritten as P(T, S)/P(S), thus leading\n to\n P(T, S)\n TÃÇ (S) = argmax (E.6)\n T s.t.S=yield(T )\n P(S)\n\n Since we are maximizing over all parse trees for the same sentence, P(S) will be a\n E.1 ‚Ä¢ P ROBABILISTIC C ONTEXT-F REE G RAMMARS 5\n\nconstant for each tree, so we can eliminate it:\n\n TÃÇ (S) = argmax P(T, S) (E.7)\n T s.t.S=yield(T )\n\nFurthermore, since we showed above that P(T, S) = P(T ), the final equation for\nchoosing the most likely parse neatly simplifies to choosing the parse with the highest probability:\n TÃÇ (S) = argmax P(T ) (E.8)\n T s.t.S=yield(T )\n\nE.1.2 PCFGs for Language Modeling\nA second attribute of a PCFG is that it assigns a probability to the string of words\nconstituting a sentence. This is important in language modeling, whether for use\nin speech recognition, machine translation, spelling correction, augmentative communication, or other applications. The probability of an unambiguous sentence is\nP(T, S) = P(T ) or just the probability of the single parse tree for that sentence. The\nprobability of an ambiguous sentence is the sum of the probabilities of all the parse\ntrees for the sentence:\n X\n P(S) = P(T, S) (E.9)\n T s.t.S=yield(T )\n X\n = P(T ) (E.10)\n T s.t.S=yield(T )\n\nAn additional feature of PCFGs that is useful for language modeling is their ability\nto assign a probability to substrings of a sentence. For example, suppose we want\nto know the probability of the next word wi in a sentence given all the words we‚Äôve\nseen so far w1 , ..., wi‚àí1 . The general formula for this is\n P(w1 , w2 , ..., wi‚àí1 , wi )\n P(wi |w1 , w2 , ..., wi‚àí1 ) = (E.11)\n P(w1 , w2 , ..., wi‚àí1 )\nWe saw in Chapter 3 a simple approximation of this probability using N-grams,\nconditioning on only the last word or two instead of the entire context; thus, the\nbigram approximation would give us\n P(wi‚àí1 , wi )\n P(wi |w1 , w2 , ..., wi‚àí1 ) ‚âà (E.12)\n P(wi‚àí1 )\nBut the fact that the N-gram model can only make use of a couple words of context\nmeans it is ignoring potentially useful prediction cues. Consider predicting the word\nafter in the following sentence from Chelba and Jelinek (2000):\n(E.13) the contract ended with a loss of 7 cents after trading as low as 9 cents\nA trigram grammar must predict after from the words 7 cents, while it seems clear\nthat the verb ended and the subject contract would be useful predictors that a PCFGbased parser could help us make use of. Indeed, it turns out that PCFGs allow us to\ncondition on the entire previous context w1 , w2 , ..., wi‚àí1 shown in Eq. E.11.\n In summary, PCFGs can be applied both to disambiguation in syntactic parsing\nand to word prediction in language modeling. Both of these applications require that\nwe be able to compute the probability of parse tree T for a given sentence S. The\nnext few sections introduce some algorithms for computing this probability.\n6 A PPENDIX E ‚Ä¢ S TATISTICAL C ONSTITUENCY PARSING\n\nE.2 Probabilistic CKY Parsing of PCFGs\n The parsing problem for PCFGs is to produce the most-likely parse TÃÇ for a given\n sentence S, that is,\n TÃÇ (S) = argmax P(T ) (E.14)\n T s.t.S=yield(T )\n\n The algorithms for computing the most likely parse are simple extensions of the\n standard algorithms for parsing; most modern probabilistic parsers are based on the\n probabilistic probabilistic CKY algorithm, first described by Ney (1991). The probabilistic CKY\n CKY\n algorithm assumes the PCFG is in Chomsky normal form. Recall from page ?? that\n in CNF, the right-hand side of each rule must expand to either two non-terminals or\n to a single terminal, i.e., rules have the form A ‚Üí B C, or A ‚Üí w.\n For the CKY algorithm, we represented each sentence as having indices between\n the words. Thus, an example sentence like\n (E.15) Book the flight through Houston.\n would assume the following indices between each word:\n (E.16) 0 Book 1 the 2 flight 3 through 4 Houston 5\n Using these indices, each constituent in the CKY parse tree is encoded in a\n two-dimensional matrix. Specifically, for a sentence of length n and a grammar\n that contains V non-terminals, we use the upper-triangular portion of an (n + 1) √ó\n (n + 1) matrix. For CKY, each cell table[i, j] contained a list of constituents that\n could span the sequence of words from i to j. For probabilistic CKY, it‚Äôs slightly\n simpler to think of the constituents in each cell as constituting a third dimension of\n maximum length V . This third dimension corresponds to each non-terminal that can\n be placed in this cell, and the value of the cell is then a probability for that nonterminal/constituent rather than a list of constituents. In summary, each cell [i, j, A]\n in this (n + 1) √ó (n + 1) √óV matrix is the probability of a constituent of type A that\n spans positions i through j of the input.\n Figure E.3 gives the probabilistic CKY algorithm.\n\n function P ROBABILISTIC -CKY(words,grammar) returns most probable parse\n and its probability\n for j ‚Üê from 1 to L ENGTH(words) do\n for all { A | A ‚Üí words[ j] ‚àà grammar}\n table[ j ‚àí 1, j, A] ‚Üê P(A ‚Üí words[ j])\n for i ‚Üê from j ‚àí 2 downto 0 do\n for k ‚Üê i + 1 to j ‚àí 1 do\n for all { A | A ‚Üí BC ‚àà grammar,\n and table[i, k, B] > 0 and table[k, j,C] > 0 }\n if (table[i,j,A] < P(A ‚Üí BC) √ó table[i,k,B] √ó table[k,j,C]) then\n table[i,j,A] ‚Üê P(A ‚Üí BC) √ó table[i,k,B] √ó table[k,j,C]\n back[i,j,A] ‚Üê {k, B,C}\n return BUILD TREE(back[1, L ENGTH(words), S]), table[1, L ENGTH(words), S]\n\n Figure E.3 The probabilistic CKY algorithm for finding the maximum probability parse\n of a string of num words words given a PCFG grammar with num rules rules in Chomsky\n normal form. back is an array of backpointers used to recover the best parse. The build tree\n function is left as an exercise to the reader.\n E.2 ‚Ä¢ P ROBABILISTIC CKY PARSING OF PCFG S 7\n\n Like the basic CKY algorithm in Fig. ??, the probabilistic CKY algorithm requires a grammar in Chomsky normal form. Converting a probabilistic grammar to\nCNF requires that we also modify the probabilities so that the probability of each\nparse remains the same under the new CNF grammar. Exercise E.2 asks you to modify the algorithm for conversion to CNF in Chapter 18 so that it correctly handles\nrule probabilities.\n In practice, a generalized CKY algorithm that handles unit productions directly\nis typically used. Recall that Exercise 13.3 asked you to make this change in CKY;\nExercise E.3 asks you to extend this change to probabilistic CKY.\n Let‚Äôs see an example of the probabilistic CKY chart, using the following minigrammar, which is already in CNF:\n\n S ‚Üí NP VP .80 Det ‚Üí the .40\n NP ‚Üí Det N .30 Det ‚Üí a .40\n VP ‚Üí V NP .20 N ‚Üí meal .01\n V ‚Üí includes .05 N ‚Üí f light .02\n\n Given this grammar, Fig. E.4 shows the first steps in the probabilistic CKY parse\nof the sentence ‚ÄúThe flight includes a meal‚Äù.\n\n The flight includes a meal\n Det: .40 NP: .30 *.40 *.02\n = .0024\n\n [0,1] [0,2] [0,3] [0,4] [0,5]\n N: .02\n\n [1,2] [1,3] [1,4] [1,5]\n V: .05\n\n [2,3] [2,4] [2,5]\n Det: .40\n\n [3,4] [3,5]\n N: .01\n\n [4,5]\n\nFigure E.4 The beginning of the probabilistic CKY matrix. Filling out the rest of the chart\nis left as Exercise E.4 for the reader.\n8 A PPENDIX E ‚Ä¢ S TATISTICAL C ONSTITUENCY PARSING\n\nE.3 Ways to Learn PCFG Rule Probabilities\n Where do PCFG rule probabilities come from? There are two ways to learn probabilities for the rules of a grammar. The simplest way is to use a treebank, a corpus\n of already parsed sentences. Recall that we introduced in Appendix D the idea\n of treebanks and the commonly used Penn Treebank, a collection of parse trees\n in English, Chinese, and other languages that is distributed by the Linguistic Data\n Consortium. Given a treebank, we can compute the probability of each expansion\n of a non-terminal by counting the number of times that expansion occurs and then\n normalizing.\n\n Count(Œ± ‚Üí Œ≤ ) Count(Œ± ‚Üí Œ≤ )\n P(Œ± ‚Üí Œ≤ |Œ±) = P = (E.17)\n Œ≥ Count(Œ± ‚Üí Œ≥) Count(Œ±)\n If we don‚Äôt have a treebank but we do have a (non-probabilistic) parser, we can\n generate the counts we need for computing PCFG rule probabilities by first parsing\n a corpus of sentences with the parser. If sentences were unambiguous, it would be\n as simple as this: parse the corpus, increment a counter for every rule in the parse,\n and then normalize to get probabilities.\n But wait! Since most sentences are ambiguous, that is, have multiple parses, we\n don‚Äôt know which parse to count the rules in. Instead, we need to keep a separate\n count for each parse of a sentence and weight each of these partial counts by the\n probability of the parse it appears in. But to get these parse probabilities to weight\n the rules, we need to already have a probabilistic parser.\n The intuition for solving this chicken-and-egg problem is to incrementally improve our estimates by beginning with a parser with equal rule probabilities, then\n parse the sentence, compute a probability for each parse, use these probabilities to\n weight the counts, re-estimate the rule probabilities, and so on, until our probabilities converge. The standard algorithm for computing this solution is called the\n inside-outside inside-outside algorithm; it was proposed by Baker (1979) as a generalization of the\n forward-backward algorithm for HMMs. Like forward-backward, inside-outside is\n a special case of the Expectation Maximization (EM) algorithm, and hence has two\n steps: the expectation step, and the maximization step. See Lari and Young (1990)\n or Manning and SchuÃàtze (1999) for more on the algorithm.\n\nE.4 Problems with PCFGs\n While probabilistic context-free grammars are a natural extension to context-free\n grammars, they have two main problems as probability estimators:\n Poor independence assumptions: CFG rules impose an independence assumption\n on probabilities that leads to poor modeling of structural dependencies across\n the parse tree.\n Lack of lexical conditioning: CFG rules don‚Äôt model syntactic facts about specific\n words, leading to problems with subcategorization ambiguities, preposition\n attachment, and coordinate structure ambiguities.\n Because of these problems, probabilistic constituent parsing models use some\n augmented version of PCFGs, or modify the Treebank-based grammar in some way.\n E.4 ‚Ä¢ P ROBLEMS WITH PCFG S 9\n\nIn the next few sections after discussing the problems in more detail we introduce\nsome of these augmentations.\n\nE.4.1 Independence Assumptions Miss Rule Dependencies\nLet‚Äôs look at these problems in more detail. Recall that in a CFG the expansion\nof a non-terminal is independent of the context, that is, of the other nearby nonterminals in the parse tree. Similarly, in a PCFG, the probability of a particular\nrule like NP ‚Üí Det N is also independent of the rest of the tree. By definition, the\nprobability of a group of independent events is the product of their probabilities.\nThese two facts explain why in a PCFG we compute the probability of a tree by just\nmultiplying the probabilities of each non-terminal expansion.\n Unfortunately, this CFG independence assumption results in poor probability\nestimates. This is because in English the choice of how a node expands can after all\ndepend on the location of the node in the parse tree. For example, in English it turns\nout that NPs that are syntactic subjects are far more likely to be pronouns, and NPs\nthat are syntactic objects are far more likely to be non-pronominal (e.g., a proper\nnoun or a determiner noun sequence), as shown by these statistics for NPs in the\nSwitchboard corpus (Francis et al., 1999):1\n\n Pronoun Non-Pronoun\n Subject 91% 9%\n Object 34% 66%\n\n Unfortunately, there is no way to represent this contextual difference in the probabilities of a PCFG. Consider two expansions of the non-terminal NP as a pronoun\nor as a determiner+noun. How shall we set the probabilities of these two rules? If\nwe set their probabilities to their overall probability in the Switchboard corpus, the\ntwo rules have about equal probability.\n NP ‚Üí DT NN .28\n NP ‚Üí PRP .25\n\n Because PCFGs don‚Äôt allow a rule probability to be conditioned on surrounding\ncontext, this equal probability is all we get; there is no way to capture the fact that in\nsubject position, the probability for NP ‚Üí PRP should go up to .91, while in object\nposition, the probability for NP ‚Üí DT NN should go up to .66.\n These dependencies could be captured if the probability of expanding an NP as\na pronoun (e.g., NP ‚Üí PRP) versus a lexical NP (e.g., NP ‚Üí DT NN) were conditioned on whether the NP was a subject or an object. Section E.5 introduces the\ntechnique of parent annotation for adding this kind of conditioning.\n\nE.4.2 Lack of Sensitivity to Lexical Dependencies\nA second class of problems with PCFGs is their lack of sensitivity to the words in\nthe parse tree. Words do play a role in PCFGs since the parse probability includes\nthe probability of a word given a part-of-speech (e.g., from rules like V ‚Üí sleep,\nNN ‚Üí book, etc.).\n1 Distribution of subjects from 31,021 declarative sentences; distribution of objects from 7,489 sentences. This tendency is caused by the use of subject position to realize the topic or old information\nin a sentence (GivoÃÅn, 1990). Pronouns are a way to talk about old information, while non-pronominal\n(‚Äúlexical‚Äù) noun-phrases are often used to introduce new referents (Chapter 23).\n10 A PPENDIX E ‚Ä¢ S TATISTICAL C ONSTITUENCY PARSING\n\n But it turns out that lexical information is useful in other places in the grammar,\n such as in resolving prepositional phrase (PP) attachment ambiguities. Since prepositional phrases in English can modify a noun phrase or a verb phrase, when a parser\n finds a prepositional phrase, it must decide where to attach it in the tree. Consider\n the following example:\n (E.18) Workers dumped sacks into a bin.\n Figure E.5 shows two possible parse trees for this sentence; the one on the left is\n the correct parse; Fig. E.6 shows another perspective on the preposition attachment\n problem, demonstrating that resolving the ambiguity in Fig. E.5 is equivalent to\n deciding whether to attach the prepositional phrase into the rest of the tree at the\n VP attachment NP or VP nodes; we say that the correct parse requires VP attachment, and the\n NP attachment incorrect parse implies NP attachment.\n\n S S\n\n NP VP NP VP\n\n NNS VBD NP PP NNS VBD NP\n\n workers dumped NNS P NP workers dumped NP PP\n\n sacks into DT NN NNS P NP\n\n a bin sacks into DT NN\n\n a bin\nFigure E.5 Two possible parse trees for a prepositional phrase attachment ambiguity. The left parse is the\nsensible one, in which ‚Äúinto a bin‚Äù describes the resulting location of the sacks. In the right incorrect parse, the\nsacks to be dumped are the ones which are already ‚Äúinto a bin‚Äù, whatever that might mean.\n\n Why doesn‚Äôt a PCFG already deal with PP attachment ambiguities? Note that\n the two parse trees in Fig. E.5 have almost exactly the same rules; they differ only\n in that the left-hand parse has this rule:\n\n V P ‚Üí V BD NP PP\n\n while the right-hand parse has these:\n\n V P ‚Üí V BD NP\n NP ‚Üí NP PP\n\n Depending on how these probabilities are set, a PCFG will always either prefer\n NP attachment or VP attachment. As it happens, NP attachment is slightly more\n common in English, so if we trained these rule probabilities on a corpus, we might\n always prefer NP attachment, causing us to misparse this sentence.\n But suppose we set the probabilities to prefer the VP attachment for this sentence. Now we would misparse the following, which requires NP attachment:\n E.5 ‚Ä¢ I MPROVING PCFG S BY S PLITTING N ON -T ERMINALS 11\n\n S\n\n NP VP\n\n PP\n NNS VBD NP\n\n P NP\n workers dumped NNS\n\n into DT NN\n sacks\n\n a bin\nFigure E.6 Another view of the preposition attachment problem. Should the PP on the right attach to the VP\nor NP nodes of the partial parse tree on the left?\n\n (E.19) fishermen caught tons of herring\n What information in the input sentence lets us know that (E.19) requires NP\n attachment while (E.18) requires VP attachment? These preferences come from\n the identities of the verbs, nouns, and prepositions. The affinity between the verb\n dumped and the preposition into is greater than the affinity between the noun sacks\n and the preposition into, thus leading to VP attachment. On the other hand, in (E.19)\n the affinity between tons and of is greater than that between caught and of, leading to\n NP attachment. Thus, to get the correct parse for these kinds of examples, we need\n a model that somehow augments the PCFG probabilities to deal with these lexical\n lexical\n dependency dependency statistics for different verbs and prepositions.\n Coordination ambiguities are another case in which lexical dependencies are\n the key to choosing the proper parse. Figure E.7 shows an example from Collins\n (1999) with two parses for the phrase dogs in houses and cats. Because dogs is\n semantically a better conjunct for cats than houses (and because most dogs can‚Äôt fit\n inside cats), the parse [dogs in [NP houses and cats]] is intuitively unnatural and\n should be dispreferred. The two parses in Fig. E.7, however, have exactly the same\n PCFG rules, and thus a PCFG will assign them the same probability.\n In summary, we have shown in this section and the previous one that probabilistic\n context-free grammars are incapable of modeling important structural and lexical\n dependencies. In the next two sections we sketch current methods for augmenting\n PCFGs to deal with both these issues.\n\nE.5 Improving PCFGs by Splitting Non-Terminals\n Let‚Äôs start with the first of the two problems with PCFGs mentioned above: their\n inability to model structural dependencies, like the fact that NPs in subject position\n tend to be pronouns, whereas NPs in object position tend to have full lexical (nonpronominal) form. How could we augment a PCFG to correctly model this fact?\n split One idea would be to split the NP non-terminal into two versions: one for subjects, one for objects. Having two nodes (e.g., NPsubject and NPobject ) would allow\n us to correctly model their different distributional properties, since we would have\n12 A PPENDIX E ‚Ä¢ S TATISTICAL C ONSTITUENCY PARSING\n\n NP NP\n\n NP Conj NP NP PP\n\n NP PP and Noun Noun Prep NP\n\n Noun Prep NP cats dogs in NP Conj NP\n\n dogs in Noun Noun and Noun\n\n houses houses cats\n Figure E.7 An instance of coordination ambiguity. Although the left structure is intuitively\n the correct one, a PCFG will assign them identical probabilities since both structures use\n exactly the same set of rules. After Collins (1999).\n\n different probabilities for the rule NPsubject ‚Üí PRP and the rule NPobject ‚Üí PRP.\n parent One way to implement this intuition of splits is to do parent annotation (Johnannotation\n son, 1998), in which we annotate each node with its parent in the parse tree. Thus,\n an NP node that is the subject of the sentence and hence has parent S would be annotated NPÀÜS, while a direct object NP whose parent is VP would be annotated NPÀÜVP.\n Figure E.8 shows an example of a tree produced by a grammar that parent-annotates\n the phrasal non-terminals (like NP and VP).\n\n a) S b) S\n\n NP VP NPÀÜS VPÀÜS\n\n PRP VBD NP PRP VBD NPÀÜVP\n\n I need DT NN I need DT NN\n\n a flight a flight\n Figure E.8 A standard PCFG parse tree (a) and one which has parent annotation on the\n nodes which aren‚Äôt pre-terminal (b). All the non-terminal nodes (except the pre-terminal\n part-of-speech nodes) in parse (b) have been annotated with the identity of their parent.\n\n In addition to splitting these phrasal nodes, we can also improve a PCFG by\n splitting the pre-terminal part-of-speech nodes (Klein and Manning, 2003b). For example, different kinds of adverbs (RB) tend to occur in different syntactic positions:\n the most common adverbs with ADVP parents are also and now, with VP parents\n n‚Äôt and not, and with NP parents only and just. Thus, adding tags like RBÀÜADVP,\n RBÀÜVP, and RBÀÜNP can be useful in improving PCFG modeling.\n Similarly, the Penn Treebank tag IN can mark a wide variety of parts-of-speech,\n including subordinating conjunctions (while, as, if), complementizers (that, for),\n and prepositions (of, in, from). Some of these differences can be captured by parent\n E.6 ‚Ä¢ P ROBABILISTIC L EXICALIZED CFG S 13\n\n annotation (subordinating conjunctions occur under S, prepositions under PP), while\n others require splitting the pre-terminal nodes. Figure E.9 shows an example from\n Klein and Manning (2003b) in which even a parent-annotated grammar incorrectly\n parses works as a noun in to see if advertising works. Splitting pre-terminals to allow\n if to prefer a sentential complement results in the correct verbal parse.\n Node-splitting is not without problems; it increases the size of the grammar and\n hence reduces the amount of training data available for each grammar rule, leading\n to overfitting. Thus, it is important to split to just the correct level of granularity for a\n particular training set. While early models employed handwritten rules to try to find\n an optimal number of non-terminals (Klein and Manning, 2003b), modern models\nsplit and merge automatically search for the optimal splits. The split and merge algorithm of Petrov\n et al. (2006), for example, starts with a simple X-bar grammar, alternately splits the\n non-terminals, and merges non-terminals, finding the set of annotated nodes that\n maximizes the likelihood of the training set treebank.\n\nE.6 Probabilistic Lexicalized CFGs\n The previous section showed that a simple probabilistic CKY algorithm for parsing raw PCFGs can achieve extremely high parsing accuracy if the grammar rule\n symbols are redesigned by automatic splits and merges.\n In this section, we discuss an alternative family of models in which instead of\n modifying the grammar rules, we modify the probabilistic model of the parser to\n allow for lexicalized rules. The resulting family of lexicalized parsers includes the\n Collins parser (Collins, 1999) and the Charniak parser (Charniak, 1997).\n We saw in Section ?? that syntactic constituents could be associated with a lexilexicalized\n grammar cal head, and we defined a lexicalized grammar in which each non-terminal in the\n tree is annotated with its lexical head, where a rule like V P ‚Üí V BD NP PP would\n be extended as\n\n VP(dumped) ‚Üí VBD(dumped) NP(sacks) PP(into) (E.20)\n\n VPÀÜS VPÀÜS\n\n TO VPÀÜVP TOÀÜVP VPÀÜVP\n\n to VB PPÀÜVP to VBÀÜVP SBARÀÜVP\n\n see IN NPÀÜPP see INÀÜSBAR SÀÜSBAR\n\n if NN NNS if NPÀÜS VPÀÜS\n\n advertising works NNÀÜNP VBZÀÜVP\n\n advertising works\n\nFigure E.9 An incorrect parse even with a parent-annotated parse (left). The correct parse (right), was produced by a grammar in which the pre-terminal nodes have been split, allowing the probabilistic grammar to\ncapture the fact that if prefers sentential complements. Adapted from Klein and Manning (2003b).\n14 A PPENDIX E ‚Ä¢ S TATISTICAL C ONSTITUENCY PARSING\n\n In the standard type of lexicalized grammar, we actually make a further extenhead tag sion, which is to associate the head tag, the part-of-speech tags of the headwords,\n with the non-terminal symbols as well. Each rule is thus lexicalized by both the\n headword and the head tag of each constituent resulting in a format for lexicalized\n rules like\n\n VP(dumped,VBD) ‚Üí VBD(dumped,VBD) NP(sacks,NNS) PP(into,P) (E.21)\n\n We show a lexicalized parse tree with head tags in Fig. E.10, extended from Fig. ??.\n\n TOP\n\n S(dumped,VBD)\n\n NP(workers,NNS) VP(dumped,VBD)\n\nNNS(workers,NNS) VBD(dumped,VBD) NP(sacks,NNS) PP(into,P)\n\n workers dumped NNS(sacks,NNS) P(into,P) NP(bin,NN)\n\n sacks into DT(a,DT) NN(bin,NN)\n\n a bin\n\n Internal Rules Lexical Rules\n TOP ‚Üí S(dumped,VBD) NNS(workers,NNS) ‚Üí workers\n S(dumped,VBD) ‚Üí NP(workers,NNS) VP(dumped,VBD) VBD(dumped,VBD) ‚Üí dumped\n NP(workers,NNS) ‚Üí NNS(workers,NNS) NNS(sacks,NNS) ‚Üí sacks\n VP(dumped,VBD) ‚Üí VBD(dumped, VBD) NP(sacks,NNS) PP(into,P) P(into,P) ‚Üí into\n PP(into,P) ‚Üí P(into,P) NP(bin,NN) DT(a,DT) ‚Üí a\n NP(bin,NN) ‚Üí DT(a,DT) NN(bin,NN) NN(bin,NN) ‚Üí bin\n\nFigure E.10 A lexicalized tree, including head tags, for a WSJ sentence, adapted from Collins (1999). Below\nwe show the PCFG rules needed for this parse tree, internal rules on the left, and lexical rules on the right.\n\n To generate such a lexicalized tree, each PCFG rule must be augmented to identify one right-hand constituent to be the head daughter. The headword for a node is\n then set to the headword of its head daughter, and the head tag to the part-of-speech\n tag of the headword. Recall that we gave in Fig. ?? a set of handwritten rules for\n identifying the heads of particular constituents.\n A natural way to think of a lexicalized grammar is as a parent annotation, that\n is, as a simple context-free grammar with many copies of each rule, one copy for\n each possible headword/head tag for each constituent. Thinking of a probabilistic\n lexicalized CFG in this way would lead to the set of simple PCFG rules shown below\n the tree in Fig. E.10.\n lexical rules Note that Fig. E.10 shows two kinds of rules: lexical rules, which express the\n internal rules expansion of a pre-terminal to a word, and internal rules, which express the other\n rule expansions. We need to distinguish these kinds of rules in a lexicalized grammar\n because they are associated with very different kinds of probabilities. The lexical\n rules are deterministic, that is, they have probability 1.0 since a lexicalized pre-\nE.6 ‚Ä¢ P ROBABILISTIC L EXICALIZED CFG S 15\n\nterminal like NN(bin, NN) can only expand to the word bin. But for the internal\nrules, we need to estimate probabilities.\n Suppose we were to treat a probabilistic lexicalized CFG like a really big CFG\nthat just happened to have lots of very complex non-terminals and estimate the\nprobabilities for each rule from maximum likelihood estimates. Thus, according\nto Eq. E.17, the MLE estimate for the probability for the rule P(VP(dumped,VBD)\n‚Üí VBD(dumped, VBD) NP(sacks,NNS) PP(into,P)) would be\n Count(VP(dumped,VBD) ‚Üí VBD(dumped, VBD) NP(sacks,NNS) PP(into,P))\n (E.22)\n Count(VP(dumped,VBD))\nBut there‚Äôs no way we can get good estimates of counts like those in Eq. E.22 because they are so specific: we‚Äôre unlikely to see many (or even any) instances of a\nsentence with a verb phrase headed by dumped that has one NP argument headed by\nsacks and a PP argument headed by into. In other words, counts of fully lexicalized\nPCFG rules like this will be far too sparse, and most rule probabilities will come out\n0.\n The idea of lexicalized parsing is to make some further independence assumptions to break down each rule so that we would estimate the probability\n P(VP(dumped,VBD) ‚Üí VBD(dumped, VBD) NP(sacks,NNS) PP(into,P))\n\nas the product of smaller independent probability estimates for which we could acquire reasonable counts. The next section summarizes one such method, the Collins\nparsing method.\n\nE.6.1 The Collins Parser\nStatistical parsers differ in exactly which independence assumptions they make.\nLet‚Äôs look at the assumptions in a simplified version of the Collins parser. The first\nintuition of the Collins parser is to think of the right-hand side of every (internal)\nCFG rule as consisting of a head non-terminal, together with the non-terminals to\nthe left of the head and the non-terminals to the right of the head. In the abstract, we\nthink about these rules as follows:\n LHS ‚Üí Ln Ln‚àí1 ... L1 H R1 ... Rn‚àí1 Rn (E.23)\nSince this is a lexicalized grammar, each of the symbols like L1 or R3 or H or LHS\nis actually a complex symbol representing the category and its head and head tag,\nlike VP(dumped,VP) or NP(sacks,NNS).\n Now, instead of computing a single MLE probability for this rule, we are going\nto break down this rule via a neat generative story, a slight simplification of what is\ncalled Collins Model 1. This new generative story is that given the left-hand side,\nwe first generate the head of the rule and then generate the dependents of the head,\none by one, from the inside out. Each of these steps will have its own probability.\n We also add a special STOP non-terminal at the left and right edges of the rule;\nthis non-terminal allows the model to know when to stop generating dependents on a\ngiven side. We generate dependents on the left side of the head until we‚Äôve generated\nSTOP on the left side of the head, at which point we move to the right side of the\nhead and start generating dependents there until we generate STOP. So it‚Äôs as if we\nare generating a rule augmented as follows:\n P(VP(dumped,VBD) ‚Üí (E.24)\n STOP VBD(dumped, VBD) NP(sacks,NNS) PP(into,P) STOP)\n16 A PPENDIX E ‚Ä¢ S TATISTICAL C ONSTITUENCY PARSING\n\n Let‚Äôs see the generative story for this augmented rule. We make use of three kinds\n of probabilities: PH for generating heads, PL for generating dependents on the left,\n and PR for generating dependents on the right.\n\n VP(dumped,VBD)\n1. Generate the head VBD(dumped,VBD) with probability\nPH (H | LHS) = PH (VBD(dumped,VBD) | VP(dumped,VBD))\n VBD(dumped,VBD)\n\n2. Generate the left dependent (which is STOP, since there isn‚Äôt VP(dumped,VBD)\none) with probability\nPL (STOP | VP(dumped,VBD) VBD(dumped,VBD)) STOP VBD(dumped,VBD)\n\n VP(dumped,VBD)\n3. Generate right dependent NP(sacks,NNS) with probability\nPR (NP(sacks,NNS) | VP(dumped,VBD), VBD(dumped,VBD))\n STOP VBD(dumped,VBD) NP(sacks,NNS)\n\n VP(dumped,VBD)\n4. Generate the right dependent PP(into,P) with probability\nPR (PP(into,P) | VP(dumped,VBD), VBD(dumped,VBD))\n STOP VBD(dumped,VBD) NP(sacks,NNS) PP(into,P)\n\n VP(dumped,VBD)\n5) Generate the right dependent STOP with probability\nPR (STOP | VP(dumped,VBD), VBD(dumped,VBD))\n STOP VBD(dumped,VBD) NP(sacks,NNS) PP(into,P) STOP\n\n In summary, the probability of this rule\n\n P(VP(dumped,VBD) ‚Üí (E.25)\n VBD(dumped, VBD) NP(sacks,NNS) PP(into,P))\n\n is estimated (simplifying the notation a bit from the steps above):\n\n PH (VBD|VP, dumped) √ó PL (STOP|VP, VBD, dumped) (E.26)\n √ó PR (NP(sacks,NNS)|VP, VBD, dumped)\n √ó PR (PP(into,P)|VP, VBD, dumped)\n √ó PR (STOP|VP, VBD, dumped)\n\n Each of these probabilities can be estimated from much smaller amounts of data than\n the full probability in Eq. E.25. For example, the maximum likelihood estimate for\n the component probability PR (NP(sacks,NNS)|VP, VBD, dumped) is\n\n Count(VP(dumped,VBD) with NNS(sacks) as a daughter somewhere on the right)\n Count( VP(dumped,VBD) )\n (E.27)\n These counts are much less subject to sparsity problems than are complex counts\n like those in Eq. E.25.\n E.7 ‚Ä¢ S UMMARY 17\n\n More generally, if H is a head with head word hw and head tag ht, lw/lt and\n rw/rt are the word/tag on the left and right respectively, and P is the parent, then the\n probability of an entire rule can be expressed as follows:\n\n 1. Generate the head of the phrase H(hw, ht) with probability:\n\n PH (H(hw, ht)|P, hw, ht)\n 2. Generate modifiers to the left of the head with total probability\n\n n+1\n Y\n PL (Li (lwi , lti )|P, H, hw, ht)\n i=1\n\n such that Ln+1 (lwn+1 , ltn+1 ) = STOP, and we stop generating once we‚Äôve generated a STOP token.\n 3. Generate modifiers to the right of the head with total probability:\n\n n+1\n Y\n PR (Ri (rwi , rti )|P, H, hw, ht)\n i=1\n\n such that Rn+1 (rwn+1 , rtn+1 ) = STOP, and we stop generating once we‚Äôve generated a STOP token.\n\n The parsing algorithm for the Collins model is an extension of probabilistic\n CKY. Extending the CKY algorithm to handle basic lexicalized probabilities is left\n as Exercises 14.5 and 14.6 for the reader.\n\nE.7 Summary\n This chapter has sketched the basics of probabilistic parsing, concentrating on\n probabilistic context-free grammars.\n ‚Ä¢ Probabilistic grammars assign a probability to a sentence or string of words\n while attempting to capture sophisticated grammatical information.\n ‚Ä¢ A probabilistic context-free grammar (PCFG) is a context-free\n grammar in which every rule is annotated with the probability of that rule\n being chosen. Each PCFG rule is treated as if it were conditionally independent; thus, the probability of a sentence is computed by multiplying the\n probabilities of each rule in the parse of the sentence.\n ‚Ä¢ The probabilistic CKY (Cocke-Kasami-Younger) algorithm is a probabilistic\n version of the CKY parsing algorithm.\n ‚Ä¢ PCFG probabilities can be learned by counting in a parsed corpus or by parsing a corpus. The inside-outside algorithm is a way of dealing with the fact\n that the sentences being parsed are ambiguous.\n ‚Ä¢ Raw PCFGs suffer from poor independence assumptions among rules and lack\n of sensitivity to lexical dependencies.\n ‚Ä¢ One way to deal with this problem is to split and merge non-terminals (automatically or by hand).\n18 A PPENDIX E ‚Ä¢ S TATISTICAL C ONSTITUENCY PARSING\n\n ‚Ä¢ Probabilistic lexicalized CFGs are another solution to this problem in which\n the basic PCFG model is augmented with a lexical head for each rule. The\n probability of a rule can then be conditioned on the lexical head or nearby\n heads.\n ‚Ä¢ Parsers for lexicalized PCFGs (like the Collins parser) are based on extensions\n to probabilistic CKY parsing.\n\nHistorical Notes\n Many of the formal properties of probabilistic context-free grammars were first\n worked out by Booth (1969) and Salomaa (1969). Baker (1979) proposed the insideoutside algorithm for unsupervised training of PCFG probabilities, and used a CKYstyle parsing algorithm to compute inside probabilities. Jelinek and Lafferty (1991)\n extended the CKY algorithm to compute probabilities for prefixes. Stolcke (1995)\n adapted the Earley algorithm to use with PCFGs.\n A number of researchers starting in the early 1990s worked on adding lexical dependencies to PCFGs and on making PCFG rule probabilities more sensitive to surrounding syntactic structure. For example, Schabes et al. (1988) and Schabes (1990)\n presented early work on the use of heads. Many papers on the use of lexical dependencies were first presented at the DARPA Speech and Natural Language Workshop\n in June 1990. A paper by Hindle and Rooth (1990) applied lexical dependencies\n to the problem of attaching prepositional phrases; in the question session to a later\n paper, Ken Church suggested applying this method to full parsing (Marcus, 1990).\n Early work on such probabilistic CFG parsing augmented with probabilistic dependency information includes Magerman and Marcus (1991), Black et al. (1992), Bod\n (1993), and Jelinek et al. (1994), in addition to Collins (1996), Charniak (1997), and\n Collins (1999) discussed above. Other recent PCFG parsing models include Klein\n and Manning (2003a) and Petrov et al. (2006).\n This early lexical probabilistic work led initially to work focused on solving\n specific parsing problems like preposition-phrase attachment by using methods including transformation-based learning (TBL) (Brill and Resnik, 1994), maximum\n entropy (Ratnaparkhi et al., 1994), memory-based learning (Zavrel and Daelemans,\n 1997), log-linear models (Franz, 1997), decision trees that used semantic distance\n between heads (computed from WordNet) (Stetina and Nagao, 1997), and boosting\n (Abney et al., 1999). Another direction extended the lexical probabilistic parsing\n work to build probabilistic formulations of grammars other than PCFGs, such as\n probabilistic TAG grammar (Resnik 1992, Schabes 1992), based on the TAG grammars discussed in Appendix D, probabilistic LR parsing (Briscoe and Carroll, 1993),\n and probabilistic link grammar (Lafferty et al., 1992). The supertagging approach\n we saw for CCG was developed for TAG grammars (Bangalore and Joshi 1999,\n Joshi and Srinivas 1994), based on the lexicalized TAG grammars of Schabes et al.\n (1988).\n\nExercises\n E.1 Implement the CKY algorithm.\n E XERCISES 19\n\nE.2 Modify the algorithm for conversion to CNF from Chapter 18 to correctly\n handle rule probabilities. Make sure that the resulting CNF assigns the same\n total probability to each parse tree.\nE.3 Recall that Exercise 13.3 asked you to update the CKY algorithm to handle unit productions directly rather than converting them to CNF. Extend this\n change to probabilistic CKY.\nE.4 Fill out the rest of the probabilistic CKY chart in Fig. E.4.\nE.5 Sketch how the CKY algorithm would have to be augmented to handle lexicalized probabilities.\nE.6 Implement your lexicalized extension of the CKY algorithm.\n20 Appendix E ‚Ä¢ Statistical Constituency Parsing\n\nAbney, S. P., R. E. Schapire, and Y. Singer. 1999. Boosting Johnson, M. 1998. PCFG models of linguistic tree represenapplied to tagging and PP attachment. EMNLP/VLC. tations. Computational Linguistics, 24(4):613‚Äì632.\nBaker, J. K. 1979. Trainable grammars for speech recogni- Joshi, A. K. 1985. Tree adjoining grammars: How\n tion. Speech Communication Papers for the 97th Meeting much context-sensitivity is required to provide reasonable\n of the Acoustical Society of America. structural descriptions? In D. R. Dowty, L. Karttunen,\nBangalore, S. and A. K. Joshi. 1999. Supertagging: An and A. Zwicky, eds, Natural Language Parsing, 206‚Äì250.\n approach to almost parsing. Computational Linguistics, Cambridge University Press.\n 25(2):237‚Äì265. Joshi, A. K. and B. Srinivas. 1994. Disambiguation of super\n parts of speech (or supertags): Almost parsing. COLING.\nBlack, E., F. Jelinek, J. D. Lafferty, D. M. Magerman, R. L.\n Mercer, and S. Roukos. 1992. Towards history-based Klein, D. and C. D. Manning. 2001. Parsing and hypergrammars: Using richer models for probabilistic parsing. graphs. IWPT-01.\n HLT. Klein, D. and C. D. Manning. 2003a. A* parsing: Fast exact\nBod, R. 1993. Using an annotated corpus as a stochastic Viterbi parse selection. HLT-NAACL.\n grammar. EACL. Klein, D. and C. D. Manning. 2003b. Accurate unlexicalized\nBooth, T. L. 1969. Probabilistic representation of formal parsing. HLT-NAACL.\n languages. IEEE Conference Record of the 1969 Tenth Lafferty, J. D., D. Sleator, and D. Temperley. 1992. Gram-\nAnnual Symposium on Switching and Automata Theory. matical trigrams: A probabilistic model of link gram-\nBooth, T. L. and R. A. Thompson. 1973. Applying proba- mar. AAAI Fall Symposium on Probabilistic Approaches\n bility measures to abstract languages. IEEE Transactions to Natural Language.\n on Computers, C-22(5):442‚Äì450. Lari, K. and S. J. Young. 1990. The estimation of stochastic context-free grammars using the Inside-Outside algo-\nBresnan, J., ed. 1982. The Mental Representation of Gramrithm. Computer Speech and Language, 4:35‚Äì56.\n matical Relations. MIT Press.\n Magerman, D. M. and M. P. Marcus. 1991. Pearl: A proba-\nBrill, E. and P. Resnik. 1994. A rule-based approach to\n bilistic chart parser. EACL.\n prepositional phrase attachment disambiguation. COL-\nING. Manning, C. D. and H. SchuÃàtze. 1999. Foundations of Statistical Natural Language Processing. MIT Press.\nBriscoe, T. and J. Carroll. 1993. Generalized probabilistic LR parsing of natural language (corpora) with Marcus, M. P. 1990. Summary of session 9: Automatic acunification-based grammars. Computational Linguistics, quisition of linguistic structure. Speech and Natural Lan-\n19(1):25‚Äì59. guage Workshop.\n Ney, H. 1991. Dynamic programming parsing for context-\nCharniak, E. 1997. Statistical parsing with a context-free\n free grammars in continuous speech recognition. IEEE\n grammar and word statistics. AAAI.\n Transactions on Signal Processing, 39(2):336‚Äì340.\nChelba, C. and F. Jelinek. 2000. Structured language model-\nPetrov, S., L. Barrett, R. Thibaux, and D. Klein. 2006. Learning. Computer Speech and Language, 14:283‚Äì332.\n ing accurate, compact, and interpretable tree annotation.\nCollins, M. 1996. A new statistical parser based on bigram COLING/ACL.\n lexical dependencies. ACL.\n Pollard, C. and I. A. Sag. 1994. Head-Driven Phrase Struc-\nCollins, M. 1999. Head-Driven Statistical Models for Natu- ture Grammar. University of Chicago Press.\n ral Language Parsing. Ph.D. thesis, University of Penn- Ratnaparkhi, A., J. C. Reynar, and S. Roukos. 1994. A maxsylvania, Philadelphia. imum entropy model for prepositional phrase attachment.\nFrancis, H. S., M. L. Gregory, and L. A. Michaelis. 1999. Are ARPA Human Language Technologies Workshop.\n lexical subjects deviant? CLS-99. University of Chicago. Resnik, P. 1992. Probabilistic tree-adjoining grammar as\nFranz, A. 1997. Independence assumptions considered a framework for statistical natural language processing.\n harmful. ACL. COLING.\nGivoÃÅn, T. 1990. Syntax: A Functional Typological Introduc- Salomaa, A. 1969. Probabilistic and weighted grammars.\n tion. John Benjamins. Information and Control, 15:529‚Äì544.\nHindle, D. and M. Rooth. 1990. Structural ambiguity and Schabes, Y. 1990. Mathematical and Computational Aslexical relations. Speech and Natural Language Work- pects of Lexicalized Grammars. Ph.D. thesis, University\n shop. of Pennsylvania, Philadelphia, PA.\nHindle, D. and M. Rooth. 1991. Structural ambiguity and Schabes, Y. 1992. Stochastic lexicalized tree-adjoining\n lexical relations. ACL. grammars. COLING.\nJelinek, F. and J. D. Lafferty. 1991. Computation of the Schabes, Y., A. AbeilleÃÅ, and A. K. Joshi. 1988. Parsing\n probability of initial substring generation by stochas- strategies with ‚Äòlexicalized‚Äô grammars: Applications to\n tic context-free grammars. Computational Linguistics, Tree Adjoining Grammars. COLING.\n 17(3):315‚Äì323. Stetina, J. and M. Nagao. 1997. Corpus based PP attachment\nJelinek, F., J. D. Lafferty, D. M. Magerman, R. L. Mercer, ambiguity resolution with a semantic dictionary. Pro-\nA. Ratnaparkhi, and S. Roukos. 1994. Decision tree pars- ceedings of the Fifth Workshop on Very Large Corpora.\n ing using a hidden derivation model. ARPA Human Lan- Stolcke, A. 1995. An efficient probabilistic context-free\n guage Technologies Workshop. parsing algorithm that computes prefix probabilities.\n Computational Linguistics, 21(2):165‚Äì202.\n Exercises 21\n\nZavrel, J. and W. Daelemans. 1997. Memory-based learning:\n Using similarity for smoothing. ACL.\n",
    "file_path": "/Users/colinsidberry/nlp-textbook-rag/data/extracted_text/E.Statistical Constituency Parsing.txt",
    "file_size_kb": 50.38
  },
  {
    "id": "4156a811ea65eb76",
    "source": "nlp_textbook",
    "chapter": "Constituency Grammars F Because the Night by Bruce Springsteen and Patty Smith The Fire Next Time by James Baldwin",
    "filename": "F.Constituency Grammars.txt",
    "content": "rights reserved. Draft of August 24, 2025.\n\nCHAPTER\n\n Constituency Grammars\nF\n Because the Night by Bruce Springsteen and Patty Smith\n The Fire Next Time by James Baldwin\n If on a winter‚Äôs night a traveler by Italo Calvino\n Love Actually by Richard Curtis\n Suddenly Last Summer by Tennessee Williams\n A Scanner Darkly by Philip K. Dick\n Six titles that are not constituents, from Geoffrey K. Pullum on\n Language Log (who was pointing out their incredible rarity).\n\n The study of grammar has an ancient pedigree. The grammar of Sanskrit was\n described by the Indian grammarian PaÃÑn.ini sometime between the 7th and 4th censyntax turies BCE, in his famous treatise the As.t.aÃÑdhyaÃÑyƒ±ÃÑ (‚Äò8 books‚Äô). And our word syntax\n comes from the Greek syÃÅntaxis, meaning ‚Äúsetting out together or arrangement‚Äù, and\n refers to the way words are arranged together. We have seen various syntactic notions in previous chapters: ordering of sequences of words (Chapter 2), probabilities\n for these word sequences (Chapter 3), and the use of part-of-speech categories as\n a grammatical equivalence class for words (Chapter 17). In this chapter and the\n next three we introduce a variety of syntactic phenomena that go well beyond these\n simpler approaches, together with formal models for capturing them in a computationally useful manner.\n The bulk of this chapter is devoted to context-free grammars. Context-free grammars are the backbone of many formal models of the syntax of natural language (and,\n for that matter, of computer languages). As such, they play a role in many computational applications, including grammar checking, semantic interpretation, dialogue\n understanding, and machine translation. They are powerful enough to express sophisticated relations among the words in a sentence, yet computationally tractable\n enough that efficient algorithms exist for parsing sentences with them (as we show in\n Chapter 18). Here we also introduce the concept of lexicalized grammars, focusing\n on one example, combinatory categorial grammar, or CCG.\n In Chapter 19 we introduce a formal model of grammar called syntactic dependencies that is an alternative to these constituency grammars, and we‚Äôll give algorithms for dependency parsing. Both constituency and dependency formalisms are\n important for language processing.\n Finally, we provide a brief overview of the grammar of English, illustrated from\n a domain with relatively simple sentences called ATIS (Air Traffic Information System) (Hemphill et al., 1990). ATIS systems were an early spoken language system\n for users to book flights, by expressing sentences like I‚Äôd like to fly to Atlanta.\n2 A PPENDIX F ‚Ä¢ C ONSTITUENCY G RAMMARS\n\nF.1 Constituency\n Syntactic constituency is the idea that groups of words can behave as single units,\n or constituents. Part of developing a grammar involves building an inventory of the\n constituents in the language. How do words group together in English? Consider\n noun phrase the noun phrase, a sequence of words surrounding at least one noun. Here are some\n examples of noun phrases (thanks to Damon Runyon):\n\n Harry the Horse a high-class spot such as Mindy‚Äôs\n the Broadway coppers the reason he comes into the Hot Box\n they three parties from Brooklyn\n What evidence do we have that these words group together (or ‚Äúform constituents‚Äù)?\n One piece of evidence is that they can all appear in similar syntactic environments,\n for example, before a verb.\n\n three parties from Brooklyn arrive. . .\n a high-class spot such as Mindy‚Äôs attracts. . .\n the Broadway coppers love. . .\n they sit\n But while the whole noun phrase can occur before a verb, this is not true of each\n of the individual words that make up a noun phrase. The following are not grammatical sentences of English (recall that we use an asterisk (*) to mark fragments that\n are not grammatical English sentences):\n\n *from arrive. . . *as attracts. . .\n *the is. . . *spot sat. . .\n Thus, to correctly describe facts about the ordering of these words in English, we\n must be able to say things like ‚ÄúNoun Phrases can occur before verbs‚Äù.\n preposed Other kinds of evidence for constituency come from what are called preposed or\n postposed postposed constructions. For example, the prepositional phrase on September seventeenth can be placed in a number of different locations in the following examples,\n including at the beginning (preposed) or at the end (postposed):\n On September seventeenth, I‚Äôd like to fly from Atlanta to Denver\n I‚Äôd like to fly on September seventeenth from Atlanta to Denver\n I‚Äôd like to fly from Atlanta to Denver on September seventeenth\n But again, while the entire phrase can be placed differently, the individual words\n making up the phrase cannot be:\n *On September, I‚Äôd like to fly seventeenth from Atlanta to Denver\n *On I‚Äôd like to fly September seventeenth from Atlanta to Denver\n *I‚Äôd like to fly on September from Atlanta to Denver seventeenth\n\nF.2 Context-Free Grammars\n The most widely used formal system for modeling constituent structure in English\n CFG and other natural languages is the Context-Free Grammar, or CFG. Context-\nF.2 ‚Ä¢ C ONTEXT-F REE G RAMMARS 3\n\n free grammars are also called Phrase-Structure Grammars, and the formalism\n is equivalent to Backus-Naur Form, or BNF. The idea of basing a grammar on\n constituent structure dates back to the psychologist Wilhelm Wundt 1900 but was\n not formalized until Chomsky (1956) and, independently, Backus (1959).\n rules A context-free grammar consists of a set of rules or productions, each of which\n expresses the ways that symbols of the language can be grouped and ordered tolexicon gether, and a lexicon of words and symbols. For example, the following productions\n NP express that an NP (or noun phrase) can be composed of either a ProperNoun or\n a determiner (Det) followed by a Nominal; a Nominal in turn can consist of one or\n more Nouns.1\n\n NP ‚Üí Det Nominal\n NP ‚Üí ProperNoun\n Nominal ‚Üí Noun | Nominal Noun\n\n Context-free rules can be hierarchically embedded, so we can combine the previous\n rules with others, like the following, that express facts about the lexicon:\n\n Det ‚Üí a\n Det ‚Üí the\n Noun ‚Üí flight\n\n The symbols that are used in a CFG are divided into two classes. The symbols\n terminal that correspond to words in the language (‚Äúthe‚Äù, ‚Äúnightclub‚Äù) are called terminal\n symbols; the lexicon is the set of rules that introduce these terminal symbols. The\nnon-terminal symbols that express abstractions over these terminals are called non-terminals. In\n each context-free rule, the item to the right of the arrow (‚Üí) is an ordered list of one\n or more terminals and non-terminals; to the left of the arrow is a single non-terminal\n symbol expressing some cluster or generalization. The non-terminal associated with\n each word in the lexicon is its lexical category, or part of speech.\n A CFG can be thought of in two ways: as a device for generating sentences\n and as a device for assigning a structure to a given sentence. Viewing a CFG as a\n generator, we can read the ‚Üí arrow as ‚Äúrewrite the symbol on the left with the string\n of symbols on the right‚Äù.\n So starting from the symbol: NP\n we can use our first rule to rewrite NP as: Det Nominal\n and then rewrite Nominal as: Noun\n and finally rewrite these parts-of-speech as: a flight\n We say the string a flight can be derived from the non-terminal NP. Thus, a CFG\n can be used to generate a set of strings. This sequence of rule expansions is called a\n derivation derivation of the string of words. It is common to represent a derivation by a parse\n parse tree tree (commonly shown inverted with the root at the top). Figure F.1 shows the tree\n representation of this derivation.\n dominates In the parse tree shown in Fig. F.1, we can say that the node NP dominates all the\n nodes in the tree (Det, Nom, Noun, a, flight). We can say further that it immediately\n dominates the nodes Det and Nom.\n The formal language defined by a CFG is the set of strings that are derivable\nstart symbol from the designated start symbol. Each grammar must have one designated start\n 1 When talking about these rules we can pronounce the rightarrow ‚Üí as ‚Äúgoes to‚Äù, and so we might\n read the first rule above as ‚ÄúNP goes to Det Nominal‚Äù.\n4 A PPENDIX F ‚Ä¢ C ONSTITUENCY G RAMMARS\n\n NP\n\n Det Nom\n\n a Noun\n\n flight\n\n Figure F.1 A parse tree for ‚Äúa flight‚Äù.\n\n symbol, which is often called S. Since context-free grammars are often used to define\n sentences, S is usually interpreted as the ‚Äúsentence‚Äù node, and the set of strings that\n are derivable from S is the set of sentences in some simplified version of English.\n Let‚Äôs add a few additional rules to our inventory. The following rule expresses\n verb phrase the fact that a sentence can consist of a noun phrase followed by a verb phrase:\n S ‚Üí NP VP I prefer a morning flight\n A verb phrase in English consists of a verb followed by assorted other things;\n for example, one kind of verb phrase consists of a verb followed by a noun phrase:\n VP ‚Üí Verb NP prefer a morning flight\n Or the verb may be followed by a noun phrase and a prepositional phrase:\n VP ‚Üí Verb NP PP leave Boston in the morning\n Or the verb phrase may have a verb followed by a prepositional phrase alone:\n VP ‚Üí Verb PP leaving on Thursday\n A prepositional phrase generally has a preposition followed by a noun phrase.\n For example, a common type of prepositional phrase in the ATIS corpus is used to\n indicate location or direction:\n PP ‚Üí Preposition NP from Los Angeles\n The NP inside a PP need not be a location; PPs are often used with times and\n dates, and with other nouns as well; they can be arbitrarily complex. Here are ten\n examples from the ATIS corpus:\n to Seattle on these flights\n in Minneapolis about the ground transportation in Chicago\n on Wednesday of the round trip flight on United Airlines\n in the evening of the AP fifty seven flight\n on the ninth of July with a stopover in Nashville\n Figure F.2 gives a sample lexicon, and Fig. F.3 summarizes the grammar rules\n we‚Äôve seen so far, which we‚Äôll call L0 . Note that we can use the or-symbol | to\n indicate that a non-terminal has alternate possible expansions.\n We can use this grammar to generate sentences of this ‚ÄúATIS-language‚Äù. We\n start with S, expand it to NP VP, then choose a random expansion of NP (let‚Äôs say, to\n I), and a random expansion of VP (let‚Äôs say, to Verb NP), and so on until we generate\n the string I prefer a morning flight. Figure F.4 shows a parse tree that represents a\n complete derivation of I prefer a morning flight.\n We can also represent a parse tree in a more compact format called bracketed\n bracketed notation; here is the bracketed representation of the parse tree of Fig. F.4:\n notation\n F.2 ‚Ä¢ C ONTEXT-F REE G RAMMARS 5\n\n Noun ‚Üí flights | flight | breeze | trip | morning\n Verb ‚Üí is | prefer | like | need | want | fly | do\n Adjective ‚Üí cheapest | non-stop | first | latest\n | other | direct\n Pronoun ‚Üí me | I | you | it\n Proper-Noun ‚Üí Alaska | Baltimore | Los Angeles\n | Chicago | United | American\n Determiner ‚Üí the | a | an | this | these | that\n Preposition ‚Üí from | to | on | near | in\n Conjunction ‚Üí and | or | but\n Figure F.2 The lexicon for L0 .\n\n Grammar Rules Examples\n S ‚Üí NP VP I + want a morning flight\n\n NP ‚Üí Pronoun I\n | Proper-Noun Los Angeles\n | Det Nominal a + flight\n Nominal ‚Üí Nominal Noun morning + flight\n | Noun flights\n\n VP ‚Üí Verb do\n | Verb NP want + a flight\n | Verb NP PP leave + Boston + in the morning\n | Verb PP leaving + on Thursday\n\n PP ‚Üí Preposition NP from + Los Angeles\n Figure F.3 The grammar for L0 , with example phrases for each rule.\n\n S\n\n NP VP\n\n Pro Verb NP\n\n I prefer Det Nom\n\n a Nom Noun\n\n Noun flight\n\n morning\n\n Figure F.4 The parse tree for ‚ÄúI prefer a morning flight‚Äù according to grammar L0 .\n\n (F.1) [S [NP [Pro I]] [VP [V prefer] [NP [Det a] [Nom [N morning] [Nom [N flight]]]]]]\n A CFG like that of L0 defines a formal language. We saw in Chapter 2 that a formal language is a set of strings. Sentences (strings of words) that can be derived by a\n grammar are in the formal language defined by that grammar, and are called gramgrammatical matical sentences. Sentences that cannot be derived by a given formal grammar are\nungrammatical not in the language defined by that grammar and are referred to as ungrammatical.\n6 A PPENDIX F ‚Ä¢ C ONSTITUENCY G RAMMARS\n\n This hard line between ‚Äúin‚Äù and ‚Äúout‚Äù characterizes all formal languages but is only\n a very simplified model of how natural languages really work. This is because determining whether a given sentence is part of a given natural language (say, English)\n often depends on the context. In linguistics, the use of formal languages to model\n generative\n grammar natural languages is called generative grammar since the language is defined by\n the set of possible sentences ‚Äúgenerated‚Äù by the grammar.\n\n F.2.1 Formal Definition of Context-Free Grammar\n We conclude this section with a quick, formal description of a context-free grammar and the language it generates. A context-free grammar G is defined by four\n parameters: N, Œ£, R, S (technically this is a ‚Äú4-tuple‚Äù).\n\n N a set of non-terminal symbols (or variables)\n Œ£ a set of terminal symbols (disjoint from N)\n R a set of rules or productions, each of the form A ‚Üí Œ≤ ,\n where A is a non-terminal,\n Œ≤ is a string of symbols from the infinite set of strings (Œ£ ‚à™ N)‚àó\n S a designated start symbol and a member of N\n\n For the remainder of the book we adhere to the following conventions when discussing the formal properties of context-free grammars (as opposed to explaining\n particular facts about English or other languages).\n Capital letters like A, B, and S Non-terminals\n S The start symbol\n Lower-case Greek letters like Œ±, Œ≤ , and Œ≥ Strings drawn from (Œ£ ‚à™ N)‚àó\n Lower-case Roman letters like u, v, and w Strings of terminals\n\n A language is defined through the concept of derivation. One string derives another one if it can be rewritten as the second one by some series of rule applications.\n More formally, following Hopcroft and Ullman (1979),\n if A ‚Üí Œ≤ is a production of R and Œ± and Œ≥ are any strings in the set\ndirectly derives (Œ£ ‚à™ N)‚àó , then we say that Œ±AŒ≥ directly derives Œ±Œ≤ Œ≥, or Œ±AŒ≥ ‚áí Œ±Œ≤ Œ≥.\n Derivation is then a generalization of direct derivation:\n Let Œ±1 , Œ±2 , . . . , Œ±m be strings in (Œ£ ‚à™ N)‚àó , m ‚â• 1, such that\n\n Œ±1 ‚áí Œ±2 , Œ±2 ‚áí Œ±3 , . . . , Œ±m‚àí1 ‚áí Œ±m\n ‚àó\n derives We say that Œ±1 derives Œ±m , or Œ±1 ‚áí Œ±m .\n We can then formally define the language LG generated by a grammar G as the\n set of strings composed of terminal symbols that can be derived from the designated\n start symbol S.\n ‚àó\n LG = {w|w is in Œ£‚àó and S ‚áí w}\n The problem of mapping from a string of words to its parse tree is called synsyntactic\n parsing tactic parsing; we define algorithms for constituency parsing in Chapter 18.\n F.3 ‚Ä¢ S OME G RAMMAR RULES FOR E NGLISH 7\n\nF.3 Some Grammar Rules for English\n In this section, we introduce a few more aspects of the phrase structure of English;\n for consistency we will continue to focus on sentences from the ATIS domain. Because of space limitations, our discussion is necessarily limited to highlights. Readers are strongly advised to consult a good reference grammar of English, such as\n Huddleston and Pullum (2002).\n\n F.3.1 Sentence-Level Constructions\n In the small grammar L0 , we provided only one sentence-level construction for\n declarative sentences like I prefer a morning flight. Among the large number of\n constructions for English sentences, four are particularly common and important:\n declaratives, imperatives, yes-no questions, and wh-questions.\n declarative Sentences with declarative structure have a subject noun phrase followed by a\n verb phrase, like ‚ÄúI prefer a morning flight‚Äù. Sentences with this structure have a\n great number of different uses; here examples from the ATIS domain:\n I want a flight from Ontario to Chicago\n The flight should be eleven a.m. tomorrow\n The return flight should leave at around seven p.m.\n imperative Sentences with imperative structure often begin with a verb phrase and have\n no subject. They are called imperative because they are almost always used for\n commands and suggestions; in the ATIS domain they are commands to the system.\n Show the lowest fare\n Give me Sunday‚Äôs flights arriving in Las Vegas from New York City\n List all flights between five and seven p.m.\n We can model this sentence structure with another rule for the expansion of S:\n S ‚Üí VP\nyes-no question Sentences with yes-no question structure are often (though not always) used to\n ask questions; they begin with an auxiliary verb, followed by a subject NP, followed\n by a VP. Here are some examples. Note that the third example is not a question at\n all but a request.\n Do any of these flights have stops?\n Does American‚Äôs flight eighteen twenty five serve dinner?\n Can you give me the same information for United?\n Here‚Äôs the rule:\n S ‚Üí Aux NP VP\n The most complex sentence-level structures we examine here are the various whwh-phrase structures. These are so named because one of their constituents is a wh-phrase, that\n wh-word is, one that includes a wh-word (who, whose, when, where, what, which, how, why).\n These may be broadly grouped into two classes of sentence-level structures. The\n wh-subject-question structure is identical to the declarative structure, except that\n the first noun phrase contains some wh-word.\n What airlines fly from Burbank to Denver?\n Which flights depart Burbank after noon and arrive in Denver by six p.m?\n Whose flights serve breakfast?\n8 A PPENDIX F ‚Ä¢ C ONSTITUENCY G RAMMARS\n\n Here is a rule. Exercise F.7 discusses rules for the constituents that make up the\n Wh-NP.\n\n S ‚Üí Wh-NP VP\n\nwh-non-subject- In the wh-non-subject-question structure, the wh-phrase is not the subject of the\n question\n sentence, and so the sentence includes another subject. In these types of sentences\n the auxiliary appears before the subject NP, just as in the yes-no question structures.\n Here is an example followed by a sample rule:\n What flights do you have from Burbank to Tacoma Washington?\n\n S ‚Üí Wh-NP Aux NP VP\n\n Constructions like the wh-non-subject-question contain what are called longlong-distance\n dependencies distance dependencies because the Wh-NP what flights is far away from the predicate that it is semantically related to, the main verb have in the VP. In some models\n of parsing and understanding compatible with the grammar rule above, long-distance\n dependencies like the relation between flights and have are thought of as a semantic\n relation. In such models, the job of figuring out that flights is the argument of have is\n done during semantic interpretation. Other models of parsing represent the relationship between flights and have as a syntactic relation, and the grammar is modified to\n insert a small marker called a trace or empty category after the verb. We discuss\n empty-category models when we introduce the Penn Treebank on page 15.\n\n F.3.2 Clauses and Sentences\n Before we move on, we should clarify the status of the S rules in the grammars we\n just described. S rules are intended to account for entire sentences that stand alone\n as fundamental units of discourse. However, S can also occur on the right-hand side\n of grammar rules and hence can be embedded within larger sentences. Clearly then,\n there‚Äôs more to being an S than just standing alone as a unit of discourse.\n What differentiates sentence constructions (i.e., the S rules) from the rest of the\n grammar is the notion that they are in some sense complete. In this way they correclause spond to the notion of a clause, which traditional grammars often describe as forming a complete thought. One way of making this notion of ‚Äúcomplete thought‚Äù more\n precise is to say an S is a node of the parse tree below which the main verb of the S\n has all of its arguments. We define verbal arguments later, but for now let‚Äôs just see\n an illustration from the tree for I prefer a morning flight in Fig. F.4 on page 5. The\n verb prefer has two arguments: the subject I and the object a morning flight. One of\n the arguments appears below the VP node, but the other one, the subject NP, appears\n only below the S node.\n\n F.3.3 The Noun Phrase\n Our L0 grammar introduced three of the most frequent types of noun phrases that\n occur in English: pronouns, proper nouns and the NP ‚Üí Det Nominal construction.\n The central focus of this section is on the last type since that is where the bulk of\n the syntactic complexity resides. These noun phrases consist of a head, the central\n noun in the noun phrase, along with various modifiers that can occur before or after\n the head noun. Let‚Äôs take a close look at the various parts.\n F.3 ‚Ä¢ S OME G RAMMAR RULES FOR E NGLISH 9\n\n The Determiner\n Noun phrases can begin with simple lexical determiners:\n a stop the flights this flight\n those flights any flights some flights\n The role of the determiner can also be filled by more complex expressions:\n United‚Äôs flight\n United‚Äôs pilot‚Äôs union\n Denver‚Äôs mayor‚Äôs mother‚Äôs canceled flight\n In these examples, the role of the determiner is filled by a possessive expression\n consisting of a noun phrase followed by an ‚Äôs as a possessive marker, as in the\n following rule.\n Det ‚Üí NP 0 s\n The fact that this rule is recursive (since an NP can start with a Det) helps us model\n the last two examples above, in which a sequence of possessive expressions serves\n as a determiner.\n Under some circumstances determiners are optional in English. For example,\n determiners may be omitted if the noun they modify is plural:\n (F.2) Show me flights from San Francisco to Denver on weekdays\n As we saw in Chapter 17, mass nouns also don‚Äôt require determination. Recall that\n mass nouns often (not always) involve something that is treated like a substance\n (including e.g., water and snow), don‚Äôt take the indefinite article ‚Äúa‚Äù, and don‚Äôt tend\n to pluralize. Many abstract nouns are mass nouns (music, homework). Mass nouns\n in the ATIS domain include breakfast, lunch, and dinner:\n (F.3) Does this flight serve dinner?\n\n The Nominal\n The nominal construction follows the determiner and contains any pre- and posthead noun modifiers. As indicated in grammar L0 , in its simplest form a nominal\n can consist of a single noun.\n Nominal ‚Üí Noun\n As we‚Äôll see, this rule also provides the basis for the bottom of various recursive\n rules used to capture more complex nominal constructions.\n\n Before the Head Noun\n A number of different kinds of word classes can appear before the head noun but\n cardinal after the determiner (the ‚Äúpostdeterminers‚Äù) in a nominal. These include cardinal\n numbers\n ordinal numbers, ordinal numbers, quantifiers, and adjectives. Examples of cardinal\n numbers\nquantifiers numbers:\n two friends one stop\n Ordinal numbers include first, second, third, and so on, but also words like next,\n last, past, other, and another:\n the first one the next day the second leg\n the last flight the other American flight\n Some quantifiers (many, (a) few, several) occur only with plural count nouns:\n10 A PPENDIX F ‚Ä¢ C ONSTITUENCY G RAMMARS\n\n many fares\n Adjectives occur after quantifiers but before nouns.\n a first-class fare a non-stop flight\n the longest layover the earliest lunch flight\n adjective\n phrase Adjectives can also be grouped into a phrase called an adjective phrase or AP.\n APs can have an adverb before the adjective (see Chapter 17 for definitions of adjectives and adverbs):\n the least expensive fare\n\n After the Head Noun\n A head noun can be followed by postmodifiers. Three kinds of nominal postmodifiers are common in English:\n prepositional phrases all flights from Cleveland\n non-finite clauses any flights arriving after eleven a.m.\n relative clauses a flight that serves breakfast\n They are especially common in the ATIS corpus since they are used to mark the\n origin and destination of flights.\n Here are some examples of prepositional phrase postmodifiers, with brackets\n inserted to show the boundaries of each PP; note that two or more PPs can be strung\n together within a single NP:\n all flights [from Cleveland] [to Newark]\n arrival [in San Jose] [before seven p.m.]\n a reservation [on flight six oh six] [from Tampa] [to Montreal]\n Here‚Äôs a new nominal rule to account for postnominal PPs:\n Nominal ‚Üí Nominal PP\n non-finite The three most common kinds of non-finite postmodifiers are the gerundive (-\n ing), -ed, and infinitive forms.\n gerundive Gerundive postmodifiers are so called because they consist of a verb phrase that\n begins with the gerundive (-ing) form of the verb. Here are some examples:\n any of those [leaving on Thursday]\n any flights [arriving after eleven a.m.]\n flights [arriving within thirty minutes of each other]\n We can define the Nominals with gerundive modifiers as follows, making use of\n a new non-terminal GerundVP:\n Nominal ‚Üí Nominal GerundVP\n We can make rules for GerundVP constituents by duplicating all of our VP productions, substituting GerundV for V.\n GerundVP ‚Üí GerundV NP\n | GerundV PP | GerundV | GerundV NP PP\n GerundV can then be defined as\n GerundV ‚Üí being | arriving | leaving | . . .\n The phrases in italics below are examples of the two other common kinds of\n non-finite clauses, infinitives and -ed forms:\n F.3 ‚Ä¢ S OME G RAMMAR RULES FOR E NGLISH 11\n\n the last flight to arrive in Boston\n I need to have dinner served\n Which is the aircraft used by this flight?\n A postnominal relative clause (more correctly a restrictive relative clause), is\n relative\n pronoun a clause that often begins with a relative pronoun (that and who are the most common). The relative pronoun functions as the subject of the embedded verb in the\n following examples:\n\n a flight that serves breakfast\n flights that leave in the morning\n the one that leaves at ten thirty five\n We might add rules like the following to deal with these:\n Nominal ‚Üí Nominal RelClause\n RelClause ‚Üí (who | that) VP\n The relative pronoun may also function as the object of the embedded verb, as\n in the following example; we leave for the reader the exercise of writing grammar\n rules for more complex relative clauses of this kind.\n\n the earliest American Airlines flight that I can get\n Various postnominal modifiers can be combined:\n a flight [from Phoenix to Detroit] [leaving Monday evening]\n evening flights [from Nashville to Houston] [that serve dinner]\n a friend [living in Denver] [that would like to visit me in DC]\n\n Before the Noun Phrase\npredeterminers Word classes that modify and appear before NPs are called predeterminers. Many\n of these have to do with number or amount; a common predeterminer is all:\n all the flights all flights all non-stop flights\n The example noun phrase given in Fig. F.5 illustrates some of the complexity\n that arises when these rules are combined.\n\n F.3.4 The Verb Phrase\n The verb phrase consists of the verb and a number of other constituents. In the\n simple rules we have built so far, these other constituents include NPs and PPs and\n combinations of the two:\n VP ‚Üí Verb disappear\n VP ‚Üí Verb NP prefer a morning flight\n VP ‚Üí Verb NP PP leave Boston in the morning\n VP ‚Üí Verb PP leaving on Thursday\n Verb phrases can be significantly more complicated than this. Many other kinds\n of constituents, such as an entire embedded sentence, can follow the verb. These are\n sentential\n complements called sentential complements:\n You [VP [V said [S you had a two hundred sixty-six dollar fare]]\n [VP [V Tell] [NP me] [S how to get from the airport to downtown]]\n I [VP [V think [S I would like to take the nine thirty flight]]\n12 A PPENDIX F ‚Ä¢ C ONSTITUENCY G RAMMARS\n\n NP\n\n PreDet NP\n\n all Det Nom\n\n the Nom GerundiveVP\n\n Nom PP leaving before 10\n\n Nom PP to Tampa\n\n Nom Noun from Denver\n\n Noun flights\n\n morning\n\nFigure F.5 A parse tree for ‚Äúall the morning flights from Denver to Tampa leaving before 10‚Äù.\n\n Here‚Äôs a rule for these:\n\n VP ‚Üí Verb S\n\n Similarly, another potential constituent of the VP is another VP. This is often the\n case for verbs like want, would like, try, intend, need:\n I want [VP to fly from Milwaukee to Orlando]\n Hi, I want [VP to arrange three flights]\n While a verb phrase can have many possible kinds of constituents, not every\n verb is compatible with every verb phrase. For example, the verb want can be used\n either with an NP complement (I want a flight . . . ) or with an infinitive VP complement (I want to fly to . . . ). By contrast, a verb like find cannot take this sort of VP\n complement (* I found to fly to Dallas).\n This idea that verbs are compatible with different kinds of complements is a very\n transitive old one; traditional grammar distinguishes between transitive verbs like find, which\n intransitive take a direct object NP (I found a flight), and intransitive verbs like disappear,\n which do not (*I disappeared a flight).\n subcategorize Where traditional grammars subcategorize verbs into these two categories (transitive and intransitive), modern grammars distinguish as many as 100 subcategories.\n subcategorizes We say that a verb like find subcategorizes for an NP, and a verb like want subfor\n categorizes for either an NP or a non-finite VP. We also call these constituents the\n complements complements of the verb (hence our use of the term sentential complement above).\n So we say that want can take a VP complement. These possible sets of complements\nsubcategorization are called the subcategorization frame for the verb. Another way of talking about\n frame\n the relation between the verb and these other constituents is to think of the verb as\n a logical predicate and the constituents as logical arguments of the predicate. So we\n can think of such predicate-argument relations as FIND (I, A FLIGHT ) or WANT (I, TO\n FLY ). We talk more about this view of verbs and arguments in Appendix F when we\n talk about predicate calculus representations of verb semantics. Subcategorization\n frames for a set of example verbs are given in Fig. F.6.\n F.3 ‚Ä¢ S OME G RAMMAR RULES FOR E NGLISH 13\n\n Frame Verb Example\n 0/ eat, sleep I ate\n NP prefer, find, leave Find [NP the flight from Pittsburgh to Boston]\n NP NP show, give Show [NP me] [NP airlines with flights from Pittsburgh]\n PPfrom PPto fly, travel I would like to fly [PP from Boston] [PP to Philadelphia]\n NP PPwith help, load Can you help [NP me] [PP with a flight]\n VPto prefer, want, need I would prefer [VPto to go by United Airlines]\n S mean Does this mean [S AA has a hub in Boston]\nFigure F.6 Subcategorization frames for a set of example verbs.\n\n We can capture the association between verbs and their complements by making\n separate subtypes of the class Verb (e.g., Verb-with-NP-complement, Verb-with-Inf-\nVP-complement, Verb-with-S-complement, and so on):\n\n Verb-with-NP-complement ‚Üí find | leave | repeat | . . .\n Verb-with-S-complement ‚Üí think | believe | say | . . .\n Verb-with-Inf-VP-complement ‚Üí want | try | need | . . .\n\n Each VP rule could then be modified to require the appropriate verb subtype:\n\n VP ‚Üí Verb-with-no-complement disappear\n VP ‚Üí Verb-with-NP-comp NP prefer a morning flight\n VP ‚Üí Verb-with-S-comp S said there were two flights\n\n A problem with this approach is the significant increase in the number of rules and\n the associated loss of generality.\n\n F.3.5 Coordination\n conjunctions The major phrase types discussed here can be conjoined with conjunctions like and,\n coordinate or, and but to form larger constructions of the same type. For example, a coordinate\n noun phrase can consist of two other noun phrases separated by a conjunction:\n Please repeat [NP [NP the flights] and [NP the costs]]\n I need to know [NP [NP the aircraft] and [NP the flight number]]\n Here‚Äôs a rule that allows these structures:\n\n NP ‚Üí NP and NP\n\n Note that the ability to form coordinate phrases through conjunctions is often\n used as a test for constituency. Consider the following examples, which differ from\n the ones given above in that they lack the second determiner.\n Please repeat the [Nom [Nom flights] and [Nom costs]]\n I need to know the [Nom [Nom aircraft] and [Nom flight number]]\n The fact that these phrases can be conjoined is evidence for the presence of the\n underlying Nominal constituent we have been making use of. Here‚Äôs a rule for this:\n\n Nominal ‚Üí Nominal and Nominal\n\n The following examples illustrate conjunctions involving VPs and Ss.\n14 A PPENDIX F ‚Ä¢ C ONSTITUENCY G RAMMARS\n\n What flights do you have [VP [VP leaving Denver] and [VP arriving in\n San Francisco]]\n [S [S I‚Äôm interested in a flight from Dallas to Washington] and [S I‚Äôm\n also interested in going to Baltimore]]\n The rules for VP and S conjunctions mirror the NP one given above.\n VP ‚Üí VP and VP\n S ‚Üí S and S\n Since all the major phrase types can be conjoined in this fashion, it is also possible\n to represent this conjunction fact more generally; a number of grammar formalisms\n metarules such as GPSG (Gazdar et al., 1985) do this using metarules like:\n X ‚Üí X and X\n This metarule states that any non-terminal can be conjoined with the same nonterminal to yield a constituent of the same type; the variable X must be designated\n as a variable that stands for any non-terminal rather than a non-terminal itself.\n\nF.4 Treebanks\n Sufficiently robust grammars consisting of context-free grammar rules can be used\n to assign a parse tree to any sentence. This means that it is possible to build a\n corpus where every sentence in the collection is paired with a corresponding parse\n treebank tree. Such a syntactically annotated corpus is called a treebank. Treebanks play\n an important role in parsing, as we discuss in Chapter 18, as well as in linguistic\n investigations of syntactic phenomena.\n A wide variety of treebanks have been created, generally through the use of\n parsers (of the sort described in the next few chapters) to automatically parse each\n sentence, followed by the use of humans (linguists) to hand-correct the parses. The\nPenn Treebank Penn Treebank project (whose POS tagset we introduced in Chapter 17) has produced treebanks from the Brown, Switchboard, ATIS, and Wall Street Journal corpora of English, as well as treebanks in Arabic and Chinese. A number of treebanks\n use the dependency representation we will introduce in Chapter 19, including many\n that are part of the Universal Dependencies project (Nivre et al., 2016).\n\n F.4.1 Example: The Penn Treebank Project\n Figure F.7 shows sentences from the Brown and ATIS portions of the Penn Treebank.2 Note the formatting differences for the part-of-speech tags; such small differences are common and must be dealt with in processing treebanks. The Penn\n Treebank part-of-speech tagset was defined in Chapter 17. The use of LISP-style\n parenthesized notation for trees is extremely common and resembles the bracketed\n notation we saw earlier in (F.1). For those who are not familiar with it we show a\n standard node-and-line tree representation in Fig. F.8.\n Figure F.9 shows a tree from the Wall Street Journal. This tree shows another featraces ture of the Penn Treebanks: the use of traces (-NONE- nodes) to mark long-distance\n 2 The Penn Treebank project released treebanks in multiple languages and in various stages; for example, there were Treebank I (Marcus et al., 1993), Treebank II (Marcus et al., 1994), and Treebank III\n releases of English treebanks. We use Treebank III for our examples.\n F.4 ‚Ä¢ T REEBANKS 15\n\n ((S\n (NP-SBJ (DT That) ((S\n (JJ cold) (, ,) (NP-SBJ The/DT flight/NN )\n (JJ empty) (NN sky) ) (VP should/MD\n (VP (VBD was) (VP arrive/VB\n (ADJP-PRD (JJ full) (PP-TMP at/IN\n (PP (IN of) (NP eleven/CD a.m/RB ))\n (NP (NN fire) (NP-TMP tomorrow/NN )))))\n (CC and)\n (NN light) ))))\n (. .) ))\n (a) (b)\n Figure F.7 Parsed sentences from the LDC Treebank3 version of the (a) Brown and (b)\n ATIS corpora.\n\n S\n\n NP-SBJ VP .\n\n DT JJ , JJ NN VBD ADJP-PRD .\n\n That cold , empty sky was JJ PP\n\n full IN NP\n\n of NN CC NN\n\n fire and light\n Figure F.8 The tree corresponding to the Brown corpus sentence in the previous figure.\n\n syntactic dependencies or syntactic movement. For example, quotations often follow a quomovement\n tative verb like say. But in this example, the quotation ‚ÄúWe would have to wait until\n we have collected on those assets‚Äù precedes the words he said. An empty S containing only the node -NONE- marks the position after said where the quotation sentence\n often occurs. This empty node is marked (in Treebanks II and III) with the index 2,\n as is the quotation S at the beginning of the sentence. Such co-indexing may make it\n easier for some parsers to recover the fact that this fronted or topicalized quotation\n is the complement of the verb said. A similar -NONE- node marks the fact that there\n is no syntactic subject right before the verb to wait; instead, the subject is the earlier\n NP We. Again, they are both co-indexed with the index 1.\n The Penn Treebank II and Treebank III releases added further information to\n make it easier to recover the relationships between predicates and arguments. Certain phrases were marked with tags indicating the grammatical function of the phrase\n (as surface subject, logical topic, cleft, non-VP predicates) its presence in particular\n text categories (headlines, titles), and its semantic function (temporal phrases, lo-\n16 A PPENDIX F ‚Ä¢ C ONSTITUENCY G RAMMARS\n\n ( (S (‚Äò‚Äò ‚Äò‚Äò)\n (S-TPC-2\n (NP-SBJ-1 (PRP We) )\n (VP (MD would)\n (VP (VB have)\n (S\n (NP-SBJ (-NONE- *-1) )\n (VP (TO to)\n (VP (VB wait)\n (SBAR-TMP (IN until)\n (S\n (NP-SBJ (PRP we) )\n (VP (VBP have)\n (VP (VBN collected)\n (PP-CLR (IN on)\n (NP (DT those)(NNS assets)))))))))))))\n (, ,) (‚Äô‚Äô ‚Äô‚Äô)\n (NP-SBJ (PRP he) )\n (VP (VBD said)\n (S (-NONE- *T*-2) ))\n (. .) ))\n\n Figure F.9 A sentence from the Wall Street Journal portion of the LDC Penn Treebank.\n Note the use of the empty -NONE- nodes.\n\n cations) (Marcus et al. 1994, Bies et al. 1995). Figure F.9 shows examples of the\n -SBJ (surface subject) and -TMP (temporal phrase) tags. Figure F.8 shows in addition the -PRD tag, which is used for predicates that are not VPs (the one in Fig. F.8\n is an ADJP). We‚Äôll return to the topic of grammatical function when we consider\n dependency grammars and parsing in Chapter 19.\n\n F.4.2 Treebanks as Grammars\n The sentences in a treebank implicitly constitute a grammar of the language represented by the corpus being annotated. For example, from the three parsed sentences\n in Fig. F.7 and Fig. F.9, we can extract each of the CFG rules in them. For simplicity,\n let‚Äôs strip off the rule suffixes (-SBJ and so on). The resulting grammar is shown in\n Fig. F.10.\n The grammar used to parse the Penn Treebank is relatively flat, resulting in very\n many and very long rules. For example, among the approximately 4,500 different\n rules for expanding VPs are separate rules for PP sequences of any length and every\n possible arrangement of verb arguments:\n VP ‚Üí VBD PP\n VP ‚Üí VBD PP PP\n VP ‚Üí VBD PP PP PP\n VP ‚Üí VBD PP PP PP PP\n VP ‚Üí VB ADVP PP\n VP ‚Üí VB PP ADVP\n VP ‚Üí ADVP VB PP\n as well as even longer rules, such as\n VP ‚Üí VBP PP PP PP PP PP ADVP PP\n F.4 ‚Ä¢ T REEBANKS 17\n\n Grammar Lexicon\n S ‚Üí NP VP . PRP ‚Üí we | he\n S ‚Üí NP VP DT ‚Üí the | that | those\n S ‚Üí ‚Äú S ‚Äù , NP VP . JJ ‚Üí cold | empty | full\n S ‚Üí -NONE- NN ‚Üí sky | fire | light | flight | tomorrow\n NP ‚Üí DT NN NNS ‚Üí assets\n NP ‚Üí DT NNS CC ‚Üí and\n NP ‚Üí NN CC NN IN ‚Üí of | at | until | on\n NP ‚Üí CD RB CD ‚Üí eleven\n NP ‚Üí DT JJ , JJ NN RB ‚Üí a.m.\n NP ‚Üí PRP VB ‚Üí arrive | have | wait\n NP ‚Üí -NONE- VBD ‚Üí was | said\n VP ‚Üí MD VP VBP ‚Üí have\n VP ‚Üí VBD ADJP VBN ‚Üí collected\n VP ‚Üí VBD S MD ‚Üí should | would\n VP ‚Üí VBN PP TO ‚Üí to\n VP ‚Üí VB S\n VP ‚Üí VB SBAR\n VP ‚Üí VBP VP\n VP ‚Üí VBN PP\n VP ‚Üí TO VP\n SBAR ‚Üí IN S\n ADJP ‚Üí JJ PP\n PP ‚Üí IN NP\n Figure F.10 A sample of the CFG grammar rules and lexical entries that would be extracted\n from the three treebank sentences in Fig. F.7 and Fig. F.9.\n\n which comes from the VP marked in italics:\n This mostly happens because we go from football in the fall to lifting in the\n winter to football again in the spring.\n Some of the many thousands of NP rules include\n NP ‚Üí DT JJ NN\n NP ‚Üí DT JJ NNS\n NP ‚Üí DT JJ NN NN\n NP ‚Üí DT JJ JJ NN\n NP ‚Üí DT JJ CD NNS\n NP ‚Üí RB DT JJ NN NN\n NP ‚Üí RB DT JJ JJ NNS\n NP ‚Üí DT JJ JJ NNP NNS\n NP ‚Üí DT NNP NNP NNP NNP JJ NN\n NP ‚Üí DT JJ NNP CC JJ JJ NN NNS\n NP ‚Üí RB DT JJS NN NN SBAR\n NP ‚Üí DT VBG JJ NNP NNP CC NNP\n NP ‚Üí DT JJ NNS , NNS CC NN NNS NN\n NP ‚Üí DT JJ JJ VBG NN NNP NNP FW NNP\n NP ‚Üí NP JJ , JJ ‚Äò‚Äò SBAR ‚Äô‚Äô NNS\n\n The last two of those rules, for example, come from the following two noun phrases:\n[DT The] [JJ state-owned] [JJ industrial] [VBG holding] [NN company] [NNP Instituto] [NNP Nacional]\n [FW de] [NNP Industria]\n[NP Shearson‚Äôs] [JJ easy-to-film], [JJ black-and-white] ‚Äú[SBAR Where We Stand]‚Äù [NNS commercials]\n Viewed as a large grammar in this way, the Penn Treebank III Wall Street Journal\n corpus, which contains about 1 million words, also has about 1 million non-lexical\n rule tokens, consisting of about 17,500 distinct rule types.\n18 A PPENDIX F ‚Ä¢ C ONSTITUENCY G RAMMARS\n\n S(dumped)\n\n NP(workers) VP(dumped)\n\n NNS(workers) VBD(dumped) NP(sacks) PP(into)\n\n workers dumped NNS(sacks) P NP(bin)\n\n sacks into DT(a) NN(bin)\n\n a bin\n Figure F.11 A lexicalized tree from Collins (1999).\n\n Various facts about the treebank grammars, such as their large numbers of flat\n rules, pose problems for probabilistic parsing algorithms. For this reason, it is common to make various modifications to a grammar extracted from a treebank. We\n discuss these further in Appendix C.\n\n F.4.3 Heads and Head-Finding\n We suggested informally earlier that syntactic constituents could be associated with\n a lexical head; N is the head of an NP, V is the head of a VP. This idea of a head\n for each constituent dates back to Bloomfield 1914, and is central to the dependency\n grammars and dependency parsing we‚Äôll introduce in Chapter 19. Heads are also\n important in probabilistic parsing (Appendix C) and in constituent-based grammar\n formalisms like Head-Driven Phrase Structure Grammar (Pollard and Sag, 1994)..\n In one simple model of lexical heads, each context-free rule is associated with\n a head (Charniak 1997, Collins 1999). The head is the word in the phrase that is\n grammatically the most important. Heads are passed up the parse tree; thus, each\n non-terminal in a parse tree is annotated with a single word, which is its lexical\n head. Figure F.11 shows an example of such a tree from Collins (1999), in which\n each non-terminal is annotated with its head.\n For the generation of such a tree, each CFG rule must be augmented to identify\n one right-side constituent to be the head child. The headword for a node is then set to\n the headword of its head child. Choosing these head children is simple for textbook\n examples (NN is the head of NP) but is complicated and indeed controversial for\n most phrases. (Should the complementizer to or the verb be the head of an infinite\n verb phrase?) Modern linguistic theories of syntax generally include a component\n that defines heads (see, e.g., (Pollard and Sag, 1994)).\n An alternative approach to finding a head is used in most practical computational\n systems. Instead of specifying head rules in the grammar itself, heads are identified\n dynamically in the context of trees for specific sentences. In other words, once\n a sentence is parsed, the resulting tree is walked to decorate each node with the\n appropriate head. Most current systems rely on a simple set of handwritten rules,\n such as a practical one for Penn Treebank grammars given in Collins (1999) but\n developed originally by Magerman (1995). For example, the rule for finding the\n head of an NP is as follows (Collins, 1999, p. 238):\n\n ‚Ä¢ If the last word is tagged POS, return last-word.\n F.5 ‚Ä¢ G RAMMAR E QUIVALENCE AND N ORMAL F ORM 19\n\n ‚Ä¢ Else search from right to left for the first child which is an NN, NNP, NNPS, NX, POS,\n or JJR.\n ‚Ä¢ Else search from left to right for the first child which is an NP.\n ‚Ä¢ Else search from right to left for the first child which is a $, ADJP, or PRN.\n ‚Ä¢ Else search from right to left for the first child which is a CD.\n ‚Ä¢ Else search from right to left for the first child which is a JJ, JJS, RB or QP.\n ‚Ä¢ Else return the last word\n\n Selected other rules from this set are shown in Fig. F.12. For example, for VP\n rules of the form VP ‚Üí Y1 ¬∑ ¬∑ ¬∑ Yn , the algorithm would start from the left of Y1 ¬∑ ¬∑ ¬∑\n Yn looking for the first Yi of type TO; if no TOs are found, it would search for the\n first Yi of type VBD; if no VBDs are found, it would search for a VBN, and so on.\n See Collins (1999) for more details.\n\nParent Direction Priority List\nADJP Left NNS QP NN $ ADVP JJ VBN VBG ADJP JJR NP JJS DT FW RBR RBS\n SBAR RB\nADVP Right RB RBR RBS FW ADVP TO CD JJR JJ IN NP JJS NN\nPRN Left\nPRT Right RP\nQP Left $ IN NNS NN JJ RB DT CD NCD QP JJR JJS\nS Left TO IN VP S SBAR ADJP UCP NP\nSBAR Left WHNP WHPP WHADVP WHADJP IN DT S SQ SINV SBAR FRAG\nVP Left TO VBD VBN MD VBZ VB VBG VBP VP ADJP NN NNS NP\nFigure F.12 Some head rules from Collins (1999). The head rules are also called a head percolation table.\n\nF.5 Grammar Equivalence and Normal Form\n A formal language is defined as a (possibly infinite) set of strings of words. This\n suggests that we could ask if two grammars are equivalent by asking if they generate the same set of strings. In fact, it is possible to have two distinct context-free\n grammars generate the same language.\n We usually distinguish two kinds of grammar equivalence: weak equivalence\n and strong equivalence. Two grammars are strongly equivalent if they generate the\n same set of strings and if they assign the same phrase structure to each sentence\n (allowing merely for renaming of the non-terminal symbols). Two grammars are\n weakly equivalent if they generate the same set of strings but do not assign the same\n phrase structure to each sentence.\n normal form It is sometimes useful to have a normal form for grammars, in which each of\n the productions takes a particular form. For example, a context-free grammar is in\n Chomsky Chomsky normal form (CNF) (Chomsky, 1963) if it is \u000f-free and if in addition\n normal form\n each production is either of the form A ‚Üí B C or A ‚Üí a. That is, the right-hand side\n of each rule either has two non-terminal symbols or one terminal symbol. Chomsky\n binary\n branching normal form grammars are binary branching, that is they have binary trees (down\n to the prelexical nodes). We make use of this binary branching property in the CKY\n parsing algorithm in Chapter 18.\n Any context-free grammar can be converted into a weakly equivalent Chomsky\n normal form grammar. For example, a rule of the form\n\n A ‚Üí B C D\n20 A PPENDIX F ‚Ä¢ C ONSTITUENCY G RAMMARS\n\n can be converted into the following two CNF rules (Exercise F.?? asks the reader to\n formulate the complete algorithm):\n\n A ‚Üí B X\n X ‚Üí C D\n\n Sometimes using binary branching can actually produce smaller grammars. For\n example, the sentences that might be characterized as\n VP -> VBD NP PP*\n are represented in the Penn Treebank by this series of rules:\n VP ‚Üí VBD NP PP\n VP ‚Üí VBD NP PP PP\n VP ‚Üí VBD NP PP PP PP\n VP ‚Üí VBD NP PP PP PP PP\n ...\n but could also be generated by the following two-rule grammar:\n VP ‚Üí VBD NP PP\n VP ‚Üí VP PP\n The generation of a symbol A with a potentially infinite sequence of symbols B with\n Chomskyadjunction a rule of the form A ‚Üí A B is known as Chomsky-adjunction.\n\nF.6 Summary\n This chapter has introduced a number of fundamental concepts in syntax through\n the use of context-free grammars.\n ‚Ä¢ In many languages, groups of consecutive words act as a group or a constituent, which can be modeled by context-free grammars (which are also\n known as phrase-structure grammars).\n ‚Ä¢ A context-free grammar consists of a set of rules or productions, expressed\n over a set of non-terminal symbols and a set of terminal symbols. Formally,\n a particular context-free language is the set of strings that can be derived\n from a particular context-free grammar.\n ‚Ä¢ A generative grammar is a traditional name in linguistics for a formal language that is used to model the grammar of a natural language.\n ‚Ä¢ There are many sentence-level grammatical constructions in English; declarative, imperative, yes-no question, and wh-question are four common types;\n these can be modeled with context-free rules.\n ‚Ä¢ An English noun phrase can have determiners, numbers, quantifiers, and\n adjective phrases preceding the head noun, which can be followed by a number of postmodifiers; gerundive and infinitive VPs are common possibilities.\n ‚Ä¢ Subjects in English agree with the main verb in person and number.\n ‚Ä¢ Verbs can be subcategorized by the types of complements they expect. Simple subcategories are transitive and intransitive; most grammars include\n many more categories than these.\n ‚Ä¢ Treebanks of parsed sentences exist for many genres of English and for many\n languages. Treebanks can be searched with tree-search tools.\n H ISTORICAL N OTES 21\n\n ‚Ä¢ Any context-free grammar can be converted to Chomsky normal form, in\n which the right-hand side of each rule has either two non-terminals or a single\n terminal.\n\nHistorical Notes\n According to Percival (1976), the idea of breaking up a sentence into a hierarchy of\n constituents appeared in the VoÃàlkerpsychologie of the groundbreaking psychologist\n Wilhelm Wundt (Wundt, 1900):\n ...den sprachlichen Ausdruck fuÃàr die willkuÃàrliche Gliederung einer Gesammtvorstellung in ihre in logische Beziehung zueinander gesetzten\n Bestandteile\n [the linguistic expression for the arbitrary division of a total idea\n into its constituent parts placed in logical relations to one another]\n Wundt‚Äôs idea of constituency was taken up into linguistics by Leonard Bloomfield in his early book An Introduction to the Study of Language (Bloomfield, 1914).\n By the time of his later book, Language (Bloomfield, 1933), what was then called\n ‚Äúimmediate-constituent analysis‚Äù was a well-established method of syntactic study\n in the United States. By contrast, traditional European grammar, dating from the\n Classical period, defined relations between words rather than constituents, and European syntacticians retained this emphasis on such dependency grammars, the subject of Chapter 19.\n American Structuralism saw a number of specific definitions of the immediate\n constituent, couched in terms of their search for a ‚Äúdiscovery procedure‚Äù: a methodological algorithm for describing the syntax of a language. In general, these attempt\n to capture the intuition that ‚ÄúThe primary criterion of the immediate constituent\n is the degree in which combinations behave as simple units‚Äù (Bazell, 1952/1966, p.\n 284). The most well known of the specific definitions is Harris‚Äô idea of distributional\n similarity to individual units, with the substitutability test. Essentially, the method\n proceeded by breaking up a construction into constituents by attempting to substitute\n simple structures for possible constituents‚Äîif a substitution of a simple form, say,\n man, was substitutable in a construction for a more complex set (like intense young\n man), then the form intense young man was probably a constituent. Harris‚Äôs test was\n the beginning of the intuition that a constituent is a kind of equivalence class.\n The first formalization of this idea of hierarchical constituency was the phrasestructure grammar defined in Chomsky (1956) and further expanded upon (and\n argued against) in Chomsky (1957) and Chomsky (1956/1975). From this time on,\n most generative linguistic theories were based at least in part on context-free grammars or generalizations of them (such as Head-Driven Phrase Structure Grammar\n (Pollard and Sag, 1994), Lexical-Functional Grammar (Bresnan, 1982), the Minimalist Program (Chomsky, 1995), and Construction Grammar (Kay and Fillmore,\n 1999), inter alia); many of these theories used schematic context-free templates\n X-bar known as X-bar schemata, which also relied on the notion of syntactic head.\n schemata\n Shortly after Chomsky‚Äôs initial work, the context-free grammar was reinvented\n by Backus (1959) and independently by Naur et al. (1960) in their descriptions of\n the ALGOL programming language; Backus (1996) noted that he was influenced by\n the productions of Emil Post and that Naur‚Äôs work was independent of his (Backus‚Äô)\n22 A PPENDIX F ‚Ä¢ C ONSTITUENCY G RAMMARS\n\n own. After this early work, a great number of computational models of natural\n language processing were based on context-free grammars because of the early development of efficient algorithms to parse these grammars (see Chapter 18).\n Thre are various classes of extensions to CFGs, many designed to handle longdistance dependencies in the syntax. (Other grammars instead treat long-distancedependent items as being related semantically rather than syntactically (Kay and\n Fillmore 1999, Culicover and Jackendoff 2005).\n One extended formalism is Tree Adjoining Grammar (TAG) (Joshi, 1985).\n The primary TAG data structure is the tree, rather than the rule. Trees come in two\n kinds: initial trees and auxiliary trees. Initial trees might, for example, represent\n simple sentential structures, and auxiliary trees add recursion into a tree. Trees are\n combined by two operations called substitution and adjunction. The adjunction\n operation handles long-distance dependencies. See Joshi (1985) for more details.\n Tree Adjoining Grammar is a member of the family of mildly context-sensitive\n languages.\n We mentioned on page 15 another way of handling long-distance dependencies,\n based on the use of empty categories and co-indexing. The Penn Treebank uses\n this model, which draws (in various Treebank corpora) from the Extended Standard\n Theory and Minimalism (Radford, 1997).\n Readers interested in the grammar of English should get one of the three large\n reference grammars of English: Huddleston and Pullum (2002), Biber et al. (1999),\n and Quirk et al. (1985).\n There are many good introductory textbooks on syntax from different perspecgenerative tives. Sag et al. (2003) is an introduction to syntax from a generative perspective,\n focusing on the use of phrase-structure rules, unification, and the type hierarchy in\n Head-Driven Phrase Structure Grammar. Van Valin, Jr. and La Polla (1997) is an\n functional introduction from a functional perspective, focusing on cross-linguistic data and on\n the functional motivation for syntactic structures.\n\nExercises\n F.1 Draw tree structures for the following ATIS phrases:\n 1. Dallas\n 2. from Denver\n 3. after five p.m.\n 4. arriving in Washington\n 5. early flights\n 6. all redeye flights\n 7. on Thursday\n 8. a one-way fare\n 9. any delays in Denver\n F.2 Draw tree structures for the following ATIS sentences:\n 1. Does American Airlines have a flight between five a.m. and six a.m.?\n 2. I would like to fly on American Airlines.\n 3. Please repeat that.\n 4. Does American 487 have a first-class section?\n 5. I need to fly between Philadelphia and Atlanta.\n 6. What is the fare from Atlanta to Denver?\n E XERCISES 23\n\n 7. Is there an American Airlines flight from Philadelphia to Dallas?\n F.3 Assume a grammar that has many VP rules for different subcategorizations, as\n expressed in Section F.3.4, and differently subcategorized verb rules like Verbwith-NP-complement. How would the rule for postnominal relative clauses\n (F.4) need to be modified if we wanted to deal properly with examples like the\n earliest flight that you have? Recall that in such examples the pronoun that is\n the object of the verb get. Your rules should allow this noun phrase but should\n correctly rule out the ungrammatical S *I get.\n F.4 Does your solution to the previous problem correctly model the NP the earliest\n flight that I can get? How about the earliest flight that I think my mother\n wants me to book for her? Hint: this phenomenon is called long-distance\n dependency.\n F.5 Write rules expressing the verbal subcategory of English auxiliaries; for example, you might have a rule verb-with-bare-stem-VP-complement ‚Üí can.\npossessive F.6 NPs like Fortune‚Äôs office or my uncle‚Äôs marks are called possessive or genitive\n genitive noun phrases. We can model possessive noun phrases by treating the sub-NP\n like Fortune‚Äôs or my uncle‚Äôs as a determiner of the following head noun. Write\n grammar rules for English possessives. You may treat ‚Äôs as if it were a separate\n word (i.e., as if there were always a space before ‚Äôs).\n F.7 Page 8 discussed the need for a Wh-NP constituent. The simplest Wh-NP is\n one of the Wh-pronouns (who, whom, whose, which). The Wh-words what\n and which can be determiners: which four will you have?, what credit do you\n have with the Duke? Write rules for the different types of Wh-NPs.\n24 Appendix F ‚Ä¢ Constituency Grammars\n\nBackus, J. W. 1959. The syntax and semantics of the Joshi, A. K. 1985. Tree adjoining grammars: How\n proposed international algebraic language of the Zurich much context-sensitivity is required to provide reasonable\n ACM-GAMM Conference. Information Processing: Pro- structural descriptions? In D. R. Dowty, L. Karttunen,\n ceedings of the International Conference on Information and A. Zwicky, eds, Natural Language Parsing, 206‚Äì250.\n Processing, Paris. UNESCO. Cambridge University Press.\nBackus, J. W. 1996. Transcript of question and answer ses- Kay, P. and C. J. Fillmore. 1999. Grammatical constructions\n sion. In R. L. Wexelblat, ed., History of Programming and linguistic generalizations: The What‚Äôs X Doing Y?\n Languages, page 162. Academic Press. construction. Language, 75(1):1‚Äì33.\nBazell, C. E. 1952/1966. The correspondence fallacy in Magerman, D. M. 1995. Statistical decision-tree models for\n structural linguistics. In E. P. Hamp, F. W. Householder, parsing. ACL.\n and R. Austerlitz, eds, Studies by Members of the En- Marcus, M. P., G. Kim, M. A. Marcinkiewicz, R. MacInglish Department, Istanbul University (3), reprinted in tyre, A. Bies, M. Ferguson, K. Katz, and B. Schasberger.\n Readings in Linguistics II (1966), 271‚Äì298. University of 1994. The Penn Treebank: Annotating predicate argu-\nChicago Press. ment structure. HLT.\nBiber, D., S. Johansson, G. Leech, S. Conrad, and E. Fine- Marcus, M. P., B. Santorini, and M. A. Marcinkiewicz. 1993.\n gan. 1999. Longman Grammar of Spoken and Written Building a large annotated corpus of English: The Penn\n English. Pearson. treebank. Computational Linguistics, 19(2):313‚Äì330.\nBies, A., M. Ferguson, K. Katz, and R. MacIntyre. 1995. Naur, P., J. W. Backus, F. L. Bauer, J. Green, C. Katz,\n Bracketing guidelines for Treebank II style Penn Tree- J. McCarthy, A. J. Perlis, H. Rutishauser, K. Samelson,\n bank Project. B. Vauquois, J. H. Wegstein, A. van Wijnagaarden, and\nBloomfield, L. 1914. An Introduction to the Study of Lan- M. Woodger. 1960. Report on the algorithmic language\n guage. Henry Holt and Company. ALGOL 60. CACM, 3(5):299‚Äì314. Revised in CACM\nBloomfield, L. 1933. Language. University of Chicago 6:1, 1-17, 1963.\n Press. Nivre, J., M.-C. de Marneffe, F. Ginter, Y. Goldberg, J. HajicÃå,\nBresnan, J., ed. 1982. The Mental Representation of Gram- C. D. Manning, R. McDonald, S. Petrov, S. Pyysalo,\n matical Relations. MIT Press. N. Silveira, R. Tsarfaty, and D. Zeman. 2016. Universal Dependencies v1: A multilingual treebank collection.\nCharniak, E. 1997. Statistical parsing with a context-free LREC.\n grammar and word statistics. AAAI.\n Percival, W. K. 1976. On the historical source of immedi-\nChomsky, N. 1956. Three models for the description of ate constituent analysis. In J. D. McCawley, ed., Syntax\n language. IRE Transactions on Information Theory, and Semantics Volume 7, Notes from the Linguistic Un-\n2(3):113‚Äì124. derground, 229‚Äì242. Academic Press.\nChomsky, N. 1956/1975. The Logical Structure of Linguistic Pollard, C. and I. A. Sag. 1994. Head-Driven Phrase Struc-\nTheory. Plenum. ture Grammar. University of Chicago Press.\nChomsky, N. 1957. Syntactic Structures. Mouton. Quirk, R., S. Greenbaum, G. Leech, and J. Svartvik. 1985.\nChomsky, N. 1963. Formal properties of grammars. In R. D. A Comprehensive Grammar of the English Language.\n Luce, R. Bush, and E. Galanter, eds, Handbook of Math- Longman.\n ematical Psychology, volume 2, 323‚Äì418. Wiley. Radford, A. 1997. Syntactic Theory and the Structure of\nChomsky, N. 1995. The Minimalist Program. MIT Press. English: A Minimalist Approach. Cambridge University\nCollins, M. 1999. Head-Driven Statistical Models for Natu- Press.\n ral Language Parsing. Ph.D. thesis, University of Penn- Sag, I. A., T. Wasow, and E. M. Bender, eds. 2003. Syntacsylvania, Philadelphia. tic Theory: A Formal Introduction. CSLI Publications,\nCulicover, P. W. and R. Jackendoff. 2005. Simpler Syntax. Stanford, CA.\n Oxford University Press. Van Valin, Jr., R. D. and R. La Polla. 1997. Syntax: Structure,\nGazdar, G., E. Klein, G. K. Pullum, and I. A. Sag. 1985. Meaning, and Function. Cambridge University Press.\n Generalized Phrase Structure Grammar. Blackwell. Wundt, W. 1900. VoÃàlkerpsychologie: eine Untersuchung der\nHarris, Z. S. 1946. From morpheme to utterance. Language, Entwicklungsgesetze von Sprache, Mythus, und Sitte. W.\n 22(3):161‚Äì183. Engelmann, Leipzig. Band II: Die Sprache, Zweiter Teil.\nHemphill, C. T., J. Godfrey, and G. Doddington. 1990. The\n ATIS spoken language systems pilot corpus. Speech and\n Natural Language Workshop.\nHopcroft, J. E. and J. D. Ullman. 1979. Introduction to Automata Theory, Languages, and Computation. Addison-\nWesley.\nHuddleston, R. and G. K. Pullum. 2002. The Cambridge\n Grammar of the English Language. Cambridge University Press.\n",
    "file_path": "/Users/colinsidberry/nlp-textbook-rag/data/extracted_text/F.Constituency Grammars.txt",
    "file_size_kb": 60.71
  },
  {
    "id": "ffde53682e9d6640",
    "source": "nlp_textbook",
    "chapter": "Combinatory Categorial G Grammar",
    "filename": "G.Combinatory Categorial Grammar.txt",
    "content": "rights reserved. Draft of August 24, 2025.\n\nCHAPTER\n\n Combinatory Categorial\nG Grammar\n categorial\n grammar In this chapter, we provide an overview of categorial grammar (Ajdukiewicz 1935,\n combinatory\n Bar-Hillel 1953), an early lexicalized grammar model, as well as an important modcategorial ern extension, combinatory categorial grammar, or CCG (Steedman 1996, Steedgrammar\n man 1989, Steedman 2000). CCG is a heavily lexicalized approach motivated by\n both syntactic and semantic considerations. It is an exemplar of a set of computationally relevant approaches to grammar that emphasize putting grammatical information in a rich lexicon, including Lexical-Functional Grammar (LFG) (Bresnan,\n 1982), Head-Driven Phrase Structure Grammar (HPSG) (Pollard and Sag, 1994),\n and Tree-Adjoining Grammar (TAG) (Joshi, 1985).\n The categorial approach consists of three major elements: a set of categories,\n a lexicon that associates words with categories, and a set of rules that govern how\n categories combine in context.\n\nG.1 CCG Categories\n Categories are either atomic elements or single-argument functions that return a category as a value when provided with a desired category as argument. More formally,\n we can define C , a set of categories for a grammar as follows:\n\n ‚Ä¢ A ‚äÜ C , where A is a given set of atomic elements\n ‚Ä¢ (X/Y), (X\\Y) ‚àà C , if X, Y ‚àà C\n\n The slash notation shown here is used to define the functions in the grammar.\n It specifies the type of the expected argument, the direction it is expected be found,\n and the type of the result. Thus, (X/Y) is a function that seeks a constituent of type\n Y to its right and returns a value of X; (X\\Y) is the same except it seeks its argument\n to the left.\n The set of atomic categories is typically very small and includes familiar elements such as sentences and noun phrases. Functional categories include verb\n phrases and complex noun phrases among others.\n\nG.2 The Lexicon\n The lexicon in a categorial approach consists of assignments of categories to words.\n These assignments can either be to atomic or functional categories, and due to lexical\n ambiguity words can be assigned to multiple categories. Consider the following\n2 A PPENDIX G ‚Ä¢ C OMBINATORY C ATEGORIAL G RAMMAR\n\n sample lexical entries.\n\n flight : N\n Miami : NP\n cancel : (S\\NP)/NP\n\n Nouns and proper nouns like flight and Miami are assigned to atomic categories,\n reflecting their typical role as arguments to functions. On the other hand, a transitive\n verb like cancel is assigned the category (S\\NP)/NP: a function that seeks an NP on\n its right and returns as its value a function with the type (S\\NP). This function can,\n in turn, combine with an NP on the left, yielding an S as the result. This captures\n subcategorization information with a computationally useful, internal structure.\n Ditransitive verbs like give, which expect two arguments after the verb, would\n have the category ((S\\NP)/NP)/NP: a function that combines with an NP on its\n right to yield yet another function corresponding to the transitive verb (S\\NP)/NP\n category such as the one given above for cancel.\n\nG.3 Rules\n The rules of a categorial grammar specify how functions and their arguments combine. The following two rule templates constitute the basis for all categorial grammars.\n\n X/Y Y ‚áí X (G.1)\n Y X\\Y ‚áí X (G.2)\n\n The first rule applies a function to its argument on the right, while the second\n looks to the left for its argument. We‚Äôll refer to the first as forward function application, and the second as backward function application. The result of applying\n either of these rules is the category specified as the value of the function being applied.\n Given these rules and a simple lexicon, let‚Äôs consider an analysis of the sentence\n United serves Miami. Assume that serves is a transitive verb with the category\n (S\\NP)/NP and that United and Miami are both simple NPs. Using both forward\n and backward function application, the derivation would proceed as follows:\n United serves Miami\n NP (S\\NP)/NP NP\n >\n S\\NP\n <\n S\n Categorial grammar derivations are illustrated growing down from the words,\n rule applications are illustrated with a horizontal line that spans the elements involved, with the type of the operation indicated at the right end of the line. In this\n example, there are two function applications: one forward function application indicated by the > that applies the verb serves to the NP on its right, and one backward\n function application indicated by the < that applies the result of the first to the NP\n United on its left.\n English permits the coordination of two constituents of the same type, resulting\n in a new constituent of the same type. The following rule provides the mechanism\n G.3 ‚Ä¢ RULES 3\n\n to handle such examples.\n\n X CONJ X ‚áí X (G.3)\n\n This rule states that when two constituents of the same category are separated by a\n constituent of type CONJ they can be combined into a single larger constituent of\n the same type. The following derivation illustrates the use of this rule.\n We flew to Geneva and drove to Chamonix\n NP (S\\NP)/PP PP/NP NP CONJ (S\\NP)/PP PP/NP NP\n > >\n PP PP\n > >\n S\\NP S\\NP\n <Œ¶>\n S\\NP\n <\n S\n\n Here the two S\\NP constituents are combined via the conjunction operator <Œ¶>\n to form a larger constituent of the same type, which can then be combined with the\n subject NP via backward function application.\n These examples illustrate the lexical nature of the categorial grammar approach.\n The grammatical facts about a language are largely encoded in the lexicon, while the\n rules of the grammar are boiled down to a set of three rules. Unfortunately, the basic\n categorial approach does not give us any more expressive power than we had with\n traditional CFG rules; it just moves information from the grammar to the lexicon. To\n move beyond these limitations CCG includes operations that operate over functions.\n The first pair of operators permit us to compose adjacent functions.\n\n X/Y Y /Z ‚áí X/Z (G.4)\n Y \\Z X\\Y ‚áí X\\Z (G.5)\n\n forward\ncomposition The first rule, called forward composition, can be applied to adjacent constituents where the first is a function seeking an argument of type Y to its right, and\n the second is a function that provides Y as a result. This rule allows us to compose\n these two functions into a single one with the type of the first constituent and the\n argument of the second. Although the notation is a little awkward, the second rule,\n backward\ncomposition backward composition is the same, except that we‚Äôre looking to the left instead of\n to the right for the relevant arguments. Both kinds of composition are signalled by a\n B in CCG diagrams, accompanied by a < or > to indicate the direction.\ntype raising The next operator is type raising. Type raising elevates simple categories to the\n status of functions. More specifically, type raising takes a category and converts it\n to a function that seeks as an argument a function that takes the original category\n as its argument. The following schema show two versions of type raising: one for\n arguments to the right, and one for the left.\n\n X ‚áí T /(T \\X) (G.6)\n X ‚áí T \\(T /X) (G.7)\n\n The category T in these rules can correspond to any of the atomic or functional\n categories already present in the grammar.\n A particularly useful example of type raising transforms a simple NP argument\n in subject position to a function that can compose with a following VP. To see how\n4 A PPENDIX G ‚Ä¢ C OMBINATORY C ATEGORIAL G RAMMAR\n\n this works, let‚Äôs revisit our earlier example of United serves Miami. Instead of classifying United as an NP which can serve as an argument to the function attached to\n serve, we can use type raising to reinvent it as a function in its own right as follows.\n\n NP ‚áí S/(S\\NP)\n\n Combining this type-raised constituent with the forward composition rule (G.4) permits the following alternative to our previous derivation.\n United serves Miami\n NP (S\\NP)/NP NP\n >T\n S/(S\\NP)\n >B\n S/NP\n >\n S\n By type raising United to S/(S\\NP), we can compose it with the transitive verb\n serves to yield the (S/NP) function needed to complete the derivation.\n There are several interesting things to note about this derivation. First, it provides a left-to-right, word-by-word derivation that more closely mirrors the way\n humans process language. This makes CCG a particularly apt framework for psycholinguistic studies. Second, this derivation involves the use of an intermediate\n unit of analysis, United serves, that does not correspond to a traditional constituent\n in English. This ability to make use of such non-constituent elements provides CCG\n with the ability to handle the coordination of phrases that are not proper constituents,\n as in the following example.\n (G.8) We flew IcelandAir to Geneva and SwissAir to London.\n Here, the segments that are being coordinated are IcelandAir to Geneva and\n SwissAir to London, phrases that would not normally be considered constituents, as\n can be seen in the following standard derivation for the verb phrase flew IcelandAir\n to Geneva.\n flew IcelandAir to Geneva\n (VP/PP)/NP NP PP/NP NP\n > >\n VP/PP PP\n >\n VP\n In this derivation, there is no single constituent that corresponds to IcelandAir\n to Geneva, and hence no opportunity to make use of the <Œ¶> operator. Note that\n complex CCG categories can get a little cumbersome, so we‚Äôll use VP as a shorthand\n for (S\\NP) in this and the following derivations.\n The following alternative derivation provides the required element through the\n use of both backward type raising (G.7) and backward function composition (G.5).\n flew IcelandAir to Geneva\n (V P/PP)/NP NP PP/NP NP\n <T >\n (V P/PP)\\((V P/PP)/NP) PP\n <T\n V P\\(V P/PP)\n <B\n V P\\((V P/PP)/NP)\n <\n VP\n Applying the same analysis to SwissAir to London satisfies the requirements for\n the <Œ¶> operator, yielding the following derivation for our original example (G.8).\n G.4 ‚Ä¢ CCG BANK 5\n\n flew IcelandAir to Geneva and SwissAir to London\n (V P/PP)/NP NP PP/NP NP CONJ NP PP/NP NP\n <T > <T >\n (V P/PP)\\((V P/PP)/NP) PP (V P/PP)\\((V P/PP)/NP) PP\n <T <T\n V P\\(V P/PP) V P\\(V P/PP)\n < <\n V P\\((V P/PP)/NP) V P\\((V P/PP)/NP)\n <Œ¶>\n V P\\((V P/PP)/NP)\n <\n VP\n\n Finally, let‚Äôs examine how these advanced operators can be used to handle longdistance dependencies (also referred to as syntactic movement or extraction). As\n mentioned in Appendix D, long-distance dependencies arise from many English\n constructions including wh-questions, relative clauses, and topicalization. What\n these constructions have in common is a constituent that appears somewhere distant from its usual, or expected, location. Consider the following relative clause as\n an example.\n the flight that United diverted\n Here, divert is a transitive verb that expects two NP arguments, a subject NP to its\n left and a direct object NP to its right; its category is therefore (S\\NP)/NP. However,\n in this example the direct object the flight has been ‚Äúmoved‚Äù to the beginning of the\n clause, while the subject United remains in its normal position. What is needed is a\n way to incorporate the subject argument, while dealing with the fact that the flight is\n not in its expected location.\n The following derivation accomplishes this, again through the combined use of\n type raising and function composition.\n the flight that United diverted\n NP/N N (NP\\NP)/(S/NP) NP (S\\NP)/NP\n > >T\n NP S/(S\\NP)\n >B\n S/NP\n >\n NP\\NP\n <\n NP\n As we saw with our earlier examples, the first step of this derivation is type raising\n United to the category S/(S\\NP) allowing it to combine with diverted via forward\n composition. The result of this composition is S/NP which preserves the fact that we\n are still looking for an NP to fill the missing direct object. The second critical piece\n is the lexical category assigned to the word that: (NP\\NP)/(S/NP). This function\n seeks a verb phrase missing an argument to its right, and transforms it into an NP\n seeking a missing element to its left, precisely where we find the flight.\n\nG.4 CCGbank\n As with phrase-structure approaches, treebanks play an important role in CCGbased approaches to parsing. CCGbank (Hockenmaier and Steedman, 2007) is the\n largest and most widely used CCG treebank. It was created by automatically translating phrase-structure trees from the Penn Treebank via a rule-based approach. The\n method produced successful translations of over 99% of the trees in the Penn Treebank resulting in 48,934 sentences paired with CCG derivations. It also provides a\n6 A PPENDIX G ‚Ä¢ C OMBINATORY C ATEGORIAL G RAMMAR\n\n lexicon of 44,000 words with over 1200 categories. Appendix C will discuss how\n these resources can be used to train CCG parsers.\n\nG.5 Ambiguity in CCG\n As is always the case in parsing, managing ambiguity is the key to successful CCG\n parsing. The difficulties with CCG parsing arise from the ambiguity caused by the\n large number of complex lexical categories combined with the very general nature of\n the grammatical rules. To see some of the ways that ambiguity arises in a categorial\n framework, consider the following example.\n (G.9) United diverted the flight to Reno.\n Our grasp of the role of the flight in this example depends on whether the prepositional phrase to Reno is taken as a modifier of the flight, as a modifier of the entire\n verb phrase, or as a potential second argument to the verb divert. In a context-free\n grammar approach, this ambiguity would manifest itself as a choice among the following rules in the grammar.\n\n Nominal ‚Üí Nominal PP\n VP ‚Üí VP PP\n VP ‚Üí Verb NP PP\n\n In a phrase-structure approach we would simply assign the word to to the category P allowing it to combine with Reno to form a prepositional phrase. The subsequent choice of grammar rules would then dictate the ultimate derivation. In the\n categorial approach, we can associate to with distinct categories to reflect the ways\n in which it might interact with other elements in a sentence. The fairly abstract\n combinatoric rules would then sort out which derivations are possible. Therefore,\n the source of ambiguity arises not from the grammar but rather from the lexicon.\n Let‚Äôs see how this works by considering several possible derivations for this\n example. To capture the case where the prepositional phrase to Reno modifies the\n flight, we assign the preposition to the category (NP\\NP)/NP, which gives rise to\n the following derivation.\n\n United diverted the flight to Reno\n NP (S\\NP)/NP NP/N N (NP\\NP)/NP NP\n > >\n NP NP\\NP\n <\n NP\n >\n S\\NP\n <\n S\n\n Here, the category assigned to to expects to find two arguments: one to the right as\n with a traditional preposition, and one to the left that corresponds to the NP to be\n modified.\n Alternatively, we could assign to to the category (S\\S)/NP, which permits the\n following derivation where to Reno modifies the preceding verb phrase.\n G.6 ‚Ä¢ CCG PARSING 7\n\n United diverted the flight to Reno\n NP (S\\NP)/NP NP/N N (S\\S)/NP NP\n > >\n NP S\\S\n >\n S\\NP\n <B\n S\\NP\n <\n S\n\n A third possibility is to view divert as a ditransitive verb by assigning it to the\n category ((S\\NP)/PP)/NP, while treating to Reno as a simple prepositional phrase.\n\n United diverted the flight to Reno\n NP ((S\\NP)/PP)/NP NP/N N PP/NP NP\n > >\n NP PP\n >\n (S\\NP)/PP\n >\n S\\NP\n <\n S\n\n While CCG parsers are still subject to ambiguity arising from the choice of grammar rules, including the kind of spurious ambiguity discussed above, it should be\n clear that the choice of lexical categories is the primary problem to be addressed in\n CCG parsing.\n\nG.6 CCG Parsing\n Since the rules in combinatory grammars are either binary or unary, a bottom-up,\n tabular approach based on the CKY algorithm should be directly applicable to CCG\n parsing. Unfortunately, the large number of lexical categories available for each\n word, combined with the promiscuity of CCG‚Äôs combinatoric rules, leads to an explosion in the number of (mostly useless) constituents added to the parsing table.\n The key to managing this explosion of zombie constituents is to accurately assess\n and exploit the most likely lexical categories possible for each word‚Äîa process\n called supertagging.\n These following sections describe an approach to CCG parsing that make use of\n supertags, structuring the parsing process as a heuristic search through the use of the\n A* algorithm.\n\n G.6.1 Supertagging\n Chapter 17 introduced the task of part-of-speech tagging, the process of assigning\n supertagging the correct lexical category to each word in a sentence. Supertagging is the corresponding task for highly lexicalized grammar frameworks, where the assigned tags\n often dictate much of the derivation for a sentence (Bangalore and Joshi, 1999).\n CCG supertaggers rely on treebanks such as CCGbank to provide both the overall set of lexical categories as well as the allowable category assignments for each\n word in the lexicon. CCGbank includes over 1000 lexical categories, however, in\n practice, most supertaggers limit their tagsets to those tags that occur at least 10\n8 A PPENDIX G ‚Ä¢ C OMBINATORY C ATEGORIAL G RAMMAR\n\n times in the training corpus. This results in a total of around 425 lexical categories\n available for use in the lexicon. Note that even this smaller number is large in contrast to the 45 POS types used by the Penn Treebank tagset.\n As with traditional part-of-speech tagging, the standard approach to building a\n CCG supertagger is to use supervised machine learning to build a sequence labeler\n from hand-annotated training data. To find the most likely sequence of tags given a\n sentence, it is most common to use a neural sequence model, either RNN or Transformer.\n It‚Äôs also possible, however, to use the CRF tagging model described in Chapter 17, using similar features; the current word wi , its surrounding words within\n l words, local POS tags and character suffixes, and the supertag from the prior\n timestep, training by maximizing log-likelihood of the training corpus and decoding\n via the Viterbi algorithm as described in Chapter 17.\n Unfortunately the large number of possible supertags combined with high perword ambiguity leads the naive CRF algorithm to error rates that are too high for\n practical use in a parser. The single best tag sequence TÃÇ will typically contain too\n many incorrect tags for effective parsing to take place. To overcome this, we instead\n return a probability distribution over the possible supertags for each word in the\n input. The following table illustrates an example distribution for a simple sentence,\n in which each column represents the probability of each supertag for a given word\n in the context of the input sentence. The ‚Äú...‚Äù represent all the remaining supertags\n possible for each word.\n\n United serves Denver\n N/N: 0.4 (S\\NP)/NP: 0.8 NP: 0.9\n NP: 0.3 N: 0.1 N/N: 0.05\n S/S: 0.1 ... ...\n S\\S: .05\n ...\n\n To get the probability of each possible word/tag pair, we‚Äôll need to sum the\n probabilities of all the supertag sequences that contain that tag at that location. This\n can be done with the forward-backward algorithm that is also used to train the CRF,\n described in Appendix A.\n\n G.6.2 CCG Parsing using the A* Algorithm\n The A* algorithm is a heuristic search method that employs an agenda to find an\n optimal solution. Search states representing partial solutions are added to an agenda\n based on a cost function, with the least-cost option being selected for further exploration at each iteration. When a state representing a complete solution is first\n selected from the agenda, it is guaranteed to be optimal and the search terminates.\n The A* cost function, f (n), is used to efficiently guide the search to a solution.\n The f -cost has two components: g(n), the exact cost of the partial solution represented by the state n, and h(n) a heuristic approximation of the cost of a solution\n that makes use of n. When h(n) satisfies the criteria of not overestimating the actual\n cost, A* will find an optimal solution. Not surprisingly, the closer the heuristic can\n get to the actual cost, the more effective A* is at finding a solution without having\n to explore a significant portion of the solution space.\n When applied to parsing, search states correspond to edges representing completed constituents. Each edge specifies a constituent‚Äôs start and end positions, its\n G.6 ‚Ä¢ CCG PARSING 9\n\ngrammatical category, and its f -cost. Here, the g component represents the current\ncost of an edge and the h component represents an estimate of the cost to complete\na derivation that makes use of that edge. The use of A* for phrase structure parsing\noriginated with Klein and Manning (2003), while the CCG approach presented here\nis based on the work of Lewis and Steedman (2014).\n Using information from a supertagger, an agenda and a parse table are initialized with states representing all the possible lexical categories for each word in the\ninput, along with their f -costs. The main loop removes the lowest cost edge from\nthe agenda and tests to see if it is a complete derivation. If it reflects a complete\nderivation it is selected as the best solution and the loop terminates. Otherwise, new\nstates based on the applicable CCG rules are generated, assigned costs, and entered\ninto the agenda to await further processing. The loop continues until a complete\nderivation is discovered, or the agenda is exhausted, indicating a failed parse. The\nalgorithm is given in Fig. G.1.\n\n function CCG-AS TAR -PARSE(words) returns table or failure\n\n supertags ‚Üê S UPERTAGGER(words)\n for i ‚Üê from 1 to L ENGTH(words) do\n for all {A | (words[i], A, score) ‚àà supertags}\n edge ‚Üê M AKE E DGE(i ‚àí 1, i, A, score)\n table ‚Üê I NSERT E DGE(table, edge)\n agenda ‚Üê I NSERT E DGE(agenda, edge)\n loop do\n if E MPTY ?(agenda) return failure\n current ‚Üê P OP(agenda)\n if C OMPLETED PARSE ?(current) return table\n table ‚Üê I NSERT E DGE(table, current)\n for each rule in A PPLICABLE RULES(current) do\n successor ‚Üê A PPLY(rule, current)\n if successor not ‚àà in agenda or chart\n agenda ‚Üê I NSERT E DGE(agenda, successor)\n else if successor ‚àà agenda with higher cost\n agenda ‚Üê R EPLACE E DGE(agenda, successor)\n\nFigure G.1 A*-based CCG parsing.\n\nG.6.3 Heuristic Functions\nBefore we can define a heuristic function for our A* search, we need to decide how\nto assess the quality of CCG derivations. We‚Äôll make the simplifying assumption\nthat the probability of a CCG derivation is just the product of the probability of\nthe supertags assigned to the words in the derivation, ignoring the rules used in the\nderivation. More formally, given a sentence S and derivation D that contains supertag\nsequence T , we have:\n\n P(D, S) = P(T, S) (G.10)\n Yn\n = P(ti |si ) (G.11)\n i=1\n10 A PPENDIX G ‚Ä¢ C OMBINATORY C ATEGORIAL G RAMMAR\n\n To better fit with the traditional A* approach, we‚Äôd prefer to have states scored by\n a cost function where lower is better (i.e., we‚Äôre trying to minimize the cost of a\n derivation). To achieve this, we‚Äôll use negative log probabilities to score derivations; this results in the following equation, which we‚Äôll use to score completed\n CCG derivations.\n\n P(D, S) = P(T, S) (G.12)\n Xn\n = ‚àí log P(ti |si ) (G.13)\n i=1\n\n Given this model, we can define our f -cost as follows. The f -cost of an edge is\n the sum of two components: g(n), the cost of the span represented by the edge, and\n h(n), the estimate of the cost to complete a derivation containing that edge (these\n are often referred to as the inside and outside costs). We‚Äôll define g(n) for an edge\n using Equation G.13. That is, it is just the sum of the costs of the supertags that\n comprise the span.\n For h(n), we need a score that approximates but never overestimates the actual\n cost of the final derivation. A simple heuristic that meets this requirement assumes\n that each of the words in the outside span will be assigned its most probable supertag. If these are the tags used in the final derivation, then its score will equal\n the heuristic. If any other tags are used in the final derivation the f -cost will be\n higher since the new tags must have higher costs, thus guaranteeing that we will not\n overestimate.\n Putting this all together, we arrive at the following definition of a suitable f -cost\n for an edge.\n\n f (wi, j ,ti, j ) = g(wi, j ) + h(wi, j ) (G.14)\n j\n X\n = ‚àí log P(tk |wk ) +\n k=i\n i‚àí1\n X N\n X\n min (‚àí log P(t|wk )) + min (‚àí log P(t|wk ))\n t‚ààtags t‚ààtags\n k=1 k= j+1\n\n As an example, consider an edge representing the word serves with the supertag N\n in the following example.\n (G.15) United serves Denver.\n The g-cost for this edge is just the negative log probability of this tag, ‚àílog10 (0.1),\n or 1. The outside h-cost consists of the most optimistic supertag assignments for\n United and Denver, which are N/N and NP respectively. The resulting f -cost for\n this edge is therefore 1.443.\n\n G.6.4 An Example\n Fig. G.2 shows the initial agenda and the progress of a complete parse for this example. After initializing the agenda and the parse table with information from the\n supertagger, it selects the best edge from the agenda‚Äîthe entry for United with the\n tag N/N and f -cost 0.591. This edge does not constitute a complete parse and is\n therefore used to generate new states by applying all the relevant grammar rules. In\n this case, applying forward application to United: N/N and serves: N results in the\n creation of the edge United serves: N[0,2], 1.795 to the agenda.\n G.6 ‚Ä¢ CCG PARSING 11\n\n Skipping ahead, at the third iteration an edge representing the complete derivation United serves Denver, S[0,3], .716 is added to the agenda. However, the algorithm does not terminate at this point since the cost of this edge (.716) does not place\n it at the top of the agenda. Instead, the edge representing Denver with the category\n NP is popped. This leads to the addition of another edge to the agenda (type-raising\n Denver). Only after this edge is popped and dealt with does the earlier state representing a complete derivation rise to the top of the agenda where it is popped, goal\n tested, and returned as a solution.\n\n Initial\n Agenda\n United: N/N United serves: N[0,2]\n .591 1.795\n Goal State\n 2 3 6\n serves: (S\\NP)/NP serves Denver: S\\NP[1,3] United serves Denver: S[0,3]\n .591 .591 .716\n\n 4 5\n Denver: NP Denver: S/(S\\NP)[0,1]\n .591 .591\n\n United: NP\n .716\n\n United: S/S\n 1.1938\n\n United: S\\S United serves Denver\n 1.494\n N/N: 0.591 N: 1.795 S: 0.716\n NP: 0.716\n serves: N S/S: 1.1938\n ‚Ä¶\n [0,1] [0,2] [0,3]\n Denver: N\n 1.795 (S\\NP)/NP: 0.591 S/NP: 0.591\n N: 1.494\n ‚Ä¶\n Denver: N/N\n 2.494\n [1,2] [1,3]\n NP: 0.591\n N: 1.795\n N/N: 2.494\n ‚Ä¶\n ‚Ä¶\n [2,3]\n\nFigure G.2 Example of an A* search for the example ‚ÄúUnited serves Denver‚Äù. The circled numbers on the\nblue boxes indicate the order in which the states are popped from the agenda. The costs in each state are based\non f-costs using negative log10 probabilities.\n\n The effectiveness of the A* approach is reflected in the coloring of the states in\n Fig. G.2 as well as the final parsing table. The edges shown in blue (including all the\n12 A PPENDIX G ‚Ä¢ C OMBINATORY C ATEGORIAL G RAMMAR\n\n initial lexical category assignments not explicitly shown) reflect states in the search\n space that never made it to the top of the agenda and, therefore, never contributed any\n edges to the final table. This is in contrast to the PCKY approach where the parser\n systematically fills the parse table with all possible constituents for all possible spans\n in the input, filling the table with myriad constituents that do not contribute to the\n final analysis.\n\nG.7 Summary\n This chapter has introduced combinatory categorial grammar (CCG):\n ‚Ä¢ Combinatorial categorial grammar (CCG) is a computationally relevant lexicalized approach to grammar and parsing.\n ‚Ä¢ Much of the difficulty in CCG parsing is disambiguating the highly rich lexical\n entries, and so CCG parsers are generally based on supertagging.\n ‚Ä¢ Supertagging is the equivalent of part-of-speech tagging in highly lexicalized\n grammar frameworks. The tags are very grammatically rich and dictate much\n of the derivation for a sentence.\n\nHistorical Notes\n Historical Notes 13\n\nAjdukiewicz, K. 1935. Die syntaktische KonnexitaÃàt. Studia Philosophica, 1:1‚Äì27. English translation ‚ÄúSyntactic\n Connexion‚Äù by H. Weber in McCall, S. (Ed.) 1967. Polish\n Logic, pp. 207‚Äì231, Oxford University Press.\nBangalore, S. and A. K. Joshi. 1999. Supertagging: An\n approach to almost parsing. Computational Linguistics,\n 25(2):237‚Äì265.\nBar-Hillel, Y. 1953. A quasi-arithmetical notation for syntactic description. Language, 29:47‚Äì58.\nBresnan, J., ed. 1982. The Mental Representation of Grammatical Relations. MIT Press.\nHockenmaier, J. and M. Steedman. 2007. CCGbank: a corpus of CCG derivations and dependency structures extracted from the penn treebank. Computational Linguistics, 33(3):355‚Äì396.\nJoshi, A. K. 1985. Tree adjoining grammars: How\n much context-sensitivity is required to provide reasonable\n structural descriptions? In D. R. Dowty, L. Karttunen,\n and A. Zwicky, eds, Natural Language Parsing, 206‚Äì250.\n Cambridge University Press.\nKlein, D. and C. D. Manning. 2003. A* parsing: Fast exact\n Viterbi parse selection. HLT-NAACL.\nLewis, M. and M. Steedman. 2014. A* ccg parsing with a\n supertag-factored model. EMNLP.\nPollard, C. and I. A. Sag. 1994. Head-Driven Phrase Structure Grammar. University of Chicago Press.\nSteedman, M. 1989. Constituency and coordination in a\n combinatory grammar. In M. R. Baltin and A. S. Kroch,\n eds, Alternative Conceptions of Phrase Structure, 201‚Äì\n 231. University of Chicago.\nSteedman, M. 1996. Surface Structure and Interpretation.\n MIT Press. Linguistic Inquiry Monograph, 30.\nSteedman, M. 2000. The Syntactic Process. The MIT Press.\n",
    "file_path": "/Users/colinsidberry/nlp-textbook-rag/data/extracted_text/G.Combinatory Categorial Grammar.txt",
    "file_size_kb": 29.16
  },
  {
    "id": "a965de244d1929d9",
    "source": "nlp_textbook",
    "chapter": "Logical Representations of H Sentence Meaning I SHMAEL : Surely all this is not without meaning.",
    "filename": "H.Logical Representations of Sentence Meaning.txt",
    "content": "rights reserved. Draft of August 24, 2025.\n\nCHAPTER\n\n Logical Representations of\nH Sentence Meaning\n I SHMAEL : Surely all this is not without meaning.\n Herman Melville, Moby Dick\n\n In this chapter we introduce the idea that the meaning of linguistic expressions can\n meaning\nrepresentations be captured in formal structures called meaning representations. Consider tasks\n that require some form of semantic processing, like learning to use a new piece of\n software by reading the manual, deciding what to order at a restaurant by reading\n a menu, or following a recipe. Accomplishing these tasks requires representations\n that link the linguistic elements to the necessary non-linguistic knowledge of the\n world. Reading a menu and deciding what to order, giving advice about where to\n go to dinner, following a recipe, and generating new recipes all require knowledge\n about food and its preparation, what people like to eat, and what restaurants are like.\n Learning to use a piece of software by reading a manual, or giving advice on using\n software, requires knowledge about the software and similar apps, computers, and\n users in general.\n In this chapter, we assume that linguistic expressions have meaning representations that are made up of the same kind of stuff that is used to represent this kind of\n everyday common-sense knowledge of the world. The process whereby such represemantic\n parsing sentations are created and assigned to linguistic inputs is called semantic parsing or\n semantic analysis, and the entire enterprise of designing meaning representations\n computational and associated semantic parsers is referred to as computational semantics.\n semantics\n\n ‚àÉe, y Having(e) ‚àß Haver(e, Speaker) ‚àß HadT hing(e, y) ‚àßCar(y)\n h / have-01\n (h / have-01 Having:\n arg0 arg1\n arg0: (i / i) Haver: Speaker\n arg1: (c / car)) HadThing: Car\n i/i c / car\n\n Figure H.1 A list of symbols, two directed graphs, and a record structure: a sampler of\n meaning representations for I have a car.\n\n Consider Fig. H.1, which shows example meaning representations for the sentence I have a car using four commonly used meaning representation languages.\n The top row illustrates a sentence in First-Order Logic, covered in detail in Section H.3; the directed graph and its corresponding textual form is an example of an\n Abstract Meaning Representation (AMR) form (Banarescu et al., 2013), and on\n the right is a frame-based or slot-filler representation, discussed in Section H.5 and\n again in Chapter 20.\n2 A PPENDIX H ‚Ä¢ L OGICAL R EPRESENTATIONS OF S ENTENCE M EANING\n\n While there are non-trivial differences among these approaches, they all share\n the notion that a meaning representation consists of structures composed from a\n set of symbols, or representational vocabulary. When appropriately arranged, these\n symbol structures are taken to correspond to objects, properties of objects, and relations among objects in some state of affairs being represented or reasoned about. In\n this case, all four representations make use of symbols corresponding to the speaker,\n a car, and a relation denoting the possession of one by the other.\n Importantly, these representations can be viewed from at least two distinct perspectives in all of these approaches: as representations of the meaning of the particular linguistic input I have a car, and as representations of the state of affairs in\n some world. It is this dual perspective that allows these representations to be used\n to link linguistic inputs to the world and to our knowledge of it.\n In the next sections we give some background: our desiderata for a meaning\n representation language and some guarantees that these representations will actually\n do what we need them to do‚Äîprovide a correspondence to the state of affairs being\n represented. In Section H.3 we introduce First-Order Logic, historically the primary\n technique for investigating natural language semantics, and see in Section H.4 how\n it can be used to capture the semantics of events and states in English.\n\nH.1 Computational Desiderata for Representations\n Let‚Äôs consider why meaning representations are needed and what they should do for\n us. To focus this discussion, let‚Äôs consider a system that gives restaurant advice to\n tourists based on a knowledge base.\n\n Verifiability\n Consider the following simple question:\n (H.1) Does Maharani serve vegetarian food?\n To answer this question, we have to know what it‚Äôs asking, and know whether what\n verifiability it‚Äôs asking is true of Maharini or not. Verifiability is a system‚Äôs ability to compare\n the state of affairs described by a representation to the state of affairs in some world\n as modeled in a knowledge base. For example, we‚Äôll need some sort of representation like Serves(Maharani,VegetarianFood), which a system can match against its\n knowledge base of facts about particular restaurants, and if it finds a representation\n matching this proposition, it can answer yes. Otherwise, it must either say No if its\n knowledge of local restaurants is complete, or say that it doesn‚Äôt know if it knows\n its knowledge is incomplete.\n\n Unambiguous Representations\n Semantics, like all the other domains we have studied, is subject to ambiguity. Words\n and sentences have different meaning representations in different contexts. Consider\n the following example:\n (H.2) I wanna eat someplace that‚Äôs close to ICSI.\n This sentence can either mean that the speaker wants to eat at some nearby location,\n or under a Godzilla-as-speaker interpretation, the speaker may want to devour some\n nearby location. The sentence is ambiguous; a single linguistic expression can have\n one of two meanings. But our meaning representations itself cannot be ambiguous.\n H.1 ‚Ä¢ C OMPUTATIONAL D ESIDERATA FOR R EPRESENTATIONS 3\n\n The representation of an input‚Äôs meaning should be free from any ambiguity, so that\n the system can reason over a representation that means either one thing or the other\n in order to decide how to answer.\n vagueness A concept closely related to ambiguity is vagueness: in which a meaning representation leaves some parts of the meaning underspecified. Vagueness does not give\n rise to multiple representations. Consider the following request:\n (H.3) I want to eat Italian food.\n While Italian food may provide enough information to provide recommendations, it\n is nevertheless vague as to what the user really wants to eat. A vague representation\n of the meaning of this phrase may be appropriate for some purposes, while a more\n specific representation may be needed for other purposes.\n\n Canonical Form\ncanonical form The doctrine of canonical form says that distinct inputs that mean the same thing\n should have the same meaning representation. This approach greatly simplifies reasoning, since systems need only deal with a single meaning representation for a\n potentially wide range of expressions.\n Consider the following alternative ways of expressing (H.1):\n (H.4) Does Maharani have vegetarian dishes?\n (H.5) Do they have vegetarian food at Maharani?\n (H.6) Are vegetarian dishes served at Maharani?\n (H.7) Does Maharani serve vegetarian fare?\n Despite the fact these alternatives use different words and syntax, we want them\n to map to a single canonical meaning representations. If they were all different,\n assuming the system‚Äôs knowledge base contains only a single representation of this\n fact, most of the representations wouldn‚Äôt match. We could, of course, store all\n possible alternative representations of the same fact in the knowledge base, but doing\n so would lead to enormous difficulty in keeping the knowledge base consistent.\n Canonical form does complicate the task of semantic parsing. Our system must\n conclude that vegetarian fare, vegetarian dishes, and vegetarian food refer to the\n same thing, that having and serving are equivalent here, and that all these parse\n structures still lead to the same meaning representation. Or consider this pair of\n examples:\n (H.8) Maharani serves vegetarian dishes.\n (H.9) Vegetarian dishes are served by Maharani.\n Despite the different placement of the arguments to serve, a system must still assign\n Maharani and vegetarian dishes to the same roles in the two examples by drawing on grammatical knowledge, such as the relationship between active and passive\n sentence constructions.\n\n Inference and Variables\n What about more complex requests such as:\n (H.10) Can vegetarians eat at Maharani?\n This request results in the same answer as the others not because they mean the same\n thing, but because there is a common-sense connection between what vegetarians eat\n and what vegetarian restaurants serve. This is a fact about the world. We‚Äôll need to\n connect the meaning representation of this request with this fact about the world in a\n4 A PPENDIX H ‚Ä¢ L OGICAL R EPRESENTATIONS OF S ENTENCE M EANING\n\n inference knowledge base. A system must be able to use inference‚Äîto draw valid conclusions\n based on the meaning representation of inputs and its background knowledge. It\n must be possible for the system to draw conclusions about the truth of propositions\n that are not explicitly represented in the knowledge base but that are nevertheless\n logically derivable from the propositions that are present.\n Now consider the following somewhat more complex request:\n (H.11) I‚Äôd like to find a restaurant where I can get vegetarian food.\n This request does not make reference to any particular restaurant; the user wants information about an unknown restaurant that serves vegetarian food. Since no restaurants are named, simple matching is not going to work. Answering this request\n variables requires the use of variables, using some representation like the following:\n\n Serves(x,VegetarianFood) (H.12)\n\n Matching succeeds only if the variable x can be replaced by some object in the\n knowledge base in such a way that the entire proposition will then match. The concept that is substituted for the variable can then be used to fulfill the user‚Äôs request.\n It is critical for any meaning representation language to be able to handle these kinds\n of indefinite references.\n\n Expressiveness\n Finally, a meaning representation scheme must be expressive enough to handle a\n wide range of subject matter, ideally any sensible natural language utterance. Although this is probably too much to expect from any single representational system,\n First-Order Logic, as described in Section H.3, is expressive enough to handle quite\n a lot of what needs to be represented.\n\nH.2 Model-Theoretic Semantics\n What is it about meaning representation languages that allows them to fulfill these\n desiderata, bridging the gap from formal representations to representations that tell\n us something about some state of affairs in the world?\n model The answer is a model. A model is a formal construct that stands for the particular state of affairs in the world. Expressions in a meaning representation language\n can be mapped to elements of the model, like objects, properties of objects, and\n relations among objects. If the model accurately captures the facts we‚Äôre interested\n in, then a consistent mapping between the meaning representation and the model\n provides the bridge between meaning representation and world. Models provide a\n surprisingly simple and powerful way to ground the expressions in meaning representation languages.\n First, some terminology. The vocabulary of a meaning representation consists of\n two parts: the non-logical vocabulary and the logical vocabulary. The non-logical\n non-logical\n vocabulary vocabulary consists of the open-ended set of names for the objects, properties, and\n relations that make up the world we‚Äôre trying to represent. These appear in various\n schemes as predicates, nodes, labels on links, or labels in slots in frames. The loglogical\n vocabulary ical vocabulary consists of the closed set of symbols, operators, quantifiers, links,\n etc., that provide the formal means for composing expressions in a given meaning\n representation language.\n H.2 ‚Ä¢ M ODEL -T HEORETIC S EMANTICS 5\n\n denotation Each element of the non-logical vocabulary must have a denotation in the model,\n meaning that every element corresponds to a fixed, well-defined part of the model.\n domain Let‚Äôs start with objects. The domain of a model is the set of objects that are being\n represented. Each distinct concept, category, or individual denotes a unique element\n in the domain.\n We represent properties of objects in a model by denoting the domain elements\n that have the property; that is, properties denote sets. The denotation of the property\n red is the set of things we think are red. Similarly, a relation among object denotes\n a set of ordered lists, or tuples, of domain elements that take part in the relation: the\n denotation of the relation Married is set of pairs of domain objects that are married.\n extensional This approach to properties and relations is called extensional, because we define\n concepts by their extension, their denotations. To summarize:\n ‚Ä¢ Objects denote elements of the domain\n ‚Ä¢ Properties denote sets of elements of the domain\n ‚Ä¢ Relations denote sets of tuples of elements of the domain\n We now need a mapping that gets us from our meaning representation to the\n corresponding denotations: a function that maps from the non-logical vocabulary of\n our meaning representation to the proper denotations in the model. We‚Äôll call such\ninterpretation a mapping an interpretation.\n Let‚Äôs return to our restaurant advice application, and let its domain consist of\n sets of restaurants, patrons, facts about the likes and dislikes of the patrons, and\n facts about the restaurants such as their cuisine, typical cost, and noise level. To\n begin populating our domain, D, let‚Äôs assume that we‚Äôre dealing with four patrons\n designated by the non-logical symbols Matthew, Franco, Katie, and Caroline. denoting four unique domain elements. We‚Äôll use the constants a, b, c and, d to stand\n for these domain elements. We‚Äôre deliberately using meaningless, non-mnemonic\n names for our domain elements to emphasize the fact that whatever it is that we\n know about these entities has to come from the formal properties of the model and\n not from the names of the symbols. Continuing, let‚Äôs assume that our application\n includes three restaurants, designated as Frasca, Med, and Rio in our meaning representation, that denote the domain elements e, f , and g. Finally, let‚Äôs assume that\n we‚Äôre dealing with the three cuisines Italian, Mexican, and Eclectic, denoted by h, i,\n and j in our model.\n Properties like Noisy denote the subset of restaurants from our domain that are\n known to be noisy. Two-place relational notions, such as which restaurants individual patrons Like, denote ordered pairs, or tuples, of the objects from the domain.\n And, since we decided to represent cuisines as objects in our model, we can capture which restaurants Serve which cuisines as a set of tuples. One possible state of\n affairs using this scheme is given in Fig. H.2.\n Given this simple scheme, we can ground our meaning representations by consulting the appropriate denotations in the corresponding model. For example, we can\n evaluate a representation claiming that Matthew likes the Rio, or that The Med serves\n Italian by mapping the objects in the meaning representations to their corresponding\n domain elements and mapping any links, predicates, or slots in the meaning representation to the appropriate relations in the model. More concretely, we can verify\n a representation asserting that Matthew likes Frasca by first using our interpretation\n function to map the symbol Matthew to its denotation a, Frasca to e, and the Likes\n relation to the appropriate set of tuples. We then check that set of tuples for the\n presence of the tuple ha, ei. If, as it is in this case, the tuple is present in the model,\n then we can conclude that Matthew likes Frasca is true; if it isn‚Äôt then we can‚Äôt.\n6 A PPENDIX H ‚Ä¢ L OGICAL R EPRESENTATIONS OF S ENTENCE M EANING\n\n Domain D = {a, b, c, d, e, f , g, h, i, j}\n Matthew, Franco, Katie and Caroline a, b, c, d\n Frasca, Med, Rio e, f , g\n Italian, Mexican, Eclectic h, i, j\n Properties\n Noisy Noisy = {e, f , g}\n Frasca, Med, and Rio are noisy\n Relations\n Likes Likes = {ha, f i, hc, f i, hc, gi, hb, ei, hd, f i, hd, gi}\n Matthew likes the Med\n Katie likes the Med and Rio\n Franco likes Frasca\n Caroline likes the Med and Rio\n Serves Serves = {h f , ji, hg, ii, he, hi}\n Med serves eclectic\n Rio serves Mexican\n Frasca serves Italian\nFigure H.2 A model of the restaurant world.\n\n This is all pretty straightforward‚Äîwe‚Äôre using sets and operations on sets to\n ground the expressions in our meaning representations. Of course, the more interesting part comes when we consider more complex examples such as the following:\n (H.13) Katie likes the Rio and Matthew likes the Med.\n (H.14) Katie and Caroline like the same restaurants.\n (H.15) Franco likes noisy, expensive restaurants.\n (H.16) Not everybody likes Frasca.\n Our simple scheme for grounding the meaning of representations is not adequate\n for examples such as these. Plausible meaning representations for these examples\n will not map directly to individual entities, properties, or relations. Instead, they\n involve complications such as conjunctions, equality, quantified variables, and negations. To assess whether these statements are consistent with our model, we‚Äôll have\n to tear them apart, assess the parts, and then determine the meaning of the whole\n from the meaning of the parts.\n Consider the first example above. A meaning representation for this example\n will include two distinct propositions expressing the individual patron‚Äôs preferences,\n conjoined with some kind of implicit or explicit conjunction operator. Our model\n doesn‚Äôt have a relation that encodes pairwise preferences for all of the patrons and\n restaurants in our model, nor does it need to. We know from our model that Matthew\n likes the Med and separately that Katie likes the Rio (that is, the tuples ha, f i and\n hc, gi are members of the set denoted by the Likes relation). All we really need to\n know is how to deal with the semantics of the conjunction operator. If we assume\n the simplest possible semantics for the English word and, the whole statement is\n true if it is the case that each of the components is true in our model. In this case,\n both components are true since the appropriate tuples are present and therefore the\n sentence as a whole is true.\n truthconditional What we‚Äôve done with this example is provide a truth-conditional semantics\n semantics\n for the assumed conjunction operator in some meaning representation. That is,\n we‚Äôve provided a method for determining the truth of a complex expression from\n H.3 ‚Ä¢ F IRST-O RDER L OGIC 7\n\n Formula ‚Üí AtomicFormula\n | Formula Connective Formula\n | Quantifier Variable, . . . Formula\n | ¬¨ Formula\n | (Formula)\n AtomicFormula ‚Üí Predicate(Term, . . .)\n Term ‚Üí Function(Term, . . .)\n | Constant\n | Variable\n Connective ‚Üí ‚àß | ‚à® | =‚áí\n Quantifier ‚Üí ‚àÄ | ‚àÉ\n Constant ‚Üí A | VegetarianFood | Maharani ¬∑ ¬∑ ¬∑\n Variable ‚Üí x | y | ¬∑ ¬∑ ¬∑\n Predicate ‚Üí Serves | Near | ¬∑ ¬∑ ¬∑\n Function ‚Üí LocationOf | CuisineOf | ¬∑ ¬∑ ¬∑\n Figure H.3 A context-free grammar specification of the syntax of First-Order Logic representations. Adapted from Russell and Norvig 2002.\n\n the meanings of the parts (by consulting a model) and the meaning of an operator by\n consulting a truth table. Meaning representation languages are truth-conditional to\n the extent that they give a formal specification as to how we can determine the meaning of complex sentences from the meaning of their parts. In particular, we need to\n know the semantics of the entire logical vocabulary of the meaning representation\n scheme being used.\n Note that although the details of how this happens depend on details of the particular meaning representation being used, it should be clear that assessing the truth\n conditions of examples like these involves nothing beyond the simple set operations\n we‚Äôve been discussing. We return to these issues in the next section in the context of\n the semantics of First-Order Logic.\n\nH.3 First-Order Logic\n First-Order Logic (FOL) is a flexible, well-understood, and computationally tractable\n meaning representation language that satisfies many of the desiderata given in Section H.1. It provides a sound computational basis for the verifiability, inference, and\n expressiveness requirements, as well as a sound model-theoretic semantics.\n An additional attractive feature of FOL is that it makes few specific commitments\n as to how things ought to be represented, and those it does are shared by many of\n the schemes mentioned earlier: the represented world consists of objects, properties\n of objects, and relations among objects.\n The remainder of this section introduces the basic syntax and semantics of FOL\n and then describes the application of FOL to the representation of events.\n\n H.3.1 Basic Elements of First-Order Logic\n Let‚Äôs explore FOL by first examining its various atomic elements and then showing\n how they can be composed to create larger meaning representations. Figure H.3,\n which provides a complete context-free grammar for the particular syntax of FOL\n that we will use, is our roadmap for this section.\n8 A PPENDIX H ‚Ä¢ L OGICAL R EPRESENTATIONS OF S ENTENCE M EANING\n\n term Let‚Äôs begin by examining the notion of a term, the FOL device for representing\n objects. As can be seen from Fig. H.3, FOL provides three ways to represent these\n basic building blocks: constants, functions, and variables. Each of these devices can\n be thought of as designating an object in the world under consideration.\n constant Constants in FOL refer to specific objects in the world being described. Such\n constants are conventionally depicted as either single capitalized letters such as A\n and B or single capitalized words that are often reminiscent of proper nouns such as\n Maharani and Harry. Like programming language constants, FOL constants refer\n to exactly one object. Objects can, however, have multiple constants that refer to\n them.\n function Functions in FOL correspond to concepts that are often expressed in English as\n genitives such as Frasca‚Äôs location. A FOL translation of such an expression might\n look like the following.\n LocationOf (Frasca) (H.17)\n FOL functions are syntactically the same as single argument predicates. It is important to remember, however, that while they have the appearance of predicates,\n they are in fact terms in that they refer to unique objects. Functions provide a convenient way to refer to specific objects without having to associate a named constant\n with them. This is particularly convenient in cases in which many named objects,\n like restaurants, have a unique concept such as a location associated with them.\n variable Variables are our final FOL mechanism for referring to objects. Variables, depicted as single lower-case letters, let us make assertions and draw inferences about\n objects without having to make reference to any particular named object. This ability\n to make statements about anonymous objects comes in two flavors: making statements about a particular unknown object and making statements about all the objects\n in some arbitrary world of objects. We return to the topic of variables after we have\n presented quantifiers, the elements of FOL that make variables useful.\n Now that we have the means to refer to objects, we can move on to the FOL\n mechanisms that are used to state relations that hold among objects. Predicates are\n symbols that refer to, or name, the relations that hold among some fixed number\n of objects in a given domain. Returning to the example introduced informally in\n Section H.1, a reasonable FOL representation for Maharani serves vegetarian food\n might look like the following formula:\n\n Serves(Maharani,VegetarianFood) (H.18)\n\n This FOL sentence asserts that Serves, a two-place predicate, holds between the\n objects denoted by the constants Maharani and VegetarianFood.\n A somewhat different use of predicates is illustrated by the following fairly typical representation for a sentence like Maharani is a restaurant:\n\n Restaurant(Maharani) (H.19)\n\n This is an example of a one-place predicate that is used, not to relate multiple objects,\n but rather to assert a property of a single object. In this case, it encodes the category\n membership of Maharani.\n With the ability to refer to objects, to assert facts about objects, and to relate\n objects to one another, we can create rudimentary composite representations. These\n representations correspond to the atomic formula level in Fig. H.3. This ability\n to compose complex representations is, however, not limited to the use of single\n predicates. Larger composite representations can also be put together through the\n H.3 ‚Ä¢ F IRST-O RDER L OGIC 9\n\n logical use of logical connectives. As can be seen from Fig. H.3, logical connectives let\nconnectives\n us create larger representations by conjoining logical formulas using one of three\n operators. Consider, for example, the following BERP sentence and one possible\n representation for it:\n (H.20) I only have five dollars and I don‚Äôt have a lot of time.\n Have(Speaker, FiveDollars) ‚àß ¬¨Have(Speaker, LotOfTime) (H.21)\n The semantic representation for this example is built up in a straightforward way\n from the semantics of the individual clauses through the use of the ‚àß and ¬¨ operators.\n Note that the recursive nature of the grammar in Fig. H.3 allows an infinite number\n of logical formulas to be created through the use of these connectives. Thus, as with\n syntax, we can use a finite device to create an infinite number of representations.\n\n H.3.2 Variables and Quantifiers\n We now have all the machinery necessary to return to our earlier discussion of variables. As noted above, variables are used in two ways in FOL: to refer to particular\n anonymous objects and to refer generically to all objects in a collection. These two\nquantifiers uses are made possible through the use of operators known as quantifiers. The two\n operators that are basic to FOL are the existential quantifier, which is denoted ‚àÉ and\n is pronounced as ‚Äúthere exists‚Äù, and the universal quantifier, which is denoted ‚àÄ and\n is pronounced as ‚Äúfor all‚Äù.\n The need for an existentially quantified variable is often signaled by the presence\n of an indefinite noun phrase in English. Consider the following example:\n (H.22) a restaurant that serves Mexican food near ICSI.\n Here, reference is being made to an anonymous object of a specified category with\n particular properties. The following would be a reasonable representation of the\n meaning of such a phrase:\n\n ‚àÉxRestaurant(x) ‚àß Serves(x, MexicanFood) (H.23)\n ‚àß Near(LocationOf (x), LocationOf (ICSI))\n\n The existential quantifier at the head of this sentence instructs us on how to\n interpret the variable x in the context of this sentence. Informally, it says that for\n this sentence to be true there must be at least one object such that if we were to\n substitute it for the variable x, the resulting sentence would be true. For example,\n if AyCaramba is a Mexican restaurant near ICSI, then substituting AyCaramba for x\n results in the following logical formula:\n\n Restaurant(AyCaramba) ‚àß Serves(AyCaramba, MexicanFood) (H.24)\n ‚àßNear((LocationOf (AyCaramba), LocationOf (ICSI))\n\n Based on the semantics of the ‚àß operator, this sentence will be true if all of its\n three component atomic formulas are true. These in turn will be true if they are\n either present in the system‚Äôs knowledge base or can be inferred from other facts in\n the knowledge base.\n The use of the universal quantifier also has an interpretation based on substitution of known objects for variables. The substitution semantics for the universal\n quantifier takes the expression for all quite literally; the ‚àÄ operator states that for the\n logical formula in question to be true, the substitution of any object in the knowledge\n10 A PPENDIX H ‚Ä¢ L OGICAL R EPRESENTATIONS OF S ENTENCE M EANING\n\n base for the universally quantified variable should result in a true formula. This is in\n marked contrast to the ‚àÉ operator, which only insists on a single valid substitution\n for the sentence to be true.\n Consider the following example:\n (H.25) All vegetarian restaurants serve vegetarian food.\n A reasonable representation for this sentence would be something like the following:\n\n ‚àÄxVegetarianRestaurant(x) =‚áí Serves(x,VegetarianFood) (H.26)\n\n For this sentence to be true, every substitution of a known object for x must result in a\n sentence that is true. We can divide the set of all possible substitutions into the set of\n objects consisting of vegetarian restaurants and the set consisting of everything else.\n Let us first consider the case in which the substituted object actually is a vegetarian\n restaurant; one such substitution would result in the following sentence:\n\n VegetarianRestaurant(Maharani) =‚áí Serves(Maharani,VegetarianFood)\n (H.27)\n If we assume that we know that the consequent clause\n\n Serves(Maharani,VegetarianFood) (H.28)\n\n is true, then this sentence as a whole must be true. Both the antecedent and the\n consequent have the value True and, therefore, according to the first two rows of\n Fig. H.4 on page 12 the sentence itself can have the value True. This result will be\n the same for all possible substitutions of Terms representing vegetarian restaurants\n for x.\n Remember, however, that for this sentence to be true, it must be true for all\n possible substitutions. What happens when we consider a substitution from the set\n of objects that are not vegetarian restaurants? Consider the substitution of a nonvegetarian restaurant such as AyCaramba for the variable x:\n\n VegetarianRestaurant(AyCaramba) =‚áí Serves(AyCaramba,VegetarianFood)\n\n Since the antecedent of the implication is False, we can determine from Fig. H.4\n that the sentence is always True, again satisfying the ‚àÄ constraint.\n Note that it may still be the case that AyCaramba serves vegetarian food without actually being a vegetarian restaurant. Note also that, despite our choice of\n examples, there are no implied categorical restrictions on the objects that can be\n substituted for x by this kind of reasoning. In other words, there is no restriction of\n x to restaurants or concepts related to them. Consider the following substitution:\n\n VegetarianRestaurant(Carburetor) =‚áí Serves(Carburetor,VegetarianFood)\n\n Here the antecedent is still false so the rule remains true under this kind of irrelevant\n substitution.\n To review, variables in logical formulas must be either existentially (‚àÉ) or universally (‚àÄ) quantified. To satisfy an existentially quantified variable, at least one\n substitution must result in a true sentence. To satisfy a universally quantified variable, all substitutions must result in true sentences.\n H.3 ‚Ä¢ F IRST-O RDER L OGIC 11\n\n H.3.3 Lambda Notation\n The final element we need to complete our discussion of FOL is called the lambda\n lambda notation (Church, 1940). This notation provides a way to abstract from fully specinotation\n fied FOL formulas in a way that will be particularly useful for semantic analysis. The\n lambda notation extends the syntax of FOL to include expressions of the following\n form:\n Œª x.P(x) (H.29)\n Such expressions consist of the Greek symbol Œª , followed by one or more variables,\n followed by a FOL formula that makes use of those variables.\n The usefulness of these Œª -expressions is based on the ability to apply them to\n logical terms to yield new FOL expressions where the formal parameter variables are\nŒª -reduction bound to the specified terms. This process is known as Œª -reduction, and consists\n of a simple textual replacement of the Œª variables and the removal of the Œª . The\n following expressions illustrate the application of a Œª -expression to the constant A,\n followed by the result of performing a Œª -reduction on this expression:\n Œª x.P(x)(A) (H.30)\n P(A)\n An important and useful variation of this technique is the use of one Œª -expression\n as the body of another as in the following expression:\n Œª x.Œª y.Near(x, y) (H.31)\n This fairly abstract expression can be glossed as the state of something being near\n something else. The following expressions illustrate a single Œª -application and subsequent reduction with this kind of embedded Œª -expression:\n Œª x.Œª y.Near(x, y)(Bacaro) (H.32)\n Œª y.Near(Bacaro, y)\n The important point here is that the resulting expression is still a Œª -expression; the\n first reduction bound the variable x and removed the outer Œª , thus revealing the\n inner expression. As might be expected, this resulting Œª -expression can, in turn,\n be applied to another term to arrive at a fully specified logical formula, as in the\n following:\n Œª y.Near(Bacaro, y)(Centro) (H.33)\n Near(Bacaro,Centro)\n currying This general technique, called currying1 (SchoÃànfinkel, 1924) is a way of converting\n a predicate with multiple arguments into a sequence of single-argument predicates.\n The Œª -notation also provides a way to incrementally gather arguments to a predicate when they do not all appear together as daughters of the predicate in a parse\n tree.\n\n H.3.4 The Semantics of First-Order Logic\n The various objects, properties, and relations represented in a FOL knowledge base\n acquire their meanings by virtue of their correspondence to objects, properties, and\n 1 Currying is the standard term, although Heim and Kratzer (1998) present an interesting argument for\n the term SchoÃànfinkelization over currying, since Curry later built on SchoÃànfinkel‚Äôs work.\n12 A PPENDIX H ‚Ä¢ L OGICAL R EPRESENTATIONS OF S ENTENCE M EANING\n\n relations out in the external world being modeled. We can accomplish this by employing the model-theoretic approach introduced in Section H.2. Recall that this\n approach employs simple set-theoretic notions to provide a truth-conditional mapping from the expressions in a meaning representation to the state of affairs being\n modeled. We can apply this approach to FOL by going through all the elements in\n Fig. H.3 on page 7 and specifying how each should be accounted for.\n We can start by asserting that the objects in our world, FOL terms, denote elements in a domain, and asserting that atomic formulas are captured either as sets of\n domain elements for properties, or as sets of tuples of elements for relations. As an\n example, consider the following:\n (H.34) Centro is near Bacaro.\n Capturing the meaning of this example in FOL involves identifying the Terms\n and Predicates that correspond to the various grammatical elements in the sentence\n and creating logical formulas that capture the relations implied by the words and\n syntax of the sentence. For this example, such an effort might yield something like\n the following:\n Near(Centro, Bacaro) (H.35)\n The meaning of this logical formula is based on whether the domain elements denoted by the terms Centro and Bacaro are contained among the tuples denoted by\n the relation denoted by the predicate Near in the current model.\n The interpretation of formulas involving logical connectives is based on the\n meanings of the components in the formulas combined with the meanings of the\n connectives they contain. Figure H.4 gives interpretations for each of the logical\n operators shown in Fig. H.3.\n\n P Q ¬¨P P‚àßQ P‚à®Q P =‚áí Q\n False False True False False True\n False True True False True True\n True False False False True False\n True True False True True True\n Figure H.4 Truth table giving the semantics of the various logical connectives.\n\n The semantics of the ‚àß (and) and ¬¨ (not) operators are fairly straightforward,\n and are correlated with at least some of the senses of the corresponding English\n terms. However, it is worth pointing out that the ‚à® (or) operator is not disjunctive\n in the same way that the corresponding English word is, and that the =‚áí (implies) operator is only loosely based on any common-sense notions of implication\n or causation.\n The final bit we need to address involves variables and quantifiers. Recall that\n there are no variables in our set-based models, only elements of the domain and\n relations that hold among them. We can provide a model-based account for formulas\n with variables by employing the notion of a substitution introduced earlier on page\n 9. Formulas involving ‚àÉ are true if a substitution of terms for variables results in\n a formula that is true in the model. Formulas involving ‚àÄ must be true under all\n possible substitutions.\n\n H.3.5 Inference\n A meaning representation language must support inference to add valid new propositions to a knowledge base or to determine the truth of propositions not explicitly\n H.3 ‚Ä¢ F IRST-O RDER L OGIC 13\n\n contained within a knowledge base (Section H.1). This section briefly discusses\n modus ponens, the most widely implemented inference method provided by FOL.\nModus ponens Modus ponens is a form of inference that corresponds to what is informally\n known as if-then reasoning. We can abstractly define modus ponens as follows,\n where Œ± and Œ≤ should be taken as FOL formulas:\n\n Œ±\n Œ± =‚áí Œ≤\n (H.36)\n Œ≤\n A schema like this indicates that the formula below the line can be inferred from the\n formulas above the line by some form of inference. Modus ponens states that if the\n left-hand side of an implication rule is true, then the right-hand side of the rule can\n be inferred. In the following discussions, we will refer to the left-hand side of an\n implication as the antecedent and the right-hand side as the consequent.\n For a typical use of modus ponens, consider the following example, which uses\n a rule from the last section:\n VegetarianRestaurant(Leaf )\n ‚àÄxVegetarianRestaurant(x) =‚áí Serves(x,VegetarianFood)\n (H.37)\n Serves(Leaf ,VegetarianFood)\n\n Here, the formula VegetarianRestaurant(Leaf ) matches the antecedent of the rule,\n thus allowing us to use modus ponens to conclude Serves(Leaf ,VegetarianFood).\n Modus ponens can be put to practical use in one of two ways: forward chaining\n forward\n chaining and backward chaining. In forward chaining systems, modus ponens is used in\n precisely the manner just described. As individual facts are added to the knowledge\n base, modus ponens is used to fire all applicable implication rules. In this kind of\n arrangement, as soon as a new fact is added to the knowledge base, all applicable\n implication rules are found and applied, each resulting in the addition of new facts to\n the knowledge base. These new propositions in turn can be used to fire implication\n rules applicable to them. The process continues until no further facts can be deduced.\n The forward chaining approach has the advantage that facts will be present in\n the knowledge base when needed, because, in a sense all inference is performed in\n advance. This can substantially reduce the time needed to answer subsequent queries\n since they should all amount to simple lookups. The disadvantage of this approach\n is that facts that will never be needed may be inferred and stored.\n backward\n chaining In backward chaining, modus ponens is run in reverse to prove specific propositions called queries. The first step is to see if the query formula is true by determining if it is present in the knowledge base. If it is not, then the next step is to search\n for applicable implication rules present in the knowledge base. An applicable rule\n is one whereby the consequent of the rule matches the query formula. If there are\n any such rules, then the query can be proved if the antecedent of any one them can\n be shown to be true. This can be performed recursively by backward chaining on\n the antecedent as a new query. The Prolog programming language is a backward\n chaining system that implements this strategy.\n To see how this works, let‚Äôs assume that we have been asked to verify the truth of\n the proposition Serves(Leaf ,VegetarianFood), assuming the facts given above the\n line in (H.37). Since this proposition is not present in the knowledge base, a search\n for an applicable rule is initiated resulting in the rule given above. After substituting\n the constant Leaf for the variable x, our next task is to prove the antecedent of the\n rule, VegetarianRestaurant(Leaf ), which, of course, is one of the facts we are given.\n14 A PPENDIX H ‚Ä¢ L OGICAL R EPRESENTATIONS OF S ENTENCE M EANING\n\n Note that it is critical to distinguish between reasoning by backward chaining\n from queries to known facts and reasoning backwards from known consequents to\n unknown antecedents. To be specific, by reasoning backwards we mean that if the\n consequent of a rule is known to be true, we assume that the antecedent will be as\n well. For example, let‚Äôs assume that we know that Serves(Leaf ,VegetarianFood) is\n true. Since this fact matches the consequent of our rule, we might reason backwards\n to the conclusion that VegetarianRestaurant(Leaf ).\n While backward chaining is a sound method of reasoning, reasoning backwards\n is an invalid, though frequently useful, form of plausible reasoning. Plausible reaabduction soning from consequents to antecedents is known as abduction, and as we show in\n Chapter 24, is often useful in accounting for many of the inferences people make\n while analyzing extended discourses.\n complete While forward and backward reasoning are sound, neither is complete. This\n means that there are valid inferences that cannot be found by systems using these\n methods alone. Fortunately, there is an alternative inference technique called resoresolution lution that is sound and complete. Unfortunately, inference systems based on resolution are far more computationally expensive than forward or backward chaining\n systems. In practice, therefore, most systems use some form of chaining and place\n a burden on knowledge base developers to encode the knowledge in a fashion that\n permits the necessary inferences to be drawn.\n\nH.4 Event and State Representations\n Much of the semantics that we wish to capture consists of representations of states\n and events. States are conditions, or properties, that remain unchanged over an\n extended period of time, and events denote changes in some state of affairs. The\n representation of both states and events may involve a host of participants, props,\n times and locations.\n The representations for events and states that we have used thus far have consisted of single predicates with as many arguments as are needed to incorporate all\n the roles associated with a given example. For example, the representation for Leaf\n serves vegetarian fare consists of a single predicate with arguments for the entity\n doing the serving and the thing served.\n\n Serves(Leaf ,VegetarianFare) (H.38)\n\n This approach assumes that the predicate used to represent an event verb has the\n same number of arguments as are present in the verb‚Äôs syntactic subcategorization\n frame. Unfortunately, this is clearly not always the case. Consider the following\n examples of the verb eat:\n (H.39) I ate.\n (H.40) I ate a turkey sandwich.\n (H.41) I ate a turkey sandwich at my desk.\n (H.42) I ate at my desk.\n (H.43) I ate lunch.\n (H.44) I ate a turkey sandwich for lunch.\n (H.45) I ate a turkey sandwich for lunch at my desk.\n H.5 ‚Ä¢ D ESCRIPTION L OGICS 15\n\n Clearly, choosing the correct number of arguments for the predicate representing the meaning of eat is a tricky problem. These examples introduce five distinct\n arguments, or roles, in an array of different syntactic forms, locations, and combinaarity tions. Unfortunately, predicates in FOL have fixed arity ‚Äì they take a fixed number\n of arguments.\nevent variable To address this problem, we introduce the notion of an event variable to allow\n us to make assertions about particular events. To do this, we can refactor our event\n predicates to have an existentially quantified variable as their first, and only, argument. Using this event variable, we can introduce additional predicates to represent\n the other information we have about the event. These predicates take an event variable as their first argument and related FOL terms as their second argument. The\n following formula illustrates this scheme with the meaning representation of H.40\n from our earlier discussion.\n\n ‚àÉe Eating(e) ‚àß Eater(e, Speaker) ‚àß Eaten(e, TurkeySandwich)\n\n Here, the quantified variable e stands for the eating event and is used to bind the\n event predicate with the core information provided via the named roles Eater and\n Eaten. To handle the more complex examples, we simply add additional relations\n to capture the provided information, as in the following for H.45.\n\n ‚àÉe Eating(e) ‚àß Eater(e, Speaker) ‚àß Eaten(e, TurkeySandwich) (H.46)\n ‚àß Meal(e, Lunch) ‚àß Location(e, Desk)\n neo-\nDavidsonian Event representations of this sort are referred to as neo-Davidsonian event representations (Davidson 1967, Parsons 1990) after the philosopher Donald Davidson\n who introduced the notion of an event variable (Davidson, 1967). To summarize, in\n the neo-Davidsonian approach to event representations:\n ‚Ä¢ Events are captured with predicates that take a single event variable as an\n argument.\n ‚Ä¢ There is no need to specify a fixed number of arguments for a given FOL\n predicate; rather, as many roles and fillers can be glued on as are provided in\n the input.\n ‚Ä¢ No more roles are postulated than are mentioned in the input.\n ‚Ä¢ The logical connections among closely related inputs that share the same predicate are satisfied without the need for additional inference.\n This approach still leaves us with the problem of determining the set of predicates needed to represent roles associated with specific events like Eater and Eaten,\n as well as more general concepts like Location and Time. We‚Äôll return to this problem in more detail in Chapter 20 and Chapter 21.\n\nH.5 Description Logics\n As noted at the beginning of this chapter, a fair number of representational schemes\n have been invented to capture the meaning of linguistic utterances. It is now widely\n accepted that meanings represented in these various approaches can, in principle, be\n translated into equivalent statements in FOL with relative ease. The difficulty is that\n in many of these approaches the semantics of a statement are defined procedurally.\n That is, the meaning arises from whatever the system that interprets it does with it.\n16 A PPENDIX H ‚Ä¢ L OGICAL R EPRESENTATIONS OF S ENTENCE M EANING\n\n Description logics are an effort to better specify the semantics of these earlier\n structured network representations and to provide a conceptual framework that is\n especially well suited to certain kinds of domain modeling. Formally, the term Description Logics refers to a family of logical approaches that correspond to varying\n subsets of FOL. The restrictions placed on the expressiveness of Description Logics\n serve to guarantee the tractability of various critical kinds of inference. Our focus\n here, however, will be on the modeling aspects of DLs rather than on computational\n complexity issues.\n When using Description Logics to model an application domain, the emphasis\n is on the representation of knowledge about categories, individuals that belong to\n those categories, and the relationships that can hold among these individuals. The\n set of categories, or concepts, that make up a particular application domain is called\n terminology its terminology. The portion of a knowledge base that contains the terminology is\n TBox traditionally called the TBox; this is in contrast to the ABox that contains facts about\n ABox individuals. The terminology is typically arranged into a hierarchical organization\n ontology called an ontology that captures the subset/superset relations among the categories.\n Returning to our earlier culinary domain, we represented domain concepts using unary predicates such as Restaurant(x); the DL equivalent omits the variable,\n so the restaurant category is simply written as Restaurant.2 To capture the fact\n that a particular domain element, such as Frasca, is a restaurant, we assert Restaurant(Frasca) in much the same way we would in FOL. The semantics of these\n categories are specified in precisely the same way that was introduced earlier in\n Section H.2: a category like Restaurant simply denotes the set of domain elements\n that are restaurants.\n Once we‚Äôve specified the categories of interest in a particular domain, the next\n step is to arrange them into a hierarchical structure. There are two ways to capture the hierarchical relationships present in a terminology: we can directly assert\n relations between categories that are related hierarchically, or we can provide complete definitions for our concepts and then rely on inference to provide hierarchical\n relationships. The choice between these methods hinges on the use to which the resulting categories will be put and the feasibility of formulating precise definitions for\n many naturally occurring categories. We‚Äôll discuss the first option here and return to\n the notion of definitions later in this section.\n subsumption To directly specify a hierarchical structure, we can assert subsumption relations\n between the appropriate concepts in a terminology. The subsumption relation is\n conventionally written as C v D and is read as C is subsumed by D; that is, all\n members of the category C are also members of the category D. Not surprisingly, the\n formal semantics of this relation are provided by a simple set relation; any domain\n element that is in the set denoted by C is also in the set denoted by D.\n Adding the following statements to the TBox asserts that all restaurants are commercial establishments and, moreover, that there are various subtypes of restaurants.\n\n Restaurant v CommercialEstablishment (H.47)\n ItalianRestaurant v Restaurant (H.48)\n ChineseRestaurant v Restaurant (H.49)\n MexicanRestaurant v Restaurant (H.50)\n\n Ontologies such as this are conventionally illustrated with diagrams such as the one\n 2 DL statements are conventionally typeset with a sans serif font. We‚Äôll follow that convention here,\n reverting to our standard mathematical notation when giving FOL equivalents of DL statements.\n H.5 ‚Ä¢ D ESCRIPTION L OGICS 17\n\nshown in Fig. H.5, where subsumption relations are denoted by links between the\nnodes representing the categories.\n\n Commercial\n Establishment\n\n Restaurant\n\n Italian Chinese Mexican\n Restaurant Restaurant Restaurant\n\nFigure H.5 A graphical network representation of a set of subsumption relations in the\nrestaurant domain.\n\n Note, that it was precisely the vague nature of semantic network diagrams like\nthis that motivated the development of Description Logics. For example, from this\ndiagram we can‚Äôt tell whether the given set of categories is exhaustive or disjoint.\nThat is, we can‚Äôt tell if these are all the kinds of restaurants that we‚Äôll be dealing with\nin our domain or whether there might be others. We also can‚Äôt tell if an individual\nrestaurant must fall into only one of these categories, or if it is possible, for example,\nfor a restaurant to be both Italian and Chinese. The DL statements given above are\nmore transparent in their meaning; they simply assert a set of subsumption relations\nbetween categories and make no claims about coverage or mutual exclusion.\n If an application requires coverage and disjointness information, then such information must be made explicitly. The simplest ways to capture this kind of information is through the use of negation and disjunction operators. For example,\nthe following assertion would tell us that Chinese restaurants can‚Äôt also be Italian\nrestaurants.\n ChineseRestaurant v not ItalianRestaurant (H.51)\nSpecifying that a set of subconcepts covers a category can be achieved with disjunction, as in the following:\n\n Restaurant v (H.52)\n (or ItalianRestaurant ChineseRestaurant MexicanRestaurant)\n\n Having a hierarchy such as the one given in Fig. H.5 tells us next to nothing\nabout the concepts in it. We certainly don‚Äôt know anything about what makes a\nrestaurant a restaurant, much less Italian, Chinese, or expensive. What is needed are\nadditional assertions about what it means to be a member of any of these categories.\nIn Description Logics such statements come in the form of relations between the\nconcepts being described and other concepts in the domain. In keeping with its\norigins in structured network representations, relations in Description Logics are\ntypically binary and are often referred to as roles, or role-relations.\n To see how such relations work, let‚Äôs consider some of the facts about restaurants\ndiscussed earlier in the chapter. We‚Äôll use the hasCuisine relation to capture information as to what kinds of food restaurants serve and the hasPriceRange relation\nto capture how pricey particular restaurants tend to be. We can use these relations\nto say something more concrete about our various classes of restaurants. Let‚Äôs start\n18 A PPENDIX H ‚Ä¢ L OGICAL R EPRESENTATIONS OF S ENTENCE M EANING\n\n with our ItalianRestaurant concept. As a first approximation, we might say something uncontroversial like Italian restaurants serve Italian cuisine. To capture these\n notions, let‚Äôs first add some new concepts to our terminology to represent various\n kinds of cuisine.\n\n MexicanCuisine v Cuisine ExpensiveRestaurant v Restaurant\n ItalianCuisine v Cuisine ModerateRestaurant v Restaurant\n ChineseCuisine v Cuisine CheapRestaurant v Restaurant\n VegetarianCuisine v Cuisine\n\n Next, let‚Äôs revise our earlier version of ItalianRestaurant to capture cuisine information.\n\n ItalianRestaurant v Restaurant u ‚àÉhasCuisine.ItalianCuisine (H.53)\n\n The correct way to read this expression is that individuals in the category Italian-\nRestaurant are subsumed both by the category Restaurant and by an unnamed\n class defined by the existential clause‚Äîthe set of entities that serve Italian cuisine.\n An equivalent statement in FOL would be\n\n ‚àÄxItalianRestaurant(x) ‚Üí Restaurant(x) (H.54)\n ‚àß(‚àÉyServes(x, y) ‚àß ItalianCuisine(y))\n\n This FOL translation should make it clear what the DL assertions given above do\n and do not entail. In particular, they don‚Äôt say that domain entities classified as Italian restaurants can‚Äôt engage in other relations like being expensive or even serving\n Chinese cuisine. And critically, they don‚Äôt say much about domain entities that we\n know do serve Italian cuisine. In fact, inspection of the FOL translation makes it\n clear that we cannot infer that any new entities belong to this category based on their\n characteristics. The best we can do is infer new facts about restaurants that we‚Äôre\n explicitly told are members of this category.\n Of course, inferring the category membership of individuals given certain characteristics is a common and critical reasoning task that we need to support. This\n brings us back to the alternative approach to creating hierarchical structures in a\n terminology: actually providing a definition of the categories we‚Äôre creating in the\n form of necessary and sufficient conditions for category membership. In this case,\n we might explicitly provide a definition for ItalianRestaurant as being those restaurants that serve Italian cuisine, and ModerateRestaurant as being those whose\n price range is moderate.\n\n ItalianRestaurant ‚â° Restaurant u ‚àÉhasCuisine.ItalianCuisine (H.55)\n ModerateRestaurant ‚â° Restaurant u hasPriceRange.ModeratePrices (H.56)\n\n While our earlier statements provided necessary conditions for membership in these\n categories, these statements provide both necessary and sufficient conditions.\n Finally, let‚Äôs now consider the superficially similar case of vegetarian restaurants.\n Clearly, vegetarian restaurants are those that serve vegetarian cuisine. But they don‚Äôt\n merely serve vegetarian fare, that‚Äôs all they serve. We can accommodate this kind of\n constraint by adding an additional restriction in the form of a universal quantifier to\n H.5 ‚Ä¢ D ESCRIPTION L OGICS 19\n\n our earlier description of VegetarianRestaurants, as follows:\n\n VegetarianRestaurant ‚â° Restaurant (H.57)\n u‚àÉhasCuisine.VegetarianCuisine\n u‚àÄhasCuisine.VegetarianCuisine\n\n Inference\n Paralleling the focus of Description Logics on categories, relations, and individuals\n is a processing focus on a restricted subset of logical inference. Rather than employing the full range of reasoning permitted by FOL, DL reasoning systems emphasize\n the closely coupled problems of subsumption and instance checking.\nsubsumption Subsumption, as a form of inference, is the task of determining, based on the\n facts asserted in a terminology, whether a superset/subset relationship exists between\n instance\n checking two concepts. Correspondingly, instance checking asks if an individual can be a\n member of a particular category given the facts we know about both the individual\n and the terminology. The inference mechanisms underlying subsumption and instance checking go beyond simply checking for explicitly stated subsumption relations in a terminology. They must explicitly reason using the relational information\n asserted about the terminology to infer appropriate subsumption and membership\n relations.\n Returning to our restaurant domain, let‚Äôs add a new kind of restaurant using the\n following statement:\n\n IlFornaio v ModerateRestaurant u ‚àÉhasCuisine.ItalianCuisine (H.58)\n\n Given this assertion, we might ask whether the IlFornaio chain of restaurants might\n be classified as an Italian restaurant or a vegetarian restaurant. More precisely, we\n can pose the following questions to our reasoning system:\n\n IlFornaio v ItalianRestaurant (H.59)\n IlFornaio v VegetarianRestaurant (H.60)\n\n The answer to the first question is positive since IlFornaio meets the criteria we\n specified for the category ItalianRestaurant: it‚Äôs a Restaurant since we explicitly\n classified it as a ModerateRestaurant, which is a subtype of Restaurant, and it\n meets the has.Cuisine class restriction since we‚Äôve asserted that directly.\n The answer to the second question is negative. Recall, that our criteria for vegetarian restaurants contains two requirements: it has to serve vegetarian fare, and\n that‚Äôs all it can serve. Our current definition for IlFornaio fails on both counts since\n we have not asserted any relations that state that IlFornaio serves vegetarian fare,\n and the relation we have asserted, hasCuisine.ItalianCuisine, contradicts the second criteria.\n A related reasoning task, based on the basic subsumption inference, is to derive\n implied\n hierarchy the implied hierarchy for a terminology given facts about the categories in the terminology. This task roughly corresponds to a repeated application of the subsumption operator to pairs of concepts in the terminology. Given our current collection of\n statements, the expanded hierarchy shown in Fig. H.6 can be inferred. You should\n convince yourself that this diagram contains all and only the subsumption links that\n should be present given our current knowledge.\n Instance checking is the task of determining whether a particular individual can\n be classified as a member of a particular category. This process takes what is known\n20 A PPENDIX H ‚Ä¢ L OGICAL R EPRESENTATIONS OF S ENTENCE M EANING\n\n Restaurant\n\n Italian Chinese Mexican Vegetarian Cheap Moderate Expensive\n Restaurant Restaurant Restaurant Restaurant Restaurant Restaurant Restaurant\n\n Il Fornaio\n\n Figure H.6 A graphical network representation of the complete set of subsumption relations in the restaurant domain given the current set of assertions in the TBox.\n\n about a given individual, in the form of relations and explicit categorical statements,\n and then compares that information with what is known about the current terminology. It then returns a list of the most specific categories to which the individual can\n belong.\n As an example of a categorization problem, consider an establishment that we‚Äôre\n told is a restaurant and serves Italian cuisine.\n\n Restaurant(Gondolier)\n hasCuisine(Gondolier, ItalianCuisine)\n\n Here, we‚Äôre being told that the entity denoted by the term Gondolier is a restaurant and serves Italian food. Given this new information and the contents of our\n current TBox, we might reasonably like to ask if this is an Italian restaurant, if it is\n a vegetarian restaurant, or if it has moderate prices.\n Assuming the definitional statements given earlier, we can indeed categorize\n the Gondolier as an Italian restaurant. That is, the information we‚Äôve been given\n about it meets the necessary and sufficient conditions required for membership in\n this category. And as with the IlFornaio category, this individual fails to match the\n stated criteria for the VegetarianRestaurant. Finally, the Gondolier might also\n turn out to be a moderately priced restaurant, but we can‚Äôt tell at this point since\n we don‚Äôt know anything about its prices. What this means is that given our current\n knowledge the answer to the query ModerateRestaurant(Gondolier) would be false\n since it lacks the required hasPriceRange relation.\n The implementation of subsumption, instance checking, as well as other kinds of\n inferences needed for practical applications, varies according to the expressivity of\n the Description Logic being used. However, for a Description Logic of even modest\n power, the primary implementation techniques are based on satisfiability methods\n that in turn rely on the underlying model-based semantics introduced earlier in this\n chapter.\n\n OWL and the Semantic Web\n The highest-profile role for Description Logics, to date, has been as a part of the\n development of the Semantic Web. The Semantic Web is an ongoing effort to provide a way to formally specify the semantics of the contents of the Web (Fensel\n et al., 2003). A key component of this effort involves the creation and deployment\n of ontologies for various application areas of interest. The meaning representation\n H.6 ‚Ä¢ S UMMARY 21\n\nWeb Ontology\n Language language used to represent this knowledge is the Web Ontology Language (OWL)\n (McGuiness and van Harmelen, 2004). OWL embodies a Description Logic that\n corresponds roughly to the one we‚Äôve been describing here.\n\nH.6 Summary\n This chapter has introduced the representational approach to meaning. The following are some of the highlights of this chapter:\n ‚Ä¢ A major approach to meaning in computational linguistics involves the creation of formal meaning representations that capture the meaning-related\n content of linguistic inputs. These representations are intended to bridge the\n gap from language to common-sense knowledge of the world.\n ‚Ä¢ The frameworks that specify the syntax and semantics of these representations are called meaning representation languages. A wide variety of such\n languages are used in natural language processing and artificial intelligence.\n ‚Ä¢ Such representations need to be able to support the practical computational\n requirements of semantic processing. Among these are the need to determine\n the truth of propositions, to support unambiguous representations, to represent variables, to support inference, and to be sufficiently expressive.\n ‚Ä¢ Human languages have a wide variety of features that are used to convey\n meaning. Among the most important of these is the ability to convey a predicateargument structure.\n ‚Ä¢ First-Order Logic is a well-understood, computationally tractable meaning\n representation language that offers much of what is needed in a meaning representation language.\n ‚Ä¢ Important elements of semantic representation including states and events\n can be captured in FOL.\n ‚Ä¢ Semantic networks and frames can be captured within the FOL framework.\n ‚Ä¢ Modern Description Logics consist of useful and computationally tractable\n subsets of full First-Order Logic. The most prominent use of a description\n logic is the Web Ontology Language (OWL), used in the specification of the\n Semantic Web.\n\nHistorical Notes\n The earliest computational use of declarative meaning representations in natural language processing was in the context of question-answering systems (Green et al.\n 1961, Raphael 1968, Lindsey 1963). These systems employed ad hoc representations for the facts needed to answer questions. Questions were then translated into\n a form that could be matched against facts in the knowledge base. Simmons (1965)\n provides an overview of these early efforts.\n Woods (1967) investigated the use of FOL-like representations in question answering as a replacement for the ad hoc representations in use at the time. Woods\n (1973) further developed and extended these ideas in the landmark Lunar system.\n22 A PPENDIX H ‚Ä¢ L OGICAL R EPRESENTATIONS OF S ENTENCE M EANING\n\n Interestingly, the representations used in Lunar had both truth-conditional and procedural semantics. Winograd (1972) employed a similar representation based on the\n Micro-Planner language in his SHRDLU system.\n During this same period, researchers interested in the cognitive modeling of language and memory had been working with various forms of associative network\n representations. Masterman (1957) was the first to make computational use of a\n semantic network-like knowledge representation, although semantic networks are\n generally credited to Quillian (1968). A considerable amount of work in the semantic network framework was carried out during this era (Norman and Rumelhart\n 1975, Schank 1972, Wilks 1975b, Wilks 1975a, Kintsch 1974). It was during this\n period that a number of researchers began to incorporate Fillmore‚Äôs notion of case\n roles (Fillmore, 1968) into their representations. Simmons (1973) was the earliest\n adopter of case roles as part of representations for natural language processing.\n Detailed analyses by Woods (1975) and Brachman (1979) aimed at figuring out\n what semantic networks actually mean led to the development of a number of more\n sophisticated network-like languages including KRL (Bobrow and Winograd, 1977)\n and KL - ONE (Brachman and Schmolze, 1985). As these frameworks became more\n sophisticated and well defined, it became clear that they were restricted variants of\n FOL coupled with specialized indexing inference procedures. A useful collection of\n papers covering much of this work can be found in Brachman and Levesque (1985).\n Russell and Norvig (2002) describe a modern perspective on these representational\n efforts.\n Linguistic efforts to assign semantic structures to natural language sentences in\n the generative era began with the work of Katz and Fodor (1963). The limitations of\n their simple feature-based representations and the natural fit of logic to many of the\n linguistic problems of the day quickly led to the adoption of a variety of predicateargument structures as preferred semantic representations (Lakoff 1972, McCawley\n 1968). The subsequent introduction by Montague (1973) of the truth-conditional\n model-theoretic framework into linguistic theory led to a much tighter integration\n between theories of formal syntax and a wide range of formal semantic frameworks.\n Good introductions to Montague semantics and its role in linguistic theory can be\n found in Dowty et al. (1981) and Partee (1976).\n The representation of events as reified objects is due to Davidson (1967). The\n approach presented here, which explicitly reifies event participants, is due to Parsons\n (1990).\n A recent comprehensive treatment of logic and language can be found in van\n Benthem and ter Meulen (1997). A classic semantics text is Lyons (1977). McCawley (1993) is an indispensable textbook covering a wide range of topics concerning\n logic and language. Chierchia and McConnell-Ginet (1991) also broadly covers\n semantic issues from a linguistic perspective. Heim and Kratzer (1998) is a more\n recent text written from the perspective of current generative theory.\n\nExercises\n H.1 Peruse your daily newspaper for three examples of ambiguous sentences or\n headlines. Describe the various sources of the ambiguities.\n H.2 Consider a domain in which the word coffee can refer to the following concepts in a knowledge-based system: a caffeinated or decaffeinated beverage,\n ground coffee used to make either kind of beverage, and the beans themselves.\n E XERCISES 23\n\n Give arguments as to which of the following uses of coffee are ambiguous and\n which are vague.\n 1. I‚Äôve had my coffee for today.\n 2. Buy some coffee on your way home.\n 3. Please grind some more coffee.\nH.3 The following rule, which we gave as a translation for Example H.25, is not a\n reasonable definition of what it means to be a vegetarian restaurant.\n\n ‚àÄxVegetarianRestaurant(x) =‚áí Serves(x,VegetarianFood)\n\n Give a FOL rule that better defines vegetarian restaurants in terms of what they\n serve.\nH.4 Give FOL translations for the following sentences:\n 1. Vegetarians do not eat meat.\n 2. Not all vegetarians eat eggs.\nH.5 Give a set of facts and inferences necessary to prove the following assertions:\n 1. McDonald‚Äôs is not a vegetarian restaurant.\n 2. Some vegetarians can eat at McDonald‚Äôs.\n Don‚Äôt just place these facts in your knowledge base. Show that they can be\n inferred from some more general facts about vegetarians and McDonald‚Äôs.\nH.6 On page 12, we gave the representation Near(Centro, Bacaro) as a translation for the sentence Centro is near Bacaro. In a truth-conditional semantics,\n this formula is either true or false given some model. Critique this truthconditional approach with respect to the meaning of words like near.\n24 Appendix H ‚Ä¢ Logical Representations of Sentence Meaning\n\nBanarescu, L., C. Bonial, S. Cai, M. Georgescu, K. Grif- McCawley, J. D. 1993. Everything that Linguists Have Alfitt, U. Hermjakob, K. Knight, P. Koehn, M. Palmer, and ways Wanted to Know about Logic, 2nd edition. Univer-\nN. Schneider. 2013. Abstract meaning representation for sity of Chicago Press, Chicago, IL.\n sembanking. 7th Linguistic Annotation Workshop and In- McGuiness, D. L. and F. van Harmelen. 2004. OWL web\n teroperability with Discourse. ontology overview. Technical Report 20040210, World\nvan Benthem, J. and A. ter Meulen, eds. 1997. Handbook of Wide Web Consortium.\n Logic and Language. MIT Press. Montague, R. 1973. The proper treatment of quantification\nBobrow, D. G. and T. Winograd. 1977. An overview of KRL, in ordinary English. In R. Thomason, ed., Formal Philosa knowledge representation language. Cognitive Science, ophy: Selected Papers of Richard Montague, 247‚Äì270.\n 1(1):3‚Äì46. Yale University Press, New Haven, CT.\nBrachman, R. J. 1979. On the epistemogical status of seman- Norman, D. A. and D. E. Rumelhart. 1975. Explorations in\n tic networks. In N. V. Findler, ed., Associative Networks: Cognition. Freeman.\n Representation and Use of Knowledge by Computers, 3‚Äì Parsons, T. 1990. Events in the Semantics of English. MIT\n 50. Academic Press. Press.\nBrachman, R. J. and H. J. Levesque, eds. 1985. Readings in Partee, B. H., ed. 1976. Montague Grammar. Academic\n Knowledge Representation. Morgan Kaufmann. Press.\nBrachman, R. J. and J. G. Schmolze. 1985. An overview of Quillian, M. R. 1968. Semantic memory. In M. Minsky, ed.,\n the KL-ONE knowledge representation system. Cogni- Semantic Information Processing, 227‚Äì270. MIT Press.\n tive Science, 9(2):171‚Äì216.\n Raphael, B. 1968. SIR: A computer program for semantic\nChierchia, G. and S. McConnell-Ginet. 1991. Meaning and information retrieval. In M. Minsky, ed., Semantic Infor-\nGrammar. MIT Press. mation Processing, 33‚Äì145. MIT Press.\nChurch, A. 1940. A formulation of a simple theory of types. Russell, S. and P. Norvig. 2002. Artificial Intelligence: A\n Journal of Symbolic Logic, 5:56‚Äì68. Modern Approach, 2nd edition. Prentice Hall.\nDavidson, D. 1967. The logical form of action sentences. In Schank, R. C. 1972. Conceptual dependency: A theory\n N. Rescher, ed., The Logic of Decision and Action. Uni- of natural language processing. Cognitive Psychology,\n versity of Pittsburgh Press. 3:552‚Äì631.\nDowty, D. R., R. E. Wall, and S. Peters. 1981. Introduction SchoÃànfinkel, M. 1924. UÃàber die Bausteine der mathemato Montague Semantics. D. Reidel. tischen Logik. Mathematische Annalen, 92:305‚Äì316.\nFensel, D., J. A. Hendler, H. Lieberman, and W. Wahlster, English translation appears in From Frege to GoÃàdel: A\n eds. 2003. Spinning the Semantic Web: Bring the World Source Book in Mathematical Logic, Harvard University\n Wide Web to its Full Potential. MIT Press, Cambridge, Press, 1967.\n MA. Simmons, R. F. 1965. Answering English questions by com-\nFillmore, C. J. 1968. The case for case. In E. W. Bach and puter: A survey. CACM, 8(1):53‚Äì70.\n R. T. Harms, eds, Universals in Linguistic Theory, 1‚Äì88. Simmons, R. F. 1973. Semantic networks: Their compu-\nHolt, Rinehart & Winston. tation and use for understanding English sentences. In\nGreen, B. F., A. K. Wolf, C. Chomsky, and K. Laughery. R. C. Schank and K. M. Colby, eds, Computer Models of\n 1961. Baseball: An automatic question answerer. Pro- Thought and Language, 61‚Äì113. W.H. Freeman & Co.\n ceedings of the Western Joint Computer Conference 19. Wilks, Y. 1975a. Preference semantics. In E. L. Keenan,\nHeim, I. and A. Kratzer. 1998. Semantics in a Generative ed., The Formal Semantics of Natural Language, 329‚Äì\n Grammar. Blackwell Publishers, Malden, MA. 350. Cambridge Univ. Press.\nKatz, J. J. and J. A. Fodor. 1963. The structure of a semantic Wilks, Y. 1975b. A preferential, pattern-seeking, semantheory. Language, 39:170‚Äì210. tics for natural language inference. Artificial Intelligence,\nKintsch, W. 1974. The Representation of Meaning in Mem- 6(1):53‚Äì74.\n ory. Wiley, New York. Winograd, T. 1972. Understanding Natural Language. Aca-\nLakoff, G. 1972. Linguistics and natural logic. In D. David- demic Press.\n son and G. Harman, eds, Semantics for Natural Lan- Woods, W. A. 1967. Semantics for a Question-Answering\n guage, 545‚Äì665. D. Reidel. System. Ph.D. thesis, Harvard University.\nLindsey, R. 1963. Inferential memory as the basis of ma- Woods, W. A. 1973. Progress in natural language underchines which understand natural language. In E. Feigen- standing. Proceedings of AFIPS National Conference.\n baum and J. Feldman, eds, Computers and Thought, 217‚Äì Woods, W. A. 1975. What‚Äôs in a link: Foundations for se-\n233. McGraw Hill. mantic networks. In D. G. Bobrow and A. M. Collins, eds,\nLyons, J. 1977. Semantics. Cambridge University Press. Representation and Understanding: Studies in Cognitive\nMasterman, M. 1957. The thesaurus in syntax and semantics. Science, 35‚Äì82. Academic Press.\n Mechanical Translation, 4(1):1‚Äì2.\nMcCawley, J. D. 1968. The role of semantics in a grammar. In E. W. Bach and R. T. Harms, eds, Universals in\n Linguistic Theory, 124‚Äì169. Holt, Rinehart & Winston.\n",
    "file_path": "/Users/colinsidberry/nlp-textbook-rag/data/extracted_text/H.Logical Representations of Sentence Meaning.txt",
    "file_size_kb": 73.41
  },
  {
    "id": "8f045282aecfce1c",
    "source": "nlp_textbook",
    "chapter": "Word Senses and WordNet I Lady Bracknell. Are your parents living? Jack. I have lost both my parents.",
    "filename": "I.Word Senses and WordNet.txt",
    "content": "rights reserved. Draft of August 24, 2025.\n\nCHAPTER\n\n Word Senses and WordNet\nI Lady Bracknell. Are your parents living?\n Jack. I have lost both my parents.\n Lady Bracknell. To lose one parent, Mr. Worthing, may be regarded as a\n misfortune; to lose both looks like carelessness.\n Oscar Wilde, The Importance of Being Earnest\n\n ambiguous Words are ambiguous: the same word can be used to mean different things. In\n Chapter 5 we saw that the word ‚Äúmouse‚Äù has (at least) two meanings: (1) a small\n rodent, or (2) a hand-operated device to control a cursor. The word ‚Äúbank‚Äù can\n mean: (1) a financial institution or (2) a sloping mound. In the quote above from\n his play The Importance of Being Earnest, Oscar Wilde plays with two meanings of\n ‚Äúlose‚Äù (to misplace an object, and to suffer the death of a close person).\n We say that the words ‚Äòmouse‚Äô or ‚Äòbank‚Äô are polysemous (from Greek ‚Äòhaving\n word sense many senses‚Äô, poly- ‚Äòmany‚Äô + sema, ‚Äòsign, mark‚Äô).1 A sense (or word sense) is\n a discrete representation of one aspect of the meaning of a word. In this chapter\n WordNet we discuss word senses in more detail and introduce WordNet, a large online thesaurus ‚Äîa database that represents word senses‚Äîwith versions in many languages.\n WordNet also represents relations between senses. For example, there is an IS-A\n relation between dog and mammal (a dog is a kind of mammal) and a part-whole\n relation between engine and car (an engine is a part of a car).\n Knowing the relation between two senses can play an important role in tasks\n involving meaning. Consider the antonymy relation. Two words are antonyms if\n they have opposite meanings, like long and short, or up and down. Distinguishing\n these is quite important; if a user asks a dialogue agent to turn up the music, it\n would be unfortunate to instead turn it down. But in fact in embedding models like\n word2vec, antonyms are easily confused with each other, because often one of the\n closest words in embedding space to a word (e.g., up) is its antonym (e.g., down).\n Thesauruses that represent this relationship can help!\n word sense\ndisambiguation We also introduce word sense disambiguation (WSD), the task of determining\n which sense of a word is being used in a particular context. We‚Äôll give supervised\n and unsupervised algorithms for deciding which sense was intended in a particular\n context. This task has a very long history in computational linguistics and many applications. In question answering, we can be more helpful to a user who asks about\n ‚Äúbat care‚Äù if we know which sense of bat is relevant. (Is the user a vampire? or\n just wants to play baseball.) And the different senses of a word often have different translations; in Spanish the animal bat is a murcieÃÅlago while the baseball bat is\n a bate, and indeed word sense algorithms may help improve MT (Pu et al., 2018).\n Finally, WSD has long been used as a tool for evaluating language processing models, and understanding how models represent different word senses is an important\n 1 The word polysemy itself is ambiguous; you may see it used in a different way, to refer only to cases\n where a word‚Äôs senses are related in some structured way, reserving the word homonymy to mean sense\n ambiguities with no relation between the senses (Haber and Poesio, 2020). Here we will use ‚Äòpolysemy‚Äô\n to mean any kind of sense ambiguity, and ‚Äòstructured polysemy‚Äô for polysemy with sense relations.\n2 A PPENDIX I ‚Ä¢ W ORD S ENSES AND W ORD N ET\n\n analytic direction.\n\nI.1 Word Senses\n word sense A sense (or word sense) is a discrete representation of one aspect of the meaning of\n a word. Loosely following lexicographic tradition, we represent each sense with a\n superscript: bank1 and bank2 , mouse1 and mouse2 . In context, it‚Äôs easy to see the\n different meanings:\n mouse1 : .... a mouse controlling a computer system in 1968.\n mouse2 : .... a quiet animal like a mouse\n bank1 : ...a bank can hold the investments in a custodial account ...\n bank2 : ...as agriculture burgeons on the east bank, the river ...\n\n I.1.1 Defining Word Senses\n How can we define the meaning of a word sense? We introduced in Chapter 5 the\n standard computational approach of representing a word as an embedding, a point\n in semantic space. The intuition of embedding models like word2vec or GloVe is\n that the meaning of a word can be defined by its co-occurrences, the counts of words\n that often occur nearby. But that doesn‚Äôt tell us how to define the meaning of a word\n sense. As we saw in Chapter 10, contextual embeddings like BERT go further by\n offering an embedding that represents the meaning of a word in its textual context,\n and we‚Äôll see that contextual embeddings lie at the heart of modern algorithms for\n word sense disambiguation.\n But first, we need to consider the alternative ways that dictionaries and thesauruses offer for defining senses. One is based on the fact that dictionaries or thegloss sauruses give textual definitions for each sense called glosses. Here are the glosses\n for two senses of bank:\n 1. financial institution that accepts deposits and channels\n the money into lending activities\n 2. sloping land (especially the slope beside a body of water)\n\n Glosses are not a formal meaning representation; they are just written for people.\n Consider the following fragments from the definitions of right, left, red, and blood\n from the American Heritage Dictionary (Morris, 1985).\n\n right adj. located nearer the right hand esp. being on the right when\n facing the same direction as the observer.\n left adj. located nearer to this side of the body than the right.\n red n. the color of blood or a ruby.\n blood n. the red liquid that circulates in the heart, arteries and veins of\n animals.\n\n Note the circularity in these definitions. The definition of right makes two direct\n references to itself, and the entry for left contains an implicit self-reference in the\n phrase this side of the body, which presumably means the left side. The entries for\n red and blood reference each other in their definitions. For humans, such entries are\n useful since the user of the dictionary has sufficient grasp of these other terms.\n I.1 ‚Ä¢ W ORD S ENSES 3\n\n Yet despite their circularity and lack of formal representation, glosses can still\n be useful for computational modeling of senses. This is because a gloss is just a sentence, and from sentences we can compute sentence embeddings that tell us something about the meaning of the sense. Dictionaries often give example sentences\n along with glosses, and these can again be used to help build a sense representation.\n The second way that thesauruses offer for defining a sense is‚Äîlike the dictionary\n definitions‚Äîdefining a sense through its relationship with other senses. For example, the above definitions make it clear that right and left are similar kinds of lemmas\n that stand in some kind of alternation, or opposition, to one another. Similarly, we\n can glean that red is a color and that blood is a liquid. Sense relations of this sort\n (IS-A, or antonymy) are explicitly listed in on-line databases like WordNet. Given\n a sufficiently large database of such relations, many applications are quite capable\n of performing sophisticated semantic tasks about word senses (even if they do not\n really know their right from their left).\n\n I.1.2 How many senses do words have?\n Dictionaries and thesauruses give discrete lists of senses. By contrast, embeddings\n (whether static or contextual) offer a continuous high-dimensional model of meaning\n that doesn‚Äôt divide up into discrete senses.\n Therefore creating a thesaurus depends on criteria for deciding when the differing uses of a word should be represented with discrete senses. We might consider\n two senses discrete if they have independent truth conditions, different syntactic behavior, and independent sense relations, or if they exhibit antagonistic meanings.\n Consider the following uses of the verb serve from the WSJ corpus:\n (I.1) They rarely serve red meat, preferring to prepare seafood.\n (I.2) He served as U.S. ambassador to Norway in 1976 and 1977.\n (I.3) He might have served his time, come out and led an upstanding life.\n The serve of serving red meat and that of serving time clearly have different truth\n conditions and presuppositions; the serve of serve as ambassador has the distinct\n subcategorization structure serve as NP. These heuristics suggest that these are probably three distinct senses of serve. One practical technique for determining if two\n senses are distinct is to conjoin two uses of a word in a single sentence; this kind\nzeugma of conjunction of antagonistic readings is called zeugma. Consider the following\n examples:\n (I.4) Which of those flights serve breakfast?\n (I.5) Does Air France serve Philadelphia?\n (I.6) ?Does Air France serve breakfast and Philadelphia?\n We use (?) to mark those examples that are semantically ill-formed. The oddness of\n the invented third example (a case of zeugma) indicates there is no sensible way to\n make a single sense of serve work for both breakfast and Philadelphia. We can use\n this as evidence that serve has two different senses in this case.\n Dictionaries tend to use many fine-grained senses so as to capture subtle meaning\n differences, a reasonable approach given that the traditional role of dictionaries is\n aiding word learners. For computational purposes, we often don‚Äôt need these fine\n distinctions, so we often group or cluster the senses; we have already done this for\n some of the examples in this chapter. Indeed, clustering examples into senses, or\n senses into broader-grained categories, is an important computational task that we‚Äôll\n discuss in Section I.7.\n4 A PPENDIX I ‚Ä¢ W ORD S ENSES AND W ORD N ET\n\nI.2 Relations Between Senses\n This section explores the relations between word senses, especially those that have\n received significant computational investigation like synonymy, antonymy, and hypernymy.\n\n Synonymy\n We introduced in Chapter 5 the idea that when two senses of two different words\n synonym (lemmas) are identical, or nearly identical, we say the two senses are synonyms.\n Synonyms include such pairs as\n couch/sofa vomit/throw up filbert/hazelnut car/automobile\n And we mentioned that in practice, the word synonym is commonly used to\n describe a relationship of approximate or rough synonymy. But furthermore, synonymy is actually a relationship between senses rather than words. Considering the\n words big and large. These may seem to be synonyms in the following sentences,\n since we could swap big and large in either sentence and retain the same meaning:\n (I.7) How big is that plane?\n (I.8) Would I be flying on a large or small plane?\n But note the following sentence in which we cannot substitute large for big:\n (I.9) Miss Nelson, for instance, became a kind of big sister to Benjamin.\n (I.10) ?Miss Nelson, for instance, became a kind of large sister to Benjamin.\n This is because the word big has a sense that means being older or grown up, while\n large lacks this sense. Thus, we say that some senses of big and large are (nearly)\n synonymous while other ones are not.\n\n Antonymy\n antonym Whereas synonyms are words with identical or similar meanings, antonyms are\n words with an opposite meaning, like:\n long/short big/little fast/slow cold/hot dark/light\n rise/fall up/down in/out\n Two senses can be antonyms if they define a binary opposition or are at opposite\n ends of some scale. This is the case for long/short, fast/slow, or big/little, which are\n reversives at opposite ends of the length or size scale. Another group of antonyms, reversives,\n describe change or movement in opposite directions, such as rise/fall or up/down.\n Antonyms thus differ completely with respect to one aspect of their meaning‚Äî\n their position on a scale or their direction‚Äîbut are otherwise very similar, sharing\n almost all other aspects of meaning. Thus, automatically distinguishing synonyms\n from antonyms can be difficult.\n\n Taxonomic Relations\n Another way word senses can be related is taxonomically. A word (or sense) is a\n hyponym hyponym of another word or sense if the first is more specific, denoting a subclass\n of the other. For example, car is a hyponym of vehicle, dog is a hyponym of animal,\n hypernym and mango is a hyponym of fruit. Conversely, we say that vehicle is a hypernym of\n car, and animal is a hypernym of dog. It is unfortunate that the two words (hypernym\n I.2 ‚Ä¢ R ELATIONS B ETWEEN S ENSES 5\n\n and hyponym) are very similar and hence easily confused; for this reason, the word\nsuperordinate superordinate is often used instead of hypernym.\n\n Superordinate vehicle fruit furniture mammal\n Subordinate car mango chair dog\n\n We can define hypernymy more formally by saying that the class denoted by\n the superordinate extensionally includes the class denoted by the hyponym. Thus,\n the class of animals includes as members all dogs, and the class of moving actions\n includes all walking actions. Hypernymy can also be defined in terms of entailment. Under this definition, a sense A is a hyponym of a sense B if everything\n that is A is also B, and hence being an A entails being a B, or ‚àÄx A(x) ‚áí B(x). Hyponymy/hypernymy is usually a transitive relation; if A is a hyponym of B and B is a\n hyponym of C, then A is a hyponym of C. Another name for the hypernym/hyponym\n IS-A structure is the IS-A hierarchy, in which we say A IS-A B, or B subsumes A.\n Hypernymy is useful for tasks like textual entailment or question answering;\n knowing that leukemia is a type of cancer, for example, would certainly be useful in\n answering questions about leukemia.\n\n Meronymy\n part-whole Another common relation is meronymy, the part-whole relation. A leg is part of a\n chair; a wheel is part of a car. We say that wheel is a meronym of car, and car is a\n holonym of wheel.\n\n Structured Polysemy\n The senses of a word can also be related semantically, in which case we call the\n structured\n polysemy relationship between them structured polysemy. Consider this sense bank:\n (I.11) The bank is on the corner of Nassau and Witherspoon.\n This sense, perhaps bank4 , means something like ‚Äúthe building belonging to\n a financial institution‚Äù. These two kinds of senses (an organization and the building associated with an organization ) occur together for many other words as well\n (school, university, hospital, etc.). Thus, there is a systematic relationship between\n senses that we might represent as\n BUILDING ‚Üî ORGANIZATION\n metonymy This particular subtype of polysemy relation is called metonymy. Metonymy is\n the use of one aspect of a concept or entity to refer to other aspects of the entity or\n to the entity itself. We are performing metonymy when we use the phrase the White\n House to refer to the administration whose office is in the White House. Other\n common examples of metonymy include the relation between the following pairings\n of senses:\n AUTHOR ‚Üî WORKS OF AUTHOR\n (Jane Austen wrote Emma) (I really love Jane Austen)\n FRUITTREE ‚Üî FRUIT\n (Plums have beautiful blossoms) (I ate a preserved plum yesterday)\n6 A PPENDIX I ‚Ä¢ W ORD S ENSES AND W ORD N ET\n\nI.3 WordNet: A Database of Lexical Relations\n The most commonly used resource for sense relations in English and many other\n WordNet languages is the WordNet lexical database (Fellbaum, 1998). English WordNet\n consists of three separate databases, one each for nouns and verbs and a third for\n adjectives and adverbs; closed class words are not included. Each database contains\n a set of lemmas, each one annotated with a set of senses. The WordNet 3.0 release\n has 117,798 nouns, 11,529 verbs, 22,479 adjectives, and 4,481 adverbs. The average noun has 1.23 senses, and the average verb has 2.16 senses. WordNet can be\n accessed on the Web or downloaded locally. Figure I.1 shows the lemma entry for\n the noun bass.\n\n The noun ‚Äúbass‚Äù has 8 senses in WordNet.\n 1. bass1 - (the lowest part of the musical range)\n 2. bass2 , bass part1 - (the lowest part in polyphonic music)\n 3. bass3 , basso1 - (an adult male singer with the lowest voice)\n 4. sea bass1 , bass4 - (the lean flesh of a saltwater fish of the family Serranidae)\n 5. freshwater bass1 , bass5 - (any of various North American freshwater fish with\n lean flesh (especially of the genus Micropterus))\n 6. bass6 , bass voice1 , basso2 - (the lowest adult male singing voice)\n 7. bass7 - (the member with the lowest range of a family of musical instruments)\n 8. bass8 - (nontechnical name for any of numerous edible marine and\n freshwater spiny-finned fishes)\n\n Figure I.1 A portion of the WordNet 3.0 entry for the noun bass.\n\n gloss Note that there are eight senses, each of which has a gloss (a dictionary-style\n definition), a list of synonyms for the sense, and sometimes also usage examples.\n WordNet doesn‚Äôt represent pronunciation, so doesn‚Äôt distinguish the pronunciation\n [b ae s] in bass4 , bass5 , and bass8 from the other senses pronounced [b ey s].\n synset The set of near-synonyms for a WordNet sense is called a synset (for synonym\n set); synsets are an important primitive in WordNet. The entry for bass includes\n synsets like {bass1 , deep6 }, or {bass6 , bass voice1 , basso2 }. We can think of a\n synset as representing a concept of the type we discussed in Appendix F. Thus,\n instead of representing concepts in logical terms, WordNet represents them as lists\n of the word senses that can be used to express the concept. Here‚Äôs another synset\n example:\n {chump1 , fool2 , gull1 , mark9 , patsy1 , fall guy1 ,\n sucker1 , soft touch1 , mug2 }\n The gloss of this synset describes it as:\n Gloss: a person who is gullible and easy to take advantage of.\n Glosses are properties of a synset, so that each sense included in the synset has the\n same gloss and can express this concept. Because they share glosses, synsets like\n this one are the fundamental unit associated with WordNet entries, and hence it is\n synsets, not wordforms, lemmas, or individual senses, that participate in most of the\n lexical sense relations in WordNet.\n WordNet also labels each synset with a lexicographic category drawn from a\n semantic field for example the 26 categories for nouns shown in Fig. I.2, as well as\n 15 for verbs (plus 2 for adjectives and 1 for adverbs). These categories are often\n I.3 ‚Ä¢ W ORD N ET: A DATABASE OF L EXICAL R ELATIONS 7\n\n supersense called supersenses, because they act as coarse semantic categories or groupings of\n senses which can be useful when word senses are too fine-grained (Ciaramita and\n Johnson 2003, Ciaramita and Altun 2006). Supersenses have also been defined for\n adjectives (Tsvetkov et al., 2014) and prepositions (Schneider et al., 2018).\n\nCategory Example Category Example Category Example\nACT service GROUP place PLANT tree\nANIMAL dog LOCATION area POSSESSION price\nARTIFACT car MOTIVE reason PROCESS process\nATTRIBUTE quality NATURAL EVENT experience QUANTITY amount\nBODY hair NATURAL OBJECT flower RELATION portion\nCOGNITION way OTHER stuff SHAPE square\nCOMMUNICATION review PERSON people STATE pain\nFEELING discomfort PHENOMENON result SUBSTANCE oil\nFOOD food TIME day\nFigure I.2 Supersenses: 26 lexicographic categories for nouns in WordNet.\n\n I.3.1 Sense Relations in WordNet\n WordNet represents all the kinds of sense relations discussed in the previous section,\n as illustrated in Fig. I.3 and Fig. I.4.\n\nRelation Also Called Definition Example\nHypernym Superordinate From concepts to superordinates breakfast1 ‚Üí meal1\nHyponym Subordinate From concepts to subtypes meal1 ‚Üí lunch1\nInstance Hypernym Instance From instances to their concepts Austen1 ‚Üí author1\nInstance Hyponym Has-Instance From concepts to their instances composer1 ‚Üí Bach1\nPart Meronym Has-Part From wholes to parts table2 ‚Üí leg3\nPart Holonym Part-Of From parts to wholes course7 ‚Üí meal1\nAntonym Semantic opposition between lemmas leader1 ‚áê‚áí follower1\nDerivation Lemmas w/same morphological root destruction1 ‚áê‚áí destroy1\nFigure I.3 Some of the noun relations in WordNet.\n\nRelation Definition Example\nHypernym From events to superordinate events fly9 ‚Üí travel5\nTroponym From events to subordinate event walk1 ‚Üí stroll1\nEntails From verbs (events) to the verbs (events) they entail snore1 ‚Üí sleep1\nAntonym Semantic opposition between lemmas increase1 ‚áê‚áí decrease1\nFigure I.4 Some verb relations in WordNet.\n\n For example WordNet represents hyponymy (page 4) by relating each synset to\n its immediately more general and more specific synsets through direct hypernym\n and hyponym relations. These relations can be followed to produce longer chains of\n more general or more specific synsets. Figure I.5 shows hypernym chains for bass3\n and bass7 ; more general synsets are shown on successively indented lines.\n WordNet has two kinds of taxonomic entities: classes and instances. An instance\n is an individual, a proper noun that is a unique entity. San Francisco is an instance\n of city, for example. But city is a class, a hyponym of municipality and eventually of\n location. Fig. I.6 shows a subgraph of WordNet demonstrating many of the relations.\n8 A PPENDIX I ‚Ä¢ W ORD S ENSES AND W ORD N ET\n\n bass3 , basso (an adult male singer with the lowest voice)\n => singer, vocalist, vocalizer, vocaliser\n => musician, instrumentalist, player\n => performer, performing artist\n => entertainer\n => person, individual, someone...\n => organism, being\n => living thing, animate thing,\n => whole, unit\n => object, physical object\n => physical entity\n => entity\n bass (member with the lowest range of a family of instruments)\n => musical instrument, instrument\n => device\n => instrumentality, instrumentation\n => artifact, artefact\n => whole, unit\n => object, physical object\n => physical entity\n => entity\n Figure I.5 Hyponymy chains for two separate senses of the lemma bass. Note that the\n chains are completely distinct, only converging at the very abstract level whole, unit.\n\n Figure I.6 WordNet viewed as a graph. Figure from Navigli (2016).\n\nI.4 Word Sense Disambiguation\n The task of selecting the correct sense for a word is called word sense disambiguaword sense\ndisambiguation tion, or WSD. WSD algorithms take as input a word in context and a fixed inventory\n WSD of potential word senses and outputs the correct word sense in context.\n I.4 ‚Ä¢ W ORD S ENSE D ISAMBIGUATION 9\n\n I.4.1 WSD: The Task and Datasets\n In this section we introduce the task setup for WSD, and then turn to algorithms.\n The inventory of sense tags depends on the task. For sense tagging in the context\n of translation from English to Spanish, the sense tag inventory for an English word\n might be the set of different Spanish translations. For automatic indexing of medical articles, the sense-tag inventory might be the set of MeSH (Medical Subject\n Headings) thesaurus entries. Or we can use the set of senses from a resource like\n WordNet, or supersenses if we want a coarser-grain set. Figure I.4.1 shows some\n such examples for the word bass.\n\n WordNet Spanish WordNet\n Sense Translation Supersense Target Word in Context\n bass4 lubina FOOD . . . fish as Pacific salmon and striped bass and. . .\n bass7 bajo ARTIFACT . . . play bass because he doesn‚Äôt have to solo. . .\n Figure I.7 Some possible sense tag inventories for bass.\n\n In some situations, we just need to disambiguate a small number of words. In\nlexical sample such lexical sample tasks, we have a small pre-selected set of target words and an\n inventory of senses for each word from some lexicon. Since the set of words and the\n set of senses are small, simple supervised classification approaches work very well.\n More commonly, however, we have a harder problem in which we have to disall-words ambiguate all the words in some text. In this all-words task, the system is given an\n entire texts and a lexicon with an inventory of senses for each entry and we have to\n disambiguate every word in the text (or sometimes just every content word). The\n all-words task is similar to part-of-speech tagging, except with a much larger set of\n tags since each lemma has its own set. A consequence of this larger set of tags is\n data sparseness.\n Supervised all-word disambiguation tasks are generally trained from a semantic\n semantic concordance, a corpus in which each open-class word in each sentence is labeled\n concordance\n with its word sense from a specific dictionary or thesaurus, most often WordNet.\n The SemCor corpus is a subset of the Brown Corpus consisting of over 226,036\n words that were manually tagged with WordNet senses (Miller et al. 1993, Landes\n et al. 1998). Other sense-tagged corpora have been built for the SENSEVAL and SemEval WSD tasks, such as the SENSEVAL-3 Task 1 English all-words test data with\n 2282 annotations (Snyder and Palmer, 2004) or the SemEval-13 Task 12 datasets.\n Large semantic concordances are also available in other languages including Dutch\n (Vossen et al., 2011) and German (Henrich et al., 2012).\n Here‚Äôs an example from the SemCor corpus showing the WordNet sense numbers of the tagged words; we‚Äôve used the standard WSD notation in which a subscript\n marks the part of speech (Navigli, 2009):\n (I.12) You will find9v that avocado1n is1v unlike1j other1j fruit1n you have ever1r tasted2v\n Given each noun, verb, adjective, or adverb word in the hand-labeled test set (say\n fruit), the SemCor-based WSD task is to choose the correct sense from the possible\n senses in WordNet. For fruit this would mean choosing between the correct answer\n fruit1n (the ripened reproductive body of a seed plant), and the other two senses fruit2n\n (yield; an amount of a product) and fruit3n (the consequence of some effort or action).\n Fig. I.8 sketches the task.\n WSD systems are typically evaluated intrinsically, by computing F1 against\n hand-labeled sense tags in a held-out set, such as the SemCor corpus or SemEval\n corpora discussed above.\n10 A PPENDIX I ‚Ä¢ W ORD S ENSES AND W ORD N ET\n\n y5 y6\n y3\n stand1: side1:\n y1 bass1: y4 upright relative\n low range ‚Ä¶ region\n electric1: ‚Ä¶ player1: stand5: ‚Ä¶\n using bass4: in game bear side3:\n electricity sea fish player2: ‚Ä¶ of body\n electric2: ‚Ä¶ musician stand10: ‚Ä¶\n tense\n y2 bass7: player3: put side11:\n electric3: instrument actor upright slope\n thrilling guitar1 ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶\n\n x1 x2 x3 x4 x5 x6\n an electric guitar and bass player stand oÔ¨Ä to one side\n\n Figure I.8 The all-words WSD task, mapping from input words (x) to WordNet senses\n (y). Only nouns, verbs, adjectives, and adverbs are mapped, and note that some words (like\n guitar in the example) only have one sense in WordNet. Figure inspired by Chaplot and\n Salakhutdinov (2018).\n\n most frequent A surprisingly strong baseline is simply to choose the most frequent sense for\n sense\n each word from the senses in a labeled corpus (Gale et al., 1992a). For WordNet, this\n corresponds to the first sense, since senses in WordNet are generally ordered from\n most frequent to least frequent based on their counts in the SemCor sense-tagged\n corpus. The most frequent sense baseline can be quite accurate, and is therefore\n often used as a default, to supply a word sense when a supervised algorithm has\n insufficient training data.\n one sense per\n discourse A second heuristic, called one sense per discourse is based on the work of\n Gale et al. (1992b), who noticed that a word appearing multiple times in a text or\n discourse often appears with the same sense. This heuristic seems to hold better for\n coarse-grained senses and particularly when a word‚Äôs senses are unrelated, so isn‚Äôt\n generally used as a baseline. Nonetheless various kinds of disambiguation tasks\n often include some such bias toward resolving an ambiguity the same way inside a\n discourse segment.\n\n I.4.2 The WSD Algorithm: Contextual Embeddings\n The best performing WSD algorithm is a simple 1-nearest-neighbor algorithm using\n contextual word embeddings, due to Melamud et al. (2016) and Peters et al. (2018).\n At training time we pass each sentence in the SemCore labeled dataset through any\n contextual embedding (e.g., BERT) resulting in a contextual embedding for each\n labeled token in SemCore. (There are various ways to compute this contextual embedding vi for a token i; for BERT it is common to pool multiple layers by summing\n the vector representations of i from the last four BERT layers). Then for each sense\n s of any word in the corpus, for each of the n tokens of that sense, we average their\n n contextual representations vi to produce a contextual sense embedding vs for s:\n\n 1X\n vs = vi ‚àÄvi ‚àà tokens(s) (I.13)\n n\n i\n\n At test time, given a token of a target word t in context, we compute its contextual\n embedding t and choose its nearest neighbor sense from the training set, i.e., the\n I.4 ‚Ä¢ W ORD S ENSE D ISAMBIGUATION 11\n\nsense whose sense embedding has the highest cosine with t:\n\n sense(t) = argmax cosine(t, vs ) (I.14)\n s‚ààsenses(t)\n\nFig. I.9 illustrates the model.\n\n find5\n find4\n v\n v\n find1\n v\n find9\n v\n\n cI cfound cthe cjar cempty\n\n ENCODER\n I found the jar empty\nFigure I.9 The nearest-neighbor algorithm for WSD. In green are the contextual embeddings precomputed for each sense of each word; here we just show a few of the senses for\nfind. A contextual embedding is computed for the target word found, and then the nearest\nneighbor sense (in this case find9v ) is chosen. Figure inspired by Loureiro and Jorge (2019).\n\n What do we do for words we haven‚Äôt seen in the sense-labeled training data?\nAfter all, the number of senses that appear in SemCor is only a small fraction of the\nwords in WordNet. The simplest algorithm is to fall back to the Most Frequent Sense\nbaseline, i.e. taking the first sense in WordNet. But that‚Äôs not very satisfactory.\n A more powerful approach, due to Loureiro and Jorge (2019), is to impute the\nmissing sense embeddings, bottom-up, by using the WordNet taxonomy and supersenses. We get a sense embedding for any higher-level node in the WordNet taxonomy by averaging the embeddings of its children, thus computing the embedding for\neach synset as the average of its sense embeddings, the embedding for a hypernym\nas the average of its synset embeddings, and the lexicographic category (supersense)\nembedding as the average of the large set of synset embeddings with that category.\nMore formally, for each missing sense in WordNet sÃÇ ‚àà W , let the sense embeddings\nfor the other members of its synset be SsÃÇ , the hypernym-specific synset embeddings\nbe HsÃÇ , and the lexicographic (supersense-specific) synset embeddings be LsÃÇ . We can\nthen compute the sense embedding for sÃÇ as follows:\n\n 1 X\n if |SsÃÇ | > 0, vsÃÇ = vs , ‚àÄvs ‚àà SsÃÇ (I.15)\n |SsÃÇ |\n 1 X\n else if |HsÃÇ | > 0, vsÃÇ = vsyn , ‚àÄvsyn ‚àà HsÃÇ (I.16)\n |HsÃÇ |\n 1 X\n else if |LsÃÇ | > 0, vsÃÇ = vsyn , ‚àÄvsyn ‚àà LsÃÇ (I.17)\n |LsÃÇ |\n\nSince all of the supersenses have some labeled data in SemCor, the algorithm is\nguaranteed to have some representation for all possible senses by the time the algorithm backs off to the most general (supersense) information, although of course\nwith a very coarse model.\n12 A PPENDIX I ‚Ä¢ W ORD S ENSES AND W ORD N ET\n\nI.5 Alternate WSD algorithms and Tasks\n\n I.5.1 Feature-Based WSD\n Feature-based algorithms for WSD are extremely simple and function almost as\n well as contextual language model algorithms. The best performing IMS algorithm\n (Zhong and Ng, 2010), augmented by embeddings (Iacobacci et al. 2016, Raganato\n et al. 2017b), uses an SVM classifier to choose the sense for each input word with\n the following simple features of the surrounding words:\n ‚Ä¢ part-of-speech tags (for a window of 3 words on each side, stopping at sentence boundaries)\n collocation ‚Ä¢ collocation features of words or n-grams of lengths 1, 2, 3 at a particular\n location in a window of 3 words on each side (i.e., exactly one word to the\n right, or the two words starting 3 words to the left, and so on).\n ‚Ä¢ weighted average of embeddings (of all words in a window of 10 words on\n each side, weighted exponentially by distance)\n Consider the ambiguous word bass in the following WSJ sentence:\n (I.18) An electric guitar and bass player stand off to one side,\n If we used a small 2-word window, a standard feature vector might include parts-ofspeech, unigram and bigram collocation features, and a weighted sum g of embeddings, that is:\n\n [wi‚àí2 , POSi‚àí2 , wi‚àí1 , POSi‚àí1 , wi+1 , POSi+1 , wi+2 , POSi+2 , wi‚àí1\n i‚àí2 ,\n wi+2\n i+1 , g(E(wi‚àí2 ), E(wi‚àí1 ), E(wi+1 ), E(wi+2 )] (I.19)\n\n would yield the following vector:\n [guitar, NN, and, CC, player, NN, stand, VB, guitar and,\n player stand, g(E(guitar),E(and),E(player),E(stand))]\n\n I.5.2 The Lesk Algorithm as WSD Baseline\n Generating sense labeled corpora like SemCor is quite difficult and expensive. An\n knowledge- alternative class of WSD algorithms, knowledge-based algorithms, rely solely on\n based\n WordNet or other such resources and don‚Äôt require labeled data. While supervised\n algorithms generally work better, knowledge-based methods can be used in languages or domains where thesauruses or dictionaries but not sense labeled corpora\n are available.\nLesk algorithm The Lesk algorithm is the oldest and most powerful knowledge-based WSD\n method, and is a useful baseline. Lesk is really a family of algorithms that choose\n the sense whose dictionary gloss or definition shares the most words with the target\n word‚Äôs neighborhood. Figure I.10 shows the simplest version of the algorithm, often\nSimplified Lesk called the Simplified Lesk algorithm (Kilgarriff and Rosenzweig, 2000).\n As an example of the Lesk algorithm at work, consider disambiguating the word\n bank in the following context:\n (I.20) The bank can guarantee deposits will eventually cover future tuition costs\n because it invests in adjustable-rate mortgage securities.\n given the following two WordNet senses:\n I.5 ‚Ä¢ A LTERNATE WSD ALGORITHMS AND TASKS 13\n\n function S IMPLIFIED L ESK(word, sentence) returns best sense of word\n\n best-sense ‚Üê most frequent sense for word\n max-overlap ‚Üê 0\n context ‚Üê set of words in sentence\n for each sense in senses of word do\n signature ‚Üê set of words in the gloss and examples of sense\n overlap ‚Üê C OMPUTE OVERLAP(signature, context)\n if overlap > max-overlap then\n max-overlap ‚Üê overlap\n best-sense ‚Üê sense\n end\n return(best-sense)\n\n Figure I.10 The Simplified Lesk algorithm. The C OMPUTE OVERLAP function returns the\n number of words in common between two sets, ignoring function words or other words on a\n stop list. The original Lesk algorithm defines the context in a more complex way.\n\n bank1 Gloss: a financial institution that accepts deposits and channels the\n money into lending activities\n Examples: ‚Äúhe cashed a check at the bank‚Äù, ‚Äúthat bank holds the mortgage\n on my home‚Äù\n bank2 Gloss: sloping land (especially the slope beside a body of water)\n Examples: ‚Äúthey pulled the canoe up on the bank‚Äù, ‚Äúhe sat on the bank of\n the river and watched the currents‚Äù\n\n Sense bank1 has two non-stopwords overlapping with the context in (I.20): deposits and mortgage, while sense bank2 has zero words, so sense bank1 is chosen.\n There are many obvious extensions to Simplified Lesk, such as weighing the\n overlapping words by IDF (inverse document frequency) (Chapter 5) to downweight\n frequent words like function words; best performing is to use word embedding cosine instead of word overlap to compute the similarity between the definition and the\n context (Basile et al., 2014). Modern neural extensions of Lesk use the definitions\n to compute sense embeddings that can be directly used instead of SemCor-training\n embeddings (Kumar et al. 2019, Luo et al. 2018a, Luo et al. 2018b).\n\n I.5.3 Word-in-Context Evaluation\n Word Sense Disambiguation is a much more fine-grained evaluation of word meaning than the context-free word similarity tasks we described in Chapter 5. Recall that\n tasks like LexSim-999 require systems to match human judgments on the contextfree similarity between two words (how similar is cup to mug?). We can think of\n WSD as a kind of contextualized similarity task, since our goal is to be able to distinguish the meaning of a word like bass in one context (playing music) from another\n context (fishing).\nword-in-context Somewhere in between lies the word-in-context task. Here the system is given\n two sentences, each with the same target word but in a different sentential context.\n The system must decide whether the target words are used in the same sense in the\n WiC two sentences or in a different sense. Fig. I.11 shows sample pairs from the WiC\n dataset of Pilehvar and Camacho-Collados (2019).\n The WiC sentences are mainly taken from the example usages for senses in\n WordNet. But WordNet senses are very fine-grained. For this reason tasks like\n14 A PPENDIX I ‚Ä¢ W ORD S ENSES AND W ORD N ET\n\n F There‚Äôs a lot of trash on the bed of the river ‚Äî\n I keep a glass of water next to my bed when I sleep\n F Justify the margins ‚Äî The end justifies the means\n T Air pollution ‚Äî Open a window and let in some air\n T The expanded window will give us time to catch the thieves ‚Äî\n You have a two-hour window of clear weather to finish working on the lawn\n Figure I.11 Positive (T) and negative (F) pairs from the WiC dataset (Pilehvar and\n Camacho-Collados, 2019).\n\n word-in-context first cluster the word senses into coarser clusters, so that the two\n sentential contexts for the target word are marked as T if the two senses are in the\n same cluster. WiC clusters all pairs of senses if they are first degree connections in\n the WordNet semantic graph, including sister senses, or if they belong to the same\n supersense; we point to other sense clustering algorithms at the end of the chapter.\n The baseline algorithm to solve the WiC task uses contextual embeddings like\n BERT with a simple thresholded cosine. We first compute the contextual embeddings for the target word in each of the two sentences, and then compute the cosine\n between them. If it‚Äôs above a threshold tuned on a devset we respond true (the two\n senses are the same) else we respond false.\n\n I.5.4 Wikipedia as a source of training data\n Datasets other than SemCor have been used for all-words WSD. One important direction is to use Wikipedia as a source of sense-labeled data. When a concept is\n mentioned in a Wikipedia article, the article text may contain an explicit link to the\n concept‚Äôs Wikipedia page, which is named by a unique identifier. This link can be\n used as a sense annotation. For example, the ambiguous word bar is linked to a\n different Wikipedia article depending on its meaning in context, including the page\n BAR (L AW ), the page BAR (M USIC ), and so on, as in the following Wikipedia\n examples (Mihalcea, 2007).\n\n In 1834, Sumner was admitted to the [[bar (law)|bar]] at the age of\n twenty-three, and entered private practice in Boston.\n It is danced in 3/4 time (like most waltzes), with the couple turning\n approx. 180 degrees every [[bar (music)|bar]].\n Jenga is a popular beer in the [[bar (establishment)|bar]]s of Thailand.\n\n These sentences can then be added to the training data for a supervised system.\n In order to use Wikipedia in this way, however, it is necessary to map from Wikipedia concepts to whatever inventory of senses is relevant for the WSD application.\n Automatic algorithms that map from Wikipedia to WordNet, for example, involve\n finding the WordNet sense that has the greatest lexical overlap with the Wikipedia\n sense, by comparing the vector of words in the WordNet synset, gloss, and related\n senses with the vector of words in the Wikipedia page title, outgoing links, and page\n category (Ponzetto and Navigli, 2010). The resulting mapping has been used to\n create BabelNet, a large sense-annotated resource (Navigli and Ponzetto, 2012).\n I.6 ‚Ä¢ U SING T HESAURUSES TO I MPROVE E MBEDDINGS 15\n\nI.6 Using Thesauruses to Improve Embeddings\n Thesauruses have also been used to improve both static and contextual word embeddings. For example, static word embeddings have a problem with antonyms.\n A word like expensive is often very similar in embedding cosine to its antonym\n like cheap. Antonymy information from thesauruses can help solve this problem;\n Fig. I.12 shows nearest neighbors to some target words in GloVe, and the improvement after one such method.\n\n Before counterfitting After counterfitting\n east west north south eastward eastern easterly\n expensive pricey cheaper costly costly pricy overpriced\n British American Australian Britain Brits London BBC\n Figure I.12 The nearest neighbors in GloVe to east, expensive, and British include\n antonyms like west. The right side showing the improvement in GloVe nearest neighbors\n after the counterfitting method (MrksÃåicÃÅ et al., 2016).\n\n There are two families of solutions. The first requires retraining: we modify the\n embedding training to incorporate thesaurus relations like synonymy, antonym, or\n supersenses. This can be done by modifying the static embedding loss function for\n word2vec (Yu and Dredze 2014, Nguyen et al. 2016) or by modifying contextual\n embedding training (Levine et al. 2020, Lauscher et al. 2019).\n The second, for static embeddings, is more light-weight; after the embeddings\n have been trained we learn a second mapping based on a thesaurus that shifts the\n embeddings of words in such a way that synonyms (according to the thesaurus) are\n retrofitting pushed closer and antonyms further apart. Such methods are called retrofitting\n (Faruqui et al. 2015, Lengerich et al. 2018) or counterfitting (MrksÃåicÃÅ et al., 2016).\n\nI.7 Word Sense Induction\n It is expensive and difficult to build large corpora in which each word is labeled for\n its word sense. For this reason, an unsupervised approach to sense disambiguation,\n word sense often called word sense induction or WSI, is an important direction. In unsuinduction\n pervised approaches, we don‚Äôt use human-defined word senses. Instead, the set of\n ‚Äúsenses‚Äù of each word is created automatically from the instances of each word in\n the training set.\n Most algorithms for word sense induction follow the early work of SchuÃàtze\n (SchuÃàtze 1992, SchuÃàtze 1998) in using some sort of clustering over word embeddings. In training, we use three steps:\n 1. For each token wi of word w in a corpus, compute a context vector c.\n 2. Use a clustering algorithm to cluster these word-token context vectors c into\n a predefined number of groups or clusters. Each cluster defines a sense of w.\n 3. Compute the vector centroid of each cluster. Each vector centroid sj is a\n sense vector representing that sense of w.\n Since this is an unsupervised algorithm, we don‚Äôt have names for each of these\n ‚Äúsenses‚Äù of w; we just refer to the jth sense of w.\n To disambiguate a particular token t of w we again have three steps:\n16 A PPENDIX I ‚Ä¢ W ORD S ENSES AND W ORD N ET\n\n 1. Compute a context vector c for t.\n 2. Retrieve all sense vectors s j for w.\n 3. Assign t to the sense represented by the sense vector s j that is closest to t.\n All we need is a clustering algorithm and a distance metric between vectors.\n Clustering is a well-studied problem with a wide number of standard algorithms that\n can be applied to inputs structured as vectors of numerical values (Duda and Hart,\n 1973). A frequently used technique in language applications is known as agglomagglomerative\n clustering erative clustering. In this technique, each of the N training instances is initially\n assigned to its own cluster. New clusters are then formed in a bottom-up fashion by\n the successive merging of the two clusters that are most similar. This process continues until either a specified number of clusters is reached, or some global goodness\n measure among the clusters is achieved. In cases in which the number of training\n instances makes this method too expensive, random sampling can be used on the\n original training set to achieve similar results.\n How can we evaluate unsupervised sense disambiguation approaches? As usual,\n the best way is to do extrinsic evaluation embedded in some end-to-end system; one\n example used in a SemEval bakeoff is to improve search result clustering and diversification (Navigli and Vannella, 2013). Intrinsic evaluation requires a way to\n map the automatically derived sense classes into a hand-labeled gold-standard set so\n that we can compare a hand-labeled test set with a set labeled by our unsupervised\n classifier. Various such metrics have been tested, for example in the SemEval tasks\n (Manandhar et al. 2010, Navigli and Vannella 2013, Jurgens and Klapaftis 2013),\n including cluster overlap metrics, or methods that map each sense cluster to a predefined sense by choosing the sense that (in some training set) has the most overlap\n with the cluster. However it is fair to say that no evaluation metric for this task has\n yet become standard.\n\nI.8 Summary\n This chapter has covered a wide range of issues concerning the meanings associated\n with lexical items. The following are among the highlights:\n ‚Ä¢ A word sense is the locus of word meaning; definitions and meaning relations\n are defined at the level of the word sense rather than wordforms.\n ‚Ä¢ Many words are polysemous, having many senses.\n ‚Ä¢ Relations between senses include synonymy, antonymy, meronymy, and\n taxonomic relations hyponymy and hypernymy.\n ‚Ä¢ WordNet is a large database of lexical relations for English, and WordNets\n exist for a variety of languages.\n ‚Ä¢ Word-sense disambiguation (WSD) is the task of determining the correct\n sense of a word in context. Supervised approaches make use of a corpus\n of sentences in which individual words (lexical sample task) or all words\n (all-words task) are hand-labeled with senses from a resource like WordNet.\n SemCor is the largest corpus with WordNet-labeled senses.\n ‚Ä¢ The standard supervised algorithm for WSD is nearest neighbors with contextual embeddings.\n ‚Ä¢ Feature-based algorithms using parts of speech and embeddings of words in\n the context of the target word also work well.\n H ISTORICAL N OTES 17\n\n ‚Ä¢ An important baseline for WSD is the most frequent sense, equivalent, in\n WordNet, to take the first sense.\n ‚Ä¢ Another baseline is a knowledge-based WSD algorithm called the Lesk algorithm which chooses the sense whose dictionary definition shares the most\n words with the target word‚Äôs neighborhood.\n ‚Ä¢ Word sense induction is the task of learning word senses unsupervised.\n\nHistorical Notes\n Word sense disambiguation traces its roots to some of the earliest applications of\n digital computers. The insight that underlies modern algorithms for word sense disambiguation was first articulated by Weaver (1949/1955) in the context of machine\n translation:\n If one examines the words in a book, one at a time as through an opaque\n mask with a hole in it one word wide, then it is obviously impossible\n to determine, one at a time, the meaning of the words. [. . . ] But if\n one lengthens the slit in the opaque mask, until one can see not only\n the central word in question but also say N words on either side, then\n if N is large enough one can unambiguously decide the meaning of the\n central word. [. . . ] The practical question is : ‚ÄúWhat minimum value of\n N will, at least in a tolerable fraction of cases, lead to the correct choice\n of meaning for the central word?‚Äù\n Other notions first proposed in this early period include the use of a thesaurus for disambiguation (Masterman, 1957), supervised training of Bayesian models for disambiguation (Madhu and Lytel, 1965), and the use of clustering in word sense analysis\n (Sparck Jones, 1986).\n Much disambiguation work was conducted within the context of early AI-oriented\n natural language processing systems. Quillian (1968) and Quillian (1969) proposed\n a graph-based approach to language processing, in which the definition of a word\n was represented by a network of word nodes connected by syntactic and semantic\n relations, and sense disambiguation by finding the shortest path between senses in\n the graph. Simmons (1973) is another influential early semantic network approach.\n Wilks proposed one of the earliest non-discrete models with his Preference Semantics (Wilks 1975c, Wilks 1975b, Wilks 1975a), and Small and Rieger (1982) and\n Riesbeck (1975) proposed understanding systems based on modeling rich procedural information for each word. Hirst‚Äôs ABSITY system (Hirst and Charniak 1982,\n Hirst 1987, Hirst 1988), which used a technique called marker passing based on semantic networks, represents the most advanced system of this type. As with these\n largely symbolic approaches, early neural network (at the time called ‚Äòconnectionist‚Äô) approaches to word sense disambiguation relied on small lexicons with handcoded representations (Cottrell 1985, Kawamoto 1988).\n The earliest implementation of a robust empirical approach to sense disambiguation is due to Kelly and Stone (1975), who directed a team that hand-crafted a set\n of disambiguation rules for 1790 ambiguous English words. Lesk (1986) was the\n first to use a machine-readable dictionary for word sense disambiguation. Fellbaum\n (1998) collects early work on WordNet. Early work using dictionaries as lexical\n18 A PPENDIX I ‚Ä¢ W ORD S ENSES AND W ORD N ET\n\n resources include Amsler‚Äôs 1981 use of the Merriam Webster dictionary and Longman‚Äôs Dictionary of Contemporary English (Boguraev and Briscoe, 1989).\n Supervised approaches to disambiguation began with the use of decision trees\n by Black (1988). In addition to the IMS and contextual-embedding based methods\n for supervised WSD, recent supervised algorithms includes encoder-decoder models\n (Raganato et al., 2017a).\n The need for large amounts of annotated text in supervised methods led early\n on to investigations into the use of bootstrapping methods (Hearst 1991, Yarowsky\n 1995). For example the semi-supervised algorithm of Diab and Resnik (2002) is\n based on aligned parallel corpora in two languages. For example, the fact that the\n French word catastrophe might be translated as English disaster in one instance\n and tragedy in another instance can be used to disambiguate the senses of the two\n English words (i.e., to choose senses of disaster and tragedy that are similar).\n The earliest use of clustering in the study of word senses was by Sparck Jones\n (1986); Pedersen and Bruce (1997), SchuÃàtze (1997), and SchuÃàtze (1998) applied discoarse senses tributional methods. Clustering word senses into coarse senses has also been used\n to address the problem of dictionary senses being too fine-grained (Section I.5.3)\n (Dolan 1994, Chen and Chang 1998, Mihalcea and Moldovan 2001, Agirre and\n de Lacalle 2003, Palmer et al. 2004, Navigli 2006, Snow et al. 2007, Pilehvar et al.\n 2013). Corpora with clustered word senses for training supervised clustering algo-\nOntoNotes rithms include Palmer et al. (2006) and OntoNotes (Hovy et al., 2006).\n See Pustejovsky (1995), Pustejovsky and Boguraev (1996), Martin (1986), and\n Copestake and Briscoe (1995), inter alia, for computational approaches to the repgenerative resentation of polysemy. Pustejovsky‚Äôs theory of the generative lexicon, and in\n lexicon\n qualia particular his theory of the qualia structure of words, is a way of accounting for the\n structure\n dynamic systematic polysemy of words in context.\n Historical overviews of WSD include Agirre and Edmonds (2006) and Navigli\n (2009).\n\nExercises\n I.1 Collect a small corpus of example sentences of varying lengths from any\n newspaper or magazine. Using WordNet or any standard dictionary, determine how many senses there are for each of the open-class words in each sentence. How many distinct combinations of senses are there for each sentence?\n How does this number seem to vary with sentence length?\n I.2 Using WordNet or a standard reference dictionary, tag each open-class word\n in your corpus with its correct tag. Was choosing the correct sense always a\n straightforward task? Report on any difficulties you encountered.\n I.3 Using your favorite dictionary, simulate the original Lesk word overlap disambiguation algorithm described on page 13 on the phrase Time flies like an\n arrow. Assume that the words are to be disambiguated one at a time, from\n left to right, and that the results from earlier decisions are used later in the\n process.\n I.4 Build an implementation of your solution to the previous exercise. Using\n WordNet, implement the original Lesk word overlap disambiguation algorithm described on page 13 on the phrase Time flies like an arrow.\n Exercises 19\n\nAgirre, E. and O. L. de Lacalle. 2003. Clustering WordNet Hirst, G. 1987. Semantic Interpretation and the Resolution\n word senses. RANLP 2003. of Ambiguity. Cambridge University Press.\nAgirre, E. and P. Edmonds, eds. 2006. Word Sense Disam- Hirst, G. 1988. Resolving lexical ambiguity computationally\n biguation: Algorithms and Applications. Kluwer. with spreading activation and polaroid words. In S. L.\nAmsler, R. A. 1981. A taxonomy for English nouns and Small, G. W. Cottrell, and M. K. Tanenhaus, eds, Lexical\n verbs. ACL. Ambiguity Resolution, 73‚Äì108. Morgan Kaufmann.\nBasile, P., A. Caputo, and G. Semeraro. 2014. An enhanced Hirst, G. and E. Charniak. 1982. Word sense and case slot\n Lesk word sense disambiguation algorithm through a dis- disambiguation. AAAI.\n tributional semantic model. COLING. Hovy, E. H., M. P. Marcus, M. Palmer, L. A. Ramshaw,\n and R. Weischedel. 2006. OntoNotes: The 90% solution.\nBlack, E. 1988. An experiment in computational discrimi-\nHLT-NAACL.\n nation of English word senses. IBM Journal of Research\n and Development, 32(2):185‚Äì194. Iacobacci, I., M. T. Pilehvar, and R. Navigli. 2016. Embeddings for word sense disambiguation: An evaluation\nBoguraev, B. K. and T. Briscoe, eds. 1989. Computational\n study. ACL.\n Lexicography for Natural Language Processing. Longman. Jurgens, D. and I. P. Klapaftis. 2013. SemEval-2013 task 13:\n Word sense induction for graded and non-graded senses.\nChaplot, D. S. and R. Salakhutdinov. 2018. Knowledge- *SEM.\n based word sense disambiguation using topic models.\n AAAI. Kawamoto, A. H. 1988. Distributed representations of ambiguous words and their resolution in connectionist net-\nChen, J. N. and J. S. Chang. 1998. Topical clustering of works. In S. L. Small, G. W. Cottrell, and M. Tanen-\nMRD senses based on information retrieval techniques. haus, eds, Lexical Ambiguity Resolution, 195‚Äì228. Mor-\nComputational Linguistics, 24(1):61‚Äì96. gan Kaufman.\nCiaramita, M. and Y. Altun. 2006. Broad-coverage sense Kelly, E. F. and P. J. Stone. 1975. Computer Recognition of\n disambiguation and information extraction with a super- English Word Senses. North-Holland.\n sense sequence tagger. EMNLP.\n Kilgarriff, A. and J. Rosenzweig. 2000. Framework and re-\nCiaramita, M. and M. Johnson. 2003. Supersense tagging of sults for English SENSEVAL. Computers and the Huunknown nouns in WordNet. EMNLP-2003. manities, 34:15‚Äì48.\nCopestake, A. and T. Briscoe. 1995. Semi-productive Kumar, S., S. Jat, K. Saxena, and P. Talukdar. 2019. Zeropolysemy and sense extension. Journal of Semantics, shot word sense disambiguation using sense definition\n 12(1):15‚Äì68. embeddings. ACL.\nCottrell, G. W. 1985. A Connectionist Approach to Word Landes, S., C. Leacock, and R. I. Tengi. 1998. Building se-\nSense Disambiguation. Ph.D. thesis, University of mantic concordances. In C. Fellbaum, ed., WordNet: An\n Rochester, Rochester, NY. Revised version published by Electronic Lexical Database, 199‚Äì216. MIT Press.\n Pitman, 1989. Lauscher, A., I. VulicÃÅ, E. M. Ponti, A. Korhonen, and\nDiab, M. and P. Resnik. 2002. An unsupervised method for G. GlavasÃå. 2019. Informing unsupervised pretraining\n word sense tagging using parallel corpora. ACL. with external linguistic knowledge. ArXiv preprint\nDolan, B. 1994. Word sense ambiguation: Clustering related arXiv:1909.02339.\n senses. COLING. Lengerich, B., A. Maas, and C. Potts. 2018. Retrofitting dis-\nDuda, R. O. and P. E. Hart. 1973. Pattern Classification and tributional embeddings to knowledge graphs with func-\nScene Analysis. John Wiley and Sons. tional relations. COLING.\nFaruqui, M., J. Dodge, S. K. Jauhar, C. Dyer, E. Hovy, and Lesk, M. E. 1986. Automatic sense disambiguation using\n N. A. Smith. 2015. Retrofitting word vectors to semantic machine readable dictionaries: How to tell a pine cone\n lexicons. NAACL HLT. from an ice cream cone. Proceedings of the 5th International Conference on Systems Documentation.\nFellbaum, C., ed. 1998. WordNet: An Electronic Lexical\n Database. MIT Press. Levine, Y., B. Lenz, O. Dagan, O. Ram, D. Padnos, O. Sharir, S. Shalev-Shwartz, A. Shashua, and\nGale, W. A., K. W. Church, and D. Yarowsky. 1992a. Es- Y. Shoham. 2020. SenseBERT: Driving some sense into\n timating upper and lower bounds on the performance of BERT. ACL.\n word-sense disambiguation programs. ACL.\n Loureiro, D. and A. Jorge. 2019. Language modelling makes\nGale, W. A., K. W. Church, and D. Yarowsky. 1992b. One sense: Propagating representations through WordNet for\n sense per discourse. HLT. full-coverage word sense disambiguation. ACL.\nHaber, J. and M. Poesio. 2020. Assessing polyseme sense Luo, F., T. Liu, Z. He, Q. Xia, Z. Sui, and B. Chang. 2018a.\n similarity through co-predication acceptability and con- Leveraging gloss knowledge in neural word sense disamtextualised embedding distance. *SEM. biguation by hierarchical co-attention. EMNLP.\nHearst, M. A. 1991. Noun homograph disambiguation. Pro- Luo, F., T. Liu, Q. Xia, B. Chang, and Z. Sui. 2018b. Incorceedings of the 7th Conference of the University of Wa- porating glosses into neural word sense disambiguation.\n terloo Centre for the New OED and Text Research. ACL.\nHenrich, V., E. Hinrichs, and T. Vodolazova. 2012. We- Madhu, S. and D. Lytel. 1965. A figure of merit technique for\n bCAGe ‚Äì a web-harvested corpus annotated with Ger- the resolution of non-grammatical ambiguity. Mechanical\n maNet senses. EACL. Translation, 8(2):9‚Äì13.\n20 Appendix I ‚Ä¢ Word Senses and WordNet\n\nManandhar, S., I. P. Klapaftis, D. Dligach, and S. Pradhan. Ponzetto, S. P. and R. Navigli. 2010. Knowledge-rich word\n 2010. SemEval-2010 task 14: Word sense induction & sense disambiguation rivaling supervised systems. ACL.\n disambiguation. SemEval. Pu, X., N. Pappas, J. Henderson, and A. Popescu-Belis.\nMartin, J. H. 1986. The acquisition of polysemy. ICML. 2018. Integrating weakly supervised word sense disam-\nMasterman, M. 1957. The thesaurus in syntax and semantics. biguation into neural machine translation. TACL, 6:635‚Äì\n Mechanical Translation, 4(1):1‚Äì2. 649.\nMelamud, O., J. Goldberger, and I. Dagan. 2016. con- Pustejovsky, J. 1995. The Generative Lexicon. MIT Press.\n text2vec: Learning generic context embedding with bidi- Pustejovsky, J. and B. K. Boguraev, eds. 1996. Lexical Serectional LSTM. CoNLL. mantics: The Problem of Polysemy. Oxford University\nMihalcea, R. 2007. Using Wikipedia for automatic word Press.\n sense disambiguation. NAACL-HLT. Quillian, M. R. 1968. Semantic memory. In M. Minsky, ed.,\nMihalcea, R. and D. Moldovan. 2001. Automatic genera- Semantic Information Processing, 227‚Äì270. MIT Press.\n tion of a coarse grained WordNet. NAACL Workshop on Quillian, M. R. 1969. The teachable language compre-\nWordNet and Other Lexical Resources. hender: A simulation program and theory of language.\nMiller, G. A., C. Leacock, R. I. Tengi, and R. T. Bunker. CACM, 12(8):459‚Äì476.\n 1993. A semantic concordance. HLT. Raganato, A., C. D. Bovi, and R. Navigli. 2017a. Neural se-\nMorris, W., ed. 1985. American Heritage Dictionary, 2nd quence learning models for word sense disambiguation.\n college edition edition. Houghton Mifflin. EMNLP.\nMrksÃåicÃÅ, N., D. OÃÅ. SeÃÅaghdha, B. Thomson, M. GasÃåicÃÅ, L. M. Raganato, A., J. Camacho-Collados, and R. Navigli. 2017b.\n Rojas-Barahona, P.-H. Su, D. Vandyke, T.-H. Wen, and Word sense disambiguation: A unified evaluation frame-\nS. Young. 2016. Counter-fitting word vectors to linguis- work and empirical comparison. EACL.\n tic constraints. NAACL HLT.\n Riesbeck, C. K. 1975. Conceptual analysis. In R. C. Schank,\nNavigli, R. 2006. Meaningful clustering of senses helps ed., Conceptual Information Processing, 83‚Äì156. Ameriboost word sense disambiguation performance. COL- can Elsevier, New York.\n ING/ACL.\n Schneider, N., J. D. Hwang, V. Srikumar, J. Prange, A. Blod-\nNavigli, R. 2009. Word sense disambiguation: A survey. gett, S. R. Moeller, A. Stern, A. Bitan, and O. Abend.\n ACM Computing Surveys, 41(2). 2018. Comprehensive supersense disambiguation of En-\nNavigli, R. 2016. Chapter 20. ontologies. In R. Mitkov, ed., glish prepositions and possessives. ACL.\n The Oxford handbook of computational linguistics. Ox- SchuÃàtze, H. 1992. Dimensions of meaning. Proceedings of\n ford University Press. Supercomputing ‚Äô92. IEEE Press.\nNavigli, R. and S. P. Ponzetto. 2012. BabelNet: The auto- SchuÃàtze, H. 1997. Ambiguity Resolution in Language Learnmatic construction, evaluation and application of a wide- ing: Computational and Cognitive Models. CSLI Publicoverage multilingual semantic network. Artificial Intel- cations, Stanford, CA.\n ligence, 193:217‚Äì250.\n SchuÃàtze, H. 1998. Automatic word sense discrimination.\nNavigli, R. and D. Vannella. 2013. SemEval-2013 task 11:\n Computational Linguistics, 24(1):97‚Äì124.\n Word sense induction and disambiguation within an enduser application. *SEM. Simmons, R. F. 1973. Semantic networks: Their computation and use for understanding English sentences. In\nNguyen, K. A., S. Schulte im Walde, and N. T. Vu. 2016.\n R. C. Schank and K. M. Colby, eds, Computer Models of\n Integrating distributional lexical contrast into word em-\nThought and Language, 61‚Äì113. W.H. Freeman & Co.\n beddings for antonym-synonym distinction. ACL.\n Small, S. L. and C. Rieger. 1982. Parsing and comprehend-\nPalmer, M., O. Babko-Malaya, and H. T. Dang. 2004. Difing with Word Experts. In W. G. Lehnert and M. H.\n ferent sense granularities for different applications. HLT-\nRingle, eds, Strategies for Natural Language Processing,\n NAACL Workshop on Scalable Natural Language Under-\n89‚Äì147. Lawrence Erlbaum.\n standing.\nPalmer, M., H. T. Dang, and C. Fellbaum. 2006. Making Snow, R., S. Prakash, D. Jurafsky, and A. Y. Ng. 2007.\n fine-grained and coarse-grained sense distinctions, both Learning to merge word senses. EMNLP/CoNLL.\n manually and automatically. Natural Language Engineer- Snyder, B. and M. Palmer. 2004. The English all-words task.\n ing, 13(2):137‚Äì163. SENSEVAL-3.\nPedersen, T. and R. Bruce. 1997. Distinguishing word senses Sparck Jones, K. 1986. Synonymy and Semantic Classificain untagged text. EMNLP. tion. Edinburgh University Press, Edinburgh. Republica-\nPeters, M., M. Neumann, M. Iyyer, M. Gardner, C. Clark, tion of 1964 PhD Thesis.\n K. Lee, and L. Zettlemoyer. 2018. Deep contextualized Tsvetkov, Y., N. Schneider, D. Hovy, A. Bhatia, M. Faruqui,\n word representations. NAACL HLT. and C. Dyer. 2014. Augmenting English adjective senses\nPilehvar, M. T. and J. Camacho-Collados. 2019. WiC: the with supersenses. LREC.\n word-in-context dataset for evaluating context-sensitive Vossen, P., A. GoÃàroÃàg, F. Laan, M. Van Gompel, R. Izquierdo,\n meaning representations. NAACL HLT. and A. Van Den Bosch. 2011. Dutch-semcor: building a\nPilehvar, M. T., D. Jurgens, and R. Navigli. 2013. Align, semantically annotated corpus for dutch. Proceedings of\n disambiguate and walk: A unified approach for measur- eLex.\n ing semantic similarity. ACL.\n Exercises 21\n\nWeaver, W. 1949/1955. Translation. In W. N. Locke and\n A. D. Boothe, eds, Machine Translation of Languages,\n 15‚Äì23. MIT Press. Reprinted from a memorandum written by Weaver in 1949.\nWilks, Y. 1975a. An intelligent analyzer and understander of\n English. CACM, 18(5):264‚Äì274.\nWilks, Y. 1975b. Preference semantics. In E. L. Keenan,\n ed., The Formal Semantics of Natural Language, 329‚Äì\n 350. Cambridge Univ. Press.\nWilks, Y. 1975c. A preferential, pattern-seeking, semantics for natural language inference. Artificial Intelligence,\n 6(1):53‚Äì74.\nYarowsky, D. 1995. Unsupervised word sense disambiguation rivaling supervised methods. ACL.\nYu, M. and M. Dredze. 2014. Improving lexical embeddings\n with semantic knowledge. ACL.\nZhong, Z. and H. T. Ng. 2010. It makes sense: A widecoverage word sense disambiguation system for free text.\n ACL.\n",
    "file_path": "/Users/colinsidberry/nlp-textbook-rag/data/extracted_text/I.Word Senses and WordNet.txt",
    "file_size_kb": 63.06
  },
  {
    "id": "bf9e6137b7bc927e",
    "source": "nlp_textbook",
    "chapter": "Pointwise Mutual Information J (PMI)",
    "filename": "J.Pointwise Mutual Information (PMI).txt",
    "content": "CHAPTER\n\n Pointwise Mutual Information\nJ (PMI)\n An alternative weighting function to tf-idf, PPMI (positive pointwise mutual information), is used for term-term-matrices, when the vector dimensions correspond to\n words rather than documents. PPMI draws on the intuition that the best way to weigh\n the association between two words is to ask how much more the two words co-occur\n in our corpus than we would have a priori expected them to appear by chance.\n pointwise\n mutual Pointwise mutual information (Fano, 1961)1 is one of the most important coninformation\n cepts in NLP. It is a measure of how often two events x and y occur, compared with\n what we would expect if they were independent:\n\n P(x, y)\n I(x, y) = log2 (J.2)\n P(x)P(y)\n The pointwise mutual information between a target word w and a context word\n c (Church and Hanks 1989, Church and Hanks 1990) is then defined as:\n P(w, c)\n PMI(w, c) = log2 (J.3)\n P(w)P(c)\n The numerator tells us how often we observed the two words together (assuming\n we compute probability by using the MLE). The denominator tells us how often\n we would expect the two words to co-occur assuming they each occurred independently; recall that the probability of two independent events both occurring is just\n the product of the probabilities of the two events. Thus, the ratio gives us an estimate of how much more the two words co-occur than we expect by chance. PMI is\n a useful tool whenever we need to find words that are strongly associated.\n PMI values range from negative to positive infinity. But negative PMI values\n (which imply things are co-occurring less often than we would expect by chance)\n tend to be unreliable unless our corpora are enormous. To distinguish whether\n two words whose individual probability is each 10‚àí6 occur together less often than\n chance, we would need to be certain that the probability of the two occurring together is significantly less than 10‚àí12 , and this kind of granularity would require an\n enormous corpus. Furthermore it‚Äôs not clear whether it‚Äôs even possible to evaluate\n such scores of ‚Äòunrelatedness‚Äô with human judgments. For this reason it is more\n PPMI common to use Positive PMI (called PPMI) which replaces all negative PMI values\n with zero (Church and Hanks 1989, Dagan et al. 1993, Niwa and Nitta 1994)2 :\n P(w, c)\n PPMI(w, c) = max(log2 , 0) (J.4)\n P(w)P(c)\n 1 PMI is based on the mutual information between two random variables X and Y , defined as:\n XX P(x, y)\n I(X,Y ) = P(x, y) log2 (J.1)\n x y\n P(x)P(y)\n In a confusion of terminology, Fano used the phrase mutual information to refer to what we now call\n pointwise mutual information and the phrase expectation of the mutual information for what we now call\n mutual information\n 2 Positive PMI also cleanly solves the problem of what to do with zero counts, using 0 to replace the\n ‚àí‚àû from log(0).\n\n2 A PPENDIX J ‚Ä¢ P OINTWISE M UTUAL I NFORMATION (PMI)\n\n More formally, let‚Äôs assume we have a co-occurrence matrix F with W rows (words)\n and C columns (contexts), where fi j gives the number of times word wi occurs with\n context c j . This can be turned into a PPMI matrix where PPMIi j gives the PPMI\n value of word wi with context c j (which we can also express as PPMI(wi , c j ) or\n PPMI(w = i, c = j)) as follows:\n PC PW\n fi j j=1 f i j i=1 f i j\n pi j = PW PC , pi‚àó = PW PC , p‚àó j = PW P C (J.5)\n i0 =1 j0 =1 f i0 j0 i0 =1 j0 =1 f i0 j0 i0 =1 j0 =1 f i0 j0\n\n pi j\n PPMIi j = max(log2 , 0) (J.6)\n pi‚àó p‚àó j\n Let‚Äôs see some PPMI calculations. We‚Äôll use Fig. J.2, which repeats Fig. J.1 plus all\n the count marginals, and let‚Äôs pretend for ease of calculation that these are the only\n words/contexts that matter.\n Here‚Äôs the original figure:\n\n aardvark ... computer data result pie sugar ...\n cherry 0 ... 2 8 9 442 25 ...\n strawberry 0 ... 0 0 1 60 19 ...\n digital 0 ... 1670 1683 85 5 4 ...\n information 0 ... 3325 3982 378 5 13 ...\n Figure J.1 Co-occurrence vectors for four words in the Wikipedia corpus, showing six of\n the dimensions (hand-picked for pedagogical purposes). The vector for digital is outlined in\n red. Note that a real vector would have vastly more dimensions and thus be much sparser, i.e.\n would have zero values in most dimensions.\n\n computer data result pie sugar count(w)\n cherry 2 8 9 442 25 486\n strawberry 0 0 1 60 19 80\n digital 1670 1683 85 5 4 3447\n information 3325 3982 378 5 13 7703\n\n count(context) 4997 5673 473 512 61 11716\n Figure J.2 Co-occurrence counts for four words in 5 contexts in the Wikipedia corpus,\n together with the marginals, pretending for the purpose of this calculation that no other\n words/contexts matter.\n\n Thus for example we could compute PPMI(information,data), assuming we pretended that Fig. J.1 encompassed all the relevant word contexts/dimensions, as follows:\n P(w=information, c=data) = = .3399\n P(w=information) = = .6575\n P(c=data) = = .4842\n PPMI(information,data) = log2 (.3399/(.6575 ‚àó .4842)) = .0944\n\n Fig. J.3 shows the joint probabilities computed from the counts in Fig. J.2, and\n Fig. J.4 shows the PPMI values. Not surprisingly, cherry and strawberry are highly\n associated with both pie and sugar, and data is mildly associated with information.\n\n p(w,context) p(w)\n computer data result pie sugar p(w)\n cherry 0.0002 0.0007 0.0008 0.0377 0.0021 0.0415\n strawberry 0.0000 0.0000 0.0001 0.0051 0.0016 0.0068\n digital 0.1425 0.1436 0.0073 0.0004 0.0003 0.2942\ninformation 0.2838 0.3399 0.0323 0.0004 0.0011 0.6575\n\n p(context) 0.4265 0.4842 0.0404 0.0437 0.0052\nFigure J.3 Replacing the counts in Fig. J.1 with joint probabilities, showing the marginals\nin the right column and the bottom row.\n\n computer data result pie sugar\n cherry 0 0 0 4.38 3.30\n strawberry 0 0 0 4.10 5.51\n digital 0.18 0.01 0 0 0\ninformation 0.02 0.09 0.28 0 0\nFigure J.4 The PPMI matrix showing the association between words and context words,\ncomputed from the counts in Fig. J.3. Note that most of the 0 PPMI values are ones that had a\nnegative PMI; for example PMI(cherry,computer) = -6.7, meaning that cherry and computer\nco-occur on Wikipedia less often than we would expect by chance, and with PPMI we replace\nnegative values by zero.\n\n PMI has the problem of being biased toward infrequent events; very rare words\ntend to have very high PMI values. One way to reduce this bias toward low frequency\nevents is to slightly change the computation for P(c), using a different function PŒ± (c)\nthat raises the probability of the context word to the power of Œ±:\n\n P(w, c)\n PPMIŒ± (w, c) = max(log2 , 0) (J.7)\n P(w)PŒ± (c)\n count(c)Œ±\n PŒ± (c) = P (J.8)\n c count(c)\n Œ±\n\n Levy et al. (2015) found that a setting of Œ± = 0.75 improved performance of\nembeddings on a wide range of tasks (drawing on a similar weighting used for skipgrams described in Chapter 5. This works because raising the count to Œ± = 0.75\nincreases the probability assigned to rare contexts, and hence lowers their PMI\n(PŒ± (c) > P(c) when c is rare).\n Another possible solution is Laplace smoothing: Before computing PMI, a small\nconstant k (values of 0.1-3 are common) is added to each of the counts, shrinking\n(discounting) all the non-zero values. The larger the k, the more the non-zero counts\nare discounted.\n4 Appendix J ‚Ä¢ Pointwise Mutual Information (PMI)\n\nChurch, K. W. and P. Hanks. 1989. Word association norms,\n mutual information, and lexicography. ACL.\nChurch, K. W. and P. Hanks. 1990. Word association norms,\n mutual information, and lexicography. Computational\n Linguistics, 16(1):22‚Äì29.\nDagan, I., S. Marcus, and S. Markovitch. 1993. Contextual\n word similarity and estimation from sparse data. ACL.\nFano, R. M. 1961. Transmission of Information: A Statistical\n Theory of Communications. MIT Press.\nLevy, O., Y. Goldberg, and I. Dagan. 2015. Improving distributional similarity with lessons learned from word embeddings. TACL, 3:211‚Äì225.\nNiwa, Y. and Y. Nitta. 1994. Co-occurrence vectors from\n corpora vs. distance vectors from dictionaries. COLING.\n",
    "file_path": "/Users/colinsidberry/nlp-textbook-rag/data/extracted_text/J.Pointwise Mutual Information (PMI).txt",
    "file_size_kb": 7.81
  },
  {
    "id": "dea598cd9d66403b",
    "source": "nlp_textbook",
    "chapter": "Frame-Based Dialogue Systems",
    "filename": "K.Frame-Based Dialogue Systems.txt",
    "content": "rights reserved. Draft of August 24, 2025.\n\nCHAPTER\n\n Frame-Based Dialogue Systems\nK A task-based dialogue system has the goal of helping a user solve a specific task\n like making a travel reservation or buying a product. Task-based dialogue systems\n frame are based around frames, first introduced in the early influential GUS system for\n GUS travel planning (Bobrow et al., 1977). Frames are knowledge structures representing\n the details of the user‚Äôs task specification. Each frame consists of a collection of\n slot slots, each of which can take a set of possible values. Together a set of frames is\n sometimes called a domain ontology.\n Here we‚Äôll describe the most well-studied frame-based architecture, the dialoguestate architecture, made up of the six components shown in Fig. K.1. In the next\n sections we‚Äôll introduce four of them, after introducing the idea of frames (deferring\n the speech recognition and synthesis components to Chapter 15).\n\nFigure K.1 Architecture of a dialogue-state system for task-oriented dialogue from Williams et al. (2016).\n\n K.0.1 Frames and Slot Filling\n The frame and its slots in a task-based dialogue system specify what the system\n needs to know to perform its task. A hotel reservation system needs dates and locations. An alarm clock system needs a time. The system‚Äôs goal is to fill the slots in\n the frame with the fillers the user intends, and then perform the relevant action for\n the user (answering a question, or booking a flight).\n Fig. K.2 shows a sample frame for booking air travel, with some sample questions used for filling slots. In the simplest frame-based systems (including most commercial assistants until quite recently), these questions are pre-written templates, but\n2 A PPENDIX K ‚Ä¢ F RAME -BASED D IALOGUE S YSTEMS\n\n in more sophisticated systems, questions are generated on-the-fly. The slot fillers are\n often constrained to a particular semantic type, like type CITY (taking on values like\n San Francisco, or Hong Kong) or DATE, AIRLINE, or TIME.\n\n Slot Type Example Question\n ORIGIN CITY city ‚ÄúFrom what city are you leaving?‚Äù\n DESTINATION CITY city ‚ÄúWhere are you going?‚Äù\n DEPARTURE TIME time ‚ÄúWhen would you like to leave?‚Äù\n DEPARTURE DATE date ‚ÄúWhat day would you like to leave?‚Äù\n ARRIVAL TIME time ‚ÄúWhen do you want to arrive?‚Äù\n ARRIVAL DATE date ‚ÄúWhat day would you like to arrive?‚Äù\n Figure K.2 A frame in a frame-based dialogue system, showing the type of each slot and a\n sample question used to fill the slot.\n\n Many domains require multiple frames. Besides frames for car or hotel reservations, we might need other frames for things like general route information (for\n questions like Which airlines fly from Boston to San Francisco?), That means the\n system must be able to disambiguate which slot of which frame a given input is\n supposed to fill.\n The task of slot-filling is usually combined with two other tasks, to extract 3\n things from each user utterance. The first is domain classification: is this user for\n example talking about airlines, programming an alarm clock, or dealing with their\n intent calendar? The second is user intent determination: what general task or goal is the\n determination\n user trying to accomplish? For example the task could be to Find a Movie, or Show\n a Flight, or Remove a Calendar Appointment. Together, the domain classification\n and intent determination tasks decide which frame we are filling. Finally, we need\n slot filling to do slot filling itself: extract the particular slots and fillers that the user intends the\n system to understand from their utterance with respect to their intent. From a user\n utterance like this one:\n Show me morning flights from Boston to San Francisco on Tuesday\n a system might want to build a representation like:\n DOMAIN: AIR-TRAVEL INTENT: SHOW-FLIGHTS\n ORIGIN-CITY: Boston DEST-CITY: San Francisco\n ORIGIN-DATE: Tuesday ORIGIN-TIME: morning\n\n Similarly an utterance like this: should give an intent like this:\n Wake me tomorrow at 6 DOMAIN: ALARM-CLOCK\n INTENT: SET-ALARM\n TIME: 2017-07-01 0600\n The simplest dialogue systems use handwritten rules for slot-filling, like this\n regular expression for recognizing the SET-ALARM intent:\n wake me (up) | set (the|an) alarm | get me up\n But most systems use supervised machine-learning: each sentence in a training\n set is annotated with slots, domain, and intent, and a sequence model maps from\n input words to slot fillers, domain and intent. For example we‚Äôll have pairs of sentences that are labeled for domain (AIRLINE) and intent (SHOWFLIGHT), and are also\n labeled with BIO representations for the slots and fillers. (Recall from Chapter 17\n that in BIO tagging we introduce a tag for the beginning (B) and inside (I) of each\n slot label, and one for tokens outside (O) any slot label.)\n\nO O O O O B-DES I-DES O B-DEPTIME I-DEPTIME O AIRLINE-SHOWFLIGHT\nI want to fly to San Francisco on Monday afternoon please EOS\n\n Fig. K.3 shows a typical architecture for inference. The input words w1 ...wn\n are passed through a pretrained language model encoder, followed by a feedforward\n layer and a softmax at each token position over possible BIO tags, with the output\n a series of BIO tags s1 ...sn . We generally combine the domain-classification and\n intent-extraction tasks with slot-filling by adding a domain concatenated with an\n intent as the desired output for the final EOS token.\n\n B-DES I-DES O B-DTIME d+i\n\n Classifier\n +softmax\n\n Encodings\n\n Encoder\n ‚Ä¶ San Francisco on Monday <EOS>\n Figure K.3 Slot filling by passing input words through an encoder, and then using a linear\n or feedforward layer followed by a softmax to generate a series of BIO tags. Here we also\n show a final state: a domain concatenated with an intent.\n\n Once the sequence labeler has tagged the user utterance, a filler string can be extracted for each slot from the tags (e.g., ‚ÄúSan Francisco‚Äù), and these word strings\n can then be normalized to the correct form in the ontology (perhaps the airport\n code ‚ÄòSFO‚Äô), for example with dictionaries that specify that SF, SFO, and San Francisco are synonyms. Often in industrial contexts, combinations of rules and machine\n learning are used for each of these components.\n We can make a very simple frame-based dialogue system by wrapping a small\n amount of code around this slot extractor. Mainly we just need to ask the user\n questions until all the slots are full, do a database query, then report back to the user,\n using hand-built templates for generating sentences.\n\n K.0.2 Evaluating Task-Based Dialogue\ntask error rate We evaluate task-based systems by computing the task error rate, or task success\n rate: the percentage of times the system booked the right plane flight, or put the\n right event on the calendar. A more fine-grained, but less extrinsic metric is the slot\nslot error rate error rate, the percentage of slots filled with the correct values:\n\n # of inserted/deleted/subsituted slots\n Slot Error Rate for a Sentence = (K.1)\n # of total reference slots for sentence\n For example a system that extracted the slot structure below from this sentence:\n (K.2) Make an appointment with Chris at 10:30 in Gates 104\n Slot Filler\n PERSON Chris\n TIME 11:30 a.m.\n ROOM Gates 104\n4 A PPENDIX K ‚Ä¢ F RAME -BASED D IALOGUE S YSTEMS\n\n has a slot error rate of 1/3, since the TIME is wrong. Instead of error rate, slot\n precision, recall, and F-score can also be used. We can also measure efficiency\nefficiency costs costs like the length of the dialogue in seconds or turns.\n\nK.1 Dialogue Acts and Dialogue State\n While the naive slot-extractor system described above can handle simple dialogues,\n often we want more complex interactions. For example, we might want to confirm\n that we‚Äôve understand the user, or ask them to repeat themselves. We can build a\n more sophisticated system using dialogue acts and dialogue state.\n\n K.1.1 Dialogue Acts\n dialogue acts Dialogue acts are a generalization of speech acts that also represent grounding. The\n set of acts can be general, or can be designed for particular dialogue tasks.\n\n Tag Sys User Description\n HELLO (a = x, b = y, ...) X X Open a dialogue and give info a = x, b = y, ...\n INFORM (a = x, b = y, ...) X X Give info a = x, b = y, ...\n REQUEST(a, b = x, ...) X X Request value for a given b = x, ...\n REQALTS (a = x, ...) œá X Request alternative with a = x, ...\n CONFIRM (a = x, b = y, ...) X X Explicitly confirm a = x, b = y, ...\n CONFREQ (a = x, ..., d) X œá Implicitly confirm a = x, ... and request value of d\n SELECT(a = x, a = y) X œá Implicitly confirm a = x, ... and request value of d\n AFFIRM (a = x, b = y, ...) X X Affirm and give further info a = x, b = y, ...\n NEGATE (a = x) œá X Negate and give corrected value a = x\n DENY (a = x) œá X Deny that a = x\n BYE() X X Close a dialogue\n Figure K.4 Dialogue acts used by the HIS restaurant recommendation system of Young\n et al. (2010). The Sys and User columns indicate which acts are valid as system outputs and\n user inputs, respectively.\n\n Figure K.4 shows a tagset for a restaurant recommendation system, and Fig. K.5\n shows these tags labeling a sample dialogue from the HIS system (Young et al.,\n 2010). This example also shows the content of each dialogue act, which are the slot\n fillers being communicated. So the user might INFORM the system that they want\n Italian food near a museum, or CONFIRM with the system that the price is reasonable.\n\n K.1.2 Dialogue State Tracking\n The job of the dialogue-state tracker is to determine the current state of the frame\n (the fillers of each slot), and the user‚Äôs most recent dialogue act. The dialogue-state\n is not just the slot-fillers in the current sentence; it includes the entire state of the\n frame at this point, summarizing all of the user‚Äôs constraints. Fig. K.6 from MrksÃåicÃÅ\n et al. (2017) shows the dialogue state after each turn.\n Dialogue act detection is done just like domain or intent classification, by passing\n the input sentence through an encoder and adding an act classifier. Often passing in\n the prior dialogue act as well can improve classification. And since dialogue acts\n place some constraints on the slots and values, the tasks of dialogue-act detection\n and slot-filling are often performed jointly. The state tracker can just take the output\n K.1 ‚Ä¢ D IALOGUE ACTS AND D IALOGUE S TATE 5\n\n Utterance Dialogue act\nU: Hi, I am looking for somewhere to eat. hello(task = find,type=restaurant)\nS: You are looking for a restaurant. What confreq(type = restaurant, food)\n type of food do you like?\nU: I‚Äôd like an Italian near the museum. inform(food = Italian, near=museum)\nS: Roma is a nice Italian restaurant near inform(name = \"Roma\", type = restaurant,\n the museum. food = Italian, near = museum)\nU: Is it reasonably priced? confirm(pricerange = moderate)\nS: Yes, Roma is in the moderate price affirm(name = \"Roma\", pricerange =\n range. moderate)\nU: What is the phone number? request(phone)\nS: The number of Roma is 385456. inform(name = \"Roma\", phone = \"385456\")\nU: Ok, thank you goodbye. bye()\nFigure K.5 A dialogue from the HIS System of Young et al. (2010) using the dialogue acts in Fig. K.4.\n\nUser: I‚Äôm looking for a cheaper restaurant\n inform(price=cheap)\nSystem: Sure. What kind - and where?\nUser: Thai food, somewhere downtown\n inform(price=cheap, food=Thai, area=centre)\nSystem: The House serves cheap Thai food\nUser: Where is it?\n inform(price=cheap, food=Thai, area=centre); request(address)\nSystem: The House is at 106 Regent Street\nFigure K.6 The output of the dialogue state tracker after each turn (MrksÃåicÃÅ et al., 2017).\n\n of a slot-filling sequence-model (Section K.0.1) after each sentence, or do something\n more complicated like training a classifier to decide if a value has been changed.\n\n A special case: detecting correction acts. If a dialogue system misrecognizes\n or misunderstands an utterance, users will repeat or reformulate the utterance. Deuser correction tecting these user correction acts is quite important, especially for spoken lanacts\n guage. Ironically, corrections are actually harder to recognize than normal sentences\n (Swerts et al., 2000), because users who are frustrated adjust their speech in a way\n that is difficult for speech recognizers (Goldberg et al., 2003). For example speakhyperarticula- ers often use a prosodic style for corrections called hyperarticulation, in which the\n tion\n utterance is louder or longer or exaggerated in pitch, such as I said BAL-TI-MORE,\n not Boston (Wade et al. 1992, Levow 1998, Hirschberg et al. 2001). Detecting acts\n can be part of the general dialogue act detection classifier, or can make use of special features beyond the words, like those shown below (Levow 1998, Litman et al.\n 1999, Hirschberg et al. 2001, Bulyko et al. 2005, Awadallah et al. 2015).\n\nfeatures examples\nsemantic embedding similarity between correction and user‚Äôs prior utterance\nphonetic phonetic overlap between candidate correction act and user‚Äôs prior utterance\n (i.e. ‚ÄúWhatsApp‚Äù may be incorrectly recognized as ‚ÄúWhat‚Äôs up‚Äù)\nprosodic hyperarticulation, increases in F0 range, pause duration, and word duration\nASR ASR confidence, language model probability\n6 A PPENDIX K ‚Ä¢ F RAME -BASED D IALOGUE S YSTEMS\n\n K.1.3 Dialogue Policy: Which act to generate\n In early commercial frame-based systems, the dialogue policy is simple: ask questions until all the slots are full, do a database query, then report back to the user. A\ndialogue policy more sophisticated dialogue policy can help a system decide when to answer the\n user‚Äôs questions, when to instead ask the user a clarification question, and so on. A\n dialogue policy thus decides what dialogue act to generate. Choosing a dialogue act\n content\n planning to generate, along with its arguments, is sometimes called content planning.\n Let‚Äôs see how to do this for some important dialogue acts. Dialogue systems, especially speech systems, often misrecognize the users‚Äô words or meaning. To ensure\n system and user share a common ground, systems must confirm understandings with\n the user or reject utterances that the system don‚Äôt understand. A system might use\n explicit an explicit confirmation act to confirm with the user, like Is that correct? below:\n confirmation\n\n U: I‚Äôd like to fly from Denver Colorado to New York City on September\n twenty first in the morning on United Airlines\n S: Let‚Äôs see then. I have you going from Denver Colorado to New York\n on September twenty first. Is that correct?\n implicit When using an implicit confirmation act, a system instead grounds more imconfirmation\n plicitly, for example by repeating the system‚Äôs understanding as part of asking the\n next question, as Shanghai is confirmed in passing in this example:\n\n U: I want to travel to to Shanghai\n S: When do you want to travel to Shanghai?\n\n There‚Äôs a tradeoff. Explicit confirmation makes it easier for users to correct\n misrecognitions by just answering ‚Äúno‚Äù to the confirmation question. But explicit\n confirmation is time-consuming and awkward (Danieli and Gerbino 1995, Walker\n et al. 1998). We also might want an act that expresses lack of understanding: rejecrejection tion, for example with a prompt like I‚Äôm sorry, I didn‚Äôt understand that. To decide\n among these acts, we can make use of the fact that ASR systems often compute\n their confidence in their transcription (often based on the log-likelihood the system\n assigns the sentence). A system can thus choose to explicitly confirm only lowconfidence sentences. Or systems might have a four-tiered level of confidence with\n three thresholds Œ±, Œ≤ , and Œ≥:\n\n < Œ± low confidence reject\n ‚â• Œ± above the threshold confirm explicitly\n ‚â• Œ≤ high confidence confirm implictly\n ‚â• Œ≥ very high confidence don‚Äôt confirm at all\n\n K.1.4 Natural language generation: Sentence Realization\n\n recommend(restaurant name= Au Midi, neighborhood = midtown,\n cuisine = french)\n 1 Au Midi is in Midtown and serves French food.\n 2 There is a French restaurant in Midtown called Au Midi.\n Figure K.7 Sample inputs to the sentence realization phase of NLG, showing the dialogue\n act and attributes prespecified by the content planner, and two distinct potential output sentences to be generated. From the restaurant recommendation system of Nayak et al. (2017).\n H ISTORICAL N OTES 7\n\n Once a dialogue act has been chosen, we need to generate the text of the response to the user. This part of the generation process is called sentence realizasentence tion. Fig. K.7 shows a sample input/output for the sentence realization phase. The\n realization\n content planner has chosen the dialogue act RECOMMEND and some slots (name,\n neighborhood, cuisine) and fillers. The sentence realizer generates a sentence like\n lines 1 or 2 (by training on examples of representation/sentence pairs from a corpus\n of labeled dialogues). Because we won‚Äôt see every restaurant or attribute in every\n delexicalize possible wording, we can delexicalize: generalize the training examples by replacing specific slot value words in the training set with a generic placeholder token\n representing the slot. Fig. K.8 shows the sentences in Fig. K.7 delexicalized.\n\n recommend(restaurant name= Au Midi, neighborhood = midtown,\n cuisine = french)\n 1 restaurant name is in neighborhood and serves cuisine food.\n 2 There is a cuisine restaurant in neighborhood called restaurant name.\n Figure K.8 Delexicalized sentences that can be used for generating many different relexicalized sentences. From the restaurant recommendation system of Nayak et al. (2017).\n\n We can map from frames to delexicalized sentences with an encoder decoder\n model (MrksÃåicÃÅ et al. 2017, inter alia), trained on hand-labeled dialogue corpora like\n MultiWOZ (Budzianowski et al., 2018). The input to the encoder is a sequence of\n tokens xt that represent the dialogue act (e.g., RECOMMEND) and its arguments (e.g.,\n service:decent, cuisine:null) (Nayak et al., 2017), as in Fig. K.9.\n\n [name] has decent service\n\n DECODER\n ENCODER\n\n RECOMMEND service: decent cuisine: null\n\n Figure K.9 An encoder decoder sentence realizer mapping slots/fillers to English.\n\n The decoder outputs the delexicalized English sentence ‚Äúname has decent serrelexicalize vice‚Äù, which we can then relexicalize, i.e. fill back in correct slot values, resulting\n in ‚ÄúAu Midi has decent service‚Äù.\n\nHistorical Notes\n The linguistic, philosophical, and psychological literature on dialogue is quite extensive. For example the idea that utterances in a conversation are a kind of action\n being performed by the speaker was due originally to the philosopher Wittgenstein\n (1953) but worked out more fully by Austin (1962) and his student John Searle.\n Various sets of speech acts have been defined over the years, and a rich linguistic\n and philosophical literature developed, especially focused on explaining the use of\n indirect speech acts. The idea of dialogue acts draws also from a number of other\n sources, including the ideas of adjacency pairs, pre-sequences, and other aspects of\n8 A PPENDIX K ‚Ä¢ F RAME -BASED D IALOGUE S YSTEMS\n\n the interactional properties of human conversation developed in the field of converconversation\n analysis sation analysis (see Levinson (1983) for an introduction to the field). This idea that\n acts set up strong local dialogue expectations was also prefigured by Firth (1935, p.\n 70), in a famous quotation:\n Most of the give-and-take of conversation in our everyday life is stereotyped\n and very narrowly conditioned by our particular type of culture. It is a sort\n of roughly prescribed social ritual, in which you generally say what the other\n fellow expects you, one way or the other, to say.\n Another important research thread modeled dialogue as a kind of collaborative\n behavior, including the ideas of common ground (Clark and Marshall, 1981), reference as a collaborative process (Clark and Wilkes-Gibbs, 1986), joint intention\n (Levesque et al., 1990), and shared plans (Grosz and Sidner, 1980).\n The earliest conversational systems were simple pattern-action chatbots like ELIZA\n (Weizenbaum, 1966). ELIZA had a widespread influence on popular perceptions of\n artificial intelligence, and brought up some of the first ethical questions in natural\n language processing ‚Äîsuch as the issues of privacy we discussed above as well the\n role of algorithms in decision-making‚Äî leading its creator Joseph Weizenbaum to\n fight for social responsibility in AI and computer science in general.\n Computational-implemented theories of dialogue blossomed in the 1970. That\n period saw the very influential GUS system (Bobrow et al., 1977), which in the late\n 1970s established the frame-based paradigm that became the dominant industrial\n paradigm for dialogue systems for over 30 years.\n Another influential line of research from that decade focused on modeling the\n hierarchical structure of dialogue. Grosz‚Äôs pioneering 1977 dissertation first showed\n that ‚Äútask-oriented dialogues have a structure that closely parallels the structure of\n the task being performed‚Äù (p. 27), leading to her work with Sidner and others showing how to use similar notions of intention and plans to model discourse structure\n and coherence in dialogue. See, e.g., Lochbaum et al. (2000) for a summary of the\n role of intentional structure in dialogue.\n Yet a third line, first suggested by Bruce (1975), suggested that since speech acts\n are actions, they should be planned like other actions, and drew on the AI planning\n literature (Fikes and Nilsson, 1971). A system seeking to find out some information\n can come up with the plan of asking the interlocutor for the information. A system\n hearing an utterance can interpret a speech act by running the planner ‚Äúin reverse‚Äù,\n using inference rules to infer from what the interlocutor said what the plan might\n BDI have been. Plan-based models of dialogue are referred to as BDI models because\n such planners model the beliefs, desires, and intentions (BDI) of the system and interlocutor. BDI models of dialogue were first introduced by Allen, Cohen, Perrault,\n and their colleagues in a number of influential papers showing how speech acts could\n be generated (Cohen and Perrault, 1979) and interpreted (Perrault and Allen 1980,\n Allen and Perrault 1980). At the same time, Wilensky (1983) introduced plan-based\n models of understanding as part of the task of interpreting stories.\n In the 1990s, machine learning models that had first been applied to natural\n language processing began to be applied to dialogue tasks like slot filling (Miller\n et al. 1994, Pieraccini et al. 1991). This period also saw lots of analytic work on the\n linguistic properties of dialogue acts and on machine-learning-based methods for\n their detection. (Sag and Liberman 1975, Hinkelman and Allen 1989, Nagata and\n Morimoto 1994, Goodwin 1996, Chu-Carroll 1998, Shriberg et al. 1998, Stolcke\n et al. 2000, Gravano et al. 2012. This work strongly informed the development\n of the dialogue-state model (Larsson and Traum, 2000). Dialogue state tracking\n H ISTORICAL N OTES 9\n\nquickly became an important problem for task-oriented dialogue, and there has been\nan influential annual evaluation of state-tracking algorithms (Williams et al., 2016).\n The turn of the century saw a line of work on applying reinforcement learning\nto dialogue, which first came out of AT&T and Bell Laboratories with work on\nMDP dialogue systems (Walker 2000, Levin et al. 2000, Singh et al. 2002) along\nwith work on cue phrases, prosody, and rejection and confirmation. Reinforcement\nlearning research turned quickly to the more sophisticated POMDP models (Roy\net al. 2000, Lemon et al. 2006, Williams and Young 2007) applied to small slotfilling dialogue tasks. Neural reinforcement learning models have been used both for\nchatbot systems, for example simulating dialogues between two dialogue systems,\nrewarding good conversational properties like coherence and ease of answering (Li\net al., 2016), and for task-oriented dialogue (Williams et al., 2017).\n By around 2010 the GUS architecture finally began to be widely used commercially in dialogue systems on phones like Apple‚Äôs SIRI (Bellegarda, 2013) and other\ndigital assistants.\n [TBD: Modern history of neural dialogue systems]\n Other important dialogue areas include the study of affect in dialogue (Rashkin\net al. 2019, Lin et al. 2019) and conversational interface design (Cohen et al. 2004,\nHarris 2005, Pearl 2017, Deibel and Evanhoe 2021).\n10 Appendix K ‚Ä¢ Frame-Based Dialogue Systems\n\nAllen, J. and C. R. Perrault. 1980. Analyzing intention in Gravano, A., J. Hirschberg, and SÃå. BenÃåusÃå. 2012. Affirmautterances. Artificial Intelligence, 15:143‚Äì178. tive cue words in task-oriented dialogue. Computational\nAustin, J. L. 1962. How to Do Things with Words. Harvard Linguistics, 38(1):1‚Äì39.\n University Press. Grosz, B. J. 1977. The Representation and Use of Focus\nAwadallah, A. H., R. G. Kulkarni, U. Ozertem, and R. Jones. in Dialogue Understanding. Ph.D. thesis, University of\n 2015. Charaterizing and predicting voice query reformu- California, Berkeley.\n lation. CIKM-15. Grosz, B. J. and C. L. Sidner. 1980. Plans for discourse. In\nBellegarda, J. R. 2013. Natural language technology in mo- P. R. Cohen, J. Morgan, and M. E. Pollack, eds, Intentions\n bile devices: Two grounding frameworks. In Mobile in Communication, 417‚Äì444. MIT Press.\n Speech and Advanced Natural Language Solutions, 185‚Äì Harris, R. A. 2005. Voice Interaction Design: Crafting the\n 196. Springer. New Conversational Speech Systems. Morgan Kaufmann.\nBobrow, D. G., R. M. Kaplan, M. Kay, D. A. Norman, Hinkelman, E. A. and J. Allen. 1989. Two constraints on\n H. Thompson, and T. Winograd. 1977. GUS, A frame speech act ambiguity. ACL.\n driven dialog system. Artificial Intelligence, 8:155‚Äì173. Hirschberg, J., D. J. Litman, and M. Swerts. 2001. Identi-\nBruce, B. C. 1975. Generation as a social action. Proceed- fying user corrections automatically in spoken dialogue\n ings of TINLAP-1 (Theoretical Issues in Natural Lan- systems. NAACL.\n guage Processing). Larsson, S. and D. R. Traum. 2000. Information state and\nBudzianowski, P., T.-H. Wen, B.-H. Tseng, I. Casanueva, dialogue management in the trindi dialogue move engine\n S. Ultes, O. Ramadan, and M. GasÃåicÃÅ. 2018. MultiWOZ - toolkit. Natural Language Engineering, 6(323-340):97‚Äì\n a large-scale multi-domain wizard-of-Oz dataset for task- 114.\n oriented dialogue modelling. EMNLP. Lemon, O., K. Georgila, J. Henderson, and M. Stuttle. 2006.\nBulyko, I., K. Kirchhoff, M. Ostendorf, and J. Goldberg. An ISU dialogue system exhibiting reinforcement learn-\n2005. Error-sensitive response generation in a spo- ing of dialogue policies: Generic slot-filling in the TALK\n ken language dialogue system. Speech Communication, in-car system. EACL.\n 45(3):271‚Äì288. Levesque, H. J., P. R. Cohen, and J. H. T. Nunes. 1990. On\nChu-Carroll, J. 1998. A statistical model for discourse act acting together. AAAI. Morgan Kaufmann.\n recognition in dialogue interactions. Applying Machine Levin, E., R. Pieraccini, and W. Eckert. 2000. A stochastic\n Learning to Discourse Processing. Papers from the 1998 model of human-machine interaction for learning dialog\n AAAI Spring Symposium. Tech. rep. SS-98-01. AAAI strategies. IEEE Transactions on Speech and Audio Pro-\nPress. cessing, 8:11‚Äì23.\nClark, H. H. and C. Marshall. 1981. Definite reference and Levinson, S. C. 1983. Conversational Analysis, chapter 6.\n mutual knowledge. In A. K. Joshi, B. L. Webber, and I. A. Cambridge University Press.\n Sag, eds, Elements of Discourse Understanding, 10‚Äì63.\n Levow, G.-A. 1998. Characterizing and recognizing spoken\n Cambridge.\n corrections in human-computer dialogue. COLING/ACL.\nClark, H. H. and D. Wilkes-Gibbs. 1986. Referring as a col- Li, J., W. Monroe, A. Ritter, D. Jurafsky, M. Galley, and\n laborative process. Cognition, 22:1‚Äì39. J. Gao. 2016. Deep reinforcement learning for dialogue\nCohen, M. H., J. P. Giangola, and J. Balogh. 2004. Voice generation. EMNLP.\n User Interface Design. Addison-Wesley. Lin, Z., A. Madotto, J. Shin, P. Xu, and P. Fung. 2019.\nCohen, P. R. and C. R. Perrault. 1979. Elements of a plan- MoEL: Mixture of empathetic listeners. EMNLP.\n based theory of speech acts. Cognitive Science, 3(3):177‚Äì Litman, D. J., M. A. Walker, and M. Kearns. 1999. Auto-\n212. matic detection of poor speech recognition at the dialogue\nDanieli, M. and E. Gerbino. 1995. Metrics for evaluating level. ACL.\n dialogue strategies in a spoken language system. AAAI Lochbaum, K. E., B. J. Grosz, and C. L. Sidner. 2000. Dis-\nSpring Symposium on Empirical Methods in Discourse course structure and intention recognition. In R. Dale,\n Interpretation and Generation. H. Moisl, and H. L. Somers, eds, Handbook of Natural\nDeibel, D. and R. Evanhoe. 2021. Conversations with Language Processing. Marcel Dekker.\n Things: UX Design for Chat and Voice. Rosenfeld. Miller, S., R. J. Bobrow, R. Ingria, and R. Schwartz. 1994.\nFikes, R. E. and N. J. Nilsson. 1971. STRIPS: A new ap- Hidden understanding models of natural language. ACL.\n proach to the application of theorem proving to problem MrksÃåicÃÅ, N., D. OÃÅ SeÃÅaghdha, T.-H. Wen, B. Thomson, and\n solving. Artificial Intelligence, 2:189‚Äì208. S. Young. 2017. Neural belief tracker: Data-driven dia-\nFirth, J. R. 1935. The technique of semantics. Transactions logue state tracking. ACL.\n of the philological society, 34(1):36‚Äì73. Nagata, M. and T. Morimoto. 1994. First steps toward statis-\nGoldberg, J., M. Ostendorf, and K. Kirchhoff. 2003. The im- tical modeling of dialogue to predict the speech act type\n pact of response wording in error correction subdialogs. of the next utterance. Speech Communication, 15:193‚Äì\n ISCA Tutorial and Research Workshop on Error Handling 203.\n in Spoken Dialogue Systems. Nayak, N., D. Hakkani-TuÃàr, M. A. Walker, and L. P. Heck.\nGoodwin, C. 1996. Transparent vision. In E. Ochs, E. A. 2017. To plan or not to plan? discourse planning in\n Schegloff, and S. A. Thompson, eds, Interaction and slot-value informed sequence to sequence models for lan-\nGrammar, 370‚Äì404. Cambridge University Press. guage generation. INTERSPEECH.\n Historical Notes 11\n\nPearl, C. 2017. Designing Voice User Interfaces: Principles Young, S. J., M. GasÃåicÃÅ, S. Keizer, F. Mairesse, J. Schatzof Conversational Experiences. O‚ÄôReilly. mann, B. Thomson, and K. Yu. 2010. The Hidden Infor-\nPerrault, C. R. and J. Allen. 1980. A plan-based analysis of mation State model: A practical framework for POMDPindirect speech acts. American Journal of Computational based spoken dialogue management. Computer Speech &\n Linguistics, 6(3-4):167‚Äì182. Language, 24(2):150‚Äì174.\nPieraccini, R., E. Levin, and C.-H. Lee. 1991. Stochastic\n representation of conceptual structure in the ATIS task.\n Speech and Natural Language Workshop.\nRashkin, H., E. M. Smith, M. Li, and Y.-L. Boureau. 2019.\n Towards empathetic open-domain conversation models:\n A new benchmark and dataset. ACL.\nRoy, N., J. Pineau, and S. Thrun. 2000. Spoken dialogue\n management using probabilistic reasoning. ACL.\nSag, I. A. and M. Y. Liberman. 1975. The intonational disambiguation of indirect speech acts. In CLS-75, 487‚Äì498.\n University of Chicago.\nShriberg, E., R. Bates, P. Taylor, A. Stolcke, D. Jurafsky,\n K. Ries, N. Coccaro, R. Martin, M. Meteer, and C. Van\n Ess-Dykema. 1998. Can prosody aid the automatic classification of dialog acts in conversational speech? Language and Speech (Special Issue on Prosody and Conversation), 41(3-4):439‚Äì487.\nSingh, S. P., D. J. Litman, M. Kearns, and M. A. Walker.\n 2002. Optimizing dialogue management with reinforcement learning: Experiments with the NJFun system.\n JAIR, 16:105‚Äì133.\nStolcke, A., K. Ries, N. Coccaro, E. Shriberg, R. Bates,\n D. Jurafsky, P. Taylor, R. Martin, M. Meteer, and C. Van\n Ess-Dykema. 2000. Dialogue act modeling for automatic\n tagging and recognition of conversational speech. Computational Linguistics, 26(3):339‚Äì371.\nSwerts, M., D. J. Litman, and J. Hirschberg. 2000. Corrections in spoken dialogue systems. ICSLP.\nWade, E., E. Shriberg, and P. J. Price. 1992. User behaviors\n affecting speech recognition. ICSLP.\nWalker, M. A. 2000. An application of reinforcement learning to dialogue strategy selection in a spoken dialogue\n system for email. JAIR, 12:387‚Äì416.\nWalker, M. A., J. C. Fromer, and S. S. Narayanan. 1998.\n Learning optimal dialogue strategies: A case study of a\n spoken dialogue agent for email. COLING/ACL.\nWeizenbaum, J. 1966. ELIZA ‚Äì A computer program for the\n study of natural language communication between man\n and machine. CACM, 9(1):36‚Äì45.\nWilensky, R. 1983. Planning and Understanding: A Computational Approach to Human Reasoning. Addison-\nWesley.\nWilliams, J. D., K. Asadi, and G. Zweig. 2017. Hybrid code\n networks: practical and efficient end-to-end dialog control with supervised and reinforcement learning. ACL.\nWilliams, J. D., A. Raux, and M. Henderson. 2016. The dialog state tracking challenge series: A review. Dialogue\n & Discourse, 7(3):4‚Äì33.\nWilliams, J. D. and S. J. Young. 2007. Partially observable markov decision processes for spoken dialog systems. Computer Speech and Language, 21(1):393‚Äì422.\nWittgenstein, L. 1953. Philosophical Investigations. (Translated by Anscombe, G.E.M.). Blackwell.\n",
    "file_path": "/Users/colinsidberry/nlp-textbook-rag/data/extracted_text/K.Frame-Based Dialogue Systems.txt",
    "file_size_kb": 32.67
  }
]