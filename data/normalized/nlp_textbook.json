[
  {
    "id": "d8044658251063b4",
    "source": "nlp_textbook",
    "chapter": "Information Retrieval and Retrieval-Augmented Generation",
    "filename": "RAG.txt",
    "content": "Speech and Language Processing. Daniel Jurafsky & James H. Martin.\nrights reserved. Draft of August 24, 2025.\n\nCopyright © 2025.\n\nAll\n\nCHAPTER\n\nInformation Retrieval and\nRetrieval-Augmented Generation\n\n11\n\nOn two occasions I have been asked,—“Pray, Mr. Babbage, if you put into\nthe machine wrong figures, will the right answers come out?” ... I am not able\nrightly to apprehend the kind of confusion of ideas that could provoke such a\nquestion.\nBabbage (1864)\nPeople need to know things. So pretty much as soon as there were computers\nwe were asking them questions. By 1961 there was a system to answer questions\nabout American baseball statistics like “How many games did the Yankees play\nin July?” (Green et al., 1961). Even fictional computers in the 1970s like Deep\nThought, invented by Douglas Adams in The Hitchhiker’s Guide to the Galaxy,\nanswered “the Ultimate Question Of Life, The Universe, and Everything”.1 And\nbecause so much knowledge is encoded in text, systems were answering questions\nat human-level performance even before LLMs: IBM’s Watson system won the TV\ngame-show Jeopardy! in 2011, surpassing humans at answering questions like:\nWILLIAM WILKINSON’S “AN ACCOUNT OF THE\nPRINCIPALITIES OF WALLACHIA AND MOLDOVIA”\nINSPIRED THIS AUTHOR’S MOST FAMOUS NOVEL 2\n\nfactoid\nquestions\n\nIt follows naturally, then, that an important function of large language models is\nto fill human information needs by answering people’s questions. And since a lot\nof information is online, answering questions is closely related to web information\nretrieval, the task performed by search engines. Indeed, the distinction is becoming ever more fuzzy, as modern search engines are integrated with large language\nmodels.\nConsider some simple information needs, for example factoid questions that\ncan be answered with facts expressed in short texts like the following:\n(11.1) Where is the Louvre Museum located?\n(11.2) Where does the energy in a nuclear explosion come from?\n(11.3) How to get a script l in latex?\nTo get an LLM to answer these questions, we can just prompt it! For example a\npretrained LLM that has been instruction-tuned on question answering (Chapter 9)\ncould directly answer the following question\nWhere is the Louvre Museum located?\nby performing conditional generation given this prefix, and take the response as the\nanswer. This works because large language models have processed a lot of facts in\ntheir pretraining data, including the location of the Louvre, and have encoded this\ninformation in their parameters. Factual knowledge of this type seems to be stored\n1\n2\n\nThe answer was 42, but unfortunately the question was never revealed.\nThe answer, of course, is ‘Who is Bram Stoker’, and the novel was Dracula.\n\n\f2\n\nC HAPTER 11\n\nhallucinate\n\ncalibrated\n\nRAG\ninformation\nretrieval\n\n•\n\nR ETRIEVAL - BASED M ODELS\n\nin the connections in the very large feedforward layers of transformer models (Geva\net al., 2021; Meng et al., 2022).\nSimply prompting an LLM can be a useful approach to answer many factoid\nquestions. But the fact that knowledge is stored in the feedforward weights of the\nLLM leads to a number of problems with prompting as a method for correctly answering factual questions.\nThe first and main problem is that LLMs often give the wrong answer to factual\nquestions! Large language models hallucinate. A hallucination is a response that is\nnot faithful to the facts of the world. That is, when asked questions, large language\nmodels sometimes make up answers that sound reasonable. For example, Dahl et al.\n(2024) found that when asked questions about the legal domain (like about particular legal cases), large language models hallucinated from 69% to 88% of the time!\nLLMs sometimes give incorrect factual responses even when the correct facts are\nstored in the parameters; this seems to be caused by the feedforward layers failing\nto recall the knowledge stored in their parameters (Jiang et al., 2024).\nAnd it’s not always possible to tell when language models are hallucinating,\npartly because LLMs aren’t well-calibrated. In a calibrated system, the confidence\nof a system in the correctness of its answer is highly correlated with the probability\nof an answer being correct. So if a calibrated system is wrong, at least it might hedge\nits answer or tell us to go check another source. But since language models are not\nwell-calibrated, they often give a very wrong answer with complete certainty (Zhou\net al., 2024).\nA second problem with answering questions with simple prompting methods\nis that prompting a large language model to answer from its pretrained parameters\ndoesn’t allow us to ask questions about proprietary data. We would like to use\nlanguage models to answer factual questions about proprietary data like personal\nemail. Or for the healthcare application we might want to apply a language model to\nmedical records. Or a company may have internal documents that contain answers\nfor customer service or internal use. Or legal firms need to ask questions about legal\ndiscovery from proprietary documents. None of this data (hopefully) was in the\nlarge web-based corpora that large language models are pretrained on.\nA final issue with using large language models to answer knowledge questions\nis that they are static; they were pretrained once, at a particular time. This means\nthat LLMs cannot answer questions about rapidly changing information (like questions about something that happened last week) since they won’t have up-to-date\ninformation from after their release data.\nOne solution to all these problems with simple prompting for answering factual\nquestions is to give a language model external sources of knowledge, for example\nproprietary texts like medical or legal records, personal emails, or corporate documents, and to use those documents in answering questions. This method is called\nretrieval-augmented generation or RAG, and that is the method we will focus on\nin this chapter. In RAG we use information retrieval (IR) techniques to retrieve\ndocuments that are likely to have information that might help answer the question.\nThen we use a large language model to generate an answer given these documents.\nBasing our answers on retrieved documents can solve some of the problems with\nusing simple prompting to answer questions. First, it helps ensure that the answer is\ngrounded in facts from some curated dataset. And the system can give the user the\nanswer accompanied by the context of the passage or document the answer came\nfrom. This information can help users have confidence in the accuracy of the answer\n(or help them spot when it is wrong!). And these retrieval techniques can be used on\n\n\f11.1\n\n•\n\nI NFORMATION R ETRIEVAL\n\n3\n\nany proprietary data we want, such as legal or medical data for those applications.\nWe’ll begin by introducing information retrieval, the task of choosing the most\nrelevant document from a document set given a user’s query expressing their information need. We’ll see the classic method based on cosines of sparse tf-idf vectors,\nmodern neural ‘dense’ retrievers based on instead representing queries and documents neurally with BERT or other language models. We then introduce retrieverbased question answering and the retrieval-augmented generation paradigm.\nFinally, we’ll discuss various datasets with questions and answers that can be\nused for finetuning LLMs in instruction tuning and for use as benchmarks for evaluation.\n\n11.1\n\nInformation Retrieval\n\ninformation\nretrieval\nIR\n\nInformation retrieval or IR is the name of the field encompassing the retrieval of all\nmanner of media based on user information needs. The resulting IR system is often\ncalled a search engine. Our goal in this section is to give a sufficient overview of IR\nto see its application to question answering. Readers with more interest specifically\nin information retrieval should see the Historical Notes section at the end of the\nchapter and textbooks like Manning et al. (2008).\nThe IR task we consider is called ad hoc retrieval, in which a user poses a\nquery to a retrieval system, which then returns an ordered set of documents from\nsome collection. A document refers to whatever unit of text the system indexes and\nretrieves (web pages, scientific papers, news articles, or even shorter passages like\nparagraphs). A collection refers to a set of documents being used to satisfy user\nrequests. A term refers to a word in a collection, but it may also include phrases.\nFinally, a query represents a user’s information need expressed as a set of terms.\nThe high-level architecture of an ad hoc retrieval engine is shown in Fig. 11.1.\n\nad hoc retrieval\ndocument\ncollection\nterm\nquery\n\nDocument\nDocument\nDocument\nDocument\nDocument\nDocument Document\n\nIndexing\n\nInverted\nIndex\n\ndocument collection\n\nSearch\n\nDocument\nDocument\nDocument\nDocument\nDocument\nRanked\n\nDocuments\n\nQuery\nProcessing\n\nquery\nFigure 11.1\n\nquery\nvector\n\nThe architecture of an ad hoc IR system.\n\nThe basic IR architecture uses the vector space model we introduced in Chapter 5, in which we map queries and document to vectors based on unigram word\ncounts, and use the cosine similarity between the vectors to rank potential documents\n(Salton, 1971). This is thus an example of the bag-of-words model introduced in\nAppendix K, since words are considered independently of their positions.\n\n11.1.1\n\nTerm weighting and document scoring\n\nLet’s look at the details of how the match between a document and query is scored.\n\n\f4\n\nC HAPTER 11\nterm weight\n\nBM25\n\n•\n\nR ETRIEVAL - BASED M ODELS\n\nWe don’t use raw word counts in IR, instead computing a term weight for each\ndocument word. Two term weighting schemes are common: the tf-idf weighting\nintroduced in Chapter 5, and a slightly more powerful variant called BM25.\nWe’ll reintroduce tf-idf here so readers don’t need to look back at Chapter 5.\nTf-idf (the ‘-’ here is a hyphen, not a minus sign) is the product of two terms, the\nterm frequency tf and the inverse document frequency idf.\nThe term frequency tells us how frequent the word is; words that occur more\noften in a document are likely to be informative about the document’s contents. We\nusually use the log10 of the word frequency, rather than the raw count. The intuition\nis that a word appearing 100 times in a document doesn’t make that word 100 times\nmore likely to be relevant to the meaning of the document. We also need to do\nsomething special with counts of 0, since we can’t take the log of 0.3\n(\n1 + log10 count(t, d)\nif count(t, d) > 0\n(11.4)\ntft, d =\n0\notherwise\nIf we use log weighting, terms which occur 0 times in a document would have tf = 0,\n1 times in a document tf = 1 + log10 (1) = 1 + 0 = 1, 10 times in a document tf =\n1 + log10 (10) = 2, 100 times tf = 1 + log10 (100) = 3, 1000 times tf = 4, and so on.\nThe document frequency dft of a term t is the number of documents it occurs in. Terms that occur in only a few documents are useful for discriminating\nthose documents from the rest of the collection; terms that occur across the entire\ncollection aren’t as helpful. The inverse document frequency or idf term weight\n(Sparck Jones, 1972) is defined as:\nidft = log10\n\nN\ndft\n\n(11.5)\n\nwhere N is the total number of documents in the collection, and dft is the number\nof documents in which term t occurs. The fewer documents in which a term occurs,\nthe higher this weight; the lowest weight of 0 is assigned to terms that occur in every\ndocument.\nHere are some idf values for some words in the corpus of Shakespeare plays,\nranging from extremely informative words that occur in only one play like Romeo,\nto those that occur in a few like salad or Falstaff, to those that are very common like\nfool or so common as to be completely non-discriminative since they occur in all 37\nplays like good or sweet.4\nWord\nRomeo\nsalad\nFalstaff\nforest\nbattle\nwit\nfool\ngood\nsweet\n3\n\ndf\n1\n2\n4\n12\n21\n34\n36\n37\n37\n\nidf\n1.57\n1.27\n0.967\n0.489\n0.246\n0.037\n0.012\n0\n0\n\nWe can also use this alternative formulation, which we have used in earlier editions: tft, d =\nlog10 (count(t, d) + 1)\n4 Sweet was one of Shakespeare’s favorite adjectives, a fact probably related to the increased use of\nsugar in European recipes around the turn of the 16th century (Jurafsky, 2014, p. 175).\n\n\f11.1\n\n•\n\nI NFORMATION R ETRIEVAL\n\n5\n\nThe tf-idf value for word t in document d is then the product of term frequency\ntft, d and IDF:\ntf-idf(t, d) = tft, d · idft\n\n11.1.2\n\n(11.6)\n\nDocument Scoring\n\nWe score document d by the cosine of its vector d with the query vector q:\nscore(q, d) = cos(q, d) =\n\nq·d\n|q||d|\n\n(11.7)\n\nAnother way to think of the cosine computation is as the dot product of unit vectors;\nwe first normalize both the query and document vector to unit vectors, by dividing\nby their lengths, and then take the dot product:\nscore(q, d) = cos(q, d) =\n\nq d\n·\n|q| |d|\n\n(11.8)\n\nWe can spell out Eq. 11.8, using the tf-idf values and spelling out the dot product as\na sum of products:\nscore(q, d) =\n\ntf-idf(t, q)\n\nX\nt∈q\n\nqP\n\nqi ∈q tf-idf\n\n2\n\n(qi , q)\n\n· qP\n\ntf-idf(t, d)\n\ndi ∈d tf-idf\n\n2\n\n(11.9)\n\n(di , d)\n\nNow let’s use Eq. 11.9 to walk through an example of a tiny query against a\ncollection of 4 nano documents, computing tf-idf values and seeing the rank of the\ndocuments. We’ll assume all words in the following query and documents are downcased and punctuation is removed:\nQuery: sweet love\nDoc 1: Sweet sweet nurse! Love?\nDoc 2: Sweet sorrow\nDoc 3: How sweet is love?\nDoc 4: Nurse!\nFig. 11.2 shows the computation of the tf-idf cosine between the query and Document 1, and the query and Document 2. The cosine is the normalized dot product\nof tf-idf values, so for the normalization we must need to compute the document\nvector lengths |q|, |d1 |, and |d2 | for the query and the first two documents using\nEq. 11.4, Eq. 11.5, Eq. 11.6, and Eq. 11.9 (computations for Documents 3 and 4 are\nalso needed but are left as an exercise for the reader). The dot product between the\nvectors is the sum over dimensions of the product, for each dimension, of the values\nof the two tf-idf vectors for that dimension. This product is only non-zero where\nboth the query and document have non-zero values, so for this example, in which\nonly sweet and love have non-zero values in the query, the dot product will be the\nsum of the products of those elements of each vector.\nDocument 1 has a higher cosine with the query (0.747) than Document 2 has\nwith the query (0.0779), and so the tf-idf cosine model would rank Document 1\nabove Document 2. This ranking is intuitive given the vector space model, since\nDocument 1 has both terms including two instances of sweet, while Document 2 is\nmissing one of the terms. We leave the computation for Documents 3 and 4 as an\nexercise for the reader.\n\n\f6\n\nC HAPTER 11\n\n•\n\nR ETRIEVAL - BASED M ODELS\n\nQuery\nword cnt tf df idf\ntf-idf n’lized = tf-idf/|q|\nsweet 1 1 3 0.125 0.125 0.383\nnurse 0 0 2 0.301 0\n0\nlove\n1 1 2 0.301 0.301 0.924\nhow\n0 0 1 0.602 0\n0\nsorrow 0 0 1 0.602 0\n0\nis\n0 0 1 0.602 0\n0\n√\n2\n2\n|q| = .125 + .301 = .326\nDocument 1\nword cnt tf\ntf-idf n’lized × q\nsweet 2 1.301 0.163 0.357\n0.137\nnurse 1 1.000 0.301 0.661\n0\nlove\n1 1.000 0.301 0.661\n0.610\nhow\n0 0\n0\n0\n0\nsorrow 0 0\n0\n0\n0\nis\n0 0\n0\n0\n0\n√\n|d1 | = .1632 + .3012 + .3012 = .456\nP\nCosine:\nof column: 0.747\n\nDocument 2\ncnt tf\ntf-idf n’lized ×q\n1 1.000 0.125 0.203\n0.0779\n0 0\n0\n0\n0\n0 0\n0\n0\n0\n0 0\n0\n0\n0\n1 1.000 0.602 0.979\n0\n0 0\n0\n0\n0\n√\n2\n2\n|d2 | = .125 + .602 = .615\nP\nCosine:\nof column: 0.0779\n\nFigure 11.2 Computation of tf-idf cosine score between the query and nano-documents 1 (0.747) and 2\n(0.0779), using Eq. 11.4, Eq. 11.5, Eq. 11.6 and Eq. 11.9.\n\nIn practice, there are many variants and approximations to Eq. 11.9. For example, we might choose to simplify processing by removing some terms. To see this,\nlet’s start by expanding the formula for tf-idf in Eq. 11.9 to explicitly mention the tf\nand idf terms from Eq. 11.6:\nscore(q, d) =\n\ntft, q · idft\n\nX\nt∈q\n\nqP\n\nqi ∈q tf-idf\n\n2\n\n(qi , q)\n\n· qP\n\ntft, d · idft\n\ndi ∈d tf-idf\n\n2\n\n(11.10)\n\n(di , d)\n\nIn one common variant of tf-idf cosine, for example, we drop the idf term for the\ndocument. Eliminating the second copy of the idf term (since the identical term is\nalready computed for the query) turns out to sometimes result in better performance:\n\nscore(q, d) =\n\nt∈q\n\nBM25\n\ntft, q ·idft\n\nX\nqP\n\n2\n\nqi ∈q tf-idf (qi , q)\n\n· qP\n\ntft, d · idft\n\n(11.11)\n\n2\ndi ∈d tf-idf (di , d)\n\nOther variants of tf-idf eliminate various other terms.\nA slightly more complex variant in the tf-idf family is the BM25 weighting\nscheme (sometimes called Okapi BM25 after the Okapi IR system in which it was\nintroduced (Robertson et al., 1995)). BM25 adds two parameters: k, a knob that\nadjust the balance between term frequency and IDF, and b, which controls the importance of document length normalization. The BM25 score of a document d given\na query q is:\nIDF\n\nweighted tf\n\n}|\n{\nz \u0012}| \u0013{ z\ntft,d\nN\n\u0010\n\u0010\n\u0011\u0011\nlog\ndft k 1 − b + b |d|\n+ tf\nt∈q\n\nX\n\n|davg |\n\nt,d\n\n(11.12)\n\n\f11.1\n\n•\n\nI NFORMATION R ETRIEVAL\n\n7\n\nwhere |davg | is the length of the average document. When k is 0, BM25 reverts to\nno use of term frequency, just a binary selection of terms in the query (plus idf).\nA large k results in raw term frequency (plus idf). b ranges from 1 (scaling by\ndocument length) to 0 (no length scaling). Manning et al. (2008) suggest reasonable\nvalues are k = [1.2,2] and b = 0.75. Kamphuis et al. (2020) is a useful summary of\nthe many minor variants of BM25.\n\nstop list\n\nStop words In the past it was common to remove high-frequency words from both\nthe query and document before representing them. The list of such high-frequency\nwords to be removed is called a stop list. The intuition is that high-frequency terms\n(often function words like the, a, to) carry little semantic weight and may not help\nwith retrieval, and can also help shrink the inverted index files we describe below.\nThe downside of using a stop list is that it makes it difficult to search for phrases\nthat contain words in the stop list. For example, common stop lists would reduce the\nphrase to be or not to be to the phrase not. In modern IR systems, the use of stop lists\nis much less common, partly due to improved efficiency and partly because much\nof their function is already handled by IDF weighting, which downweights function\nwords that occur in every document. Nonetheless, stop word removal is occasionally\nuseful in various NLP tasks so is worth keeping in mind.\n\n11.1.3\n\ninverted index\n\npostings\n\nInverted Index\n\nIn order to compute scores, we need to efficiently find documents that contain words\nin the query. (Any document that contains none of the query terms will have a score\nof 0 and can be ignored.) The basic search problem in IR is thus to find all documents\nd ∈ C that contain a term q ∈ Q.\nThe data structure for this task is the inverted index, which we use for making this search efficient, and also conveniently storing useful information like the\ndocument frequency and the count of each term in each document.\nAn inverted index, given a query term, gives a list of documents that contain the\nterm. It consists of two parts, a dictionary and the postings. The dictionary is a list\nof terms (designed to be efficiently accessed), each pointing to a postings list for the\nterm. A postings list is the list of document IDs associated with each term, which\ncan also contain information like the term frequency or even the exact positions of\nterms in the document. The dictionary can also store the document frequency for\neach term. For example, a simple inverted index for our 4 sample documents above,\nwith each word containing its document frequency in {}, and a pointer to a postings\nlist that contains document IDs and term counts in [], might look like the following:\nhow {1} → 3 [1]\nis {1}\n→ 3 [1]\nlove {2} → 1 [1] → 3 [1]\nnurse {2} → 1 [1] → 4 [1]\nsorry {1} → 2 [1]\nsweet {3} → 1 [2] → 2 [1] → 3 [1]\nGiven a list of terms in query, we can very efficiently get lists of all candidate\ndocuments, together with the information necessary to compute the tf-idf scores we\nneed.\nThere are alternatives to the inverted index. For the question-answering domain\nof finding Wikipedia pages to match a user query, Chen et al. (2017) show that\nindexing based on bigrams works better than unigrams, and use efficient hashing\nalgorithms rather than the inverted index to make the search efficient.\n\n\f8\n\nC HAPTER 11\n\n•\n\n11.1.4\n\nR ETRIEVAL - BASED M ODELS\n\nEvaluation of Information-Retrieval Systems\n\nWe measure the performance of ranked retrieval systems using the same precision\nand recall metrics we have been using. We make the assumption that each document returned by the IR system is either relevant to our purposes or not relevant.\nPrecision is the fraction of the returned documents that are relevant, and recall is the\nfraction of all relevant documents that are returned. More formally, let’s assume a\nsystem returns T ranked documents in response to an information request, a subset\nR of these are relevant, a disjoint subset, N, are the remaining irrelevant documents,\nand U documents in the collection as a whole are relevant to this request. Precision\nand recall are then defined as:\nPrecision =\n\n|R|\n|T |\n\nRecall =\n\n|R|\n|U|\n\n(11.13)\n\nUnfortunately, these metrics don’t adequately measure the performance of a system\nthat ranks the documents it returns. If we are comparing the performance of two\nranked retrieval systems, we need a metric that prefers the one that ranks the relevant\ndocuments higher. We need to adapt precision and recall to capture how well a\nsystem does at putting relevant documents higher in the ranking.\nRank\nJudgment\nPrecisionRank\nRecallRank\n1\nR\n1.0\n.11\n2\nN\n.50\n.11\n3\nR\n.66\n.22\n4\nN\n.50\n.22\n5\nR\n.60\n.33\n6\nR\n.66\n.44\n7\nN\n.57\n.44\n8\nR\n.63\n.55\n9\nN\n.55\n.55\n10\nN\n.50\n.55\n11\nR\n.55\n.66\n12\nN\n.50\n.66\n13\nN\n.46\n.66\n14\nN\n.43\n.66\n15\nR\n.47\n.77\n16\nN\n.44\n.77\n17\nN\n.44\n.77\n18\nR\n.44\n.88\n19\nN\n.42\n.88\n20\nN\n.40\n.88\n21\nN\n.38\n.88\n22\nN\n.36\n.88\n23\nN\n.35\n.88\n24\nN\n.33\n.88\n25\nR\n.36\n1.0\nFigure 11.3 Rank-specific precision and recall values calculated as we proceed down\nthrough a set of ranked documents (assuming the collection has 9 relevant documents).\n\nLet’s turn to an example. Assume the table in Fig. 11.3 gives rank-specific precision and recall values calculated as we proceed down through a set of ranked documents for a particular query; the precisions are the fraction of relevant documents\nseen at a given rank, and recalls the fraction of relevant documents found at the same\n\n\f11.1\n\n•\n\nI NFORMATION R ETRIEVAL\n\n9\n\n1.0\n\nPrecision\n\n0.8\n0.6\n0.4\n0.2\n0.00.0\nFigure 11.4\n\nprecision-recall\ncurve\n\ninterpolated\nprecision\n\n0.2\n\n0.4\n\nRecall\n\n0.6\n\n0.8\n\nThe precision recall curve for the data in table 11.3.\n\nrank. The recall measures in this example are based on this query having 9 relevant\ndocuments in the collection as a whole.\nNote that recall is non-decreasing; when a relevant document is encountered,\nrecall increases, and when a non-relevant document is found it remains unchanged.\nPrecision, on the other hand, jumps up and down, increasing when relevant documents are found, and decreasing otherwise. The most common way to visualize\nprecision and recall is to plot precision against recall in a precision-recall curve,\nlike the one shown in Fig. 11.4 for the data in table 11.3.\nFig. 11.4 shows the values for a single query. But we’ll need to combine values\nfor all the queries, and in a way that lets us compare one system to another. One way\nof doing this is to plot averaged precision values at 11 fixed levels of recall (0 to 100,\nin steps of 10). Since we’re not likely to have datapoints at these exact levels, we\nuse interpolated precision values for the 11 recall values from the data points we do\nhave. We can accomplish this by choosing the maximum precision value achieved\nat any level of recall at or above the one we’re calculating. In other words,\nIntPrecision(r) = max Precision(i)\ni>=r\n\nmean average\nprecision\n\n1.0\n\n(11.14)\n\nThis interpolation scheme not only lets us average performance over a set of queries,\nbut also helps smooth over the irregular precision values in the original data. It is\ndesigned to give systems the benefit of the doubt by assigning the maximum precision value achieved at higher levels of recall from the one being measured. Fig. 11.5\nand Fig. 11.6 show the resulting interpolated data points from our example.\nGiven curves such as that in Fig. 11.6 we can compare two systems or approaches\nby comparing their curves. Clearly, curves that are higher in precision across all\nrecall values are preferred. However, these curves can also provide insight into the\noverall behavior of a system. Systems that are higher in precision toward the left\nmay favor precision over recall, while systems that are more geared towards recall\nwill be higher at higher levels of recall (to the right).\nA second way to evaluate ranked retrieval is mean average precision (MAP),\nwhich provides a single metric that can be used to compare competing systems or\napproaches. In this approach, we again descend through the ranked list of items,\nbut now we note the precision only at those points where a relevant item has been\nencountered (for example at ranks 1, 3, 5, 6 but not 2 or 4 in Fig. 11.3). For a single\n\n\fC HAPTER 11\n\n•\n\nR ETRIEVAL - BASED M ODELS\nInterpolated Precision\n1.0\n1.0\n.66\n.66\n.66\n.63\n.55\n.47\n.44\n.36\n.36\nInterpolated data points from Fig. 11.3.\n\nFigure 11.5\n\nRecall\n0.0\n.10\n.20\n.30\n.40\n.50\n.60\n.70\n.80\n.90\n1.0\n\nInterpolated Precision Recall Curve\n1\n0.9\n0.8\n0.7\n0.6\nPrecision\n\n10\n\n0.5\n0.4\n0.3\n0.2\n0.1\n0\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\nRecall\n\nFigure 11.6 An 11 point interpolated precision-recall curve. Precision at each of the 11\nstandard recall levels is interpolated for each query from the maximum at any higher level of\nrecall. The original measured precision recall points are also shown.\n\nquery, we average these individual precision measurements over the return set (up\nto some fixed cutoff). More formally, if we assume that Rr is the set of relevant\ndocuments at or above r, then the average precision (AP) for a single query is\nAP =\n\n1 X\nPrecisionr (d)\n|Rr |\n\n(11.15)\n\nd∈Rr\n\nwhere Precisionr (d) is the precision measured at the rank at which document d was\nfound. For an ensemble of queries Q, we then average over these averages, to get\nour final MAP measure:\nMAP =\n\n1 X\nAP(q)\n|Q|\nq∈Q\n\nThe MAP for the single query (hence = AP) in Fig. 11.3 is 0.6.\n\n(11.16)\n\n\f11.2\n\n11.2\n\n•\n\nI NFORMATION R ETRIEVAL WITH D ENSE V ECTORS\n\n11\n\nInformation Retrieval with Dense Vectors\nThe classic tf-idf or BM25 algorithms for IR have long been known to have a conceptual flaw: they work only if there is exact overlap of words between the query\nand document. In other words, the user posing a query (or asking a question) needs\nto guess exactly what words the writer of the answer might have used, an issue called\nthe vocabulary mismatch problem (Furnas et al., 1987).\nThe solution to this problem is to use an approach that can handle synonymy:\ninstead of (sparse) word-count vectors, using (dense) embeddings. This idea was\nfirst proposed for retrieval in the last century under the name of Latent Semantic\nIndexing approach (Deerwester et al., 1990), but is implemented in modern times\nvia encoders like BERT.\nThe most powerful approach is to present both the query and the document to a\nsingle encoder, allowing the transformer self-attention to see all the tokens of both\nthe query and the document, and thus building a representation that is sensitive to\nthe meanings of both query and document. Then a linear layer can be put on top of\nthe [CLS] token to predict a similarity score for the query/document tuple:\nz = BERT(q; [SEP]; d)[CLS]\nscore(q, d) = softmax(U(z))\n\n(11.17)\n\nThis architecture is shown in Fig. 11.7a. Usually the retrieval step is not done on\nan entire document. Instead documents are broken up into smaller passages, such\nas non-overlapping fixed-length chunks of say 100 tokens, and the retriever encodes\nand retrieves these passages rather than entire documents. The query and document\nhave to be made to fit in the BERT 512-token window, for example by truncating\nthe query to 64 tokens and truncating the document if necessary so that it, the query,\n[CLS], and [SEP] fit in 512 tokens. The BERT system together with the linear layer\nU can then be fine-tuned for the relevance task by gathering a tuning dataset of\nrelevant and non-relevant passages.\nThe problem with the full BERT architecture in Fig. 11.7a is the expense in\ncomputation and time. With this architecture, every time we get a query, we have to\npass every single document in our entire collection through a BERT encoder jointly\nwith the new query! This enormous use of resources is impractical for real cases.\nAt the other end of the computational spectrum is a much more efficient architecture, the bi-encoder. In this architecture we can encode the documents in the\ncollection only one time by using two separate encoder models, one to encode the\nquery and one to encode the document. We encode each document, and store all\nthe encoded document vectors in advance. When a query comes in, we encode just\nthis query and then use the dot product between the query vector and the precomputed document vectors as the score for each candidate document (Fig. 11.7b). For\nexample, if we used BERT, we would have two encoders BERTQ and BERTD and\nwe could represent the query and document as the [CLS] token of the respective\nencoders (Karpukhin et al., 2020):\nzq = BERTQ (q)[CLS]\nzd = BERTD (d)[CLS]\nscore(q, d) = zq · zd\n\n(11.18)\n\nThe bi-encoder is much cheaper than a full query/document encoder, but is also\nless accurate, since its relevance decision can’t take full advantage of all the possi-\n\n\f12\n\nC HAPTER 11\n\n•\n\nR ETRIEVAL - BASED M ODELS\n\ns(q,d)\n\ns(q,d)\n\n•\n\nU\n\nzCLS_Q\n\nzCLS\n\nQuery\n\n[sep]\n\nzCLS_D\n\n…\n\n…\n\n…\n\n…\n\n…\n\n…\n\n…\n\n…\n\n…\n\n…\n\n…\n\n…\n\nQuery\n\nDocument\n\n(a)\n\nDocument\n\n(b)\n\nFigure 11.7 Two ways to do dense retrieval, illustrated by using lines between layers to schematically represent self-attention: (a) Use a single encoder to jointly encode query and document and finetune to produce a\nrelevance score with a linear layer over the CLS token. This is too compute-expensive to use except in rescoring\n(b) Use separate encoders for query and document, and use the dot product between CLS token outputs for the\nquery and document as the score. This is less compute-expensive, but not as accurate.\n\nColBERT\n\nble meaning interactions between all the tokens in the query and the tokens in the\ndocument.\nThere are numerous approaches that lie in between the full encoder and the biencoder. One intermediate alternative is to use cheaper methods (like BM25) as the\nfirst pass relevance ranking for each document, take the top N ranked documents,\nand use expensive methods like the full BERT scoring to rerank only the top N\ndocuments rather than the whole set.\nAnother intermediate approach is the ColBERT approach of Khattab and Zaharia (2020) and Khattab et al. (2021), shown in Fig. 11.8. This method separately\nencodes the query and document, but rather than encoding the entire query or document into one vector, it separately encodes each of them into contextual representations for each token. These BERT representations of each document word can be\npre-stored for efficiency. The relevance score between a query q and a document d is\na sum of maximum similarity (MaxSim) operators between tokens in q and tokens\nin d. Essentially, for each token in q, ColBERT finds the most contextually similar token in d, and then sums up these similarities. A relevant document will have\ntokens that are contextually very similar to the query.\nMore formally, a question q is tokenized as [q1 , . . . , qn ], prepended with a [CLS]\nand a special [Q] token, truncated to N=32 tokens (or padded with [MASK] tokens if\nit is shorter), and passed through BERT to get output vectors q = [q1 , . . . , qN ]. The\npassage d with tokens [d1 , . . . , dm ], is processed similarly, including a [CLS] and\nspecial [D] token. A linear layer is applied on top of d and q to control the output\ndimension, so as to keep the vectors small for storage efficiency, and vectors are\nrescaled to unit length, producing the final vector sequences Eq (length N) and Ed\n(length m). The ColBERT scoring mechanism is:\nscore(q, d) =\n\nN\nX\ni=1\n\nm\n\nmax Eqi · Ed j\nj=1\n\n(11.19)\n\nWhile the interaction mechanism has no tunable parameters, the ColBERT ar-\n\n\f11.2\n\n•\n\nI NFORMATION R ETRIEVAL WITH D ENSE V ECTORS\n\n13\n\ns(q,d)\n\n∑\nMaxSim\n\nnorm\n\nnorm\n\nMaxSim\n\nnorm\n\nnorm\n\nMaxSim\n\nnorm\n\nnorm\n\n…\n…\n…\n…\n…\n…\n\nQuery\n\nDocument\n\nFigure 11.8 A sketch of the ColBERT algorithm at inference time. The query and document are first passed through separate BERT encoders. Similarity between query and document is computed by summing a soft alignment between the contextual representations of\ntokens in the query and the document. Training is end-to-end. (Various details aren’t depicted; for example the query is prepended by a [CLS] and [Q:] tokens, and the document\nby [CLS] and [D:] tokens). Figure adapted from Khattab and Zaharia (2020).\n\nFaiss\n\nchitecture still needs to be trained end-to-end to fine-tune the BERT encoders and\ntrain the linear layers (and the special [Q] and [D] embeddings) from scratch. It is\ntrained on triples hq, d + , d − i of query q, positive document d + and negative document d − to produce a score for each document using Eq. 11.19, optimizing model\nparameters using a cross-entropy loss.\nAll the supervised algorithms (like ColBERT or the full-interaction version of\nthe BERT algorithm applied for reranking) need training data in the form of queries\ntogether with relevant and irrelevant passages or documents (positive and negative\nexamples). There are various semi-supervised ways to get labels; some datasets\n(like MS MARCO Ranking, Section 11.4) contain gold positive examples. Negative\nexamples can be sampled randomly from the top-1000 results from some existing\nIR system. If datasets don’t have labeled positive examples, iterative methods like\nrelevance-guided supervision can be used (Khattab et al., 2021) which rely on the\nfact that many datasets contain short answer strings. In this method, an existing IR\nsystem is used to harvest examples that do contain short answer strings (the top few\nare taken as positives) or don’t contain short answer strings (the top few are taken as\nnegatives), these are used to train a new retriever, and then the process is iterated.\nEfficiency is an important issue, since every possible document must be ranked\nfor its similarity to the query. For sparse word-count vectors, the inverted index\nallows this very efficiently. For dense vector algorithms finding the set of dense\ndocument vectors that have the highest dot product with a dense query vector is\nan instance of the problem of nearest neighbor search. Modern systems therefore make use of approximate nearest neighbor vector search algorithms like Faiss\n(Johnson et al., 2017).\n\n\f14\n\nC HAPTER 11\n\n11.3\n\n•\n\nR ETRIEVAL - BASED M ODELS\n\nAnswering Questions with RAG\nHere we introduce an important paradigm for using LLMs to answer knowledgebased questions, based on first finding supportive text segments from the web or\nanother other large collection of documents, and then generating an answer based\non the documents. The method of generating based on retrieved documents is\ncalled retrieval-augmented generation or RAG, and the two components are sometimes called, for historical reasons, the retriever and the reader (Chen et al., 2017).\nFig. 11.9 sketches out this standard model for answering questions.\nquery\n\nRetriever\nQ: When was\nthe premiere of\nThe Magic Flute?\n\ndocs\n\nReader/\nGenerator\nLLM\n\nIndexed Docs\n\nRelevant\nDocs\n\nA: 1791\n\nprompt\n\nFigure 11.9 Retrieval-based question answering has two stages: retrieval, which returns relevant documents\nfrom the collection, and reading, in which an LLM generates answers given the documents as a prompt.\n\nIn the first stage of the 2-stage retrieve and read model in Fig. 11.9 we retrieve\nrelevant passages from a text collection, for example using the dense retrievers of the\nprevious section. In the second reader stage, we generate the answer via retrievalaugmented generation. In this method, we take a large pretrained language model,\ngive it the set of retrieved passages and other text as its prompt, and autoregressively\ngenerate a new answer token by token.\n\n11.3.1\nretrievalaugmented\ngeneration\nRAG\n\nRetrieval-Augmented Generation\n\nThe standard reader algorithm is to generate from a large language model, conditioned on the retrieved passages. This method is known as retrieval-augmented\ngeneration, or RAG.\nRecall that in simple conditional generation, we can cast the task of question\nanswering as word prediction by giving a language model a question and a token\nlike A: suggesting that an answer should come next:\nQ: Who wrote the book ‘‘The Origin of Species\"?\n\nA:\n\nThen we generate autoregressively conditioned on this text.\nMore formally, recall that simple autoregressive language modeling computes\nthe probability of a string from the previous tokens:\np(x1 , . . . , xn ) =\n\nn\nY\n\np(xi |x<i )\n\ni=1\n\nAnd simple conditional generation for question answering adds a prompt like Q: ,\nfollowed by a query q , and A:, all concatenated:\np(x1 , . . . , xn ) =\n\nn\nY\ni=1\n\np([Q:] ; q ; [A:] ; x<i )\n\n\f11.4\n\n•\n\nQ UESTION A NSWERING DATASETS\n\n15\n\nThe advantage of using a large language model is the enormous amount of\nknowledge encoded in its parameters from the text it was pretrained on. But as\nwe mentioned at the start of the chapter, while this kind of simple prompted generation can work fine for many simple factoid questions, it is not a general solution\nfor QA, because it leads to hallucination, is unable to show users textual evidence to\nsupport the answer, and is unable to answer questions from proprietary data.\nThe idea of retrieval-augmented generation is to address these problems by conditioning on the retrieved passages as part of the prefix, perhaps with some prompt\ntext like “Based on these texts, answer this question:”. Let’s suppose we have a\nquery q, and call the set of retrieved passages based on it R(q). For example, we\ncould have a prompt like:\nSchematic of a RAG Prompt\nretrieved passage 1\nretrieved passage 2\n...\nretrieved passage n\nBased on these texts, answer this question:\nthe book ‘‘The Origin of Species\"? A:\n\nQ: Who wrote\n\nOr more formally,\np(x1 , . . . , xn ) =\n\nn\nY\n\np(xi |R(q) ; prompt ; [Q:] ; q ; [A:] ; x<i )\n\ni=1\n\nmulti-hop\n\n11.4\n\nAs with the span-based extraction reader, successfully applying the retrievalaugmented generation algorithm for QA requires a successful retriever, and often\na two-stage retrieval algorithm is used in which the retrieval is reranked. Some\ncomplex questions may require multi-hop architectures, in which a query is used to\nretrieve documents, which are then appended to the original query for a second stage\nof retrieval. Details of prompt engineering also have to be worked out, like deciding\nwhether to demarcate passages, for example with [SEP] tokens, and so on. Combinations of private data and public data involving an externally hosted large language\nmodel may lead to privacy concerns that need to be worked out (Arora et al., 2023).\nMuch research in this area also focuses on ways to more tightly integrate the retrieval\nand reader stages.\n\nQuestion Answering Datasets\nThere are scores of question answering datasets, used both for instruction tuning and\nfor evaluation of the question answering abilities of language models.\nWe can distinguish the datasets along many dimensions, summarized nicely in\nRogers et al. (2023). One is the original purpose of the questions in the data, whether\nthey were natural information-seeking questions, or whether they were questions\ndesigned for probing: evaluating or testing systems or humans.\n\n\f16\n\nC HAPTER 11\nNatural\nQuestions\n\nMS MARCO\n\nTyDi QA\n\nMMLU\n\nreading\ncomprehension\n\nopen book\nclosed book\n\n•\n\nR ETRIEVAL - BASED M ODELS\n\nOn the natural side there are datasets like Natural Questions (Kwiatkowski\net al., 2019), a set of anonymized English queries to the Google search engine and\ntheir answers. The answers are created by annotators based on Wikipedia information, and include a paragraph-length long answer and a short span answer. For\nexample the question “When are hops added to the brewing process?” has the short\nanswer the boiling process and a long answer which is an entire paragraph from the\nWikipedia page on Brewing.\nA similar natural question set is the MS MARCO (Microsoft Machine Reading\nComprehension) collection of datasets, including 1 million real anonymized English\nquestions from Microsoft Bing query logs together with a human generated answer\nand 9 million passages (Bajaj et al., 2016), that can be used both to test retrieval\nranking and question answering.\nAlthough many datasets focus on English, natural information-seeking question datasets exist in other languages. The DuReader dataset is a Chinese QA resource based on search engine queries and community QA (He et al., 2018). TyDi\nQA dataset contains 204K question-answer pairs from 11 typologically diverse languages, including Arabic, Bengali, Kiswahili, Russian, and Thai (Clark et al., 2020).\nIn the T Y D I QA task, a system is given a question and the passages from a Wikipedia article and must (a) select the passage containing the answer (or N ULL if no\npassage contains the answer), and (b) mark the minimal answer span (or N ULL).\nOn the probing side are datasets like MMLU (Massive Multitask Language Understanding), a commonly-used dataset of 15908 knowledge and reasoning questions in 57 areas including medicine, mathematics, computer science, law, and others. MMLU questions are sourced from various exams for humans, such as the US\nGraduate Record Exam, Medical Licensing Examination, and Advanced Placement\nexams. So the questions don’t represent people’s information needs, but rather are\ndesigned to test human knowledge for academic or licensing purposes. Fig. 11.10\nshows some examples, with the correct answers in bold.\nSome of the question datasets described above augment each question with passage(s) from which the answer can be extracted. These datasets were mainly created\nfor an earlier QA task called reading comprehension in which a model is given\na question and a document and is required to extract the answer from the given\ndocument. We sometimes call the task of question answering given one or more\ndocuments (for example via RAG), the open book QA task, while the task of answering directly from the LM with no retrieval component at all is the closed book\nQA task.5 Thus datasets like Natural Questions can be treated as open book if the\nsolver uses each question’s attached document, or closed book if the documents are\nnot used, while datasets like MMLU are solely closed book.\nAnother dimension of variation is the format of the answer: multiple-choice\nversus freeform. And of course there are variations in prompting, like whether the\nmodel is just the question (zero-shot) or also given demonstrations of answers to\nsimilar questions (few-shot). MMLU offers both zero-shot and few-shot prompt\noptions.\n\n5\n\nThis repurposes the word for types of exams in which students are allowed to ‘open their books’ or\nnot.\n\n\f11.5\n\n•\n\nE VALUATING Q UESTION A NSWERING\n\n17\n\nMMLU examples\nCollege Computer Science\nAny set of Boolean operators that is sufficient to represent all Boolean expressions is said to be complete. Which of the following is NOT complete?\n(A) AND, NOT\n(B) NOT, OR\n(C) AND, OR\n(D) NAND\nCollege Physics\nThe primary source of the Sun’s energy is a series of thermonuclear\nreactions in which the energy produced is c2 times the mass difference\nbetween\n(A) two hydrogen atoms and one helium atom\n(B) four hydrogen atoms and one helium atom\n(C) six hydrogen atoms and two helium atoms\n(D) three helium atoms and one carbon atom\nInternational Law\nWhich of the following is a treaty-based human rights mechanism?\n(A) The UN Human Rights Committee\n(B) The UN Human Rights Council\n(C) The UN Universal Periodic Review\n(D) The UN special mandates\nPrehistory\nUnlike most other early civilizations, Minoan culture shows little evidence\nof\n(A) trade.\n(B) warfare.\n(C) the development of a common religion.\n(D) conspicuous consumption by elites.\nFigure 11.10\n\n11.5\n\nExample problems from MMLU\n\nEvaluating Question Answering\nThree techniques are commonly employed to evaluate question-answering systems,\nwith the choice depending on the type of question and QA situation. For multiple\nchoice questions like in MMLU, we report exact match:\nExact match: The % of predicted answers that match the gold answer\nexactly.\nFor questions with free text answers, like Natural Questions, we commonly evaluated with token F1 score to roughly measure the partial string overlap between the\nanswer and the reference answer:\nF1 score: The average token overlap between predicted and gold answers. Treat the prediction and gold as a bag of tokens, and compute F1\nfor each question, then return the average F1 over all questions.\n\n\f18\n\nC HAPTER 11\n\nmean\nreciprocal rank\nMRR\n\n•\n\nR ETRIEVAL - BASED M ODELS\n\nFinally, in some situations QA systems give multiple ranked answers. In such cases\nwe evaluated using mean reciprocal rank, or MRR (Voorhees, 1999). MRR is\ndesigned for systems that return a short ranked list of answers or passages for each\ntest set question, which we can compare against the (human-labeled) correct answer.\nFirst, each test set question is scored with the reciprocal of the rank of the first\ncorrect answer. For example if the system returned five answers to a question but\nthe first three are wrong (so the highest-ranked correct answer is ranked fourth), the\nreciprocal rank for that question is 41 . The score for questions that return no correct\nanswer is 0. The MRR of a system is the average of the scores for each question in\nthe test set. In some versions of MRR, questions with a score of zero are ignored\nin this calculation. More formally, for a system returning ranked answers to each\nquestion in a test set Q, (or in the alternate version, let Q be the subset of test set\nquestions that have non-zero scores). MRR is then defined as\n|Q|\n\n1 X 1\nMRR =\n|Q|\nranki\n\n(11.20)\n\ni=1\n\n11.6\n\nSummary\nThis chapter introduced the tasks of question answering and information retrieval.\n• Question answering (QA) is the task of answering a user’s questions.\n• We focus in this chapter on the task of retrieval-based question answering,\nin which the user’s questions are intended to be answered by the material in\nsome set of documents (which might be the web).\n• Information Retrieval (IR) is the task of returning documents to a user based\non their information need as expressed in a query. In ranked retrieval, the\ndocuments are returned in ranked order.\n• The match between a query and a document can be done by first representing\neach of them with a sparse vector that represents the frequencies of words,\nweighted by tf-idf or BM25. Then the similarity can be measured by cosine.\n• Documents or queries can instead be represented by dense vectors, by encoding the question and document with an encoder-only model like BERT, and in\nthat case computing similarity in embedding space.\n• The inverted index is a storage mechanism that makes it very efficient to find\ndocuments that have a particular word.\n• Ranked retrieval is generally evaluated by mean average precision or interpolated precision.\n• Question answering systems generally use the retriever/reader architecture.\nIn the retriever stage, an IR system is given a query and returns a set of\ndocuments.\n• The reader stage is implemented by retrieval-augmented generation, in\nwhich a large language model is prompted with the query and a set of documents and then conditionally generates a novel answer.\n• QA can be evaluated by exact match with a known answer if only a single\nanswer is given, with token F1 score for free text answers, or with mean reciprocal rank if a ranked set of answers is given.\n\n\fH ISTORICAL N OTES\n\n19\n\nHistorical Notes\nQuestion answering was one of the earliest NLP tasks. By 1961 the BASEBALL\nsystem (Green et al., 1961) answered questions about baseball games like “Where\ndid the Red Sox play on July 7” by querying a structured database of game information. The database was stored as a kind of attribute-value matrix with values for\nattributes of each game:\nMonth = July\nPlace = Boston\nDay = 7\nGame Serial No. = 96\n(Team = Red Sox, Score = 5)\n(Team = Yankees, Score = 3)\nEach question was constituency-parsed using the algorithm of Zellig Harris’s\nTDAP project at the University of Pennsylvania, essentially a cascade of finite-state\ntransducers (see the historical discussion in Joshi and Hopely 1999 and Karttunen\n1999). Then in a content analysis phase each word or phrase was associated with a\nprogram that computed parts of its meaning. Thus the phrase ‘Where’ had code to\nassign the semantics Place = ?, with the result that the question “Where did the\nRed Sox play on July 7” was assigned the meaning\nPlace = ?\nTeam = Red Sox\nMonth = July\nDay = 7\nThe question is then matched against the database to return the answer.\nThe Protosynthex system of Simmons et al. (1964), given a question, formed a\nquery from the content words in the question, and then retrieved candidate answer\nsentences in the document, ranked by their frequency-weighted term overlap with\nthe question. The query and each retrieved sentence were then parsed with dependency parsers, and the sentence whose structure best matches the question structure\nselected. Thus the question What do worms eat? would match worms eat grass:\nboth have the subject worms as a dependent of eat, in the version of dependency\ngrammar used at the time, while birds eat worms has birds as the subject:\n\nWhat do worms eat\n\nLUNAR\n\nWorms eat grass\n\nBirds eat worms\n\nSimmons (1965) summarizes other early QA systems.\nBy the 1970s, systems used predicate calculus as the meaning representation\nlanguage. The LUNAR system (Woods et al. 1972, Woods 1978) was designed to\nbe a natural language interface to a database of chemical facts about lunar geology. It\ncould answer questions like Do any samples have greater than 13 percent aluminum\nby parsing them into a logical form\n(TEST (FOR SOME X16 / (SEQ SAMPLES) : T ; (CONTAIN’ X16\n(NPR* X17 / (QUOTE AL203)) (GREATERTHAN 13 PCT))))\nBy the 1990s question answering shifted to machine learning. Zelle and Mooney\n(1996) proposed to treat question answering as a semantic parsing task, by creat-\n\n\f20\n\nC HAPTER 11\n\n•\n\nR ETRIEVAL - BASED M ODELS\n\ning the Prolog-based GEOQUERY dataset of questions about US geography. This\nmodel was extended by Zettlemoyer and Collins (2005) and 2007. By a decade\nlater, neural models were applied to semantic parsing (Dong and Lapata 2016, Jia\nand Liang 2016), and then to knowledge-based question answering by mapping text\nto SQL (Iyer et al., 2017).\n[TBD: History of IR.]\nMeanwhile, a paradigm for answering questions that drew more on informationretrieval was influenced by the rise of the web in the 1990s. The U.S. governmentsponsored TREC (Text REtrieval Conference) evaluations, run annually since 1992,\nprovide a testbed for evaluating information-retrieval tasks and techniques (Voorhees\nand Harman, 2005). TREC added an influential QA track in 1999, which led to a\nwide variety of factoid and non-factoid question answering systems competing in\nannual evaluations.\nAt that same time, Hirschman et al. (1999) introduced the idea of using children’s reading comprehension tests to evaluate machine text comprehension algorithms. They acquired a corpus of 120 passages with 5 questions each designed for\n3rd-6th grade children, built an answer extraction system, and measured how well\nthe answers given by their system corresponded to the answer key from the test’s\npublisher. Their algorithm focused on word overlap as a feature; later algorithms\nadded named entity features and more complex similarity between the question and\nthe answer span (Riloff and Thelen 2000, Ng et al. 2000).\nThe DeepQA component of the Watson Jeopardy! system was a large and sophisticated feature-based system developed just before neural systems became common. It is described in a series of papers in volume 56 of the IBM Journal of Research and Development, e.g., Ferrucci (2012).\nEarly neural reading comprehension systems drew on the insight common to\nearly systems that answer finding should focus on question-passage similarity. Many\nof the architectural outlines of these neural systems were laid out in Hermann et al.\n(2015), Chen et al. (2017), and Seo et al. (2017). These systems focused on datasets\nlike Rajpurkar et al. (2016) and Rajpurkar et al. (2018) and their successors, usually\nusing separate IR algorithms as input to neural reading comprehension systems. The\nparadigm of using dense retrieval with a span-based reader, often with a single endto-end architecture, is exemplified by systems like Lee et al. (2019) or Karpukhin\net al. (2020). An important research area with dense retrieval for open-domain QA\nis training data: using self-supervised methods to avoid having to label positive and\nnegative passages (Sachan et al., 2023).\nEarly work on large language models showed that they stored sufficient knowledge in the pretraining process to answer questions (Petroni et al., 2019; Raffel\net al., 2020; Radford et al., 2019; Roberts et al., 2020), at first not competitively\nwith special-purpose question answerers, but quickly surpassing them. Retrievalaugmented generation algorithms were first introduced as a way to improve language modeling word prediction (Khandelwal et al., 2019), but were quickly applied\nto question answering (Izacard et al., 2022; Ram et al., 2023; Shi et al., 2023).\n\nExercises\n\n\fExercises\n\n21\n\nArora, S., P. Lewis, A. Fan, J. Kahn, and C. Ré. 2023. Reasoning over public and private data in retrieval-based systems. TACL, 11:902–921.\n\nJohnson, J., M. Douze, and H. Jégou. 2017. Billionscale similarity search with GPUs. ArXiv preprint\narXiv:1702.08734.\n\nBabbage, C. 1864. Passages from the Life of a Philosopher.\nLongman.\n\nJoshi, A. K. and P. Hopely. 1999. A parser from antiquity.\nIn A. Kornai, ed., Extended Finite State Models of Language, 6–15. Cambridge University Press.\n\nBajaj, P., D. Campos, N. Craswell, L. Deng, J. G. ando\nXiaodong Liu, R. Majumder, A. McNamara, B. Mitra,\nT. Nguye, M. Rosenberg, X. Song, A. Stoica, S. Tiwary,\nand T. Wang. 2016. MS MARCO: A human generated\nMAchine Reading COmprehension dataset. NeurIPS.\nChen, D., A. Fisch, J. Weston, and A. Bordes. 2017. Reading\nWikipedia to answer open-domain questions. ACL.\nClark, J. H., E. Choi, M. Collins, D. Garrette,\nT. Kwiatkowski, V. Nikolaev, and J. Palomaki. 2020.\nTyDi QA: A benchmark for information-seeking question answering in typologically diverse languages. TACL,\n8:454–470.\nDahl, M., V. Magesh, M. Suzgun, and D. E. Ho. 2024. Large\nlegal fictions: Profiling legal hallucinations in large language models. Journal of Legal Analysis, 16:64–93.\nDeerwester, S. C., S. T. Dumais, T. K. Landauer, G. W. Furnas, and R. A. Harshman. 1990. Indexing by latent semantics analysis. JASIS, 41(6):391–407.\nDong, L. and M. Lapata. 2016. Language to logical form\nwith neural attention. ACL.\nFerrucci, D. A. 2012. Introduction to “This is Watson”. IBM\nJournal of Research and Development, 56(3/4):1:1–1:15.\nFurnas, G. W., T. K. Landauer, L. M. Gomez, and S. T.\nDumais. 1987. The vocabulary problem in humansystem communication. Communications of the ACM,\n30(11):964–971.\nGeva, M., R. Schuster, J. Berant, and O. Levy. 2021.\nTransformer feed-forward layers are key-value memories.\nEMNLP.\nGreen, B. F., A. K. Wolf, C. Chomsky, and K. Laughery.\n1961. Baseball: An automatic question answerer. Proceedings of the Western Joint Computer Conference 19.\nHe, W., K. Liu, J. Liu, Y. Lyu, S. Zhao, X. Xiao, Y. Liu,\nY. Wang, H. Wu, Q. She, X. Liu, T. Wu, and H. Wang.\n2018. DuReader: a Chinese machine reading comprehension dataset from real-world applications. Workshop\non Machine Reading for Question Answering.\nHermann, K. M., T. Kocisky, E. Grefenstette, L. Espeholt,\nW. Kay, M. Suleyman, and P. Blunsom. 2015. Teaching\nmachines to read and comprehend. NeurIPS.\n\nJurafsky, D. 2014. The Language of Food. W. W. Norton,\nNew York.\nKamphuis, C., A. P. de Vries, L. Boytsov, and J. Lin. 2020.\nWhich BM25 do you mean? a large-scale reproducibility study of scoring variants. European Conference on\nInformation Retrieval.\nKarpukhin, V., B. Oğuz, S. Min, P. Lewis, L. Wu, S. Edunov,\nD. Chen, and W.-t. Yih. 2020. Dense passage retrieval for\nopen-domain question answering. EMNLP.\nKarttunen, L. 1999. Comments on Joshi. In A. Kornai, ed.,\nExtended Finite State Models of Language, 16–18. Cambridge University Press.\nKhandelwal, U., O. Levy, D. Jurafsky, L. Zettlemoyer, and\nM. Lewis. 2019. Generalization through memorization:\nNearest neighbor language models. ICLR.\nKhattab, O., C. Potts, and M. Zaharia. 2021. Relevanceguided supervision for OpenQA with ColBERT. TACL,\n9:929–944.\nKhattab, O. and M. Zaharia. 2020. ColBERT: Efficient and\neffective passage search via contextualized late interaction over BERT. SIGIR.\nKwiatkowski, T., J. Palomaki, O. Redfield, M. Collins,\nA. Parikh, C. Alberti, D. Epstein, I. Polosukhin, J. Devlin, K. Lee, K. Toutanova, L. Jones, M. Kelcey, M.-W.\nChang, A. M. Dai, J. Uszkoreit, Q. Le, and S. Petrov.\n2019. Natural questions: A benchmark for question answering research. TACL, 7:452–466.\nLee, K., M.-W. Chang, and K. Toutanova. 2019. Latent retrieval for weakly supervised open domain question answering. ACL.\nManning, C. D., P. Raghavan, and H. Schütze. 2008. Introduction to Information Retrieval. Cambridge.\nMeng, K., D. Bau, A. Andonian, and Y. Belinkov. 2022. Locating and editing factual associations in GPT. NeurIPS,\nvolume 36.\nNg, H. T., L. H. Teo, and J. L. P. Kwan. 2000. A machine learning approach to answering questions for reading comprehension tests. EMNLP.\n\nHirschman, L., M. Light, E. Breck, and J. D. Burger. 1999.\nDeep Read: A reading comprehension system. ACL.\n\nPetroni, F., T. Rocktäschel, S. Riedel, P. Lewis, A. Bakhtin,\nY. Wu, and A. Miller. 2019. Language models as knowledge bases? EMNLP.\n\nIyer, S., I. Konstas, A. Cheung, J. Krishnamurthy, and\nL. Zettlemoyer. 2017. Learning a neural semantic parser\nfrom user feedback. ACL.\n\nRadford, A., J. Wu, R. Child, D. Luan, D. Amodei, and\nI. Sutskever. 2019. Language models are unsupervised\nmultitask learners. OpenAI tech report.\n\nIzacard, G., P. Lewis, M. Lomeli, L. Hosseini, F. Petroni,\nT. Schick, J. Dwivedi-Yu, A. Joulin, S. Riedel, and\nE. Grave. 2022. Few-shot learning with retrieval augmented language models. ArXiv preprint.\n\nRaffel, C., N. Shazeer, A. Roberts, K. Lee, S. Narang,\nM. Matena, Y. Zhou, W. Li, and P. J. Liu. 2020. Exploring\nthe limits of transfer learning with a unified text-to-text\ntransformer. JMLR, 21(140):1–67.\n\nJia, R. and P. Liang. 2016. Data recombination for neural\nsemantic parsing. ACL.\n\nRajpurkar, P., R. Jia, and P. Liang. 2018. Know what you\ndon’t know: Unanswerable questions for SQuAD. ACL.\n\nJiang, C., B. Qi, X. Hong, D. Fu, Y. Cheng, F. Meng, M. Yu,\nB. Zhou, and J. Zhou. 2024. On large language models’\nhallucination with regard to known facts. NAACL HLT.\n\nRajpurkar, P., J. Zhang, K. Lopyrev, and P. Liang. 2016.\nSQuAD: 100,000+ questions for machine comprehension\nof text. EMNLP.\n\n\f22\n\nChapter 11\n\n•\n\nRetrieval-based Models\n\nRam, O., Y. Levine, I. Dalmedigos, D. Muhlgay, A. Shashua,\nK. Leyton-Brown, and Y. Shoham. 2023. In-context\nretrieval-augmented language models. ArXiv preprint.\nRiloff, E. and M. Thelen. 2000. A rule-based question answering system for reading comprehension tests.\nANLP/NAACL workshop on reading comprehension tests.\nRoberts, A., C. Raffel, and N. Shazeer. 2020. How much\nknowledge can you pack into the parameters of a language model? EMNLP.\nRobertson, S., S. Walker, S. Jones, M. M. HancockBeaulieu, and M. Gatford. 1995. Okapi at TREC-3.\nOverview of the Third Text REtrieval Conference (TREC3).\nRogers, A., M. Gardner, and I. Augenstein. 2023. QA dataset\nexplosion: A taxonomy of NLP resources for question\nanswering and reading comprehension. ACM Computing\nSurveys, 55(10):1–45.\nSachan, D. S., M. Lewis, D. Yogatama, L. Zettlemoyer,\nJ. Pineau, and M. Zaheer. 2023. Questions are all you\nneed to train a dense passage retriever. TACL, 11:600–\n616.\nSalton, G. 1971. The SMART Retrieval System: Experiments\nin Automatic Document Processing. Prentice Hall.\nSeo, M., A. Kembhavi, A. Farhadi, and H. Hajishirzi. 2017.\nBidirectional attention flow for machine comprehension.\nICLR.\nShi, W., S. Min, M. Yasunaga, M. Seo, R. James, M. Lewis,\nL. Zettlemoyer, and W.-t. Yih. 2023. REPLUG: Retrievalaugmented black-box language models. ArXiv preprint.\nSimmons, R. F. 1965. Answering English questions by computer: A survey. CACM, 8(1):53–70.\nSimmons, R. F., S. Klein, and K. McConlogue. 1964. Indexing and dependency logic for answering English questions. American Documentation, 15(3):196–204.\nSparck Jones, K. 1972. A statistical interpretation of term\nspecificity and its application in retrieval. Journal of Documentation, 28(1):11–21.\nVoorhees, E. M. 1999. TREC-8 question answering track\nreport. Proceedings of the 8th Text Retrieval Conference.\nVoorhees, E. M. and D. K. Harman. 2005. TREC: Experiment and Evaluation in Information Retrieval. MIT Press.\nWoods, W. A. 1978. Semantics and quantification in natural\nlanguage question answering. In M. Yovits, ed., Advances\nin Computers, 2–64. Academic.\nWoods, W. A., R. M. Kaplan, and B. L. Nash-Webber. 1972.\nThe lunar sciences natural language information system:\nFinal report. Technical Report 2378, BBN.\nZelle, J. M. and R. J. Mooney. 1996. Learning to parse\ndatabase queries using inductive logic programming.\nAAAI.\nZettlemoyer, L. and M. Collins. 2005. Learning to map\nsentences to logical form: Structured classification with\nprobabilistic categorial grammars. Uncertainty in Artificial Intelligence, UAI’05.\nZettlemoyer, L. and M. Collins. 2007. Online learning\nof relaxed CCG grammars for parsing to logical form.\nEMNLP/CoNLL.\nZhou, K., J. Hwang, X. Ren, and M. Sap. 2024. Relying\non the unreliable: The impact of language models’ reluctance to express uncertainty. ACL.\n\n\f",
    "file_path": "/Users/colinsidberry/Downloads/NLP_Textbook/RAG.txt",
    "file_size_kb": 61.27
  },
  {
    "id": "aac54d248e46f619",
    "source": "nlp_textbook",
    "chapter": "Embeddings",
    "filename": "embeddings.txt",
    "content": "Speech and Language Processing. Daniel Jurafsky & James H. Martin.\nrights reserved. Draft of August 24, 2025.\n\nCopyright © 2025.\n\nAll\n\nCHAPTER\n\n5\n\nEmbeddings\n荃者所以在鱼，得鱼而忘荃 Nets are for fish;\nOnce you get the fish, you can forget the net.\n言者所以在意，得意而忘言 Words are for meaning;\nOnce you get the meaning, you can forget the words\n庄子(Zhuangzi), Chapter 26\n\ndistributional\nhypothesis\n\nembeddings\n\nvector\nsemantics\nrepresentation\nlearning\n\nThe asphalt that Los Angeles is famous for occurs mainly on its freeways. But\nin the middle of the city is another patch of asphalt, the La Brea tar pits, and this\nasphalt preserves millions of fossil bones from the last of the Ice Ages of the Pleistocene Epoch. One of these fossils is the Smilodon, or saber-toothed tiger, instantly\nrecognizable by its long canines. Five million years ago or so, a completely different\nsaber-tooth tiger called Thylacosmilus lived\nin Argentina and other parts of South America. Thylacosmilus was a marsupial whereas\nSmilodon was a placental mammal, but Thylacosmilus had the same long upper canines\nand, like Smilodon, had a protective bone\nflange on the lower jaw. The similarity of\nthese two mammals is one of many examples\nof parallel or convergent evolution, in which particular contexts or environments\nlead to the evolution of very similar structures in different species (Gould, 1980).\nThe role of context is also important in the similarity of a less biological kind\nof organism: the word. Words that occur in similar contexts tend to have similar\nmeanings. This link between similarity in how words are distributed and similarity\nin what they mean is called the distributional hypothesis. The hypothesis was\nfirst formulated in the 1950s by linguists like Joos (1950), Harris (1954), and Firth\n(1957), who noticed that words which are synonyms (like oculist and eye-doctor)\ntended to occur in the same environment (e.g., near words like eye or examined)\nwith the amount of meaning difference between two words “corresponding roughly\nto the amount of difference in their environments” (Harris, 1954, p. 157).\nIn this chapter we introduce embeddings, vector representations of the meaning\nof words that are learned directly from word distributions in texts. Embeddings lie\nat the heart of large language models and other modern applications. The static embeddings we introduce here underlie the more powerful dynamic or contextualized\nembeddings like BERT that we will see in Chapter 10 and Chapter 8.\nThe linguistic field that studies embeddings and their meanings is called vector\nsemantics. Embeddings are also the first example in this book of representation\nlearning, automatically learning useful representations of the input text. Finding\nsuch self-supervised ways to learn representations of language, instead of creating representations by hand via feature engineering, is an important principle of\nmodern NLP (Bengio et al., 2013).\n\n\f2\n\nC HAPTER 5\n\n5.1\n\n•\n\nE MBEDDINGS\n\nLexical Semantics\nLet’s begin by introducing some basic principles of word meaning. How should\nwe represent the meaning of a word? In the n-gram models of Chapter 3, and in\nclassical NLP applications, our only representation of a word is as a string of letters,\nor an index in a vocabulary list. This representation is not that different from a\ntradition in philosophy, perhaps you’ve seen it in introductory logic classes, in which\nthe meaning of words is represented by just spelling the word with small capital\nletters; representing the meaning of “dog” as DOG, and “cat” as CAT, or by using an\napostrophe (DOG ’).\nRepresenting the meaning of a word by capitalizing it is a pretty unsatisfactory\nmodel. You might have seen a version of a joke due originally to semanticist Barbara\nPartee (Carlson, 1977):\nQ: What’s the meaning of life?\nA: LIFE ’\n\nlexical\nsemantics\n\nSurely we can do better than this! After all, we’ll want a model of word meaning\nto do all sorts of things for us. It should tell us that some words have similar meanings (cat is similar to dog), others are antonyms (cold is the opposite of hot), some\nhave positive connotations (happy) while others have negative connotations (sad). It\nshould represent the fact that the meanings of buy, sell, and pay offer differing perspectives on the same underlying purchasing event. (If I buy something from you,\nyou’ve probably sold it to me, and I likely paid you.) More generally, a model of\nword meaning should allow us to draw inferences to address meaning-related tasks\nlike question-answering or dialogue.\nIn this section we summarize some of these desiderata, drawing on results in the\nlinguistic study of word meaning, which is called lexical semantics; we’ll return to\nand expand on this list in Appendix G and Chapter 21.\nLemmas and Senses Let’s start by looking at how one word (we’ll choose mouse)\nmight be defined in a dictionary (simplified from the online dictionary WordNet):\nmouse (N)\n1. any of numerous small rodents...\n2. a hand-operated device that controls a cursor...\n\nlemma\ncitation form\n\nwordform\n\nHere the form mouse is the lemma, also called the citation form. The form\nmouse would also be the lemma for the word mice; dictionaries don’t have separate\ndefinitions for inflected forms like mice. Similarly sing is the lemma for sing, sang,\nsung. In many languages the infinitive form is used as the lemma for the verb, so\nSpanish dormir “to sleep” is the lemma for duermes “you sleep”. The specific forms\nsung or carpets or sing or duermes are called wordforms.\nAs the example above shows, each lemma can have multiple meanings; the\nlemma mouse can refer to the rodent or the cursor control device. We call each\nof these aspects of the meaning of mouse a word sense. The fact that lemmas can\nbe polysemous (have multiple senses) can make interpretation difficult (is someone who searches for “mouse info” looking for a pet or a widget?). Chapter 10\nand Appendix G will discuss the problem of polysemy, and introduce word sense\ndisambiguation, the task of determining which sense of a word is being used in a\nparticular context.\nSynonymy One important component of word meaning is the relationship between word senses. For example when one word has a sense whose meaning is\n\n\f5.1\n\nsynonym\n\n•\n\nL EXICAL S EMANTICS\n\n3\n\nidentical to a sense of another word, or nearly identical, we say the two senses of\nthose two words are synonyms. Synonyms include such pairs as\ncouch/sofa vomit/throw up filbert/hazelnut car/automobile\n\nprinciple of\ncontrast\n\nsimilarity\n\nA more formal definition of synonymy (between words rather than senses) is that\ntwo words are synonymous if they are substitutable for one another in any sentence\nwithout changing the truth conditions of the sentence, the situations in which the\nsentence would be true.\nWhile substitutions between some pairs of words like car / automobile or water / H2 O are truth preserving, the words are still not identical in meaning. Indeed,\nprobably no two words are absolutely identical in meaning. One of the fundamental\ntenets of semantics, called the principle of contrast (Girard 1718, Bréal 1897, Clark\n1987), states that a difference in linguistic form is always associated with some difference in meaning. For example, the word H2 O is used in scientific contexts and\nwould be inappropriate in a hiking guide—water would be more appropriate— and\nthis genre difference is part of the meaning of the word. In practice, the word synonym is therefore used to describe a relationship of approximate or rough synonymy.\nWord Similarity While words don’t have many synonyms, most words do have\nlots of similar words. Cat is not a synonym of dog, but cats and dogs are certainly\nsimilar words. In moving from synonymy to similarity, it will be useful to shift from\ntalking about relations between word senses (like synonymy) to relations between\nwords (like similarity). Dealing with words avoids having to commit to a particular\nrepresentation of word senses, which will turn out to simplify our task.\nThe notion of word similarity is very useful in larger semantic tasks. Knowing\nhow similar two words are can help in computing how similar the meaning of two\nphrases or sentences are, a very important component of tasks like question answering, paraphrasing, and summarization. One way of getting values for word similarity\nis to ask humans to judge how similar one word is to another. A number of datasets\nhave resulted from such experiments. For example the SimLex-999 dataset (Hill\net al., 2015) gives values on a scale from 0 to 10, like the examples below, which\nrange from near-synonyms (vanish, disappear) to pairs that scarcely seem to have\nanything in common (hole, agreement):\nvanish disappear 9.8\nbelief impression 5.95\nmuscle bone\n3.65\nmodest flexible\n0.98\nhole\nagreement 0.3\n\nrelatedness\nassociation\n\nsemantic field\n\nWord Relatedness The meaning of two words can be related in ways other than\nsimilarity. One such class of connections is called word relatedness (Budanitsky\nand Hirst, 2006), also traditionally called word association in psychology.\nConsider the meanings of the words coffee and cup. Coffee is not similar to cup;\nthey share practically no features (coffee is a plant or a beverage, while a cup is a\nmanufactured object with a particular shape). But coffee and cup are clearly related;\nthey are associated by co-participating in an everyday event (the event of drinking\ncoffee out of a cup). Similarly scalpel and surgeon are not similar but are related\neventively (a surgeon tends to make use of a scalpel).\nOne common kind of relatedness between words is if they belong to the same\nsemantic field. A semantic field is a set of words which cover a particular semantic\ndomain and bear structured relations with each other. For example, words might be\n\n\f4\n\nC HAPTER 5\n\ntopic models\n\nconnotations\n\nsentiment\n\n•\n\nE MBEDDINGS\n\nrelated by being in the semantic field of hospitals (surgeon, scalpel, nurse, anesthetic, hospital), restaurants (waiter, menu, plate, food, chef), or houses (door, roof,\nkitchen, family, bed). Semantic fields are also related to topic models, like Latent\nDirichlet Allocation, LDA, which apply unsupervised learning on large sets of texts\nto induce sets of associated words from text. Semantic fields and topic models are\nvery useful tools for discovering topical structure in documents.\nIn Appendix G we’ll introduce more relations between senses like hypernymy\nor IS-A, antonymy (opposites) and meronymy (part-whole relations).\nConnotation Finally, words have affective meanings or connotations. The word\nconnotation has different meanings in different fields, but here we use it to mean the\naspects of a word’s meaning that are related to a writer or reader’s emotions, sentiment, opinions, or evaluations. For example some words have positive connotations\n(wonderful) while others have negative connotations (dreary). Even words whose\nmeanings are similar in other ways can vary in connotation; consider the difference\nin connotations between fake, knockoff, forgery, on the one hand, and copy, replica,\nreproduction on the other, or innocent (positive connotation) and naive (negative\nconnotation). Some words describe positive evaluation (great, love) and others negative evaluation (terrible, hate). Positive or negative evaluation language is called\nsentiment, as we saw in Appendix K, and word sentiment plays a role in important tasks like sentiment analysis, stance detection, and applications of NLP to the\nlanguage of politics and consumer reviews.\nEarly work on affective meaning (Osgood et al., 1957) found that words varied\nalong three important dimensions of affective meaning:\nvalence: the pleasantness of the stimulus\narousal: the intensity of emotion provoked by the stimulus\ndominance: the degree of control exerted by the stimulus\nThus words like happy or satisfied are high on valence, while unhappy or annoyed are low on valence. Excited is high on arousal, while calm is low on arousal.\nControlling is high on dominance, while awed or influenced are low on dominance.\nEach word is thus represented by three numbers, corresponding to its value on each\nof the three dimensions:\nValence Arousal Dominance\ncourageous 8.0\n5.5\n7.4\nmusic\n7.7\n5.6\n6.5\nheartbreak 2.5\n5.7\n3.6\ncub\n6.7\n4.0\n4.2\nOsgood et al. (1957) noticed that in using these 3 numbers to represent the\nmeaning of a word, the model was representing each word as a point in a threedimensional space, a vector whose three dimensions corresponded to the word’s\nrating on the three scales. This revolutionary idea that word meaning could be represented as a point in space (e.g., that part of the meaning of heartbreak can be\nrepresented as the point [2.5, 5.7, 3.6]) was the first expression of the vector semantics models that we introduce next.\n\n5.2\n\nVector Semantics: The Intuition\n\nvector\nsemantics\n\nVector semantics is the standard way to represent word meaning in NLP, helping\n\n\f5.2\n\n•\n\nV ECTOR S EMANTICS : T HE I NTUITION\n\n5\n\nus model many of the aspects of word meaning we saw in the previous section. The\nroots of the model lie in the 1950s when two big ideas converged: Osgood’s 1957\nidea mentioned above to use a point in three-dimensional space to represent the\nconnotation of a word, and the proposal by linguists like Joos (1950), Harris (1954),\nand Firth (1957) to define the meaning of a word by its distribution in language\nuse, meaning its neighboring words or grammatical environments. Their idea was\nthat two words that occur in very similar distributions (whose neighboring words are\nsimilar) have similar meanings.\nFor example, suppose you didn’t know the meaning of the word ongchoi (a recent borrowing from Cantonese) but you see it in the following contexts:\n(5.1) Ongchoi is delicious sauteed with garlic.\n(5.2) Ongchoi is superb over rice.\n(5.3) ...ongchoi leaves with salty sauces...\nAnd suppose that you had seen many of these context words in other contexts:\n(5.4) ...spinach sauteed with garlic over rice...\n(5.5) ...chard stems and leaves are delicious...\n(5.6) ...collard greens and other salty leafy greens\nThe fact that ongchoi occurs with words like rice and garlic and delicious and\nsalty, as do words like spinach, chard, and collard greens might suggest that ongchoi\nis a leafy green similar to these other leafy greens.1 We can implement the same\nintuition computationally by just counting words in the context of ongchoi.\n\nFigure 5.1 A two-dimensional (t-SNE) visualization of 200-dimensional word2vec embeddings for some words close to the word sweet, showing that words with similar meanings are nearby in space. Visualization created using the TensorBoard Embedding Projector\nhttps://projector.tensorflow.org/.\n\nembeddings\n\nThe idea of vector semantics is to represent a word as a point in a multidimensional semantic space that is derived (in different ways we’ll see) from the distributions of word neighbors. Vectors for representing words are called embeddings.\nThe word “embedding” derives historically from its mathematical sense as a mapping from one space or structure to another, although the meaning has shifted; see\nthe end of the chapter.\nFig. 5.1 shows a visualization of embeddings learned by the word2vec algorithm,\nshowing the location of selected words (neighbors of “sweet”) projected down from\n1\n\nIt’s in fact Ipomoea aquatica, a relative of morning glory sometimes called water spinach in English.\n\n\f6\n\nC HAPTER 5\n\n•\n\nE MBEDDINGS\n\n200-dimensional space into a 2-dimensional space. Note that the nearest neighbors\nof sweet are semantically related words like honey, candy, juice, chocolate. This idea\nthat similar words are near each other in high-dimensional space is an important\nthat offers enormous power to language models and other NLP applications. For\nexample the sentiment classifiers of Chapter 4 depend on the same words appearing\nin the training and test sets. But by representing words as embeddings, a classifier\ncan assign sentiment as long as it sees some words with similar meanings. And as\nwe’ll see, vector semantic models like the ones showed in Fig. 5.1 can be learned\nautomatically from text without supervision.\nIn this chapter we’ll begin with a simple pedagogical model of embeddings in\nwhich the meaning of a word is defined by a vector with the counts of nearby words.\nWe introduce this model as a helpful way to understand the concept of vectors and\nwhat it means for a vector to be a representation of word meaning, but more sophisticated variants like the tf-idf model we will introduce in Chapter 11 are important\nmethods you should understand. We will see that this method results in very long\nvectors that are sparse, i.e. mostly zeros (since most words simply never occur in the\ncontext of others). We’ll then introduce the word2vec model family for constructing\nshort, dense vectors that have even more useful semantic properties.\nWe’ll also introduce the cosine, the standard way to use embeddings to compute semantic similarity, between two words, two sentences, or two documents, an\nimportant tool in practical applications.\n\n5.3\n\nSimple count-based embeddings\n“The most important attributes of a vector in 3-space are {Location, Location, Location}”\nRandall Munroe, the hover from https://xkcd.com/2358/\n\nword-context\nmatrix\n\nLet’s now introduce the first way to compute word vector embeddings. This simplest vector model of meaning is based on the co-occurrence matrix, a way of representing how often words co-occur. We’ll define a particular kind of co-occurrence\nmatrix, the word-context matrix, in which each row in the matrix represents a word\nin the vocabulary and each column represents how often each other word in the vocabulary appears nearby. This matrix is thus of dimensionality |V | × |V | and each\ncell records the number of times the row (target) word and the column (context)\nword co-occur nearby in some training corpus.\nWhat do we mean by ‘nearby’? We could implement various methods, but let’s\nstart with a very simple one: a context window around the word, let’s say of 4 words\nto the left and 4 words to the right. If we do that, each cell will represents the\nnumber of times (in some training corpus) the column word occurs in such a ±4\nword window around the row word.\nLet’s see how this works for 4 words: cherry, strawberry, digital, and information. For each word we took a single instance from a corpus, and we show the ±4\nword window from that instance:\nis traditionally followed by cherry\npie, a traditional dessert\noften mixed, such as strawberry rhubarb pie. Apple pie\ncomputer peripherals and personal digital\nassistants. These devices usually\na computer. This includes information available on the internet\nIf we then take every occurrence of each word in a large corpus and count the\ncontext words around it, we get a word-context co-occurrence matrix. The full word-\n\n\f5.3\n\n•\n\nS IMPLE COUNT- BASED EMBEDDINGS\n\n7\n\ncontext co-occurrence matrix is very large, because for each word in the vocabulary\n(since |V |) we have to count how often it occurs with every other word in the vocabulary, hence dimensionality |V | × |V |. Let’s therefore instead sketch the process\non a smaller scale. Imagine that we are going to look at only the 4 words, and only\nconsider the following 3 context words: a, computer, and pie. Furthermore let’s\nassume we only count occurrences in the mini-corpus above.\nSo before looking at Fig. 5.2, compute by hand the counts for these 3 context\nwords for the four words cherry, strawberry, digital, and information.\na\ncomputer\npie\ncherry\n1\n0\n1\nstrawberry\n0\n0\n2\ndigital\n0\n1\n0\ninformation\n1\n1\n0\nFigure 5.2 Co-occurrence vectors for four words with counts from the 4 windows above,\nshowing just 3 of the potential context word dimensions. The vector for cherry is outlined in\nred. Note that a real vector would have vastly more dimensions and thus be even sparser.\n\nvector\n\nvector space\ndimension\n\nHopefully your count matches what is shown in Fig. 5.2, so that each cell represents the number of times a particular word (defined by the row) occurs in a particular context (defined by the word column).\nEach row, then, is a vector representing a word. To review some basic linear\nalgebra, a vector is, at heart, just a list or array of numbers. So cherry is represented\nas the list [1,0,1] (the first row vector in Fig. 5.2) and information is represented as\nthe list [1,1,0] (the fourth row vector).\nA vector space is a collection of vectors, and is characterized by its dimension.\nVectors in a 3-dimensional vector space have an element for each dimension of the\nspace. We will loosely refer to a vector in a 3-dimensional space as a 3-dimensional\nvector, with one element along each dimension. In the example in Fig. 5.2, we’ve\nchosen to make the document vectors of dimension 3, just so they fit on the page; in\nreal term-document matrices, the document vectors would have dimensionality |V |,\nthe vocabulary size.\nThe ordering of the numbers in a vector space indicates the different dimensions\non which documents vary. The third dimension for all these vectors corresponds\nto the number of times pie occurs in the context. The second dimension for all of\nthem corresponds to the number of times the word computer occurs. Notice that\nthe vectors for information and digital have the same value (1) for this “computer”\ndimension.\nIn reality, we don’t compute word vectors on a single context window. Instead,\nwe compute them over an entire corpus. Let’s see what some real counts look like.\nLet’s look at some vectors computed in this way. Fig. 5.3 shows a subset of the\nword-word co-occurrence matrix for these four words, where, again because it’s\nimpossible to visualize all |V | possible context words on the page of this textbook,\nwe show a subset of 6 of the dimensions, with counts computed from the Wikipedia\ncorpus (Davies, 2015).\nNote in Fig. 5.3 that the two words cherry and strawberry are more similar to\neach other (both pie and sugar tend to occur in their window) than they are to other\nwords like digital; conversely, digital and information are more similar to each other\nthan, say, to strawberry.\nWe can think of the vector for a document as a point in |V |-dimensional space;\nthus the documents in Fig. 5.3 are points in 3-dimensional space. Fig. 5.4 shows a\nspatial visualization.\n\n\f8\n\nC HAPTER 5\n\n•\n\nE MBEDDINGS\n\naardvark\n...\ncomputer\ndata\nresult\npie\nsugar\n...\ncherry\n0\n...\n2\n8\n9\n442\n25\n...\nstrawberry\n0\n...\n0\n0\n1\n60\n19\n...\ndigital\n0\n...\n1670\n1683\n85\n5\n4\n...\ninformation\n0\n...\n3325\n3982\n378\n5\n13\n...\nFigure 5.3 Co-occurrence vectors for four words in the Wikipedia corpus, showing six of\nthe dimensions (hand-picked for pedagogical purposes). The vector for digital is outlined in\nred. Note that a real vector would have vastly more dimensions and thus be much sparser, i.e.\nwould have zero values in most dimensions.\n\ncomputer\n\n4000\n\ninformation\n[3982,3325]\n\n3000\n\ndigital\n2000 [1683,1670]\n1000\n\n1000 2000 3000 4000\n\ndata\nFigure 5.4 A spatial visualization of word vectors for digital and information, showing just\ntwo of the dimensions, corresponding to the words data and computer.\n\nNote that |V |, the dimensionality of the vector, is generally the size of the vocabulary, often between 10,000 and 50,000 words (using the most frequent words\nin the training corpus; keeping words after about the most frequent 50,000 or so is\ngenerally not helpful). Since most of these numbers are zero these are sparse vector\nrepresentations; there are efficient algorithms for storing and computing with sparse\nmatrices.\nIt’s also possible to applying various kinds of weighting functions to the counts\nin these cells. The most popular such weighting is tf-idf, which we’ll introduce in\nChapter 11, but there have historically been a wide variety of other weightings.\nNow that we have some intuitions, let’s move on to examine the details of computing word similarity.\n\n5.4\n\nCosine for measuring similarity\n\ndot product\n\nTo measure similarity between two target words v and w, we need a metric that\ntakes two vectors (of the same dimensionality, either both with words as dimensions,\nhence of length |V |, or both with documents as dimensions, of length |D|) and gives\na measure of their similarity. By far the most common similarity metric is the cosine\nof the angle between the vectors.\nThe cosine—like most measures for vector similarity used in NLP—is based on\nthe dot product operator from linear algebra, also called the inner product:\n\ninner product\n\ndot product(v, w) = v · w =\n\nN\nX\n\nvi wi = v1 w1 + v2 w2 + ... + vN wN\n\n(5.7)\n\ni=1\n\nThe dot product acts as a similarity metric because it will tend to be high just when\nthe two vectors have large values in the same dimensions. Alternatively, vectors that\n\n\f5.4\n\nvector length\n\n•\n\nC OSINE FOR MEASURING SIMILARITY\n\n9\n\nhave zeros in different dimensions—orthogonal vectors—will have a dot product of\n0, representing their strong dissimilarity.\nThis raw dot product, however, has a problem as a similarity metric: it favors\nlong vectors. The vector length is defined as\nv\nu N\nuX\n|v| = t\nv2i\n(5.8)\ni=1\n\nThe dot product is higher if a vector is longer, with higher values in each dimension.\nMore frequent words have longer vectors, since they tend to co-occur with more\nwords and have higher co-occurrence values with each of them. The raw dot product\nthus will be higher for frequent words. But this is a problem; we’d like a similarity\nmetric that tells us how similar two words are regardless of their frequency.\nWe modify the dot product to normalize for the vector length by dividing the\ndot product by the lengths of each of the two vectors. This normalized dot product\nturns out to be the same as the cosine of the angle between the two vectors, following\nfrom the definition of the dot product between two vectors a and b:\na · b = |a||b| cos θ\na·b\n= cos θ\n|a||b|\ncosine\n\n(5.9)\n\nThe cosine similarity metric between two vectors v and w thus can be computed as:\nN\nX\n\nvi wi\nv·w\ni=1\nv\ncosine(v, w) =\n=v\nu N\n|v||w| u\nN\nuX\nuX\nt\nv2 t\nw2\ni\n\ni=1\n\nunit vector\n\n(5.10)\n\ni\n\ni=1\n\nFor some applications we pre-normalize each vector, by dividing it by its length,\ncreating a unit vector of length 1. Thus we could compute a unit vector from a by\ndividing it by |a|. For unit vectors, the dot product is the same as the cosine.\nThe cosine value ranges from 1 for vectors pointing in the same direction, through\n0 for orthogonal vectors, to -1 for vectors pointing in opposite directions. But since\nraw frequency values are non-negative, the cosine for these vectors ranges from 0–1.\nLet’s see how the cosine computes which of the words cherry or digital is closer\nin meaning to information, just using raw counts from the following shortened table:\npie data computer\ncherry\n442 8\n2\ndigital\n5 1683\n1670\ninformation 5 3982\n3325\n\n442 ∗ 5 + 8 ∗ 3982 + 2 ∗ 3325\n√\ncos(cherry, information) = √\n= .018\n4422 + 82 + 22 52 + 39822 + 33252\n5 ∗ 5 + 1683 ∗ 3982 + 1670 ∗ 3325\n√\ncos(digital, information) = √\n= .996\n2\n5 + 16832 + 16702 52 + 39822 + 33252\nThe model decides that information is way closer to digital than it is to cherry, a\nresult that seems sensible. Fig. 5.5 shows a visualization.\n\n\f10\n\n•\n\nE MBEDDINGS\n\nDimension 1: ‘pie’\n\nC HAPTER 5\n\n500\n\ncherry\ndigital\n500\n\n1000\n\n1500\n\ninformation\n2000\n\n2500\n\n3000\n\nDimension 2: ‘computer’\n\nFigure 5.5 A (rough) graphical demonstration of cosine similarity, showing vectors for\nthree words (cherry, digital, and information) in the two dimensional space defined by counts\nof the words computer and pie nearby. The figure doesn’t show the cosine, but it highlights the\nangles; note that the angle between digital and information is smaller than the angle between\ncherry and information. When two vectors are more similar, the cosine is larger but the angle\nis smaller; the cosine has its maximum (1) when the angle between two vectors is smallest\n(0◦ ); the cosine of all other angles is less than 1.\n\ncan be used to compute word similarity, for tasks like finding word paraphrases,\ntracking changes in word meaning, or automatically discovering meanings of words\nin different corpora. For example, we can find the 10 most similar words to any\ntarget word w by computing the cosines between w and each of the |V | − 1 other\nwords, sorting, and looking at the top 10.\n\n5.5\n\nWord2vec\n\nskip-gram\nSGNS\nword2vec\n\nIn the previous sections we saw how to represent a word as a sparse, long vector with\ndimensions corresponding to words in the vocabulary. We now introduce a more\npowerful word representation: embeddings, short dense vectors. Unlike the vectors\nwe’ve seen so far, embeddings are short, with number of dimensions d ranging from\n50-1000, rather than the much larger vocabulary size |V |.These d dimensions don’t\nhave a clear interpretation. And the vectors are dense: instead of vector entries\nbeing sparse, mostly-zero counts or functions of counts, the values will be realvalued numbers that can be negative.\nIt turns out that dense vectors work better in every NLP task than sparse vectors.\nWhile we don’t completely understand all the reasons for this, we have some intuitions. Representing words as 300-dimensional dense vectors requires our classifiers\nto learn far fewer weights than if we represented words as 50,000-dimensional vectors, and the smaller parameter space possibly helps with generalization and avoiding overfitting. Dense vectors may also do a better job of capturing synonymy.\nFor example, in a sparse vector representation, dimensions for synonyms like car\nand automobile dimension are distinct and unrelated; sparse vectors may thus fail\nto capture the similarity between a word with car as a neighbor and a word with\nautomobile as a neighbor.\nIn this section we introduce one method for computing embeddings: skip-gram\nwith negative sampling, sometimes called SGNS. The skip-gram algorithm is one\nof two algorithms in a software package called word2vec, and so sometimes the\nalgorithm is loosely referred to as word2vec (Mikolov et al. 2013a, Mikolov et al.\n2013b). The word2vec methods are fast, efficient to train, and easily available online with code and pretrained embeddings. Word2vec embeddings are static em-\n\n\f5.5\nstatic\nembeddings\n\nself-supervision\n\n•\n\nW ORD 2 VEC\n\n11\n\nbeddings, meaning that the method learns one fixed embedding for each word in the\nvocabulary. In Chapter 10 we’ll introduce methods for learning dynamic contextual\nembeddings like the popular family of BERT representations, in which the vector\nfor each word is different in different contexts.\nThe intuition of word2vec is that instead of counting how often each word w occurs near, say, apricot, we’ll instead train a classifier on a binary prediction task: “Is\nword w likely to show up near apricot?” We don’t actually care about this prediction\ntask; instead we’ll take the learned classifier weights as the word embeddings.\nThe revolutionary intuition here is that we can just use running text as implicitly\nsupervised training data for such a classifier; a word c that occurs near the target\nword apricot acts as gold ‘correct answer’ to the question “Is word c likely to show\nup near apricot?” This method, often called self-supervision, avoids the need for\nany sort of hand-labeled supervision signal. This idea was first proposed in the task\nof neural language modeling, when Bengio et al. (2003) and Collobert et al. (2011)\nshowed that a neural language model (a neural network that learned to predict the\nnext word from prior words) could just use the next word in running text as its\nsupervision signal, and could be used to learn an embedding representation for each\nword as part of doing this prediction task.\nWe’ll see how to do neural networks in the next chapter, but word2vec is a\nmuch simpler model than the neural network language model, in two ways. First,\nword2vec simplifies the task (making it binary classification instead of word prediction). Second, word2vec simplifies the architecture (training a logistic regression\nclassifier instead of a multi-layer neural network with hidden layers that demand\nmore sophisticated training algorithms). The intuition of skip-gram is:\n1. Treat the target word and a neighboring context word as positive examples.\n2. Randomly sample other words in the lexicon to get negative samples.\n3. Use logistic regression to train a classifier to distinguish those two cases.\n4. Use the learned weights as the embeddings.\n\n5.5.1\n\nThe classifier\n\nLet’s start by thinking about the classification task, and then turn to how to train.\nImagine a sentence like the following, with a target word apricot, and assume we’re\nusing a window of ±2 context words:\n... lemon,\n\na [tablespoon of apricot jam,\nc1\nc2\nw\nc3\n\na] pinch ...\nc4\n\nOur goal is to train a classifier such that, given a tuple (w, c) of a target word\nw paired with a candidate context word c (for example (apricot, jam), or perhaps\n(apricot, aardvark)) it will return the probability that c is a real context word (true\nfor jam, false for aardvark):\nP(+|w, c)\n\n(5.11)\n\nThe probability that word c is not a real context word for w is just 1 minus\nEq. 5.11:\nP(−|w, c) = 1 − P(+|w, c)\n\n(5.12)\n\nHow does the classifier compute the probability P? The intuition of the skipgram model is to base this probability on embedding similarity: a word is likely to\n\n\f12\n\nC HAPTER 5\n\n•\n\nE MBEDDINGS\n\noccur near the target if its embedding vector is similar to the target embedding. To\ncompute similarity between these dense embeddings, we rely on the intuition that\ntwo vectors are similar if they have a high dot product (after all, cosine is just a\nnormalized dot product). In other words:\nSimilarity(w, c) ≈ c · w\n\n(5.13)\n\nThe dot product c · w is not a probability, it’s just a number ranging from −∞ to ∞\n(since the elements in word2vec embeddings can be negative, the dot product can be\nnegative). To turn the dot product into a probability, we’ll use the logistic or sigmoid\nfunction σ (x), the fundamental core of logistic regression:\nσ (x) =\n\n1\n1 + exp (−x)\n\n(5.14)\n\nWe model the probability that word c is a real context word for target word w as:\nP(+|w, c) = σ (c · w) =\n\n1\n1 + exp (−c · w)\n\n(5.15)\n\nThe sigmoid function returns a number between 0 and 1, but to make it a probability\nwe’ll also need the total probability of the two possible events (c is a context word,\nand c isn’t a context word) to sum to 1. We thus estimate the probability that word c\nis not a real context word for w as:\nP(−|w, c) = 1 − P(+|w, c)\n= σ (−c · w) =\n\n1\n1 + exp (c · w)\n\n(5.16)\n\nEquation 5.15 gives us the probability for one word, but there are many context\nwords in the window. Skip-gram makes the simplifying assumption that all context\nwords are independent, allowing us to just multiply their probabilities:\nP(+|w, c1:L ) =\n\nlog P(+|w, c1:L ) =\n\nL\nY\n\nσ (ci · w)\n\ni=1\nL\nX\n\nlog σ (ci · w)\n\n(5.17)\n\n(5.18)\n\ni=1\n\nIn summary, skip-gram trains a probabilistic classifier that, given a test target word\nw and its context window of L words c1:L , assigns a probability based on how similar\nthis context window is to the target word. The probability is based on applying the\nlogistic (sigmoid) function to the dot product of the embeddings of the target word\nwith each context word. To compute this probability, we just need embeddings for\neach target word and context word in the vocabulary.\nFig. 5.6 shows the intuition of the parameters we’ll need. Skip-gram actually\nstores two embeddings for each word, one for the word as a target, and one for the\nword considered as context. Thus the parameters we need to learn are two matrices\nW and C, each containing an embedding for every one of the |V | words in the\nvocabulary V .2 Let’s now turn to learning these embeddings (which is the real goal\nof training this classifier in the first place).\n2\n\nIn principle the target matrix and the context matrix could use different vocabularies, but we’ll simplify\nby assuming one shared vocabulary V .\n\n\f5.5\n\n•\n\nW ORD 2 VEC\n\n13\n\n1..d\naardvark\n\n1\n\napricot\n\n𝜽=\n\n…\n\n…\n\nzebra\n\n|V|\n\naardvark\n\nW\n\ntarget words\n\nC\n\ncontext & noise\nwords\n\n|V|+1\n\napricot\n…\n\n…\n\nzebra\n\n2|V|\n\nFigure 5.6 The embeddings learned by the skipgram model. The algorithm stores two embeddings for each word, the target embedding (sometimes called the input embedding) and\nthe context embedding (sometimes called the output embedding). The parameter θ that the algorithm learns is thus a matrix of 2|V | vectors, each of dimension d, formed by concatenating\ntwo matrices, the target embeddings W and the context+noise embeddings C.\n\n5.5.2\n\nLearning skip-gram embeddings\n\nThe learning algorithm for skip-gram embeddings takes as input a corpus of text,\nand a chosen vocabulary size N. It begins by assigning a random embedding vector\nfor each of the N vocabulary words, and then proceeds to iteratively shift the embedding of each word w to be more like the embeddings of words that occur nearby\nin texts, and less like the embeddings of words that don’t occur nearby. Let’s start\nby considering a single piece of training data:\n... lemon,\n\na [tablespoon of apricot jam,\nc1\nc2\nw\nc3\n\na] pinch ...\nc4\n\nThis example has a target word w (apricot), and 4 context words in the L = ±2\nwindow, resulting in 4 positive training instances (on the left below):\npositive examples +\nw\ncpos\napricot tablespoon\napricot of\napricot jam\napricot a\n\nnegative examples w\ncneg\nw\ncneg\napricot aardvark apricot seven\napricot my\napricot forever\napricot where\napricot dear\napricot coaxial apricot if\n\nFor training a binary classifier we also need negative examples. In fact skipgram with negative sampling (SGNS) uses more negative examples than positive\nexamples (with the ratio between them set by a parameter k). So for each of these\n(w, cpos ) training instances we’ll create k negative samples, each consisting of the\ntarget w plus a ‘noise word’ cneg . A noise word is a random word from the lexicon,\nconstrained not to be the target word w. The table right above shows the setting\nwhere k = 2, so we’ll have 2 negative examples in the negative training set − for\neach positive example w, cpos .\nThe noise words are chosen according to their weighted unigram probability\npα (w), where α is a weight. If we were sampling according to unweighted probability P(w), it would mean that with unigram probability P(“the”) we would choose\nthe word the as a noise word, with unigram probability P(“aardvark”) we would\nchoose aardvark, and so on. But in practice it is common to set α = 0.75, i.e. use\n\n\f14\n\nC HAPTER 5\n\n•\n\nE MBEDDINGS\n\nthe weighting P3 (w):\n4\n\nPα (w) = P\n\ncount(w)α\n0 α\nw0 count(w )\n\n(5.19)\n\nSetting α = .75 gives better performance because it gives rare noise words slightly\nhigher probability: for rare words, Pα (w) > P(w). To illustrate this intuition, it\nmight help to work out the probabilities for an example with α = .75 and two events,\nP(a) = 0.99 and P(b) = 0.01:\n.99.75\n= 0.97\n.99.75 + .01.75\n.01.75\nPα (b) =\n= 0.03\n.75\n.99 + .01.75\n\nPα (a) =\n\n(5.20)\n\nThus using α = .75 increases the probability of the rare event b from 0.01 to 0.03.\nGiven the set of positive and negative training instances, and an initial set of\nembeddings, the goal of the learning algorithm is to adjust those embeddings to\n• Maximize the similarity of the target word, context word pairs (w, cpos ) drawn\nfrom the positive examples\n• Minimize the similarity of the (w, cneg ) pairs from the negative examples.\nIf we consider one word/context pair (w, cpos ) with its k noise words cneg1 ...cnegk ,\nwe can express these two goals as the following loss function L to be minimized\n(hence the −); here the first term expresses that we want the classifier to assign the\nreal context word cpos a high probability of being a neighbor, and the second term\nexpresses that we want to assign each of the noise words cnegi a high probability of\nbeing a non-neighbor, all multiplied because we assume independence:\n\"\nL = − log P(+|w, cpos )\n\nk\nY\n\n#\nP(−|w, cnegi )\n\ni=1\n\n\"\n= − log P(+|w, cpos ) +\n\nk\nX\n\n#\nlog P(−|w, cnegi )\n\ni=1\n\n\"\n= − log P(+|w, cpos ) +\n\nk\nX\n\n#\n\u0001\nlog 1 − P(+|w, cnegi )\n\ni=1\n\n\"\n= − log σ (cpos · w) +\n\nk\nX\n\n#\nlog σ (−cnegi · w)\n\n(5.21)\n\ni=1\n\nThat is, we want to maximize the dot product of the word with the actual context\nwords, and minimize the dot products of the word with the k negative sampled nonneighbor words.\nWe minimize this loss function using stochastic gradient descent. Fig. 5.7 shows\nthe intuition of one step of learning.\nTo get the gradient, we need to take the derivative of Eq. 5.21 with respect to\nthe different embeddings. It turns out the derivatives are the following (we leave the\n\n\f5.5\n\n•\n\nW ORD 2 VEC\n\n15\n\naardvark\napricot\n\nw\n\nmove apricot and jam closer,\nincreasing cpos z w\n\nW\n“…apricot jam…”\nzebra\n\n!\n\naardvark\n\nC\n\nk=2\n\njam\n\ncpos\n\nmatrix\n\ncneg1\n\nTolstoy\n\ncneg2\n\nzebra\n\nmove apricot and matrix apart\ndecreasing cneg1 z w\n\nmove apricot and Tolstoy apart\ndecreasing cneg2 z w\n\nFigure 5.7 Intuition of one step of gradient descent. The skip-gram model tries to shift embeddings so the target embeddings (here for apricot) are closer to (have a higher dot product\nwith) context embeddings for nearby words (here jam) and further from (lower dot product\nwith) context embeddings for noise words that don’t occur nearby (here Tolstoy and matrix).\n\nproof as an exercise at the end of the chapter):\n∂L\n= [σ (cpos · w) − 1]w\n∂ cpos\n∂L\n= [σ (cneg · w)]w\n∂ cneg\n\n(5.22)\n(5.23)\nk\n\nX\n∂L\n[σ (cnegi · w)]cnegi\n= [σ (cpos · w) − 1]cpos +\n∂w\n\n(5.24)\n\ni=1\n\nThe update equations going from time step t to t + 1 in stochastic gradient descent\nare thus:\nt\nt\nt\nt\nct+1\npos = cpos − η[σ (cpos · w ) − 1]w\n\nct+1\nneg\n\n=\n\n(5.25)\n\nctneg − η[σ (ctneg · wt )]wt\n\"\n\nwt+1 = wt − η [σ (ctpos · wt ) − 1]ctpos +\n\n(5.26)\nk\nX\n[σ (ctnegi · wt )]ctnegi\n\n#\n(5.27)\n\ni=1\n\ntarget\nembedding\ncontext\nembedding\n\nJust as in logistic regression, then, the learning algorithm starts with randomly initialized W and C matrices, and then walks through the training corpus using gradient\ndescent to move W and C so as to minimize the loss in Eq. 5.21 by making the updates in (Eq. 5.25)-(Eq. 5.27).\nRecall that the skip-gram model learns two separate embeddings for each word i:\nthe target embedding wi and the context embedding ci , stored in two matrices, the\ntarget matrix W and the context matrix C. It’s common to just add them together,\nrepresenting word i with the vector wi + ci . Alternatively we can throw away the C\nmatrix and just represent each word i by the vector wi .\nAs with the simple count-based methods like tf-idf, the context window size L\naffects the performance of skip-gram embeddings, and experiments often tune the\nparameter L on a devset.\n\n\f16\n\nC HAPTER 5\n\n•\n\nE MBEDDINGS\n\n5.5.3\nfasttext\n\nOther kinds of static embeddings\n\nThere are many kinds of static embeddings. An extension of word2vec, fasttext\n(Bojanowski et al., 2017), addresses a problem with word2vec as we have presented\nit so far: it has no good way to deal with unknown words—words that appear in\na test corpus but were unseen in the training corpus. A related problem is word\nsparsity, such as in languages with rich morphology, where some of the many forms\nfor each noun and verb may only occur rarely. Fasttext deals with these problems\nby using subword models, representing each word as itself plus a bag of constituent\nn-grams, with special boundary symbols < and > added to each word. For example,\nwith n = 3 the word where would be represented by the sequence <where> plus the\ncharacter n-grams:\n<wh, whe, her, ere, re>\nThen a skipgram embedding is learned for each constituent n-gram, and the word\nwhere is represented by the sum of all of the embeddings of its constituent n-grams.\nUnknown\nwords\ncan then\npresented\nonly by the sum of the constituent n-grams.\nModeling\nWord Meaning\nUsingbe\nLexical\nCo-Occurrence\nA fasttext open-source library, including pretrained embeddings for 157 languages,\nis available at https://fasttext.cc.\nAnother very widely used static embedding model is GloVe (Pennington et al.,\n2014), short for Global Vectors, because the model is based on capturing global\ncorpus statistics. GloVe is based on ratios of probabilities from the word-word cooccurrence matrix.\nIt turns out that dense embeddings like word2vec actually have an elegant mathematical relationship with count-based embeddings, in which word2vec can be seen\nas implicitly optimizing a function of a count matrix with a particular (PPMI) weighting (Levy and Goldberg, 2014c).\n\nhde, Gonnerman, Plaut\n\nRUSSIA\nFRANCE\nCHINA\n\nEUROPE\nASIA\nAFRICA\nAMERICA\n\nWRIST\nANKLE\nARM\n\nBRAZIL\n\nSHOULDER\nFINGER\nEYE\nFACE\nHAND\nEAR\nTOE\n\nMOSCOW\n\nLEG\nFOOT\n\nHAWAII\n\nTOOTH\nNOSE\nHEAD\n\nTOKYO\n\nMONTREAL\nCHICAGO\nATLANTA\n\nMOUSE\n\n5.6\n\nVisualizing Embeddings\n\nDOG\nCAT\n\nPUPPY\nKITTEN\n\nTURTLE\nCOW\n\nNASHVILLE\n\nLION\n\nOYSTER\n\n“I see well in many dimensions as long as the dimensions are around two.”\nThe late economist Martin Shubik\n\nBULL\n\nFigure 8: Multidimensional scaling for three noun classes.\n\nVisualizing embeddings is an important goal in helping understand, apply, and\nimprove these models of word meaning. But how can we visualize a (for example)\n100-dimensional vector?\nThe simplest way to visualize the meaning of a word\nWRIST\nANKLE\nSHOULDER\nw\nembedded\nin a space is to list the most similar words to\nARM\nLEG\nw by sorting the vectors for all words in the vocabulary by\nHAND\nFOOT\nHEAD\ntheir cosine with the vector for w. For example the 7 closest\nNOSE\nFINGER\nwords to frog using a particular embeddings computed with\nTOE\nFACE\nEAR\nthe GloVe algorithm are: frogs, toad, litoria, leptodactyliEYE\nTOOTH\ndae, rana, lizard, and eleutherodactylus (Pennington et al.,\nDOG\nCAT\nPUPPY\n2014).\nKITTEN\nCOW\nYet another visualization method is to use a clustering\nMOUSE\nTURTLE\nOYSTER\nalgorithm\nto show a hierarchical representation of which\nLION\nBULL\nwords are similar to others in the embedding space. The\nCHICAGO\nATLANTA\nMONTREAL\nuncaptioned figure on the left uses hierarchical clustering\nNASHVILLE\nTOKYO\nof some embedding vectors for nouns as a visualization\nCHINA\nRUSSIA\nAFRICA\nmethod (Rohde et al., 2006).\nASIA\nEUROPE\nAMERICA\nBRAZIL\nMOSCOW\nFRANCE\nHAWAII\n\nFigure 9: Hierarchical clustering for three noun classes using distances based on vector correlations.\n\n\f5.7\n\n•\n\nS EMANTIC PROPERTIES OF EMBEDDINGS\n\n17\n\nProbably the most common visualization method, however, is to project the 100 dimensions of a word down into 2\ndimensions. Fig. 5.1 showed one such visualization, as does\nFig. 5.9, using a projection method called t-SNE (van der\nMaaten and Hinton, 2008).\n\n5.7\n\nSemantic properties of embeddings\nIn this section we briefly summarize some of the semantic properties of embeddings\nthat have been studied.\n\nfirst-order\nco-occurrence\n\nsecond-order\nco-occurrence\n\nparallelogram\nmodel\n\nDifferent types of similarity or association: One parameter of vector semantic\nmodels that is relevant to both sparse PPMI vectors and dense word2vec vectors is\nthe size of the context window used to collect counts. This is generally between 1\nand 10 words on each side of the target word (for a total context of 2-20 words).\nThe choice depends on the goals of the representation. Shorter context windows\ntend to lead to representations that are a bit more syntactic, since the information is\ncoming from immediately nearby words. When the vectors are computed from short\ncontext windows, the most similar words to a target word w tend to be semantically\nsimilar words with the same parts of speech. When vectors are computed from long\ncontext windows, the highest cosine words to a target word w tend to be words that\nare topically related but not similar.\nFor example Levy and Goldberg (2014a) showed that using skip-gram with a\nwindow of ±2, the most similar words to the word Hogwarts (from the Harry Potter\nseries) were names of other fictional schools: Sunnydale (from Buffy the Vampire\nSlayer) or Evernight (from a vampire series). With a window of ±5, the most similar\nwords to Hogwarts were other words topically related to the Harry Potter series:\nDumbledore, Malfoy, and half-blood.\nIt’s also often useful to distinguish two kinds of similarity or association between\nwords (Schütze and Pedersen, 1993). Two words have first-order co-occurrence\n(sometimes called syntagmatic association) if they are typically nearby each other.\nThus wrote is a first-order associate of book or poem. Two words have second-order\nco-occurrence (sometimes called paradigmatic association) if they have similar\nneighbors. Thus wrote is a second-order associate of words like said or remarked.\nAnalogy/Relational Similarity: Another semantic property of embeddings is their\nability to capture relational meanings. In an important early vector space model of\ncognition, Rumelhart and Abrahamson (1973) proposed the parallelogram model\nfor solving simple analogy problems of the form a is to b as a* is to what?. In such\nproblems, a system is given a problem like apple:tree::grape:?, i.e., apple is to tree\n, and must fill in the word vine. In the parallelogram model, ilas grape is to\n# » # »\nlustrated in Fig. 5.8, the vector from the word apple to the word tree (= tree − apple)\n# » the nearest word to that point is returned.\nis added to the vector for grape (grape);\nIn early work with sparse embeddings, scholars showed that sparse vector models of meaning could solve such analogy problems (Turney and Littman, 2005),\nbut the parallelogram method received more modern attention because of its success with word2vec or GloVe vectors (Mikolov et al. 2013c, Levy and Goldberg\n# »\n2014b, Pennington et al. 2014). For example, the result of the expression king −\n#\n»\n#\n»\n#\n»\n# » + woman\n#\n» is a vector close to queen.\n# » Similarly, Paris − France + Italy results\nman\n# »\nin a vector that is close to Rome. The embedding model thus seems to be extract-\n\n\f18\n\nC HAPTER 5\n\n•\n\nE MBEDDINGS\n\ntree\napple\n\nvine\ngrape\nFigure 5.8 The parallelogram model for analogy problems (Rumelhart and Abrahamson,\n# »\n# »\n# »\n# »\n1973): the location of vine can be found by subtracting apple from tree and adding grape.\n\ning representations of relations like MALE - FEMALE, or CAPITAL - CITY- OF, or even\nCOMPARATIVE / SUPERLATIVE , as shown in Fig. 5.9 from GloVe.\n\n(a)\n\n(b)\n\nFigure 5.9 Relational properties of the GloVe vector space, shown by projecting vectors onto two dimensions.\n# » # » #\n» is close to queen.\n# » (b) offsets seem to capture comparative and superlative morphology\n(a) king − man\n+ woman\n(Pennington et al., 2014).\n\nFor a a : b :: a∗ : b∗ problem, meaning the algorithm is given vectors a, b, and\na∗ and must find b∗ , the parallelogram method is thus:\nb̂∗ = argmin distance(x, b − a + a∗ )\n\n(5.28)\n\nx\n\nwith some distance function, such as Euclidean distance.\nThere are some caveats. For example, the closest value returned by the parallelogram algorithm in word2vec or GloVe embedding spaces is usually not in fact\nb* but one of the 3 input words or their morphological variants (i.e., cherry:red ::\npotato:x returns potato or potatoes instead of brown), so these must be explicitly\nexcluded. Furthermore while embedding spaces perform well if the task involves\nfrequent words, small distances, and certain relations (like relating countries with\ntheir capitals or verbs/nouns with their inflected forms), the parallelogram method\nwith embeddings doesn’t work as well for other relations (Linzen 2016, Gladkova\net al. 2016, Schluter 2018, Ethayarajh et al. 2019a), and indeed Peterson et al. (2020)\nargue that the parallelogram method is in general too simple to model the human\ncognitive process of forming analogies of this kind.\n\n\f5.8\n\n5.7.1\n\n•\n\nB IAS AND E MBEDDINGS\n\n19\n\nEmbeddings and Historical Semantics\n\nEmbeddings can also be a useful tool for studying how meaning changes over time,\nby computing multiple embedding spaces, each from texts written in a particular\ntime period. For example Fig. 5.10 shows a visualization of changes in meaning in\nEnglish words over the last two centuries, computed by building separate embedding\nspaces for each\ndecade fromSOCIAL\nhistoricalREPRESENTATIONS\ncorpora like Google n-grams\n(Lin etMEANING79\nal., 2012)\nCHAPTER\n5. DYNAMIC\nOF WORD\nand the Corpus of Historical American English (Davies, 2012).\n\nFigure 5.10\n\nA t-SNE visualization of the semantic change of 3 words in English using\n\nFigure\nTwo-dimensional\nvisualization\nof semantic\nin English\nSGNS\nword2vec5.1:\nvectors.\nThe modern sense\nof each word,\nand the change\ngrey context\nwords,using\nare comvectors\n(seetheSection\n5.8 for\nthe visualization\nalgorithm).\nA,Earlier\nThe word\nshifted\nputed from\nmost recent\n(modern)\ntime-point embedding\nspace.\npointsgay\nare comfrom\n“cheerful”\n“frolicsome”\nto referring\nto homosexuality.\nA, In the\nearly\nputed meaning\nfrom earlier\nhistoricalorembedding\nspaces.\nThe visualizations\nshow the changes\nin the\n20th\ncentury\nbroadcast\nreferred\nto\n“casting\nout\nseeds”;\nwith\nthe\nrise\nof\ntelevision\nand\nword gay from meanings related to “cheerful” or “frolicsome” to referring to homosexuality,\nradio\nits\nmeaning\nshifted\nto\n“transmitting\nsignals”.\nC,\nAwful\nunderwent\na\nprocess\nthe development of the modern “transmission” sense of broadcast from its original sense of of\npejoration,\nit shifted\nfrom meaning\n“full\nof awe”\nto meaning\n“terrible“full\nor appalling”\nsowing seeds,asand\nthe pejoration\nof the word\nawful\nas it shifted\nfrom meaning\nof awe”\n[212].\nto meaning “terrible or appalling” (Hamilton et al., 2016).\nthat adverbials (e.g., actually) have a general tendency to undergo subjectification\n\n5.8\n\nshift from objective statements about the world (e.g., “Sorry, the car is\nBiaswhere\nandtheyEmbeddings\n\nallocational\nharm\n\nbias\namplification\n\nactually broken”) to subjective statements (e.g., “I can’t believe he actually did that”,\n\nindicating surprise/disbelief).\nIn addition to their ability to learn word meaning from text, embeddings, alas,\nalso reproduce the implicit biases and stereotypes that were latent in the text. As\n5.2.2\nstudies\nthe prior Computational\nsection just showed, linguistic\nembeddings can\nroughly model relational similarity: ‘queen’ as the closest word to ‘king’ - ‘man’ + ‘woman’ implies the analogy\nThere are also a number of recent works analyzing semantic change using computational\nman:woman::king:queen. But these same embedding analogies also exhibit gender\nmethods.\n[200]\nlatent semantic\nto analyze\nhow the\nword\nmeanings\nbroaden\nstereotypes.\nForuse\nexample\nBolukbasianalysis\net al. (2016)\nfind that\nclosest\noccupation\nto ‘computer\nprogrammer’\n- ‘man’\n+ ‘woman’\nin word2vec\non of\nand\nnarrow over\ntime. [113]\nuse raw\nco-occurrence\nvectorsembeddings\nto perform trained\na number\nnews\ntext\nis\n‘homemaker’,\nand\nthat\nthe\nembeddings\nsimilarly\nsuggest\nthe\nanalogy\nhistorical case-studies on semantic change, and [252] perform a similar set of small‘father’ is to ‘doctor’ as ‘mother’ is to ‘nurse’. This could result in what Crawford\nscale case-studies using temporal topic models. [87] construct point-wise mutual\n(2017) and Blodgett et al. (2020) call an allocational harm, when a system alloinformation-based\nembeddings\nand found\nthat semantic\nuncovered\nby their\ncates resources (jobs\nor credit) unfairly\nto different\ngroups.changes\nFor example\nalgorithms\nthat use had\nembeddings\nas agreement\npart of a search\nfor hiring\npotential [129]\nprogrammers\ndoctors\nmethod\nreasonable\nwith human\njudgments.\nand [119]oruse\n“neural”\nmight\nthus\nincorrectly\ndownweight\ndocuments\nwith\nwomen’s\nnames.\nword-embedding methods to detect linguistic change points. Finally, [257] analyze\nIt turns out that embeddings don’t just reflect the statistics of their input, but also\nhistorical co-occurrences to test whether synonyms tend to change in similar ways.\namplify bias; gendered terms become more gendered in embedding space than they\nwere in the input text statistics (Zhao et al. 2017, Ethayarajh et al. 2019b, Jia et al.\n2020), and biases are more exaggerated than in actual labor employment statistics\n(Garg et al., 2018).\nEmbeddings also encode the implicit associations that are a property of human\nreasoning. The Implicit Association Test (Greenwald et al., 1998) measures peo-\n\n\f20\n\nC HAPTER 5\n\nrepresentational\nharm\n\ndebiasing\n\n5.9\n\n•\n\nE MBEDDINGS\n\nple’s associations between concepts (like ‘flowers’ or ‘insects’) and attributes (like\n‘pleasantness’ and ‘unpleasantness’) by measuring differences in the latency with\nwhich they label words in the various categories.3 Using such methods, people\nin the United States have been shown to associate African-American names with\nunpleasant words (more than European-American names), male names more with\nmathematics and female names with the arts, and old people’s names with unpleasant words (Greenwald et al. 1998, Nosek et al. 2002a, Nosek et al. 2002b). Caliskan\net al. (2017) replicated all these findings of implicit associations using GloVe vectors\nand cosine similarity instead of human latencies. For example African-American\nnames like ‘Leroy’ and ‘Shaniqua’ had a higher GloVe cosine with unpleasant words\nwhile European-American names (‘Brad’, ‘Greg’, ‘Courtney’) had a higher cosine\nwith pleasant words. These problems with embeddings are an example of a representational harm (Crawford 2017, Blodgett et al. 2020), which is a harm caused by\na system demeaning or even ignoring some social groups. Any embedding-aware algorithm that made use of word sentiment could thus exacerbate bias against African\nAmericans.\nRecent research focuses on ways to try to remove these kinds of biases, for\nexample by developing a transformation of the embedding space that removes gender stereotypes but preserves definitional gender (Bolukbasi et al. 2016, Zhao et al.\n2017) or changing the training procedure (Zhao et al., 2018). However, although\nthese sorts of debiasing may reduce bias in embeddings, they do not eliminate it\n(Gonen and Goldberg, 2019), and this remains an open problem.\nHistorical embeddings are also being used to measure biases in the past. Garg\net al. (2018) used embeddings from historical texts to measure the association between embeddings for occupations and embeddings for names of various ethnicities or genders (for example the relative cosine similarity of women’s names versus\nmen’s to occupation words like ‘librarian’ or ‘carpenter’) across the 20th century.\nThey found that the cosines correlate with the empirical historical percentages of\nwomen or ethnic groups in those occupations. Historical embeddings also replicated old surveys of ethnic stereotypes; the tendency of experimental participants in\n1933 to associate adjectives like ‘industrious’ or ‘superstitious’ with, e.g., Chinese\nethnicity, correlates with the cosine between Chinese last names and those adjectives\nusing embeddings trained on 1930s text. They also were able to document historical\ngender biases, such as the fact that embeddings for adjectives related to competence\n(‘smart’, ‘wise’, ‘thoughtful’, ‘resourceful’) had a higher cosine with male than female words, and showed that this bias has been slowly decreasing since 1960. We\nreturn in later chapters to this question about the role of bias in natural language\nprocessing.\n\nEvaluating Vector Models\nThe most important evaluation metric for vector models is extrinsic evaluation on\ntasks, i.e., using vectors in an NLP task and seeing whether this improves performance over some other model.\n3\n\nRoughly speaking, if humans associate ‘flowers’ with ‘pleasantness’ and ‘insects’ with ‘unpleasantness’, when they are instructed to push a green button for ‘flowers’ (daisy, iris, lilac) and ‘pleasant words’\n(love, laughter, pleasure) and a red button for ‘insects’ (flea, spider, mosquito) and ‘unpleasant words’\n(abuse, hatred, ugly) they are faster than in an incongruous condition where they push a red button for\n‘flowers’ and ‘unpleasant words’ and a green button for ‘insects’ and ‘pleasant words’.\n\n\f5.10\n\n•\n\nS UMMARY\n\n21\n\nNonetheless it is useful to have intrinsic evaluations. The most common metric\nis to test their performance on similarity, computing the correlation between an\nalgorithm’s word similarity scores and word similarity ratings assigned by humans.\nWordSim-353 (Finkelstein et al., 2002) is a commonly used set of ratings from 0\nto 10 for 353 noun pairs; for example (plane, car) had an average score of 5.77.\nSimLex-999 (Hill et al., 2015) is a more complex dataset that quantifies similarity\n(cup, mug) rather than relatedness (cup, coffee), and includes concrete and abstract\nadjective, noun and verb pairs. The TOEFL dataset is a set of 80 questions, each\nconsisting of a target word with 4 additional word choices; the task is to choose\nwhich is the correct synonym, as in the example: Levied is closest in meaning to:\nimposed, believed, requested, correlated (Landauer and Dumais, 1997). All of these\ndatasets present words without context.\nSlightly more realistic are intrinsic similarity tasks that include context. The\nStanford Contextual Word Similarity (SCWS) dataset (Huang et al., 2012) and the\nWord-in-Context (WiC) dataset (Pilehvar and Camacho-Collados, 2019) offer richer\nevaluation scenarios. SCWS gives human judgments on 2,003 pairs of words in\ntheir sentential context, while WiC gives target words in two sentential contexts that\nare either in the same or different senses; see Appendix G. The semantic textual\nsimilarity task (Agirre et al. 2012, Agirre et al. 2015) evaluates the performance of\nsentence-level similarity algorithms, consisting of a set of pairs of sentences, each\npair with human-labeled similarity scores.\nAnother task used for evaluation is the analogy task, discussed on page 17, where\nthe system has to solve problems of the form a is to b as a* is to b*, given a, b, and a*\nand having to find b* (Turney and Littman, 2005). A number of sets of tuples have\nbeen created for this task (Mikolov et al. 2013a, Mikolov et al. 2013c, Gladkova\net al. 2016), covering morphology (city:cities::child:children), lexicographic relations (leg:table::spout:teapot) and encyclopedia relations (Beijing:China::Dublin:Ireland),\nsome drawing from the SemEval-2012 Task 2 dataset of 79 different relations (Jurgens et al., 2012).\nAll embedding algorithms suffer from inherent variability. For example because\nof randomness in the initialization and the random negative sampling, algorithms\nlike word2vec may produce different results even from the same dataset, and individual documents in a collection may strongly impact the resulting embeddings\n(Tian et al. 2016, Hellrich and Hahn 2016, Antoniak and Mimno 2018). When embeddings are used to study word associations in particular corpora, therefore, it is\nbest practice to train multiple embeddings with bootstrap sampling over documents\nand average the results (Antoniak and Mimno, 2018).\n\n5.10\n\nSummary\n• In vector semantics, a word is modeled as a vector—a point in high-dimensional\nspace, also called an embedding. In this chapter we focus on static embeddings, where each word is mapped to a fixed embedding.\n• Vector semantic models fall into two classes: sparse and dense. In sparse\nmodels each dimension corresponds to a word in the vocabulary V and cells\nare functions of co-occurrence counts. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each\ncontext term in the vocabulary.\n\n\f22\n\nC HAPTER 5\n\n•\n\nE MBEDDINGS\n\n• Dense vector models typically have dimensionality 50–1000. Word2vec algorithms like skip-gram are a popular way to compute dense embeddings.\nSkip-gram trains a logistic regression classifier to compute the probability that\ntwo words are ‘likely to occur nearby in text’. This probability is computed\nfrom the dot product between the embeddings for the two words.\n• Skip-gram uses stochastic gradient descent to train the classifier, by learning\nembeddings that have a high dot product with embeddings of words that occur\nnearby and a low dot product with noise words.\n• Other important embedding algorithms include GloVe, a method based on\nratios of word co-occurrence probabilities.\n• Whether using sparse or dense vectors, word and document similarities are\ncomputed by some function of the dot product between vectors. The cosine\nof two vectors—a normalized dot product—is the most popular such metric.\n\nHistorical Notes\nThe idea of vector semantics arose out of research in the 1950s in three distinct\nfields: linguistics, psychology, and computer science, each of which contributed a\nfundamental aspect of the model.\nThe idea that meaning is related to the distribution of words in context was\nwidespread in linguistic theory of the 1950s, among distributionalists like Zellig\nHarris, Martin Joos, and J. R. Firth, and semioticians like Thomas Sebeok. As Joos\n(1950) put it,\nthe linguist’s “meaning” of a morpheme. . . is by definition the set of conditional\nprobabilities of its occurrence in context with all other morphemes.\n\nmechanical\nindexing\n\nThe idea that the meaning of a word might be modeled as a point in a multidimensional semantic space came from psychologists like Charles E. Osgood, who\nhad been studying how people responded to the meaning of words by assigning values along scales like happy/sad or hard/soft. Osgood et al. (1957) proposed that the\nmeaning of a word in general could be modeled as a point in a multidimensional\nEuclidean space, and that the similarity of meaning between two words could be\nmodeled as the distance between these points in the space.\nA final intellectual source in the 1950s and early 1960s was the field then called\nmechanical indexing, now known as information retrieval. In what became known\nas the vector space model for information retrieval (Salton 1971, Sparck Jones\n1986), researchers demonstrated new ways to define the meaning of words in terms\nof vectors (Switzer, 1965), and refined methods for word similarity based on measures of statistical association between words like mutual information (Giuliano,\n1965) and idf (Sparck Jones, 1972), and showed that the meaning of documents\ncould be represented in the same vector spaces used for words. Around the same\ntime, (Cordier, 1965) showed that factor analysis of word association probabilities\ncould be used to form dense vector representations of words.\nSome of the philosophical underpinning of the distributional way of thinking\ncame from the late writings of the philosopher Wittgenstein, who was skeptical of\nthe possibility of building a completely formal theory of meaning definitions for\neach word. Wittgenstein suggested instead that “the meaning of a word is its use in\nthe language” (Wittgenstein, 1953, PI 43). That is, instead of using some logical language to define each word, or drawing on denotations or truth values, Wittgenstein’s\n\n\fH ISTORICAL N OTES\n\nsemantic\nfeature\n\n23\n\nidea is that we should define a word by how it is used by people in speaking and understanding in their day-to-day interactions, thus prefiguring the movement toward\nembodied and experiential models in linguistics and NLP (Glenberg and Robertson\n2000, Lake and Murphy 2021, Bisk et al. 2020, Bender and Koller 2020).\nMore distantly related is the idea of defining words by a vector of discrete features, which has roots at least as far back as Descartes and Leibniz (Wierzbicka 1992,\nWierzbicka 1996). By the middle of the 20th century, beginning with the work of\nHjelmslev (Hjelmslev, 1969) (originally 1943) and fleshed out in early models of\ngenerative grammar (Katz and Fodor, 1963), the idea arose of representing meaning with semantic features, symbols that represent some sort of primitive meaning.\nFor example words like hen, rooster, or chick, have something in common (they all\ndescribe chickens) and something different (their age and sex), representable as:\nhen\n+female, +chicken, +adult\nrooster -female, +chicken, +adult\nchick +chicken, -adult\n\nSVD\n\nThe dimensions used by vector models of meaning to define words, however, are\nonly abstractly related to this idea of a small fixed number of hand-built dimensions.\nNonetheless, there has been some attempt to show that certain dimensions of embedding models do contribute some specific compositional aspect of meaning like\nthese early semantic features.\nThe use of dense vectors to model word meaning, and indeed the term embedding, grew out of the latent semantic indexing (LSI) model (Deerwester et al.,\n1988) recast as LSA (latent semantic analysis) (Deerwester et al., 1990). In LSA\nsingular value decomposition—SVD— is applied to a term-document matrix (each\ncell weighted by log frequency and normalized by entropy), and then the first 300\ndimensions are used as the LSA embedding. Singular Value Decomposition (SVD)\nis a method for finding the most important dimensions of a data set, those dimensions along which the data varies the most. LSA was then quickly widely applied:\nas a cognitive model (Landauer and Dumais, 1997), and for tasks like spell checking\n(Jones and Martin, 1997), language modeling (Bellegarda 1997, Coccaro and Jurafsky 1998, Bellegarda 2000), morphology induction (Schone and Jurafsky 2000,\nSchone and Jurafsky 2001b), multiword expressions (MWEs) (Schone and Jurafsky,\n2001a), and essay grading (Rehder et al., 1998). Related models were simultaneously developed and applied to word sense disambiguation by Schütze (1992). LSA\nalso led to the earliest use of embeddings to represent words in a probabilistic classifier, in the logistic regression document router of Schütze et al. (1995). The idea of\nSVD on the term-term matrix (rather than the term-document matrix) as a model of\nmeaning for NLP was proposed soon after LSA by Schütze (1992). Schütze applied\nthe low-rank (97-dimensional) embeddings produced by SVD to the task of word\nsense disambiguation, analyzed the resulting semantic space, and also suggested\npossible techniques like dropping high-order dimensions. See Schütze (1997).\nA number of alternative matrix models followed on from the early SVD work,\nincluding Probabilistic Latent Semantic Indexing (PLSI) (Hofmann, 1999), Latent\nDirichlet Allocation (LDA) (Blei et al., 2003), and Non-negative Matrix Factorization (NMF) (Lee and Seung, 1999).\nThe LSA community seems to have first used the word “embedding” in Landauer\net al. (1997), in a variant of its mathematical meaning as a mapping from one space\nor mathematical structure to another. In LSA, the word embedding seems to have\ndescribed the mapping from the space of sparse count vectors to the latent space of\nSVD dense vectors. Although the word thus originally meant the mapping from one\n\n\f24\n\nC HAPTER 5\n\n•\n\nE MBEDDINGS\n\nspace to another, it has metonymically shifted to mean the resulting dense vector in\nthe latent space, and it is in this sense that we currently use the word.\nBy the next decade, Bengio et al. (2003) and Bengio et al. (2006) showed that\nneural language models could also be used to develop embeddings as part of the task\nof word prediction. Collobert and Weston (2007), Collobert and Weston (2008), and\nCollobert et al. (2011) then demonstrated that embeddings could be used to represent\nword meanings for a number of NLP tasks. Turian et al. (2010) compared the value\nof different kinds of embeddings for different NLP tasks. Mikolov et al. (2011)\nshowed that recurrent neural nets could be used as language models. The idea of\nsimplifying the hidden layer of these neural net language models to create the skipgram (and also CBOW) algorithms was proposed by Mikolov et al. (2013a). The\nnegative sampling training algorithm was proposed in Mikolov et al. (2013b). There\nare numerous surveys of static embeddings and their parameterizations (Bullinaria\nand Levy 2007, Bullinaria and Levy 2012, Lapesa and Evert 2014, Kiela and Clark\n2014, Levy et al. 2015).\nSee Manning et al. (2008) and Chapter 11 for a deeper understanding of the role\nof vectors in information retrieval, including how to compare queries with documents, more details on tf-idf, and issues of scaling to very large datasets. See Kim\n(2019) for a clear and comprehensive tutorial on word2vec. Cruse (2004) is a useful\nintroductory linguistic text on lexical semantics.\n\nExercises\n\n\fExercises\nAgirre, E., C. Banea, C. Cardie, D. Cer, M. Diab,\nA. Gonzalez-Agirre, W. Guo, I. Lopez-Gazpio, M. Maritxalar, R. Mihalcea, G. Rigau, L. Uria, and J. Wiebe.\n2015. SemEval-2015 task 2: Semantic textual similarity,\nEnglish, Spanish and pilot on interpretability. SemEval15.\nAgirre, E., M. Diab, D. Cer, and A. Gonzalez-Agirre. 2012.\nSemEval-2012 task 6: A pilot on semantic textual similarity. SemEval-12.\nAntoniak, M. and D. Mimno. 2018. Evaluating the stability\nof embedding-based word similarities. TACL, 6:107–119.\nBellegarda, J. R. 1997. A latent semantic analysis framework\nfor large-span language modeling. EUROSPEECH.\nBellegarda, J. R. 2000. Exploiting latent semantic information in statistical language modeling. Proceedings of the\nIEEE, 89(8):1279–1296.\nBender, E. M. and A. Koller. 2020. Climbing towards NLU:\nOn meaning, form, and understanding in the age of data.\nACL.\nBengio, Y., A. Courville, and P. Vincent. 2013. Representation learning: A review and new perspectives. IEEE\nTransactions on Pattern Analysis and Machine Intelligence, 35(8):1798–1828.\nBengio, Y., R. Ducharme, P. Vincent, and C. Jauvin. 2003.\nA neural probabilistic language model. JMLR, 3:1137–\n1155.\nBengio, Y., H. Schwenk, J.-S. Senécal, F. Morin, and J.-L.\nGauvain. 2006. Neural probabilistic language models. In\nInnovations in Machine Learning, 137–186. Springer.\nBisk, Y., A. Holtzman, J. Thomason, J. Andreas, Y. Bengio,\nJ. Chai, M. Lapata, A. Lazaridou, J. May, A. Nisnevich,\nN. Pinto, and J. Turian. 2020. Experience grounds language. EMNLP.\nBlei, D. M., A. Y. Ng, and M. I. Jordan. 2003. Latent Dirichlet allocation. JMLR, 3(5):993–1022.\nBlodgett, S. L., S. Barocas, H. Daumé III, and H. Wallach.\n2020. Language (technology) is power: A critical survey\nof “bias” in NLP. ACL.\nBojanowski, P., E. Grave, A. Joulin, and T. Mikolov. 2017.\nEnriching word vectors with subword information. TACL,\n5:135–146.\nBolukbasi, T., K.-W. Chang, J. Zou, V. Saligrama, and A. T.\nKalai. 2016. Man is to computer programmer as woman\nis to homemaker? Debiasing word embeddings. NeurIPS.\nBréal, M. 1897. Essai de Sémantique: Science des significations. Hachette.\nBudanitsky, A. and G. Hirst. 2006. Evaluating WordNetbased measures of lexical semantic relatedness. Computational Linguistics, 32(1):13–47.\nBullinaria, J. A. and J. P. Levy. 2007. Extracting semantic representations from word co-occurrence statistics:\nA computational study. Behavior research methods,\n39(3):510–526.\nBullinaria, J. A. and J. P. Levy. 2012. Extracting semantic\nrepresentations from word co-occurrence statistics: stoplists, stemming, and SVD. Behavior research methods,\n44(3):890–907.\nCaliskan, A., J. J. Bryson, and A. Narayanan. 2017. Semantics derived automatically from language corpora contain\nhuman-like biases. Science, 356(6334):183–186.\n\n25\n\nCarlson, G. N. 1977. Reference to kinds in English. Ph.D.\nthesis, University of Massachusetts, Amherst. Forward.\nClark, E. 1987. The principle of contrast: A constraint on\nlanguage acquisition. In B. MacWhinney, ed., Mechanisms of language acquisition, 1–33. LEA.\nCoccaro, N. and D. Jurafsky. 1998. Towards better integration of semantic predictors in statistical language modeling. ICSLP.\nCollobert, R. and J. Weston. 2007. Fast semantic extraction\nusing a novel neural network architecture. ACL.\nCollobert, R. and J. Weston. 2008. A unified architecture for\nnatural language processing: Deep neural networks with\nmultitask learning. ICML.\nCollobert, R., J. Weston, L. Bottou, M. Karlen,\nK. Kavukcuoglu, and P. Kuksa. 2011. Natural language\nprocessing (almost) from scratch. JMLR, 12:2493–2537.\nCordier, B. 1965. Factor-analysis of correspondences. COLING 1965.\nCrawford, K. 2017. The trouble with bias. Keynote at\nNeurIPS.\nCruse, D. A. 2004. Meaning in Language: an Introduction\nto Semantics and Pragmatics. Oxford University Press.\nSecond edition.\nDavies, M. 2012. Expanding horizons in historical linguistics with the 400-million word Corpus of Historical\nAmerican English. Corpora, 7(2):121–157.\nDavies, M. 2015. The Wikipedia Corpus: 4.6 million articles, 1.9 billion words. Adapted from Wikipedia. https:\n//www.english-corpora.org/wiki/.\nDeerwester, S. C., S. T. Dumais, G. W. Furnas, R. A. Harshman, T. K. Landauer, K. E. Lochbaum, and L. Streeter.\n1988. Computer information retrieval using latent semantic structure: US Patent 4,839,853.\nDeerwester, S. C., S. T. Dumais, T. K. Landauer, G. W. Furnas, and R. A. Harshman. 1990. Indexing by latent semantics analysis. JASIS, 41(6):391–407.\nEthayarajh, K., D. Duvenaud, and G. Hirst. 2019a. Towards\nunderstanding linear word analogies. ACL.\nEthayarajh, K., D. Duvenaud, and G. Hirst. 2019b. Understanding undesirable word embedding associations. ACL.\nFinkelstein, L., E. Gabrilovich, Y. Matias, E. Rivlin,\nZ. Solan, G. Wolfman, and E. Ruppin. 2002. Placing\nsearch in context: The concept revisited. ACM Transactions on Information Systems, 20(1):116—-131.\nFirth, J. R. 1957. A synopsis of linguistic theory 1930–\n1955. In Studies in Linguistic Analysis. Philological Society. Reprinted in Palmer, F. (ed.) 1968. Selected Papers\nof J. R. Firth. Longman, Harlow.\nGarg, N., L. Schiebinger, D. Jurafsky, and J. Zou. 2018.\nWord embeddings quantify 100 years of gender and ethnic stereotypes. Proceedings of the National Academy of\nSciences, 115(16):E3635–E3644.\nGirard, G. 1718. La justesse de la langue françoise: ou les\ndifférentes significations des mots qui passent pour synonimes. Laurent d’Houry, Paris.\n\n\f26\n\nChapter 5\n\n•\n\nEmbeddings\n\nGiuliano, V. E. 1965.\nThe interpretation of word\nassociations.\nStatistical Association Methods For\nMechanized Documentation. Symposium Proceedings. Washington, D.C., USA, March 17, 1964.\nhttps://nvlpubs.nist.gov/nistpubs/Legacy/\nMP/nbsmiscellaneouspub269.pdf.\nGladkova, A., A. Drozd, and S. Matsuoka. 2016. Analogybased detection of morphological and semantic relations\nwith word embeddings: what works and what doesn’t.\nNAACL Student Research Workshop.\nGlenberg, A. M. and D. A. Robertson. 2000. Symbol grounding and meaning: A comparison of high-dimensional and\nembodied theories of meaning. Journal of memory and\nlanguage, 43(3):379–401.\nGonen, H. and Y. Goldberg. 2019. Lipstick on a pig: Debiasing methods cover up systematic gender biases in word\nembeddings but do not remove them. NAACL HLT.\nGould, S. J. 1980. The Panda’s Thumb. Penguin Group.\nGreenwald, A. G., D. E. McGhee, and J. L. K. Schwartz.\n1998. Measuring individual differences in implicit cognition: the implicit association test. Journal of personality\nand social psychology, 74(6):1464–1480.\n\nKim, E. 2019.\nOptimize computational efficiency\nof skip-gram with negative sampling.\nhttps://\naegis4048.github.io/optimize_computational_\nefficiency_of_skip-gram_with_negative_\nsampling.\nLake, B. M. and G. L. Murphy. 2021. Word meaning in\nminds and machines. Psychological Review. In press.\nLandauer, T. K. and S. T. Dumais. 1997. A solution to Plato’s\nproblem: The Latent Semantic Analysis theory of acquisition, induction, and representation of knowledge. Psychological Review, 104:211–240.\nLandauer, T. K., D. Laham, B. Rehder, and M. E. Schreiner.\n1997. How well can passage meaning be derived without using word order? A comparison of Latent Semantic\nAnalysis and humans. COGSCI.\nLapesa, G. and S. Evert. 2014. A large scale evaluation of\ndistributional semantic models: Parameters, interactions\nand model selection. TACL, 2:531–545.\nLee, D. D. and H. S. Seung. 1999. Learning the parts of\nobjects by non-negative matrix factorization. Nature,\n401(6755):788–791.\nLevy, O. and Y. Goldberg. 2014a. Dependency-based word\nembeddings. ACL.\n\nHamilton, W. L., J. Leskovec, and D. Jurafsky. 2016. Diachronic word embeddings reveal statistical laws of semantic change. ACL.\n\nLevy, O. and Y. Goldberg. 2014b. Linguistic regularities in\nsparse and explicit word representations. CoNLL.\n\nHarris, Z. S. 1954. Distributional structure. Word, 10:146–\n162.\n\nLevy, O. and Y. Goldberg. 2014c. Neural word embedding\nas implicit matrix factorization. NeurIPS.\n\nHellrich, J. and U. Hahn. 2016.\nBad company—\nNeighborhoods in neural embedding spaces considered\nharmful. COLING.\n\nLevy, O., Y. Goldberg, and I. Dagan. 2015. Improving distributional similarity with lessons learned from word embeddings. TACL, 3:211–225.\n\nHill, F., R. Reichart, and A. Korhonen. 2015. Simlex-999:\nEvaluating semantic models with (genuine) similarity estimation. Computational Linguistics, 41(4):665–695.\n\nLin, Y., J.-B. Michel, E. Lieberman Aiden, J. Orwant,\nW. Brockman, and S. Petrov. 2012. Syntactic annotations\nfor the Google Books NGram corpus. ACL.\n\nHjelmslev, L. 1969. Prologomena to a Theory of Language.\nUniversity of Wisconsin Press. Translated by Francis J.\nWhitfield; original Danish edition 1943.\n\nLinzen, T. 2016. Issues in evaluating semantic spaces using word analogies. 1st Workshop on Evaluating VectorSpace Representations for NLP.\n\nHofmann, T. 1999. Probabilistic latent semantic indexing.\nSIGIR-99.\n\nManning, C. D., P. Raghavan, and H. Schütze. 2008. Introduction to Information Retrieval. Cambridge.\n\nHuang, E. H., R. Socher, C. D. Manning, and A. Y. Ng. 2012.\nImproving word representations via global context and\nmultiple word prototypes. ACL.\n\nMikolov, T., K. Chen, G. S. Corrado, and J. Dean. 2013a. Efficient estimation of word representations in vector space.\nICLR 2013.\n\nJia, S., T. Meng, J. Zhao, and K.-W. Chang. 2020. Mitigating gender bias amplification in distribution by posterior\nregularization. ACL.\n\nMikolov, T., S. Kombrink, L. Burget, J. H. Černockỳ, and\nS. Khudanpur. 2011. Extensions of recurrent neural network language model. ICASSP.\n\nJones, M. P. and J. H. Martin. 1997. Contextual spelling correction using latent semantic analysis. ANLP.\n\nMikolov, T., I. Sutskever, K. Chen, G. S. Corrado, and\nJ. Dean. 2013b. Distributed representations of words and\nphrases and their compositionality. NeurIPS.\n\nJoos, M. 1950.\n22:701–708.\n\nDescription of language design.\n\nJASA,\n\nJurgens, D., S. M. Mohammad, P. Turney, and K. Holyoak.\n2012. SemEval-2012 task 2: Measuring degrees of relational similarity. *SEM 2012.\nKatz, J. J. and J. A. Fodor. 1963. The structure of a semantic\ntheory. Language, 39:170–210.\nKiela, D. and S. Clark. 2014. A systematic study of semantic\nvector space model parameters. EACL 2nd Workshop on\nContinuous Vector Space Models and their Compositionality (CVSC).\n\nMikolov, T., W.-t. Yih, and G. Zweig. 2013c. Linguistic regularities in continuous space word representations.\nNAACL HLT.\nNosek, B. A., M. R. Banaji, and A. G. Greenwald. 2002a.\nHarvesting implicit group attitudes and beliefs from a\ndemonstration web site. Group Dynamics: Theory, Research, and Practice, 6(1):101.\nNosek, B. A., M. R. Banaji, and A. G. Greenwald. 2002b.\nMath=male, me=female, therefore math6= me. Journal of\npersonality and social psychology, 83(1):44.\nOsgood, C. E., G. J. Suci, and P. H. Tannenbaum. 1957. The\nMeasurement of Meaning. University of Illinois Press.\n\n\fExercises\nPennington, J., R. Socher, and C. D. Manning. 2014. GloVe:\nGlobal vectors for word representation. EMNLP.\n\n27\n\nTurney, P. D. and M. L. Littman. 2005. Corpus-based learning of analogies and semantic relations. Machine Learning, 60(1-3):251–278.\n\nPeterson, J. C., D. Chen, and T. L. Griffiths. 2020. Parallelograms revisited: Exploring the limitations of vector space\nmodels for simple analogies. Cognition, 205.\n\nvan der Maaten, L. and G. E. Hinton. 2008. Visualizing highdimensional data using t-SNE. JMLR, 9:2579–2605.\n\nPilehvar, M. T. and J. Camacho-Collados. 2019. WiC: the\nword-in-context dataset for evaluating context-sensitive\nmeaning representations. NAACL HLT.\n\nWierzbicka, A. 1992. Semantics, Culture, and Cognition:\nUniversity Human Concepts in Culture-Specific Configurations. Oxford University Press.\n\nRehder, B., M. E. Schreiner, M. B. W. Wolfe, D. Laham,\nT. K. Landauer, and W. Kintsch. 1998. Using Latent\nSemantic Analysis to assess knowledge: Some technical\nconsiderations. Discourse Processes, 25(2-3):337–354.\n\nWierzbicka, A. 1996. Semantics: Primes and Universals.\nOxford University Press.\n\nRohde, D. L. T., L. M. Gonnerman, and D. C. Plaut. 2006.\nAn improved model of semantic similarity based on lexical co-occurrence. CACM, 8:627–633.\n\nZhao, J., T. Wang, M. Yatskar, V. Ordonez, and K.W. Chang. 2017. Men also like shopping: Reducing\ngender bias amplification using corpus-level constraints.\nEMNLP.\n\nRumelhart, D. E. and A. A. Abrahamson. 1973. A model for\nanalogical reasoning. Cognitive Psychology, 5(1):1–28.\nSalton, G. 1971. The SMART Retrieval System: Experiments\nin Automatic Document Processing. Prentice Hall.\nSchluter, N. 2018. The word analogy testing caveat. NAACL\nHLT.\nSchone, P. and D. Jurafsky. 2000. Knowlege-free induction\nof morphology using latent semantic analysis. CoNLL.\nSchone, P. and D. Jurafsky. 2001a. Is knowledge-free induction of multiword unit dictionary headwords a solved\nproblem? EMNLP.\nSchone, P. and D. Jurafsky. 2001b. Knowledge-free induction of inflectional morphologies. NAACL.\nSchütze, H. 1992. Dimensions of meaning. Proceedings of\nSupercomputing ’92. IEEE Press.\nSchütze, H. 1997. Ambiguity Resolution in Language Learning – Computational and Cognitive Models. CSLI, Stanford, CA.\nSchütze, H., D. A. Hull, and J. Pedersen. 1995. A comparison of classifiers and document representations for the\nrouting problem. SIGIR-95.\nSchütze, H. and J. Pedersen. 1993. A vector model for syntagmatic and paradigmatic relatedness. 9th Annual Conference of the UW Centre for the New OED and Text Research.\nSparck Jones, K. 1972. A statistical interpretation of term\nspecificity and its application in retrieval. Journal of Documentation, 28(1):11–21.\nSparck Jones, K. 1986. Synonymy and Semantic Classification. Edinburgh University Press, Edinburgh. Republication of 1964 PhD Thesis.\nSwitzer, P. 1965. Vector images in document retrieval.\nStatistical Association Methods For Mechanized Documentation. Symposium Proceedings. Washington, D.C.,\nUSA, March 17, 1964. https://nvlpubs.nist.gov/\nnistpubs/Legacy/MP/nbsmiscellaneouspub269.\npdf.\nTian, Y., V. Kulkarni, B. Perozzi, and S. Skiena. 2016. On\nthe convergent properties of word embedding methods.\nArXiv preprint arXiv:1605.03956.\nTurian, J., L. Ratinov, and Y. Bengio. 2010. Word representations: a simple and general method for semi-supervised\nlearning. ACL.\n\nWittgenstein, L. 1953. Philosophical Investigations. (Translated by Anscombe, G.E.M.). Blackwell.\n\nZhao, J., Y. Zhou, Z. Li, W. Wang, and K.-W. Chang. 2018.\nLearning gender-neutral word embeddings. EMNLP.\n\n\f",
    "file_path": "/Users/colinsidberry/Downloads/NLP_Textbook/embeddings.txt",
    "file_size_kb": 85.34
  },
  {
    "id": "03b8d296c665e3e8",
    "source": "nlp_textbook",
    "chapter": "Logistic Regression and Text Classification",
    "filename": "logistic-regression.txt",
    "content": "Speech and Language Processing. Daniel Jurafsky & James H. Martin.\nrights reserved. Draft of August 24, 2025.\n\nCopyright © 2025.\n\nAll\n\nCHAPTER\n\n4\n\nLogistic Regression and Text\nClassification\nEn sus remotas páginas está escrito que los animales se dividen en:\na. pertenecientes al Emperador h. incluidos en esta clasificación\nb. embalsamados\ni. que se agitan como locos\nc. amaestrados\nj. innumerables\nd. lechones\nk. dibujados con un pincel finı́simo de pelo de camello\ne. sirenas\nl. etcétera\nf. fabulosos\nm. que acaban de romper el jarrón\ng. perros sueltos\nn. que de lejos parecen moscas\nBorges (1964)\n\nClassification lies at the heart of language processing and intelligence. Recognizing a letter, a word, or a face, sorting mail, assigning grades to homeworks; these\nare all examples of assigning a category to an input. The challenges of classification\nwere famously highlighted by the fabulist Jorge Luis Borges (1964), who imagined\nan ancient mythical encyclopedia that classified animals into:\n(a) those that belong to the Emperor, (b) embalmed ones, (c) those that\nare trained, (d) suckling pigs, (e) mermaids, (f) fabulous ones, (g) stray\ndogs, (h) those that are included in this classification, (i) those that\ntremble as if they were mad, (j) innumerable ones, (k) those drawn with\na very fine camel’s hair brush, (l) others, (m) those that have just broken\na flower vase, (n) those that resemble flies from a distance.\n\ntext\ncategorization\nsentiment\nanalysis\n\nLuckily, the classes we use for language processing are easier to define than\nthose of Borges. In this chapter we introduce the logistic regression algorithm for\nclassification, and apply it to text categorization, the task of assigning a label or\ncategory to a text or document. We’ll focus on one text categorization task, sentiment analysis, the categorization of sentiment, the positive or negative orientation\nthat a writer expresses toward some object. A review of a movie, book, or product\nexpresses the author’s sentiment toward the product, while an editorial or political\ntext expresses sentiment toward an action or candidate. Extracting sentiment is thus\nrelevant for fields from marketing to politics.\nFor the binary task of labeling a text as indicating positive or negative stance,\nwords (like awesome and love, or awful and ridiculously are very informative, as we\ncan see from these sample extracts from movie/restaurant reviews:\n+ ...awesome caramel sauce and sweet toasty almonds. I love this place!\n− ...awful pizza and ridiculously overpriced...\n\nspam detection\nlanguage id\nauthorship\nattribution\n\nThere are many text classification tasks. In spam detection we assign an email\nto one of the two classes spam or not-spam. Language id is the task of determining what language a text is written in, while authorship attribution is the task of\ndetermining a text’s author, relevant to both humanistic and forensic analysis.\n\n\f2\n\nC HAPTER 4\n\nsigmoid\nsoftmax\nlogit\n\n4.1\n\n•\n\nL OGISTIC R EGRESSION\n\nBut what makes classification so important is that language modeling can also\nbe viewed as classification: each word can be thought of as a class, and so predicting\nthe next word is classifying the context-so-far into a class for each next word. As\nwe’ll see, this intuition underlies large language models.\nThe algorithm for classification we introduce in this chapter, logistic regression,\nis equally important, in a number of ways. First, logistic regression has a close\nrelationship with neural networks. As we will see in Chapter 6, a neural network\ncan be viewed as a series of logistic regression classifiers stacked on top of each\nother. Second, logistic regression introduces ideas that are fundamental to neural\nnetworks and language models, like the sigmoid and softmax functions, the logit,\nand the key gradient descent algorithm for learning. Finally, logistic regression is\nalso one of the most important analytic tools in the social and natural sciences.\n\nMachine learning and classification\n\nobservation\n\nhat\n\nThe goal of classification is to take a single input (we call each input an observation), extract some useful features or properties of the input, and thereby classify\nthe observation into one of a set of discrete classes. We’ll call the input x, and say\nthat the output comes from a fixed set of output classes Y = {y1 , y2 , ..., yM }. Our goal\nis return a predicted class ŷ ∈ Y . The hat or circumflex notation ŷ is used to refer to\nan estimated or predicted value. Sometimes you’ll see the output classes referred to\nas the set C instead of Y .\nFor sentiment analysis, the input x might be a review, or some other text. And\nthe output set Y might be the set:\n{positive, negative}\nor the set\n{0, 1}\nFor language id, the input might be a text that we need to know what language it was\nwritten in, and the output set Y is the set of languages, i.e.,\nY = {Abkhaz, Ainu, Albanian, Amharic, ...Zulu, Zuñi}\nThere are many ways to do classification. One method is to use rules handwritten\nby humans. For example, we might have a rule like:\nIf the word ‘‘love’’ appears in x, and it’s not preceded by the\nword ‘‘don’t\", classify as positive\n\nsupervised\nmachine\nlearning\n\nHandwritten rules can be components of modern NLP systems, such as the handwritten lists of positive and negative words that can be used in sentiment analysis,\nas we’ll see below. But rules can be fragile, as situations or data change over time,\nand for many tasks there are complex interactions between different features (like\nthe example of negation with “don’t” in the rule above), so it can be quite hard for\nhumans to come up with rules that are successful over many situations.\nAnother method that we will introduce later is to ask a large language model (of\nthe type we will introduce in Chapter 7) by prompting the model to give a label to\nsome text. Prompting can be powerful, but again has weaknesses: language models\noften hallucinate, and may not be able to explain why they chose the class they did.\nFor these reasons the most common way to do classification is to use supervised machine learning. Supervised machine learning is a paradigm in which, in\n\n\f4.1\n\n•\n\nM ACHINE LEARNING AND CLASSIFICATION\n\n3\n\naddition to the input and the set of output classes, we have a labeled training set\nand a learning algorithm. We talked about training sets in Chapter 3 as a locus for\ncomputing n-gram statistics. But in supervised machine learning the training set is\nlabeled, meaning that it contains a set of input observations, each observation associated with the correct output (a ‘supervision signal’). We can generally refer to a\ntraining set of m input/output pairs, where each input x is a text, in the case of text\nclassification, and each is hand-labeled with an associated class (the correct label):.\ntraining set: {(x(1) , y(1) ), (x(2) , y(2) ), . . . , (x(m) , y(m) )}\n\n(4.1)\n\nWe’ll use superscripts in parentheses to refer to individual observations or instances\nin the training set. So for sentiment classification, a training set might be a set of\nsentences or other texts, each with their correct sentiment label.\nOur goal is to learn from this training set a classifier that is capable of mapping\nfrom a new input x to its correct class y ∈ Y . It does this by learn to find features in\nthese training sentences (perhaps words like “awesome” or “awful”). Probabilistic\nclassifiers are the subset of machine learning classifiers that in addition to giving an\nanswer (which class is this observation in?), additionally will tell us the probability\nof the observation being in the class. This full distribution over the classes can be\nuseful information for downstream decisions; avoiding making discrete decisions\nearly on can be useful when combining systems.\nThere are many algorithms for achieving this supervised machine learning task,\n(naive Bayes, support vector machines, neural networks, fine-tuned language models), but logistic regression has the advantages we discussed above and so we’ll\nintroduce it! Any machine learning classifier thus has four components:\n1. A feature representation of the input. For each input observation x(i) , this\nwill be a vector of features [x1 , x2 , ..., xn ]. We will generally refer to feature\n( j)\ni for input x( j) as xi , sometimes simplified as xi , but we will also see the\nnotation fi , fi (x), or, for multiclass classification, fi (c, x).\n2. A classification function that computes ŷ, the estimated class, via P(y|x). We\nwill introduce the sigmoid and softmax tools for classification.\n3. An objective function that we want to optimize for learning, usually involving minimizing a loss function corresponding to error on training examples.\nWe will introduce the cross-entropy loss function.\n4. An algorithm for optimizing the objective function. We introduce the stochastic gradient descent algorithm.\nAt the highest level, logistic regression, and really any supervised machine learning classifier, has two phases\ntraining: We train the system (in the case of logistic regression that means training the weights w and b, introduced below) using stochastic gradient descent\nand the cross-entropy loss.\ntest: Given a test example x we compute the probability P(yi |x) of each class yi ,\nand return the higher probability label y = 1 or y = 0.\nLogistic regression can be used to classify an observation into one of two classes\n(like ‘positive sentiment’ and ‘negative sentiment’), or into one of many classes.\nBecause the mathematics for the two-class case is simpler, we’ll first describe this\nspecial case of logistic regression in the next few sections, beginning with the sigmoid function, and then turn to multinomial logistic regression for more than two\nclasses and the use of the softmax function in Section 4.4.\n\n\f4\n\nC HAPTER 4\n\n4.2\n\n•\n\nL OGISTIC R EGRESSION\n\nThe sigmoid function\n\nbias term\nintercept\n\nThe goal of binary logistic regression is to train a classifier that can make a binary\ndecision about the class of a new input observation. Here we introduce the sigmoid\nclassifier that will help us make this decision.\nConsider a single input observation x, which we will represent by a vector of\nfeatures [x1 , x2 , ..., xn ]. (We’ll show sample features in the next subsection.) The\nclassifier output y can be 1 (meaning the observation is a member of the class) or\n0 (the observation is not a member of the class). We want to know the probability\nP(y = 1|x) that this observation is a member of the class. So perhaps the decision\nis “positive sentiment” versus “negative sentiment”, the features represent counts of\nwords in a document, P(y = 1|x) is the probability that the document has positive\nsentiment, and P(y = 0|x) is the probability that the document has negative sentiment.\nLogistic regression solves this task by learning, from a training set, a vector of\nweights and a bias term. Each weight wi is a real number, and is associated with one\nof the input features xi . The weight wi represents how important that input feature\nis to the classification decision, and can be positive (providing evidence that the instance being classified belongs in the positive class) or negative (providing evidence\nthat the instance being classified belongs in the negative class). Thus we might\nexpect in a sentiment task the word awesome to have a high positive weight, and\nabysmal to have a very negative weight. The bias term, also called the intercept, is\nanother real number that’s added to the weighted inputs.\nTo make a decision on a test instance—after we’ve learned the weights in training—\nthe classifier first multiplies each xi by its weight wi , sums up the weighted features,\nand adds the bias term b. The resulting single number z expresses the weighted sum\nof the evidence for the class.\n!\nn\nX\nz =\nwi xi + b\n(4.2)\ni=1\n\ndot product\n\nIn the rest of the book we’ll represent such sums using the dot product notation\nfrom linear algebra. The dot product of two vectors a and b, written as a · b, is the\nsum of the products of the corresponding elements of each vector. (Notice that we\nrepresent vectors using the boldface notation b). Thus the following is an equivalent\nformation to Eq. 4.2:\nz = w·x+b\n\nsigmoid\nlogistic\nfunction\n\n(4.3)\n\nBut note that nothing in Eq. 4.3 forces z to be a legal probability, that is, to lie\nbetween 0 and 1. In fact, since weights are real-valued, the output might even be\nnegative; z ranges from −∞ to ∞.\nTo create a probability, we’ll pass z through the sigmoid function, σ (z). The\nsigmoid function (named because it looks like an s) is also called the logistic function, and gives logistic regression its name. The sigmoid has the following equation,\nshown graphically in Fig. 4.1:\nσ (z) =\n\n1\n1\n=\n−z\n1+e\n1 + exp (−z)\n\n(4.4)\n\n(For the rest of the book, we’ll use the notation exp(x) to mean ex .) The sigmoid\nhas a number of advantages; it takes a real-valued number and maps it into the range\n\n\f4.3\n\n•\n\nC LASSIFICATION WITH L OGISTIC R EGRESSION\n\n5\n\n1\nFigure 4.1 The sigmoid function σ (z) = 1+e\n−z takes a real value and maps it to the range\n(0, 1). It is nearly linear around 0 but outlier values get squashed toward 0 or 1.\n\n(0, 1), which is just what we want for a probability. Because it is nearly linear around\n0 but flattens toward the ends, it tends to squash outlier values toward 0 or 1. And\nit’s differentiable, which as we’ll see in Section 4.15 will be handy for learning.\nWe’re almost there. If we apply the sigmoid to the sum of the weighted features,\nwe get a number between 0 and 1. To make it a probability, we just need to make\nsure that the two cases, P(y = 1) and P(y = 0), sum to 1. We can do this as follows:\nP(y = 1) = σ (w · x + b)\n1\n=\n1 + exp (−(w · x + b))\nP(y = 0) = 1 − σ (w · x + b)\n1\n= 1−\n1 + exp (−(w · x + b))\nexp (−(w · x + b))\n=\n1 + exp (−(w · x + b))\n\n(4.5)\n\nThe sigmoid function has the property\n1 − σ (x) = σ (−x)\n\nlogit\n\n(4.6)\n\nso we could also have expressed P(y = 0) as σ (−(w · x + b)).\nFinally, one terminological point. The input to the sigmoid function, the score\nz = w · x + b from Eq. 4.3, is often called the logit. This is because the logit function\np\nis the inverse of the sigmoid. The logit function is the log of the odds ratio 1−p\n:\nlogit(p) = σ −1 (p) = ln\n\np\n1− p\n\n(4.7)\n\nUsing the term logit for z is a way of reminding us that by using the sigmoid to turn\nz (which ranges from −∞ to ∞) into a probability, we are implicitly interpreting z as\nnot just any real-valued number, but as specifically a log odds.\n\n4.3\n\nClassification with Logistic Regression\n\ndecision\nboundary\n\nThe sigmoid function from the prior section thus gives us a way to take an instance\nx and compute the probability P(y = 1|x).\nHow do we make a decision about which class to apply to a test instance x? For\na given x, we say yes if the probability P(y = 1|x) is more than .5, and no otherwise.\nWe call .5 the decision boundary:\n\n\f6\n\nC HAPTER 4\n\n•\n\nL OGISTIC R EGRESSION\n\u001a\ndecision(x) =\n\n1 if P(y = 1|x) > 0.5\n0 otherwise\n\nLet’s have some examples of applying logistic regression as a classifier for language\ntasks.\n\n4.3.1\n\nSentiment Classification\n\nSuppose we are doing binary sentiment classification on movie review text, and\nwe would like to know whether to assign the sentiment class + or − to a review\ndocument doc. We’ll represent each input observation by the 6 features x1 . . . x6 of\nthe input shown in the following table; Fig. 4.2 shows features in a sample mini test\ndocument.\nVar\nx1\nx2\nx3\nx4\nx5\nx6\n\nDefinition\ncount(positive lexicon words ∈ doc)\ncount(negative\nlexicon words ∈ doc)\n\u001a\n1 if “no” ∈ doc\n0 otherwise\ncount(1st\nand 2nd pronouns ∈ doc)\n\u001a\n1 if “!” ∈ doc\n0 otherwise\nln(word+punctuation count of doc)\n\nValue in Fig. 4.2\n3\n2\n1\n3\n0\nln(66) = 4.19\n\nx2=2\nx3=1\nIt's hokey . There are virtually no surprises , and the writing is second-rate .\nSo why was it so enjoyable ? For one thing , the cast is\ngreat . Another nice touch is the music . I was overcome with the urge to get off\nthe couch and start dancing . It sucked me in , and it'll do the same to you .\nx1=3\nFigure 4.2\n\nx5=0\n\nx6=4.19\n\nx4=3\n\nA sample mini test document showing the extracted features in the vector x.\n\nLet’s assume for the moment that we’ve already learned a real-valued weight\nfor each of these features, and that the 6 weights corresponding to the 6 features\nare [2.5, −5.0, −1.2, 0.5, 2.0, 0.7], while b = 0.1. (We’ll discuss in the next section\nhow the weights are learned.) The weight w1 , for example indicates how important\na feature the number of positive lexicon words (great, nice, enjoyable, etc.) is to\na positive sentiment decision, while w2 tells us the importance of negative lexicon\nwords. Note that w1 = 2.5 is positive, while w2 = −5.0, meaning that negative words\nare negatively associated with a positive sentiment decision, and are about twice as\nimportant as positive words.\nGiven these 6 features and the input review x, P(+|x) and P(−|x) can be com-\n\n\f4.3\n\n•\n\nC LASSIFICATION WITH L OGISTIC R EGRESSION\n\n7\n\nputed using Eq. 4.5:\nP(+|x) = P(y = 1|x) = σ (w · x + b)\n= σ ([2.5, −5.0, −1.2, 0.5, 2.0, 0.7] · [3, 2, 1, 3, 0, 4.19] + 0.1)\n= σ (.833)\n= 0.70\n\n(4.8)\n\nP(−|x) = P(y = 0|x) = 1 − σ (w · x + b)\n= 0.30\n\n4.3.2\nperiod\ndisambiguation\n\nfeature\ninteractions\n\nfeature\ntemplates\n\nOther classification tasks and features\n\nLogistic regression is applied to all sorts of NLP tasks, and any property of the input\ncan be a feature. Consider the task of period disambiguation: deciding if a period\nis the end of a sentence or part of a word, by classifying each period into one of two\nclasses, EOS (end-of-sentence) and not-EOS. We might use features like x1 below\nexpressing that the current word is lower case, perhaps with a positive weight. Or a\nfeature expressing that the current word is in our abbreviations dictionary (“Prof.”),\nperhaps with a negative weight. A feature can also express a combination of properties. For example a period following an upper case word is likely to be an EOS, but\nif the word itself is St. and the previous word is capitalized then the period is likely\npart of a shortening of the word street following a street name.\n\u001a\n1 if “Case(wi ) = Lower”\nx1 =\n0 otherwise\n\u001a\n1 if “wi ∈ AcronymDict”\nx2 =\n0 otherwise\n\u001a\n1 if “wi = St. & Case(wi−1 ) = Upper”\nx3 =\n0 otherwise\nDesigning versus learning features: In classic models, features are designed by\nhand by examining the training set with an eye to linguistic intuitions and literature,\nsupplemented by insights from error analysis on the training set of an early version\nof a system. We can also consider feature interactions, complex features that are\ncombinations of more primitive features. We saw such a feature for period disambiguation above, where a period on the word St. was less likely to be the end of the\nsentence if the previous word was capitalized. Features can be created automatically\nvia feature templates, abstract specifications of features. For example a bigram\ntemplate for period disambiguation might create a feature for every pair of words\nthat occurs before a period in the training set. Thus the feature space is sparse, since\nwe only have to create a feature if that n-gram exists in that position in the training\nset. The feature is generally created as a hash from the string descriptions. A user\ndescription of a feature as, “bigram(American breakfast)” is hashed into a unique\ninteger i that becomes the feature number fi .\nIt should be clear from the prior paragraph that designing features by hand requires extensive human effort. For this reason, recent NLP systems avoid handdesigned features and instead focus on representation learning: ways to learn features automatically in an unsupervised way from the input. We’ll introduce methods\nfor representation learning in Chapter 5 and Chapter 6.\nScaling input features: When different input features have extremely different\nranges of values, it’s common to rescale them so they have comparable ranges. We\n\n\f8\n\nC HAPTER 4\nstandardize\nz-score\n\n•\n\nL OGISTIC R EGRESSION\n\nstandardize input values by centering them to result in a zero mean and a standard\ndeviation of one (this transformation is sometimes called the z-score). That is, if µi\nis the mean of the values of feature xi across the m observations in the input dataset,\nand σi is the standard deviation of the values of features xi across the input dataset,\nwe can replace each feature xi by a new feature xi0 computed as follows:\nv\nu X\n\u00112\nu 1 m \u0010 ( j)\nσi = t\nxi − µi\nm\n\nm\n\n1 X ( j)\nxi\nµi =\nm\nj=1\n\nj=1\n\nxi − µi\nxi0 =\nσi\nnormalize\n\n(4.9)\n\nAlternatively, we can normalize the input features values to lie between 0 and 1:\nxi0 =\n\nxi − min(xi )\nmax(xi ) − min(xi )\n\n(4.10)\n\nHaving input data with comparable range is useful when comparing values across\nfeatures. Data scaling is especially important in large neural networks, since it helps\nspeed up gradient descent.\n\n4.3.3\n\nProcessing many examples at once\n\nWe’ve shown the equations for logistic regression for a single example. But in practice we’ll of course want to process an entire test set with many examples. Let’s\nsuppose we have a test set consisting of m test examples each of which we’d like to\nclassify. We’ll continue to use the notation from page 3, in which a superscript value\nin parentheses refers to the example index in some set of data (either for training or\nfor test). So in this case each test example x(i) has a feature vector x(i) , 1 ≤ i ≤ m.\n(As usual, we’ll represent vectors and matrices in bold.)\nOne way to compute each output value ŷ(i) is just to have a for-loop, and compute\neach test example one at a time:\nforeach x(i) in input [x(1) , x(2) , ..., x(m) ]\ny(i) = σ (w · x(i) + b)\n\n(4.11)\n\nFor the first 3 test examples, then, we would be separately computing the predicted ŷ(i) as follows:\nP(y(1) = 1|x(1) ) = σ (w · x(1) + b)\nP(y(2) = 1|x(2) ) = σ (w · x(2) + b)\nP(y(3) = 1|x(3) ) = σ (w · x(3) + b)\nBut it turns out that we can slightly modify our original equation Eq. 4.5 to do\nthis much more efficiently. We’ll use matrix arithmetic to assign a class to all the\nexamples with one matrix operation!\nFirst, we’ll pack all the input feature vectors for each input x into a single input\nmatrix X, where each row i is a row vector consisting of the feature vector for input example x(i) (i.e., the vector x(i) ). Assuming each example has f features and\n\n\f4.4\n\n•\n\nM ULTINOMIAL LOGISTIC REGRESSION\n\nweights, X will therefore be a matrix of shape [m × f ], as follows:\n\n (1) (1)\n(1)\nx1 x2 . . . x f\n (2) (2)\n(2) \nx\nx2 . . . x f \n1\n\n\nX = \n\n\n x1(3) x2(3) . . . x(3)\nf\n...\n\n9\n\n(4.12)\n\nNow if we introduce b as a vector of length m which consists of the scalar bias\nterm b repeated m times, b = [b, b, ..., b], and ŷ = [ŷ(1) , ŷ(2) ..., ŷ(m) ] as the vector of\noutputs (one scalar ŷ(i) for each input x(i) and its feature vector x(i) ), and represent\nthe weight vector w as a column vector, we can compute all the outputs with a single\nmatrix multiplication and one addition:\nŷ = σ (Xw + b)\n\n(4.13)\n\nYou should convince yourself that Eq. 4.13 computes the same thing as our for-loop\nin Eq. 4.11. For example ŷ(1) , the first entry of the output vector y, will correctly be:\n(1)\n\n(1)\n\n(1)\n\nŷ(1) = [x1 , x2 , ..., x f ] · [w1 , w2 , ..., w f ] + b\n\n(4.14)\n\nNote that we had to reorder X and w from the order they appeared in in Eq. 4.5 to\nmake the multiplications come out properly. Here is Eq. 4.13 again with the shapes\nshown:\nŷ = σ (X\n(m × 1)\n\nw\n\n+\n\nb)\n\n(m × f ) ( f × 1) (m × 1)\n\n(4.15)\n\nModern compilers and compute hardware can compute this matrix operation very\nefficiently, making the computation much faster, which becomes important when\ntraining or testing on very large datasets.\nNote by the way that we could have kept X and w in the original order (as\nŷ = σ (wX + b)) if we had chosen to define X differently as a matrix of column\nvectors, one vector for each input example, instead of row vectors, and then it would\nhave shape [ f × m]. But we conventionally represent inputs as rows.\n\n4.4\n\nMultinomial logistic regression\n\nmultinomial\nlogistic\nregression\n\nSometimes we need more than two classes. Perhaps we might want to do 3-way\nsentiment classification (positive, negative, or neutral). Or we could be assigning\nsome of the labels we will introduce in Chapter 17, like the part of speech of a word\n(choosing from 10, 30, or even 50 different parts of speech), or the named entity\ntype of a phrase (choosing from tags like person, location, organization). Or, for\nlarge language models, we’ll be predicting the next word out of the |V | possible\nwords in the vocabulary, so it’s |V |-way classification.\nIn such cases we use multinomial logistic regression, also called softmax regression (in older NLP literature you will sometimes see the name maxent classifier). In multinomial logistic regression we want to label each observation with a\nclass k from a set of K classes, under the stipulation that only one of these classes is\nthe correct one (sometimes called hard classification; an observation can not be in\n\n\f10\n\nC HAPTER 4\n\n•\n\nL OGISTIC R EGRESSION\n\nmultiple classes). Let’s use the following representation: the output y for each input\nx will be a vector of length K. If class c is the correct class, we’ll set yc = 1, and\nset all the other elements of y to be 0, i.e., yc = 1 and y j = 0 ∀ j 6= c. A vector like\nthis y, with one value=1 and the rest 0, is called a one-hot vector. The job of the\nclassifier is to produce an estimate vector ŷ. For each class k, the value ŷk will be\nthe classifier’s estimate of the probability P(yk = 1|x).\n\n4.4.1\nsoftmax\n\nSoftmax\n\nThe multinomial logistic classifier uses a generalization of the sigmoid, called the\nsoftmax function, to compute p(yk = 1|x). The softmax function takes a vector\nz = [z1 , z2 , ..., zK ] of K arbitrary values and maps them to a probability distribution,\nwith each value in the range [0,1], and all the values summing to 1. Like the sigmoid,\nit is an exponential function.\nFor a vector z of dimensionality K, the softmax is defined as:\nexp (zi )\n1≤i≤K\nsoftmax(zi ) = PK\nj=1 exp (z j )\n\n(4.16)\n\nThe softmax of an input vector z = [z1 , z2 , ..., zK ] is thus a vector itself:\n\"\n#\nexp (z1 )\nexp (z2 )\nexp (zK )\nsoftmax(z) = PK\n, PK\n, ..., PK\n(4.17)\ni=1 exp (zi )\ni=1 exp (zi )\ni=1 exp (zi )\nP\nThe denominator Ki=1 exp (zi ) is used to normalize all the values into probabilities.\nThus for example given a vector:\nz = [0.6, 1.1, −1.5, 1.2, 3.2, −1.1]\nthe resulting (rounded) softmax(z) is\n[0.05, 0.09, 0.01, 0.1, 0.74, 0.01]\nLike the sigmoid, the softmax has the property of squashing values toward 0 or 1.\nThus if one of the inputs is larger than the others, it will tend to push its probability\ntoward 1, and suppress the probabilities of the smaller inputs.\nFinally, note that, just as for the sigmoid, we refer to z, the vector of scores that\nis the input to the softmax, as logits (see Eq. 4.7).\n\n4.4.2\n\nApplying softmax in logistic regression\n\nWhen we apply softmax for logistic regression, the input will (just as for the sigmoid) be the dot product between a weight vector w and an input vector x (plus a\nbias). But now we’ll need separate weight vectors wk and bias bk for each of the K\nclasses. The probability of each of our output classes ŷk can thus be computed as:\nP(yk = 1|x) =\n\nexp (wk · x + bk )\nK\nX\n\n(4.18)\n\nexp (w j · x + b j )\n\nj=1\n\nThe form of Eq. 4.18 makes it seem that we would compute each output separately. Instead, it’s more common to set up the equation for more efficient computation by modern vector processing hardware. We’ll do this by representing the\n\n\f4.4\n\n•\n\nM ULTINOMIAL LOGISTIC REGRESSION\n\n11\n\nset of K weight vectors as a weight matrix W and a bias vector b. Each row k of\nW corresponds to the vector of weights wk . W thus has shape [K × f ], for K the\nnumber of output classes and f the number of input features. The bias vector b has\none value for each of the K output classes. If we represent the weights in this way,\nwe can compute ŷ, the vector of output probabilities for each of the K classes, by a\nsingle elegant equation:\nŷ = softmax(Wx + b)\n\nprototype\n\n(4.19)\n\nIf you work out the matrix arithmetic, you can see that the estimated score of\nthe first output class ŷ1 (before we take the softmax) will correctly turn out to be\nw1 · x + b1 .\nOne helpful interpretation of the weight matrix W is to see each row wk as a\nprototype of class k. The weight vector wk that is learned represents the class as\na kind of template. Since two vectors that are more similar to each other have a\nhigher dot product with each other, the dot product acts as a similarity function.\nLogistic regression is thus learning an exemplar representation for each class, such\nthat incoming vectors are assigned the class k they are most similar to from the K\nclasses (Doumbouya et al., 2025).\nFig. 4.3 shows the difference between binary and multinomial logistic regression\nby illustrating the weight vector versus weight matrix in the computation of the\noutput class probabilities.\n\n4.4.3\n\nFeatures in Multinomial Logistic Regression\n\nFeatures in multinomial logistic regression act like features in binary logistic regression, with the difference mentioned above that we’ll need separate weight vectors\nand biases for each of the K classes. Recall our binary exclamation point feature x5\nfrom page 6:\n\u001a\n1 if “!” ∈ doc\nx5 =\n0 otherwise\nIn binary classification a positive weight w5 on a feature influences the classifier\ntoward y = 1 (positive sentiment) and a negative weight influences it toward y = 0\n(negative sentiment) with the absolute value indicating how important the feature\nis. For multinomial logistic regression, by contrast, with separate weights for each\nclass, a feature can be evidence for or against each individual class.\nIn 3-way multiclass sentiment classification, for example, we must assign each\ndocument one of the 3 classes +, −, or 0 (neutral). Now a feature related to exclamation marks might have a negative weight for 0 documents, and a positive weight\nfor + or − documents:\nFeature\nf5 (x)\n\nDefinition\n\u001a\n1 if “!” ∈ doc\n0 otherwise\n\nw5,+ w5,− w5,0\n3.5\n\n3.1\n\n−5.3\n\nBecause these feature weights are dependent both on the input text and the output\nclass, we sometimes make this dependence explicit and represent the features themselves as f (x, y): a function of both the input and the class. Using such a notation\nf5 (x) above could be represented as three features f5 (x, +), f5 (x, −), and f5 (x, 0),\neach of which has a single weight. We’ll use this kind of notation in our description\nof the CRF in Chapter 17.\n\n\f12\n\nC HAPTER 4\n\n•\n\nL OGISTIC R EGRESSION\nBinary Logistic Regression\np(+) = 1- p(-)\n\ny\n\nOutput\nsigmoid\n\ny^\n\n[scalar]\n\nw\n\nWeight vector\n\n[1⨉f]\n\nInput feature x\n[f ⨉1]\nvector\n\nx1\n\nx2\n\nx3\n\n… xf\n\nwordcount positive lexicon count of\n=3\nwords = 1\n“no” = 0\n\ndessert\n\nInput words\n\nwas\n\ngreat\n\nMultinomial Logistic Regression\n\ny\n\nOutput\nsoftmax\n\n[K⨉1]\n\nWeight\nmatrix\n\n[K⨉f]\n\nInput feature\nvector\n\nInput words\n\np(+)\n\np(-)\n\np(neut)\n\ny^1\n\n^y\n2\n\ny^3\n\nThese f red weights\nare a row of W\ncorresponding\nto weight vector w3,\n\nW\nx\n[f⨉1]\n\n(= weights for class 3,\n= a prototype of class 3)\n\nx1\n\nx2\n\nx3\n\nwordcount positive lexicon\n=3\nwords = 1\n\ndessert\n\n…\n\ncount of\n“no” = 0\n\nwas\n\nxf\n\ngreat\n\nFigure 4.3 Binary versus multinomial logistic regression. Binary logistic regression uses a\nsingle weight vector w, and has a scalar output ŷ. In multinomial logistic regression we have\nK separate weight vectors corresponding to the K classes, all packed into a single weight\nmatrix W, and a vector output ŷ. We omit the biases from both figures for clarity.\n\n4.5\n\nLearning in Logistic Regression\n\nloss\n\nHow are the parameters of the model, the weights w and bias b, learned? Logistic\nregression is an instance of supervised classification in which we know the correct\nlabel y (either 0 or 1) for each observation x. What the system produces via Eq. 4.5\nis ŷ, the system’s estimate of the true y. We want to learn parameters (meaning w\nand b) that make ŷ for each training observation as close as possible to the true y.\nThis requires two components that we foreshadowed in the introduction to the\nchapter. The first is a metric for how close the current label (ŷ) is to the true gold\nlabel y. Rather than measure similarity, we usually talk about the opposite of this:\nthe distance between the system output and the gold output, and we call this distance\nthe loss function or the cost function. In the next section we’ll introduce the loss\nfunction that is commonly used for logistic regression and also for neural networks,\n\n\f4.6\n\n•\n\nT HE CROSS - ENTROPY LOSS FUNCTION\n\n13\n\nthe cross-entropy loss.\nThe second thing we need is an optimization algorithm for iteratively updating\nthe weights so as to minimize this loss function. The standard algorithm for this is\ngradient descent; we’ll introduce the stochastic gradient descent algorithm in the\nfollowing section.\nWe’ll describe these algorithms for the simpler case of binary logistic regression in the next two sections, and then turn to multinomial logistic regression in\nSection 4.8.\n\n4.6\n\nThe cross-entropy loss function\nWe need a loss function that expresses, for an observation x, how close the classifier\noutput (ŷ = σ (w · x + b)) is to the correct output (y, which is 0 or 1). We’ll call this:\nL(ŷ, y) = How much ŷ differs from the true y\n\ncross-entropy\nloss\n\n(4.20)\n\nWe do this via a loss function that prefers the correct class labels of the training examples to be more likely. This is called conditional maximum likelihood\nestimation: we choose the parameters w, b that maximize the log probability of\nthe true y labels in the training data given the observations x. The resulting loss\nfunction is the negative log likelihood loss, generally called the cross-entropy loss.\nLet’s derive this loss function, applied to a single observation x. We’d like to\nlearn weights that maximize the probability of the correct label p(y|x). Since there\nare only two discrete outcomes (1 or 0), this is a Bernoulli distribution, and we can\nexpress the probability p(y|x) that our classifier produces for one observation as the\nfollowing (keeping in mind that if y = 1, Eq. 4.21 simplifies to ŷ; if y = 0, Eq. 4.21\nsimplifies to 1 − ŷ):\np(y|x) = ŷ y (1 − ŷ)1−y\n\n(4.21)\n\nNow we take the log of both sides. This will turn out to be handy mathematically,\nand doesn’t hurt us; whatever values maximize a probability will also maximize the\nlog of the probability:\n\u0002\n\u0003\nlog p(y|x) = log ŷ y (1 − ŷ)1−y\n= y log ŷ + (1 − y) log(1 − ŷ)\n\n(4.22)\n\nEq. 4.22 describes a log likelihood that should be maximized. In order to turn this\ninto a loss function (something that we need to minimize), we’ll just flip the sign on\nEq. 4.22. The result is the cross-entropy loss LCE :\nLCE (ŷ, y) = − log p(y|x) = − [y log ŷ + (1 − y) log(1 − ŷ)]\n\n(4.23)\n\nFinally, we can plug in the definition of ŷ = σ (w · x + b):\nLCE (ŷ, y) = − [y log σ (w · x + b) + (1 − y) log (1 − σ (w · x + b))]\n\n(4.24)\n\nLet’s see if this loss function does the right thing for our example from Fig. 4.2. We\nwant the loss to be smaller if the model’s estimate is close to correct, and bigger if\nthe model is confused. So first let’s suppose the correct gold label for the sentiment\nexample in Fig. 4.2 is positive, i.e., y = 1. In this case our model is doing well, since\n\n\f14\n\nC HAPTER 4\n\n•\n\nL OGISTIC R EGRESSION\n\nfrom Eq. 4.8 it indeed gave the example a higher probability of being positive (.70)\nthan negative (.30). If we plug σ (w · x + b) = .70 and y = 1 into Eq. 4.24, the right\nside of the equation drops out, leading to the following loss (we’ll use log to mean\nnatural log when the base is not specified):\n−[y log σ (w · x + b) + (1 − y) log (1 − σ (w · x + b))]\n\nLCE (ŷ, y) =\n=\n\n− [log σ (w · x + b)]\n\n=\n\n− log(.70)\n\n=\n\n.36\n\nBy contrast, let’s pretend instead that the example in Fig. 4.2 was actually negative,\ni.e., y = 0 (perhaps the reviewer went on to say “But bottom line, the movie is\nterrible! I beg you not to see it!”). In this case our model is confused and we’d want\nthe loss to be higher. Now if we plug y = 0 and 1 − σ (w · x + b) = .30 from Eq. 4.8\ninto Eq. 4.24, the left side of the equation drops out:\n−[y log σ (w · x + b)+(1 − y) log (1 − σ (w · x + b))]\n\nLCE (ŷ, y) =\n=\n\n− [log (1 − σ (w · x + b))]\n\n=\n\n− log (.30)\n\n=\n\n1.2\n\nSure enough, the loss for the first classifier (.36) is less than the loss for the second\nclassifier (1.2).\nWhy does minimizing this negative log probability do what we want? A perfect\nclassifier would assign probability 1 to the correct outcome (y = 1 or y = 0) and\nprobability 0 to the incorrect outcome. That means if y equals 1, the higher ŷ is (the\ncloser it is to 1), the better the classifier; the lower ŷ is (the closer it is to 0), the\nworse the classifier. If y equals 0, instead, the higher 1 − ŷ is (closer to 1), the better\nthe classifier. The negative log of ŷ (if the true y equals 1) or 1 − ŷ (if the true y\nequals 0) is a convenient loss metric since it goes from 0 (negative log of 1, no loss)\nto infinity (negative log of 0, infinite loss). This loss function also ensures that as\nthe probability of the correct answer is maximized, the probability of the incorrect\nanswer is minimized; since the two sum to one, any increase in the probability of the\ncorrect answer is coming at the expense of the incorrect answer. It’s called the crossentropy loss, because Eq. 4.22 is also the formula for the cross-entropy between the\ntrue probability distribution y and our estimated distribution ŷ.\nNow we know what we want to minimize; in the next section, we’ll see how to\nfind the minimum.\n\n4.7\n\nGradient Descent\nOur goal with gradient descent is to find the optimal weights: minimize the loss\nfunction we’ve defined for the model. In Eq. 4.25 below, we’ll explicitly represent\nthe fact that the cross-entropy loss function LCE is parameterized by the weights. In\nmachine learning in general we refer to the parameters being learned as θ ; in the\ncase of logistic regression θ = {w, b}. So the goal is to find the set of weights which\nminimizes the loss function, averaged over all examples:\nm\n\nθ̂ = argmin\nθ\n\n1X\nLCE ( f (x(i) ; θ ), y(i) )\nm\ni=1\n\n(4.25)\n\n\f4.7\n\nconvex\n\n•\n\nG RADIENT D ESCENT\n\n15\n\nHow shall we find the minimum of this (or any) loss function? Gradient descent is\na method that finds a minimum of a function by figuring out in which direction (in\nthe space of the parameters θ ) the function’s slope is rising the most steeply, and\nmoving in the opposite direction. The intuition is that if you are hiking in a canyon\nand trying to descend most quickly down to the river at the bottom, you might look\naround yourself in all directions, find the direction where the ground is sloping the\nsteepest, and walk downhill in that direction.\nFor logistic regression, this loss function is conveniently convex. A convex function has at most one minimum; there are no local minima to get stuck in, so gradient\ndescent starting from any point is guaranteed to find the minimum. (By contrast,\nthe loss for multi-layer neural networks is non-convex, and gradient descent may\nget stuck in local minima for neural network training and never find the global optimum.)\nAlthough the algorithm (and the concept of gradient) are designed for direction\nvectors, let’s first consider a visualization of the case where the parameter of our\nsystem is just a single scalar w, shown in Fig. 4.4.\nGiven a random initialization of w at some value w1 , and assuming the loss\nfunction L happened to have the shape in Fig. 4.4, we need the algorithm to tell us\nwhether at the next iteration we should move left (making w2 smaller than w1 ) or\nright (making w2 bigger than w1 ) to reach the minimum.\n\nLoss\none step\nof gradient\ndescent\n\nslope of loss at w1\nis negative\n\nw1\n0\n\nwmin\n(goal)\n\nw\n\nFigure 4.4 The first step in iteratively finding the minimum of this loss function, by moving\nw in the reverse direction from the slope of the function. Since the slope is negative, we need\nto move w in a positive direction, to the right. Here superscripts are used for learning steps,\nso w1 means the initial value of w (which is 0), w2 the value at the second step, and so on.\ngradient\n\nlearning rate\n\nThe gradient descent algorithm answers this question by finding the gradient\nof the loss function at the current point and moving in the opposite direction. The\ngradient of a function of many variables is a vector pointing in the direction of the\ngreatest increase in a function. The gradient is a multi-variable generalization of the\nslope, so for a function of one variable like the one in Fig. 4.4, we can informally\nthink of the gradient as the slope. The dotted line in Fig. 4.4 shows the slope of this\nhypothetical loss function at point w = w1 . You can see that the slope of this dotted\nline is negative. Thus to find the minimum, gradient descent tells us to go in the\nopposite direction: moving w in a positive direction.\nThe magnitude of the amount to move in gradient descent is the value of the\nd\nL( f (x; w), y) weighted by a learning rate η. A higher (faster) learning\nslope dw\n\n\f16\n\nC HAPTER 4\n\n•\n\nL OGISTIC R EGRESSION\n\nrate means that we should move w more on each step. The change we make in our\nparameter is the learning rate times the gradient (or the slope, in our single-variable\nexample):\nd\nL( f (x; w), y)\n(4.26)\ndw\nNow let’s extend the intuition from a function of one scalar variable w to many\nvariables, because we don’t just want to move left or right, we want to know where\nin the N-dimensional space (of the N parameters that make up θ ) we should move.\nThe gradient is just such a vector; it expresses the directional components of the\nsharpest slope along each of those N dimensions. If we’re just imagining two weight\ndimensions (say for one weight w and one bias b), the gradient might be a vector with\ntwo orthogonal components, each of which tells us how much the ground slopes in\nthe w dimension and in the b dimension. Fig. 4.5 shows a visualization of the value\nof a 2-dimensional gradient vector taken at the red point.\nIn an actual logistic regression, the parameter vector w is much longer than 1 or\n2, since the input feature vector x can be quite long, and we need a weight wi for\neach xi . For each dimension/variable wi in w (plus the bias b), the gradient will have\na component that tells us the slope with respect to that variable. In each dimension\nwi , we express the slope as a partial derivative ∂∂wi of the loss function. Essentially\nwe’re asking: “How much would a small change in that variable wi influence the\ntotal loss function L?”\nFormally, then, the gradient of a multi-variable function f is a vector in which\neach component expresses the partial derivative of f with respect to one of the variables. We’ll use the inverted Greek delta symbol ∇ to refer to the gradient, and\nrepresent ŷ as f (x; θ ) to make the dependence on θ more obvious:\n ∂\n\n∂ w1 L( f (x; θ ), y)\n ∂ L( f (x; θ ), y)\n ∂ w2\n\n\n\n..\n\n∇L( f (x; θ ), y) = \n(4.27)\n\n\n.\n ∂\n\n ∂ w L( f (x; θ ), y)\nn\n∂\n∂ b L( f (x; θ ), y)\nwt+1 = wt − η\n\nThe final equation for updating θ based on the gradient is thus\nθ t+1 = θ t − η∇L( f (x; θ ), y)\n\n(4.28)\n\nCost(w,b)\n\nw\n\nb\n\nFigure 4.5 Visualization of the gradient vector at the red point in two dimensions w and\nb, showing a red arrow in the x-y plane pointing in the direction we will go to look for the\nminimum: the opposite direction of the gradient (recall that the gradient points in the direction\nof increase not decrease).\n\n\f4.7\n\n4.7.1\n\n•\n\nG RADIENT D ESCENT\n\n17\n\nThe Gradient for Logistic Regression\n\nIn order to update θ , we need a definition for the gradient ∇L( f (x; θ ), y). Recall that\nfor logistic regression, the cross-entropy loss function is:\nLCE (ŷ, y) = − [y log σ (w · x + b) + (1 − y) log (1 − σ (w · x + b))]\n\n(4.29)\n\nIt turns out that the derivative of this function for one observation vector x is Eq. 4.30\n(the interested reader can see Section 4.15 for the derivation of this equation):\n∂ LCE (ŷ, y)\n= [σ (w · x + b) − y]x j\n∂wj\n= (ŷ − y)x j\n\n(4.30)\n\nYou’ll also sometimes see this equation in the equivalent form:\n∂ LCE (ŷ, y)\n= −(y − ŷ)x j\n∂wj\n\n(4.31)\n\nNote in these equations that the gradient with respect to a single weight w j represents a very intuitive value: the difference between the true y and our estimated\nŷ = σ (w · x + b) for that observation, multiplied by the corresponding input value\nx j.\n\n4.7.2\n\nhyperparameter\n\nThe Stochastic Gradient Descent Algorithm\n\nStochastic gradient descent is an online algorithm that minimizes the loss function\nby computing its gradient after each training example, and nudging θ in the right\ndirection (the opposite direction of the gradient). (An “online algorithm” is one that\nprocesses its input example by example, rather than waiting until it sees the entire\ninput.) Stochastic gradient descent is called stochastic because it chooses a single\nrandom example at a time; in Section 4.7.4 we’ll discuss other versions of gradient\ndescent that batch many examples at once. Fig. 4.6 shows the algorithm.\nThe learning rate η is a hyperparameter that must be adjusted. If it’s too high,\nthe learner will take steps that are too large, overshooting the minimum of the loss\nfunction. If it’s too low, the learner will take steps that are too small, and take too\nlong to get to the minimum. It is common to start with a higher learning rate and then\nslowly decrease it, so that it is a function of the iteration k of training; the notation\nηk can be used to mean the value of the learning rate at iteration k.\nWe’ll discuss hyperparameters in more detail in Chapter 6, but in short, they are\na special kind of parameter for any machine learning model. Unlike regular parameters of a model (weights like w and b), which are learned by the algorithm from\nthe training set, hyperparameters are special parameters chosen by the algorithm\ndesigner that affect how the algorithm works.\n\n4.7.3\n\nWorking through an example\n\nLet’s walk through a single step of the gradient descent algorithm. We’ll use a\nsimplified version of the example in Fig. 4.2 as it sees a single observation x, whose\ncorrect value is y = 1 (this is a positive review), and with a feature vector x = [x1 , x2 ]\nconsisting of these two features:\nx1 = 3\n\n(count of positive lexicon words)\n\nx2 = 2\n\n(count of negative lexicon words)\n\n\f18\n\nC HAPTER 4\n\n•\n\nL OGISTIC R EGRESSION\n\nfunction S TOCHASTIC G RADIENT D ESCENT(L(), f (), x, y) returns θ\n# where: L is the loss function\n#\nf is a function parameterized by θ\n#\nx is the set of training inputs x(1) , x(2) , ..., x(m)\n#\ny is the set of training outputs (labels) y(1) , y(2) , ..., y(m)\nθ ←0\n# (or small random values)\nrepeat til done # see caption\nFor each training tuple (x(i) , y(i) ) (in random order)\n1. Optional (for reporting):\n# How are we doing on this tuple?\nCompute ŷ (i) = f (x(i) ; θ ) # What is our estimated output ŷ?\nCompute the loss L(ŷ (i) , y(i) ) # How far off is ŷ(i) from the true output y(i) ?\n2. g ← ∇θ L( f (x(i) ; θ ), y(i) )\n# How should we move θ to maximize loss?\n3. θ ← θ − η g\n# Go the other way instead\nreturn θ\nFigure 4.6 The stochastic gradient descent algorithm. Step 1 (computing the loss) is used\nmainly to report how well we are doing on the current tuple; we don’t need to compute the\nloss in order to compute the gradient. The algorithm can terminate when it converges (when\nthe gradient norm < \u000f), or when progress halts (for example when the loss starts going up on\na held-out set). Weights are initialized to 0 for logistic regression, but to small random values\nfor neural networks, as we’ll see in Chapter 6.\n\nLet’s assume the initial weights and bias in θ 0 are all set to 0, and the initial learning\nrate η is 0.1:\nw1 = w2 = b = 0\nη = 0.1\nThe single update step requires that we compute the gradient, multiplied by the\nlearning rate\nθ t+1 = θ t − η∇θ L( f (x(i) ; θ ), y(i) )\nIn our mini example there are three parameters, so the gradient vector has 3 dimensions, for w1 , w2 , and b. We can compute the first gradient as follows:\n ∂ LCE (ŷ,y)  \n \n \n \n\n(σ (w · x + b) − y)x1\n(σ (0) − 1)x1\n−0.5x1\n−1.5\n∂ w1\n\n(ŷ,y) \n∇w,b L =  ∂ LCE\n =  (σ (w · x + b) − y)x2  =  (σ (0) − 1)x2  =  −0.5x2  =  −1.0 \n∂ w2\n∂ LCE (ŷ,y)\nσ (0) − 1\n−0.5\nσ (w · x + b) − y\n−0.5\n∂b\n\nNow that we have a gradient, we compute the new parameter vector θ 1 by moving\nθ 0 in the opposite direction from the gradient:\n\n\n \n\n\nw1\n−1.5\n.15\nθ 1 =  w2  − η  −1.0  =  .1 \nb\n−0.5\n.05\nSo after one step of gradient descent, the weights have shifted to be: w1 = .15,\nw2 = .1, and b = .05.\nNote that this observation x happened to be a positive example. We would expect\nthat after seeing more negative examples with high counts of negative words, that\nthe weight w2 would shift to have a negative value.\n\n\f4.7\n\n4.7.4\n\nbatch training\n\nmini-batch\n\n•\n\n19\n\nG RADIENT D ESCENT\n\nMini-batch training\n\nStochastic gradient descent is called stochastic because it chooses a single random\nexample at a time, moving the weights so as to improve performance on that single\nexample. That can result in very choppy movements, so it’s common to compute the\ngradient over batches of training instances rather than a single instance.\nFor example in batch training we compute the gradient over the entire dataset.\nBy seeing so many examples, batch training offers a superb estimate of which direction to move the weights, at the cost of spending a lot of time processing every\nsingle example in the training set to compute this perfect direction.\nA compromise is mini-batch training: we train on a group of m examples (perhaps 512, or 1024) that is less than the whole dataset. (If m is the size of the dataset,\nthen we are doing batch gradient descent; if m = 1, we are back to doing stochastic gradient descent.) Mini-batch training also has the advantage of computational\nefficiency. The mini-batches can easily be vectorized, choosing the size of the minibatch based on the computational resources. This allows us to process all the examples in one mini-batch in parallel and then accumulate the loss, something that’s not\npossible with individual or batch training.\nWe just need to define mini-batch versions of the cross-entropy loss function\nwe defined in Section 4.6 and the gradient in Section 4.7.1. Let’s extend the crossentropy loss for one example from Eq. 4.23 to mini-batches of size m. We’ll continue\nto use the notation that x(i) and y(i) mean the ith training features and training label,\nrespectively. We make the assumption that the training examples are independent:\nlog p(training labels) = log\n\nm\nY\n\np(y(i) |x(i) )\n\ni=1\n\n=\n\nm\nX\n\nlog p(y(i) |x(i) )\n\ni=1\n\n= −\n\nm\nX\n\nLCE (ŷ(i) , y(i) )\n\n(4.32)\n\ni=1\n\nNow the cost function for the mini-batch of m examples is the average loss for each\nexample:\nm\n\nCost(ŷ, y) =\n\n1X\nLCE (ŷ(i) , y(i) )\nm\n\n= −\n\ni=1\nm\nX\n\n1\nm\n\n\u0010\n\u0011\ny(i) log σ (w · x(i) + b) + (1 − y(i) ) log 1 − σ (w · x(i) + b) (4.33)\n\ni=1\n\nThe mini-batch gradient is the average of the individual gradients from Eq. 4.30:\nm\ni\n∂Cost(ŷ, y)\n1 Xh\n(i)\n=\nσ (w · x(i) + b) − y(i) x j\n∂wj\nm\n\n(4.34)\n\ni=1\n\nInstead of using the sum notation, we can more efficiently compute the gradient\nin its matrix form, following the vectorization we saw on page 9, where we have a\nmatrix X of size [m × f ] representing the m inputs in the batch, and a vector y of size\n[m × 1] representing the correct outputs:\n\n\f20\n\nC HAPTER 4\n\n•\n\nL OGISTIC R EGRESSION\n\n1\n∂Cost(ŷ, y)\n=\n(ŷ − y)| X\n∂w\nm\n1\n=\n(σ (Xw + b) − y)| X\nm\n\n4.8\n\n(4.35)\n\nLearning in Multinomial Logistic Regression\nThe loss function for multinomial logistic regression generalizes the loss function\nfor binary logistic regression from 2 to K classes. Recall that that the cross-entropy\nloss for binary logistic regression (repeated from Eq. 4.23) is:\nLCE (ŷ, y) = − log p(y|x) = − [y log ŷ + (1 − y) log(1 − ŷ)]\n\n(4.36)\n\nThe loss function for multinomial logistic regression generalizes the two terms in\nEq. 4.36 (one that is non-zero when y = 1 and one that is non-zero when y = 0) to\nK terms. As we mentioned above, for multinomial regression we’ll represent both y\nand ŷ as vectors. The true label y is a vector with K elements, each corresponding\nto a class, with yc = 1 if the correct class is c, with all other elements of y being 0.\nAnd our classifier will produce an estimate vector with K elements ŷ, each element\nŷk of which represents the estimated probability p(yk = 1|x).\nThe loss function for a single example x, generalizing from binary logistic regression, is the sum of the logs of the K output classes, each weighted by the indicator function yk (Eq. 4.37). This turns out to be just the negative log probability of\nthe correct class c (Eq. 4.38):\nLCE (ŷ, y) = −\n\nK\nX\n\nyk log ŷk\n\n(4.37)\n\nk=1\n\n= − log ŷc ,\n\n(where c is the correct class)\n\n= − log p̂(yc = 1|x)\n\n(where c is the correct class)\n\nexp (wc · x + bc )\n= − log PK\nj=1 exp (wj · x + b j )\n\nnegative log\nlikelihood loss\n\n(4.38)\n\n(c is the correct class)\n\n(4.39)\n\nHow did we get from Eq. 4.37 to Eq. 4.38? Because only one class (let’s call it c) is\nthe correct one, the vector y takes the value 1 only for this value of k, i.e., has yc = 1\nand y j = 0 ∀ j 6= c. That means the terms in the sum in Eq. 4.37 will all be 0 except\nfor the term corresponding to the true class c. Hence the cross-entropy loss is simply\nthe log of the output probability corresponding to the correct class, and we therefore\nalso call Eq. 4.38 the negative log likelihood loss.\nOf course for gradient descent we don’t need the loss, we need its gradient. The\ngradient for a single example turns out to be very similar to the gradient for binary\nlogistic regression, (ŷ − y)x, that we saw in Eq. 4.30. Let’s consider one piece of the\ngradient, the derivative for a single weight. For each class k, the weight of the ith\nelement of input x is wk,i . What is the partial derivative of the loss with respect to\nwk,i ? This derivative turns out to be just the difference between the true value for the\nclass k (which is either 1 or 0) and the probability the classifier outputs for class k,\nweighted by the value of the input xi corresponding to the ith element of the weight\n\n\f4.9\n\n•\n\nE VALUATION : P RECISION , R ECALL , F- MEASURE\n\n21\n\nvector for class k:\n∂ LCE\n= −(yk − ŷk )xi\n∂ wk,i\n= −(yk − p(yk = 1|x))xi\nexp (wk · x + bk )\n\n= − yk − PK\n\nj=1 exp (wj · x + b j )\n\n!\nxi\n\n(4.40)\n\nWe’ll return to this case of the gradient for softmax regression when we introduce\nneural networks in Chapter 6, and at that time we’ll also discuss the derivation of\nthis gradient in equations Eq. ??–Eq. ??.\n\n4.9\n\nEvaluation: Precision, Recall, F-measure\n\ngold labels\n\nconfusion\nmatrix\n\nTo introduce the methods for evaluating text classification, let’s first consider some\nsimple binary detection tasks. For example, in spam detection, our goal is to label\nevery text as being in the spam category (“positive”) or not in the spam category\n(“negative”). For each item (email document) we therefore need to know whether\nour system called it spam or not. We also need to know whether the email is actually\nspam or not, i.e. the human-defined labels for each document that we are trying to\nmatch. We will refer to these human labels as the gold labels.\nOr imagine you’re the CEO of the Delicious Pie Company and you need to know\nwhat people are saying about your pies on social media, so you build a system that\ndetects tweets concerning Delicious Pie. Here the positive class is tweets about\nDelicious Pie and the negative class is all other tweets.\nIn both cases, we need a metric for knowing how well our spam detector (or\npie-tweet-detector) is doing. To evaluate any system for detecting things, we start\nby building a confusion matrix like the one shown in Fig. 4.7. A confusion matrix\nis a table for visualizing how an algorithm performs with respect to the human gold\nlabels, using two dimensions (system output and gold labels), and each cell labeling\na set of possible outcomes. In the spam detection case, for example, true positives\nare documents that are indeed spam (indicated by human-created gold labels) that\nour system correctly said were spam. False negatives are documents that are indeed\nspam but our system incorrectly labeled as non-spam.\nTo the bottom right of the table is the equation for accuracy, which asks what\npercentage of all the observations (for the spam or pie examples that means all emails\nor tweets) our system labeled correctly. Although accuracy might seem a natural\nmetric, we generally don’t use it for text classification tasks. That’s because accuracy\ndoesn’t work well when the classes are unbalanced (as indeed they are with spam,\nwhich is a large majority of email, or with tweets, which are mainly not about pie).\nTo make this more explicit, imagine that we looked at a million tweets, and\nlet’s say that only 100 of them are discussing their love (or hatred) for our pie,\nwhile the other 999,900 are tweets about something completely unrelated. Imagine a\nsimple classifier that stupidly classified every tweet as “not about pie”. This classifier\nwould have 999,900 true negatives and only 100 false negatives for an accuracy of\n999,900/1,000,000 or 99.99%! What an amazing accuracy level! Surely we should\nbe happy with this classifier? But of course this fabulous ‘no pie’ classifier would\nbe completely useless, since it wouldn’t find a single one of the customer comments\nwe are looking for. In other words, accuracy is not a good metric when the goal is\n\n\f22\n\nC HAPTER 4\n\n•\n\nL OGISTIC R EGRESSION\ngold standard labels\ngold positive\n\nsystem\noutput\nlabels\n\ngold negative\n\ntp\nsystem\npositive true positive false positive precision = tp+fp\nsystem\nnegative false negative true negative\nrecall =\n\ntp\ntp+fn\n\naccuracy =\n\ntp+tn\ntp+fp+tn+fn\n\nFigure 4.7 A confusion matrix for visualizing how well a binary classification system performs against gold standard labels.\n\nprecision\n\nto discover something that is rare, or at least not completely balanced in frequency,\nwhich is a very common situation in the world.\nThat’s why instead of accuracy we generally turn to two other metrics shown in\nFig. 4.7: precision and recall. Precision measures the percentage of the items that\nthe system detected (i.e., the system labeled as positive) that are in fact positive (i.e.,\nare positive according to the human gold labels). Precision is defined as\nPrecision =\n\nrecall\n\nRecall measures the percentage of items actually present in the input that were\ncorrectly identified by the system. Recall is defined as\nRecall =\n\nF-measure\n\ntrue positives\ntrue positives + false positives\n\ntrue positives\ntrue positives + false negatives\n\nPrecision and recall will help solve the problem with the useless “nothing is\npie” classifier. This classifier, despite having a fabulous accuracy of 99.99%, has\na terrible recall of 0 (since there are no true positives, and 100 false negatives, the\nrecall is 0/100). You should convince yourself that the precision at finding relevant\ntweets is equally problematic. Thus precision and recall, unlike accuracy, emphasize\ntrue positives: finding the things that we are supposed to be looking for.\nThere are many ways to define a single metric that incorporates aspects of both\nprecision and recall. The simplest of these combinations is the F-measure (van\nRijsbergen, 1975) , defined as:\nFβ =\n\nF1\n\n(β 2 + 1)PR\nβ 2P + R\n\nThe β parameter differentially weights the importance of recall and precision,\nbased perhaps on the needs of an application. Values of β > 1 favor recall, while\nvalues of β < 1 favor precision. When β = 1, precision and recall are equally balanced; this is the most frequently used metric, and is called Fβ =1 or just F1 :\n2PR\n(4.41)\nP+R\nF-measure comes from a weighted harmonic mean of precision and recall. The\nharmonic mean of a set of numbers is the reciprocal of the arithmetic mean of recipF1 =\n\n\f4.9\n\n•\n\nE VALUATION : P RECISION , R ECALL , F- MEASURE\n\n23\n\nrocals:\nn\n\nHarmonicMean(a1 , a2 , a3 , a4 , ..., an ) = 1\n\n1\n1\n1\na1 + a2 + a3 + ... + an\n\n(4.42)\n\nand hence F-measure is\n1\nF= 1\nα P + (1 − α) R1\n\n\u0012\n\n1−α\nor with β =\nα\n2\n\n\u0013\nF=\n\n(β 2 + 1)PR\nβ 2P + R\n\n(4.43)\n\nHarmonic mean is used because the harmonic mean of two values is closer to the\nminimum of the two values than the arithmetic mean is. Thus it weighs the lower of\nthe two numbers more heavily, which is more conservative in this situation.\n\n4.9.1\n\nEvaluating with more than two classes\n\nUp to now we have been describing text classification tasks with only two classes.\nBut lots of classification tasks in language processing have more than two classes.\nFor sentiment analysis we generally have 3 classes (positive, negative, neutral) and\neven more classes are common for tasks like part-of-speech tagging, word sense\ndisambiguation, semantic role labeling, emotion detection, and so on. Luckily the\nnaive Bayes algorithm is already a multi-class classification algorithm.\n\nurgent\n\ngold labels\nnormal\nspam\n\nsystem\noutput normal\n\n8\n5\n\n10\n60\n\n1\n50\n\nspam\n\n3\n\n30\n\n200\n\nurgent\n\n8\n8+10+1\n60\nprecisionn=\n5+60+50\n200\nprecisions=\n3+30+200\n\nprecisionu=\n\nrecallu = recalln = recalls =\n8\n\n60\n\n200\n\n8+5+3 10+60+30 1+50+200\n\nFigure 4.8 Confusion matrix for a three-class categorization task, showing for each pair of\nclasses (c1 , c2 ), how many documents from c1 were (in)correctly assigned to c2 .\n\nmacroaveraging\nmicroaveraging\n\nBut we’ll need to slightly modify our definitions of precision and recall. Consider the sample confusion matrix for a hypothetical 3-way one-of email categorization decision (urgent, normal, spam) shown in Fig. 4.8. The matrix shows, for\nexample, that the system mistakenly labeled one spam document as urgent, and we\nhave shown how to compute a distinct precision and recall value for each class. In\norder to derive a single metric that tells us how well the system is doing, we can combine these values in two ways. In macroaveraging, we compute the performance\nfor each class, and then average over classes. In microaveraging, we collect the decisions for all classes into a single confusion matrix, and then compute precision and\nrecall from that table. Fig. 4.9 shows the confusion matrix for each class separately,\nand shows the computation of microaveraged and macroaveraged precision.\nAs the figure shows, a microaverage is dominated by the more frequent class (in\nthis case spam), since the counts are pooled. The macroaverage better reflects the\nstatistics of the smaller classes, and so is more appropriate when performance on all\nthe classes is equally important.\n\n\f24\n\nC HAPTER 4\n\n•\n\nL OGISTIC R EGRESSION\n\nClass 1: Urgent\n\nClass 2: Normal\n\ntrue true\nurgent not\n\ntrue true\nnormal not\n\ntrue\nspam\n\ntrue\nnot\n\ntrue\nyes\n\nsystem\nspam 200\nsystem\n51\nnot\n\n33\n83\n\nsystem\nyes 268 99\nsystem\n99 635\nno\n\nsystem\nurgent\nsystem\nnot\n\n8 11\n8 340\n\nsystem\nnormal\nsystem\nnot\n\n60 55\n40 212\n\nprecision =\n\n8\n= .42\n8+11\n\nprecision =\n\n60\n= .52\n60+55\n\nClass 3: Spam\n\nprecision =\n\n200\n= .86\n200+33\n\nPooled\ntrue\nno\n\nmicroaverage = 268\n= .73\nprecision\n268+99\n\nmacroaverage = .42+.52+.86\n= .60\nprecision\n3\n\nFigure 4.9 Separate confusion matrices for the 3 classes from the previous figure, showing the pooled confusion matrix and the microaveraged and macroaveraged precision.\n\n4.10\n\nTest sets and Cross-validation\n\ndevelopment\ntest set\ndevset\n\ncross-validation\nfolds\n\n10-fold\ncross-validation\n\nThe training and testing procedure for text classification follows what we saw with\nlanguage modeling (Section ??): we use the training set to train the model, then use\nthe development test set (also called a devset) to perhaps tune some parameters,\nand in general decide what the best model is. Once we come up with what we think\nis the best model, we run it on the (hitherto unseen) test set to report its performance.\nWhile the use of a devset avoids overfitting the test set, having a fixed training set, devset, and test set creates another problem: in order to save lots of data\nfor training, the test set (or devset) might not be large enough to be representative.\nWouldn’t it be better if we could somehow use all our data for training and still use\nall our data for test? We can do this by cross-validation.\nIn cross-validation, we choose a number k, and partition our data into k disjoint\nsubsets called folds. Now we choose one of those k folds as a test set, train our\nclassifier on the remaining k − 1 folds, and then compute the error rate on the test\nset. Then we repeat with another fold as the test set, again training on the other k − 1\nfolds. We do this sampling process k times and average the test set error rate from\nthese k runs to get an average error rate. If we choose k = 10, we would train 10\ndifferent models (each on 90% of our data), test the model 10 times, and average\nthese 10 values. This is called 10-fold cross-validation.\nThe only problem with cross-validation is that because all the data is used for\ntesting, we need the whole corpus to be blind; we can’t examine any of the data\nto suggest possible features and in general see what’s going on, because we’d be\npeeking at the test set, and such cheating would cause us to overestimate the performance of our system. However, looking at the corpus to understand what’s going\non is important in designing NLP systems! What to do? For this reason, it is common to create a fixed training set and test set, then do 10-fold cross-validation inside\nthe training set, but compute error rate the normal way in the test set, as shown in\nFig. 4.10.\n\n\f4.11\n\n•\n\nS TATISTICAL S IGNIFICANCE T ESTING\n\nTraining Iterations\n1\n2\n3\n\nTesting\nTraining\n\nDev\n\nTraining\n\nDev\n\n6\n7\n8\n9\n10\n\nFigure 4.10\n\n4.11\n\nTraining\n\nDev\n\n4\n5\n\n25\n\nTraining\n\nDev\nTraining\nTraining\n\nTest\nSet\n\nTraining\n\nDev\nDev\n\nTraining\n\nDev\n\nTraining\n\nDev\n\nTraining\n\nDev\n\nTraining\n\nDev\n\n10-fold cross-validation\n\nStatistical Significance Testing\nIn building systems we often need to compare the performance of two systems. How\ncan we know if the new system we just built is better than our old one? Or better\nthan some other system described in the literature? This is the domain of statistical\nhypothesis testing, and in this section we introduce tests for statistical significance\nfor NLP classifiers, drawing especially on the work of Dror et al. (2020) and BergKirkpatrick et al. (2012).\nSuppose we’re comparing the performance of classifiers A and B on a metric M\nsuch as F1 , or accuracy. Perhaps we want to know if our new sentiment classifier\nA gets a higher F1 score than our previous sentiment classifier B on a particular test\nset x. Let’s call M(A, x) the score that system A gets on test set x, and δ (x) the\nperformance difference between A and B on x:\nδ (x) = M(A, x) − M(B, x)\n\neffect size\n\n(4.44)\n\nWe would like to know if δ (x) > 0, meaning that our logistic regression classifier\nhas a higher F1 than our naive Bayes classifier on x. δ (x) is called the effect size; a\nbigger δ means that A seems to be way better than B; a small δ means A seems to\nbe only a little better.\nWhy don’t we just check if δ (x) is positive? Suppose we do, and we find that\nthe F1 score of A is higher than B’s by .04. Can we be certain that A is better? We\ncannot! That’s because A might just be accidentally better than B on this particular x.\nWe need something more: we want to know if A’s superiority over B is likely to hold\nagain if we checked another test set x0 , or under some other set of circumstances.\nIn the paradigm of statistical hypothesis testing, we test this by formalizing two\nhypotheses.\nH0 : δ (x) ≤ 0\nH1 : δ (x) > 0\n\nnull hypothesis\n\n(4.45)\n\nThe hypothesis H0 , called the null hypothesis, supposes that δ (x) is actually negative or zero, meaning that A is not better than B. We would like to know if we can\nconfidently rule out this hypothesis, and instead support H1 , that A is better.\nWe do this by creating a random variable X ranging over all test sets. Now we\nask how likely is it, if the null hypothesis H0 was correct, that among these test sets\n\n\f26\n\nC HAPTER 4\n\np-value\n\n•\n\nL OGISTIC R EGRESSION\n\nwe would encounter the value of δ (x) that we found, if we repeated the experiment\na great many times. We formalize this likelihood as the p-value: the probability,\nassuming the null hypothesis H0 is true, of seeing the δ (x) that we saw or one even\ngreater\nP(δ (X) ≥ δ (x)|H0 is true)\n\nstatistically\nsignificant\n\napproximate\nrandomization\npaired\n\nSo in our example, this p-value is the probability that we would see δ (x) assuming\nA is not better than B. If δ (x) is huge (let’s say A has a very respectable F1 of .9\nand B has a terrible F1 of only .2 on x), we might be surprised, since that would be\nextremely unlikely to occur if H0 were in fact true, and so the p-value would be low\n(unlikely to have such a large δ if A is in fact not better than B). But if δ (x) is very\nsmall, it might be less surprising to us even if H0 were true and A is not really better\nthan B, and so the p-value would be higher.\nA very small p-value means that the difference we observed is very unlikely\nunder the null hypothesis, and we can reject the null hypothesis. What counts as very\nsmall? It is common to use values like .05 or .01 as the thresholds. A value of .01\nmeans that if the p-value (the probability of observing the δ we saw assuming H0 is\ntrue) is less than .01, we reject the null hypothesis and assume that A is indeed better\nthan B. We say that a result (e.g., “A is better than B”) is statistically significant if\nthe δ we saw has a probability that is below the threshold and we therefore reject\nthis null hypothesis.\nHow do we compute this probability we need for the p-value? In NLP we generally don’t use simple parametric tests like t-tests or ANOVAs that you might be\nfamiliar with. Parametric tests make assumptions about the distributions of the test\nstatistic (such as normality) that don’t generally hold in our cases. So in NLP we\nusually use non-parametric tests based on sampling: we artificially create many versions of the experimental setup. For example, if we had lots of different test sets x0\nwe could just measure all the δ (x0 ) for all the x0 . That gives us a distribution. Now\nwe set a threshold (like .01) and if we see in this distribution that 99% or more of\nthose deltas are smaller than the delta we observed, i.e., that p-value(x)—the probability of seeing a δ (x) as big as the one we saw—is less than .01, then we can reject\nthe null hypothesis and agree that δ (x) was a sufficiently surprising difference and\nA is really a better algorithm than B.\nThere are two common non-parametric tests used in NLP: approximate randomization (Noreen, 1989) and the bootstrap test. We will describe bootstrap\nbelow, showing the paired version of the test, which again is most common in NLP.\nPaired tests are those in which we compare two sets of observations that are aligned:\neach observation in one set can be paired with an observation in another. This happens naturally when we are comparing the performance of two systems on the same\ntest set; we can pair the performance of system A on an individual observation xi\nwith the performance of system B on the same xi .\n\n4.11.1\nbootstrap test\nbootstrapping\n\n(4.46)\n\nThe Paired Bootstrap Test\n\nThe bootstrap test (Efron and Tibshirani, 1993) can apply to any metric; from precision, recall, or F1 to the BLEU metric used in machine translation. The word\nbootstrapping refers to repeatedly drawing large numbers of samples with replacement (called bootstrap samples) from an original set. The intuition of the bootstrap\ntest is that we can create many virtual test sets from an observed test set by repeatedly sampling from it. The method only makes the assumption that the sample is\nrepresentative of the population.\n\n\f4.11\n\n•\n\nS TATISTICAL S IGNIFICANCE T ESTING\n\n27\n\nConsider a tiny text classification example with a test set x of 10 documents. The\nfirst row of Fig. 4.11 shows the results of two classifiers (A and B) on this test set.\nEach document is labeled by one of the four possibilities (A and B both right, both\nwrong, A right and B wrong, A wrong and B right). A slash through a letter (\u0013\nB)\nmeans that that classifier got the answer wrong. On the first document both A and\nB get the correct class (AB), while on the second document A got it right but B got\nit wrong (A\u0013\nB). If we assume for simplicity that our metric is accuracy, A has an\naccuracy of .70 and B of .50, so δ (x) is .20.\nNow we create a large number b (perhaps 105 ) of virtual test sets x(i) , each of size\nn = 10. Fig. 4.11 shows a couple of examples. To create each virtual test set x(i) , we\nrepeatedly (n = 10 times) select a cell from row x with replacement. For example, to\ncreate the first cell of the first virtual test set x(1) , if we happened to randomly select\nthe second cell of the x row, we would copy the value A\u0013\nB into our new cell, and\nmove on to create the second cell of x(1) , each time sampling (randomly choosing)\nfrom the original x with replacement.\n\nx\nx(1)\nx(2)\n...\nx(b)\n\n1\n2 3 4 5 6 7 8 9 10 A% B% δ ()\nAB AB\n\u0013 AB AB AB\n\u0013 AB AB\n\u0013 AB AB\n\u0013 AB\n\u0013 .70 .50 .20\nAB\n\u0013 AB AB\n\u0013 AB AB AB\n\u0013 AB AB AB\n\u0013 AB .60 .60 .00\nAB\n\u0013 AB AB\n\u0013 AB AB AB AB AB\n\u0013 AB AB .60 .70 -.10\n\nFigure 4.11 The paired bootstrap test: Examples of b pseudo test sets x(i) being created\nfrom an initial true test set x. Each pseudo test set is created by sampling n = 10 times with\nreplacement; thus an individual sample is a single cell, a document with its gold label and\nthe correct or incorrect performance of classifiers A and B. Of course real test sets don’t have\nonly 10 examples, and b needs to be large as well.\n\nNow that we have the b test sets, providing a sampling distribution, we can do\nstatistics on how often A has an accidental advantage. There are various ways to\ncompute this advantage; here we follow the version laid out in Berg-Kirkpatrick\net al. (2012). Assuming H0 (A isn’t better than B), we would expect that δ (X),\nestimated over many test sets, would be zero or negative; a much higher value would\nbe surprising, since H0 specifically assumes A isn’t better than B. To measure exactly\nhow surprising our observed δ (x) is, we would in other circumstances compute the\np-value by counting over many test sets how often δ (x(i) ) exceeds the expected zero\nvalue by δ (x) or more:\n\np-value(x) =\n\nb\n\u0011\n1 X \u0010 (i)\n1 δ (x ) − δ (x) ≥ 0\nb\ni=1\n\n(We use the notation 1(x) to mean “1 if x is true, and 0 otherwise”.) However,\nalthough it’s generally true that the expected value of δ (X) over many test sets,\n(again assuming A isn’t better than B) is 0, this isn’t true for the bootstrapped test\nsets we created. That’s because we didn’t draw these samples from a distribution\nwith 0 mean; we happened to create them from the original test set x, which happens\nto be biased (by .20) in favor of A. So to measure how surprising is our observed\nδ (x), we actually compute the p-value by counting over many test sets how often\n\n\f28\n\nC HAPTER 4\n\n•\n\nL OGISTIC R EGRESSION\n\nδ (x(i) ) exceeds the expected value of δ (x) by δ (x) or more:\np-value(x) =\n\nb\n\u0011\n1 X \u0010 (i)\n1 δ (x ) − δ (x) ≥ δ (x)\nb\ni=1\n\n=\n\nb\n1X \u0010\n\nb\n\n\u0011\n\n1 δ (x(i) ) ≥ 2δ (x)\n\n(4.47)\n\ni=1\n\nSo if for example we have 10,000 test sets x(i) and a threshold of .01, and in only 47\nof the test sets do we find that A is accidentally better δ (x(i) ) ≥ 2δ (x), the resulting\np-value of .0047 is smaller than .01, indicating that the delta we found, δ (x) is indeed\nsufficiently surprising and unlikely to have happened by accident, and we can reject\nthe null hypothesis and conclude A is better than B.\n\nfunction B OOTSTRAP(test set x, num of samples b) returns p-value(x)\nCalculate δ (x) # how much better does algorithm A do than B on x\ns=0\nfor i = 1 to b do\nfor j = 1 to n do # Draw a bootstrap sample x(i) of size n\nSelect a member of x at random and add it to x(i)\nCalculate δ (x(i) ) # how much better does algorithm A do than B on x(i)\ns ← s + 1 if δ (x(i) ) ≥ 2δ (x)\np-value(x) ≈ bs # on what % of the b samples did algorithm A beat expectations?\nreturn p-value(x) # if very few did, our observed δ is probably not accidental\nFigure 4.12\n(2012).\n\nA version of the paired bootstrap algorithm after Berg-Kirkpatrick et al.\n\nThe full algorithm for the bootstrap is shown in Fig. 4.12. It is given a test set\nx, a number of samples b, and counts the percentage of the b bootstrap test sets in\nwhich δ (x(i) ) > 2δ (x). This percentage then acts as a one-sided empirical p-value.\n\n4.12\n\nAvoiding Harms in Classification\n\nrepresentational\nharms\n\nIt is important to avoid harms that may result from classifiers, harms that exist both\nfor naive Bayes classifiers and for the other classification algorithms we introduce\nin later chapters.\nOne class of harms is representational harms (Crawford 2017, Blodgett et al.\n2020), harms caused by a system that demeans a social group, for example by perpetuating negative stereotypes about them. For example Kiritchenko and Mohammad (2018) examined the performance of 200 sentiment analysis systems on pairs of\nsentences that were identical except for containing either a common African American first name (like Shaniqua) or a common European American first name (like\nStephanie), chosen from the Caliskan et al. (2017) study discussed in Chapter 5.\nThey found that most systems assigned lower sentiment and more negative emotion\nto sentences with African American names, reflecting and perpetuating stereotypes\nthat associate African Americans with negative emotions (Popp et al., 2003).\n\n\f4.13\n\ntoxicity\ndetection\n\nmodel card\n\n•\n\nI NTERPRETING MODELS\n\n29\n\nIn other tasks classifiers may lead to both representational harms and other\nharms, such as silencing. For example the important text classification task of toxicity detection is the task of detecting hate speech, abuse, harassment, or other\nkinds of toxic language. While the goal of such classifiers is to help reduce societal harm, toxicity classifiers can themselves cause harms. For example, researchers\nhave shown that some widely used toxicity classifiers incorrectly flag as being toxic\nsentences that are non-toxic but simply mention identities like women (Park et al.,\n2018), blind people (Hutchinson et al., 2020) or gay people (Dixon et al., 2018;\nDias Oliva et al., 2021), or simply use linguistic features characteristic of varieties\nlike African-American Vernacular English (Sap et al. 2019, Davidson et al. 2019).\nSuch false positive errors could lead to the silencing of discourse by or about these\ngroups.\nThese model problems can be caused by biases or other problems in the training\ndata; in general, machine learning systems replicate and even amplify the biases\nin their training data. But these problems can also be caused by the labels (for\nexample due to biases in the human labelers), by the resources used (like lexicons,\nor model components like pretrained embeddings), or even by model architecture\n(like what the model is trained to optimize). While the mitigation of these biases\n(for example by carefully considering the training data sources) is an important area\nof research, we currently don’t have general solutions. For this reason it’s important,\nwhen introducing any NLP model, to study these kinds of factors and make them\nclear. One way to do this is by releasing a model card (Mitchell et al., 2019) for\neach version of a model. A model card documents a machine learning model with\ninformation like:\n• training algorithms and parameters\n• training data sources, motivation, and preprocessing\n• evaluation data sources, motivation, and preprocessing\n• intended use and users\n• model performance across different demographic or other groups and environmental situations\n\n4.13\n\nInterpreting models\n\ninterpretable\n\nOften we want to know more than just the correct classification of an observation.\nWe want to know why the classifier made the decision it did. That is, we want our\ndecision to be interpretable. Interpretability can be hard to define strictly, but the\ncore idea is that as humans we should know why our algorithms reach the conclusions they do. Because the features to logistic regression are often human-designed,\none way to understand a classifier’s decision is to understand the role each feature\nplays in the decision. Logistic regression can be combined with statistical tests (the\nlikelihood ratio test, or the Wald test); investigating whether a particular feature is\nsignificant by one of these tests, or inspecting its magnitude (how large is the weight\nw associated with the feature?) can help us interpret why the classifier made the\ndecision it makes. This is enormously important for building transparent models.\nFurthermore, in addition to its use as a classifier, logistic regression in NLP and\nmany other fields is widely used as an analytic tool for testing hypotheses about the\neffect of various explanatory variables (features). In text classification, perhaps we\nwant to know if logically negative words (no, not, never) are more likely to be asso-\n\n\f30\n\nC HAPTER 4\n\n•\n\nL OGISTIC R EGRESSION\n\nciated with negative sentiment, or if negative reviews of movies are more likely to\ndiscuss the cinematography. However, in doing so it’s necessary to control for potential confounds: other factors that might influence sentiment (the movie genre, the\nyear it was made, perhaps the length of the review in words). Or we might be studying the relationship between NLP-extracted linguistic features and non-linguistic\noutcomes (hospital readmissions, political outcomes, or product sales), but need to\ncontrol for confounds (the age of the patient, the county of voting, the brand of the\nproduct). In such cases, logistic regression allows us to test whether some feature is\nassociated with some outcome above and beyond the effect of other features.\n\n4.14\n\nAdvanced: Regularization\nNumquam ponenda est pluralitas sine necessitate\n‘Plurality should never be proposed unless needed’\nWilliam of Occam\n\noverfitting\ngeneralize\nregularization\n\nThere is a problem with learning weights that make the model perfectly match the\ntraining data. If a feature is perfectly predictive of the outcome because it happens\nto only occur in one class, it will be assigned a very high weight. The weights for\nfeatures will attempt to perfectly fit details of the training set, in fact too perfectly,\nmodeling noisy factors that just accidentally correlate with the class. This problem is\ncalled overfitting. A good model should be able to generalize well from the training\ndata to the unseen test set, but a model that overfits will have poor generalization.\nTo avoid overfitting, a new regularization term R(θ ) is added to the loss function in Eq. 4.25, resulting in the following loss for a batch of m examples (slightly\nrewritten from Eq. 4.25 to be maximizing log probability rather than minimizing\nloss, and removing the m1 term which doesn’t affect the argmax):\nθ̂ = argmax\nθ\n\nL2\nregularization\n\nm\nX\n\nlog P(y(i) |x(i) ) − αR(θ )\n\n(4.48)\n\ni=1\n\nThe new regularization term R(θ ) is used to penalize large weights. Thus a setting of\nthe weights that matches the training data perfectly— but uses many weights with\nhigh values to do so—will be penalized more than a setting that matches the data\na little less well, but does so using smaller weights. The higher the regularization\nstrength parameter α, the lower the model’s weights will be, reducing its reliance on\nthe training data.\nThere are two common ways to compute this regularization term R(θ ). L2 regularization is a quadratic function of the weight values, named because it uses the\n(square of the) L2 norm of the weight values. The L2 norm, ||θ ||2 , is the same as\nthe Euclidean distance of the vector θ from the origin. If θ consists of n weights,\nthen:\nR(θ ) = ||θ ||22 =\n\nn\nX\nj=1\n\nθ j2\n\n(4.49)\n\n\f4.14\n\n•\n\nA DVANCED : R EGULARIZATION\n\nThe L2 regularized loss function becomes:\n\" m\n#\nn\nX\nX\n(i) (i)\nθ̂ = argmax\nlog P(y |x ; θ ) − α\nθ j2\nθ\nL1\nregularization\n\ni=1\n\n31\n\n(4.50)\n\nj=1\n\nL1 regularization is a linear function of the weight values, named after the L1 norm\n||W ||1 , the sum of the absolute values of the weights, or Manhattan distance (the\nManhattan distance is the distance you’d have to walk between two points in a city\nwith a street grid like New York):\nR(θ ) = ||θ ||1 =\n\nn\nX\n\n|θi |\n\n(4.51)\n\ni=1\n\nThe L1 regularized loss function becomes:\n\" m\n#\nn\nX\nX\n(i) (i)\nθ̂ = argmax\nlog P(y |x ; θ ) − α\n|θ j |\nθ\n\nlasso\nridge\n\ni=1\n\n(4.52)\n\nj=1\n\nThese kinds of regularization come from statistics, where L1 regularization is called\nlasso regression (Tibshirani, 1996) and L2 regularization is called ridge regression,\nand both are commonly used in language processing. L2 regularization is easier to\noptimize because of its simple derivative (the derivative of θ 2 is just 2θ ), while\nL1 regularization is more complex (the derivative of |θ | is non-continuous at zero).\nBut while L2 prefers weight vectors with many small weights, L1 prefers sparse\nsolutions with some larger weights but many more weights set to zero. Thus L1\nregularization leads to much sparser weight vectors, that is, far fewer features.\nBoth L1 and L2 regularization have Bayesian interpretations as constraints on\nthe prior of how weights should look. L1 regularization can be viewed as a Laplace\nprior on the weights. L2 regularization corresponds to assuming that weights are\ndistributed according to a Gaussian distribution with mean µ = 0. In a Gaussian\nor normal distribution, the further away a value is from the mean, the lower its\nprobability (scaled by the variance σ ). By using a Gaussian prior on the weights, we\nare saying that weights prefer to have the value 0. A Gaussian for a weight θ j is\n!\n(θ j − µ j )2\n1\nq\nexp −\n(4.53)\n2σ 2j\n2πσ 2\nj\n\nIf we multiply each weight by a Gaussian prior on the weight, we are thus maximizing the following constraint:\n!\nm\nn\nY\nY\n(θ j − µ j )2\n1\n(i) (i)\nq\nθ̂ = argmax\nP(y |x ) ×\nexp −\n(4.54)\n2σ 2j\nθ\n2πσ 2\ni=1\nj=1\nj\n\nwhich in log space, with µ = 0, and assuming 2σ 2 = 1, corresponds to\nθ̂ = argmax\nθ\n\nm\nX\n\n(i)\n\n(i)\n\nlog P(y |x ) − α\n\ni=1\n\nwhich is in the same form as Eq. 4.50.\n\nn\nX\nj=1\n\nθ j2\n\n(4.55)\n\n\f32\n\nC HAPTER 4\n\n4.15\n\n•\n\nL OGISTIC R EGRESSION\n\nAdvanced: Deriving the Gradient Equation\nIn this section we give the derivation of the gradient of the cross-entropy loss function LCE for logistic regression. Let’s start with some quick calculus refreshers.\nFirst, the derivative of ln(x):\nd\n1\nln(x) =\ndx\nx\n\n(4.56)\n\nSecond, the (very elegant) derivative of the sigmoid:\ndσ (z)\n= σ (z)(1 − σ (z))\ndz\nchain rule\n\n(4.57)\n\nFinally, the chain rule of derivatives. Suppose we are computing the derivative\nof a composite function f (x) = u(v(x)). The derivative of f (x) is the derivative of\nu(x) with respect to v(x) times the derivative of v(x) with respect to x:\ndu dv\ndf\n=\n·\ndx\ndv dx\n\n(4.58)\n\nFirst, we want to know the derivative of the loss function with respect to a single\nweight w j (we’ll need to compute it for each weight, and for the bias):\n∂ LCE\n∂\n=\n− [y log σ (w · x + b) + (1 − y) log (1 − σ (w · x + b))]\n∂wj\n∂wj\n\u0014\n\u0015\n∂\n∂\n= −\ny log σ (w · x + b) +\n(1 − y) log [1 − σ (w · x + b)]\n∂wj\n∂wj\n(4.59)\n\nNext, using the chain rule, and relying on the derivative of log:\n∂ LCE\n∂\n∂\ny\n1−y\n= −\nσ (w · x + b) −\n[1 − σ (w · x + b)]\n∂wj\nσ (w · x + b) ∂ w j\n1 − σ (w · x + b) ∂ w j\n(4.60)\n\nRearranging terms:\n\u0014\n\u0015\ny\n1−y\n∂\n∂ LCE\n= −\n−\nσ (w · x + b)\n∂wj\nσ (w · x + b) 1 − σ (w · x + b) ∂ w j\nAnd now plugging in the derivative of the sigmoid, and using the chain rule one\nmore time, we end up with Eq. 4.61:\n\u0014\n\u0015\n∂ LCE\ny − σ (w · x + b)\n∂ (w · x + b)\n= −\nσ (w · x + b)[1 − σ (w · x + b)]\n∂wj\nσ (w · x + b)[1 − σ (w · x + b)]\n∂wj\n\u0015\n\u0014\ny − σ (w · x + b)\n= −\nσ (w · x + b)[1 − σ (w · x + b)]x j\nσ (w · x + b)[1 − σ (w · x + b)]\n= −[y − σ (w · x + b)]x j\n= [σ (w · x + b) − y]x j\n\n(4.61)\n\n\f4.16\n\n4.16\n\n•\n\nS UMMARY\n\n33\n\nSummary\nThis chapter introduced the logistic regression model of classification.\n• Logistic regression is a supervised machine learning classifier that extracts\nreal-valued features from the input, multiplies each by a weight, sums them,\nand passes the sum through a sigmoid function to generate a probability. A\nthreshold is used to make a decision.\n• Logistic regression can be used with two classes (e.g., positive and negative\nsentiment) or with multiple classes (multinomial logistic regression, for example for n-ary text classification, part-of-speech labeling, etc.).\n• Multinomial logistic regression uses the softmax function to compute probabilities.\n• The weights (vector w and bias b) are learned from a labeled training set via a\nloss function, such as the cross-entropy loss, that must be minimized.\n• Minimizing this loss function is a convex optimization problem, and iterative\nalgorithms like gradient descent are used to find the optimal weights.\n• Regularization is used to avoid overfitting.\n• Logistic regression is also one of the most useful analytic tools, because of its\nability to transparently study the importance of individual features.\n\nHistorical Notes\n\nmaximum\nentropy\n\nLogistic regression was developed in the field of statistics, where it was used for\nthe analysis of binary data by the 1960s, and was particularly common in medicine\n(Cox, 1969). Starting in the late 1970s it became widely used in linguistics as one\nof the formal foundations of the study of linguistic variation (Sankoff and Labov,\n1979).\nNonetheless, logistic regression didn’t become common in natural language processing until the 1990s, when it seems to have appeared simultaneously from two\ndirections. The first source was the neighboring fields of information retrieval and\nspeech processing, both of which had made use of regression, and both of which\nlent many other statistical techniques to NLP. Indeed a very early use of logistic\nregression for document routing was one of the first NLP applications to use (LSI)\nembeddings as word representations (Schütze et al., 1995).\nAt the same time in the early 1990s logistic regression was developed and applied to NLP at IBM Research under the name maximum entropy modeling or\nmaxent (Berger et al., 1996), seemingly independent of the statistical literature. Under that name it was applied to language modeling (Rosenfeld, 1996), part-of-speech\ntagging (Ratnaparkhi, 1996), parsing (Ratnaparkhi, 1997), coreference resolution\n(Kehler, 1997), and text classification (Nigam et al., 1999).\nThere are a variety of sources covering the many kinds of text classification\ntasks. For sentiment analysis see Pang and Lee (2008), and Liu and Zhang (2012).\nStamatatos (2009) surveys authorship attribute algorithms. On language identification see Jauhiainen et al. (2019); Jaech et al. (2016) is an important early neural\nsystem. The task of newswire indexing was often used as a test case for text classification algorithms, based on the Reuters-21578 collection of newswire articles.\nSee Manning et al. (2008) and Aggarwal and Zhai (2012) on text classification;\nclassification in general is covered in machine learning textbooks (Hastie et al. 2001,\n\n\f34\n\nC HAPTER 4\n\ninformation\ngain\n\n•\n\nL OGISTIC R EGRESSION\n\nWitten and Frank 2005, Bishop 2006, Murphy 2012).\nNon-parametric methods for computing statistical significance were used first in\nNLP in the MUC competition (Chinchor et al., 1993), and even earlier in speech\nrecognition (Gillick and Cox 1989, Bisani and Ney 2004). Our description of the\nbootstrap draws on the description in Berg-Kirkpatrick et al. (2012). Recent work\nhas focused on issues including multiple test sets and multiple metrics (Søgaard et al.\n2014, Dror et al. 2017).\nFeature selection is a method of removing features that are unlikely to generalize\nwell. Features are generally ranked by how informative they are about the classification decision. A very common metric, information gain, tells us how many bits of\ninformation the presence of the word gives us for guessing the class. Other feature\nselection metrics include χ 2 , pointwise mutual information, and GINI index; see\nYang and Pedersen (1997) for a comparison and Guyon and Elisseeff (2003) for an\nintroduction to feature selection.\n\nExercises\n\n\fExercises\nAggarwal, C. C. and C. Zhai. 2012. A survey of text classification algorithms. In C. C. Aggarwal and C. Zhai, eds,\nMining text data, 163–222. Springer.\nBerg-Kirkpatrick, T., D. Burkett, and D. Klein. 2012. An\nempirical investigation of statistical significance in NLP.\nEMNLP.\nBerger, A., S. A. Della Pietra, and V. J. Della Pietra. 1996. A\nmaximum entropy approach to natural language processing. Computational Linguistics, 22(1):39–71.\nBisani, M. and H. Ney. 2004. Bootstrap estimates for confidence intervals in ASR performance evaluation. ICASSP.\nBishop, C. M. 2006. Pattern recognition and machine learning. Springer.\nBlodgett, S. L., S. Barocas, H. Daumé III, and H. Wallach.\n2020. Language (technology) is power: A critical survey\nof “bias” in NLP. ACL.\nBorges, J. L. 1964. The analytical language of john wilkins.\nIn Other inquisitions 1937–1952. University of Texas\nPress. Trans. Ruth L. C. Simms.\nCaliskan, A., J. J. Bryson, and A. Narayanan. 2017. Semantics derived automatically from language corpora contain\nhuman-like biases. Science, 356(6334):183–186.\nChinchor, N., L. Hirschman, and D. L. Lewis. 1993. Evaluating Message Understanding systems: An analysis of\nthe third Message Understanding Conference. Computational Linguistics, 19(3):409–449.\nCox, D. 1969. Analysis of Binary Data. Chapman and Hall,\nLondon.\nCrawford, K. 2017. The trouble with bias. Keynote at\nNeurIPS.\nDavidson, T., D. Bhattacharya, and I. Weber. 2019. Racial\nbias in hate speech and abusive language detection\ndatasets. Third Workshop on Abusive Language Online.\nDias Oliva, T., D. Antonialli, and A. Gomes. 2021. Fighting\nhate speech, silencing drag queens? artificial intelligence\nin content moderation and risks to lgbtq voices online.\nSexuality & Culture, 25:700–732.\nDixon, L., J. Li, J. Sorensen, N. Thain, and L. Vasserman.\n2018. Measuring and mitigating unintended bias in text\nclassification. 2018 AAAI/ACM Conference on AI, Ethics,\nand Society.\nDoumbouya, M. K. B., D. Jurafsky, and C. D. Manning.\n2025. Tversky neural networks: Psychologically plausible deep learning with differentiable tversky similarity.\nArXiv preprint.\nDror, R., G. Baumer, M. Bogomolov, and R. Reichart. 2017.\nReplicability analysis for natural language processing:\nTesting significance with multiple datasets. TACL, 5:471–\n–486.\nDror, R., L. Peled-Cohen, S. Shlomov, and R. Reichart.\n2020. Statistical Significance Testing for Natural Language Processing, volume 45 of Synthesis Lectures on\nHuman Language Technologies. Morgan & Claypool.\n\n35\n\nHastie, T., R. J. Tibshirani, and J. H. Friedman. 2001. The\nElements of Statistical Learning. Springer.\nHutchinson, B., V. Prabhakaran, E. Denton, K. Webster,\nY. Zhong, and S. Denuyl. 2020. Social biases in NLP\nmodels as barriers for persons with disabilities. ACL.\nJaech, A., G. Mulcaire, S. Hathi, M. Ostendorf, and N. A.\nSmith. 2016. Hierarchical character-word models for language identification. ACL Workshop on NLP for Social\nMedia.\nJauhiainen, T., M. Lui, M. Zampieri, T. Baldwin, and\nK. Lindén. 2019. Automatic language identification in\ntexts: A survey. JAIR, 65(1):675–682.\nKehler, A. 1997. Probabilistic coreference in information\nextraction. EMNLP.\nKiritchenko, S. and S. M. Mohammad. 2018. Examining\ngender and race bias in two hundred sentiment analysis\nsystems. *SEM.\nLiu, B. and L. Zhang. 2012. A survey of opinion mining and\nsentiment analysis. In C. C. Aggarwal and C. Zhai, eds,\nMining text data, 415–464. Springer.\nManning, C. D., P. Raghavan, and H. Schütze. 2008. Introduction to Information Retrieval. Cambridge.\nMitchell, M., S. Wu, A. Zaldivar, P. Barnes, L. Vasserman,\nB. Hutchinson, E. Spitzer, I. D. Raji, and T. Gebru. 2019.\nModel cards for model reporting. ACM FAccT.\nMurphy, K. P. 2012. Machine learning: A probabilistic perspective. MIT Press.\nNigam, K., J. D. Lafferty, and A. McCallum. 1999. Using\nmaximum entropy for text classification. IJCAI-99 workshop on machine learning for information filtering.\nNoreen, E. W. 1989. Computer Intensive Methods for Testing\nHypothesis. Wiley.\nPang, B. and L. Lee. 2008. Opinion mining and sentiment\nanalysis. Foundations and trends in information retrieval,\n2(1-2):1–135.\nPark, J. H., J. Shin, and P. Fung. 2018. Reducing gender bias\nin abusive language detection. EMNLP.\nPopp, D., R. A. Donovan, M. Crawford, K. L. Marsh, and\nM. Peele. 2003. Gender, race, and speech style stereotypes. Sex Roles, 48(7-8):317–325.\nRatnaparkhi, A. 1996. A maximum entropy part-of-speech\ntagger. EMNLP.\nRatnaparkhi, A. 1997. A linear observed time statistical\nparser based on maximum entropy models. EMNLP.\nRosenfeld, R. 1996. A maximum entropy approach to adaptive statistical language modeling. Computer Speech and\nLanguage, 10:187–228.\nSankoff, D. and W. Labov. 1979. On the uses of variable\nrules. Language in society, 8(2-3):189–222.\nSap, M., D. Card, S. Gabriel, Y. Choi, and N. A. Smith. 2019.\nThe risk of racial bias in hate speech detection. ACL.\n\nEfron, B. and R. J. Tibshirani. 1993. An introduction to the\nbootstrap. CRC press.\n\nSchütze, H., D. A. Hull, and J. Pedersen. 1995. A comparison of classifiers and document representations for the\nrouting problem. SIGIR-95.\n\nGillick, L. and S. J. Cox. 1989. Some statistical issues in the\ncomparison of speech recognition algorithms. ICASSP.\n\nSøgaard, A., A. Johannsen, B. Plank, D. Hovy, and H. M.\nAlonso. 2014. What’s in a p-value in NLP? CoNLL.\n\nGuyon, I. and A. Elisseeff. 2003. An introduction to variable\nand feature selection. JMLR, 3:1157–1182.\n\nStamatatos, E. 2009. A survey of modern authorship attribution methods. JASIST, 60(3):538–556.\n\n\f36\n\nChapter 4\n\n•\n\nLogistic Regression\n\nTibshirani, R. J. 1996. Regression shrinkage and selection\nvia the lasso. Journal of the Royal Statistical Society. Series B (Methodological), 58(1):267–288.\nvan Rijsbergen, C. J. 1975. Information Retrieval. Butterworths.\nWitten, I. H. and E. Frank. 2005. Data Mining: Practical Machine Learning Tools and Techniques, 2nd edition.\nMorgan Kaufmann.\nYang, Y. and J. Pedersen. 1997. A comparative study on\nfeature selection in text categorization. ICML.\n\n\f",
    "file_path": "/Users/colinsidberry/Downloads/NLP_Textbook/logistic-regression.txt",
    "file_size_kb": 97.37
  },
  {
    "id": "6ed9c6bd613ae4df",
    "source": "nlp_textbook",
    "chapter": "N-gram Language Models",
    "filename": "n-gram.txt",
    "content": "Speech and Language Processing. Daniel Jurafsky & James H. Martin.\nrights reserved. Draft of August 24, 2025.\n\nCopyright © 2025.\n\nAll\n\nCHAPTER\n\nN-gram Language Models\n\n3\n\n“You are uniformly charming!” cried he, with a smile of associating and now\nand then I bowed and they perceived a chaise and four to wish for.\nRandom sentence generated from a Jane Austen trigram model\n\nPredicting is difficult—especially about the future, as the old quip goes. But how\nabout predicting something that seems much easier, like the next word someone is\ngoing to say? What word, for example, is likely to follow\nThe water of Walden Pond is so beautifully ...\n\nlanguage model\nLM\n\nYou might conclude that a likely word is blue, or green, or clear, but probably\nnot refrigerator nor this. In this chapter we formalize this intuition by introducing n-gram language models or LMs. A language model is a machine learning\nmodel that predicts upcoming words. More formally, a language model assigns a\nprobability to each possible next word, or equivalently gives a probability distribution over possible next words. Language models can also assign a probability to an\nentire sentence. Thus an LM could tell us that the following sequence has a much\nhigher probability of appearing in a text:\nall of a sudden I notice three guys standing on the sidewalk\n\nthan does this same set of words in a different order:\non guys all I of notice sidewalk three a sudden standing the\n\nAAC\n\nn-gram\n\nWhy would we want to predict upcoming words? The main reason is that large\nlanguage models are built just by training them to predict words!! As we’ll see\nin chapters 5-10, large language models learn an enormous amount about language\nsolely from being trained to predict upcoming words from neighboring words.\nThis probabilistic knowledge can be very practical. Consider correcting grammar or spelling errors like Their are two midterms, in which There was mistyped\nas Their, or Everything has improve, in which improve should have been\nimproved. The phrase There are is more probable than Their are, and has\nimproved than has improve, so a language model can help users select the more\ngrammatical variant.\nOr for a speech system to recognize that you said I will be back soonish\nand not I will be bassoon dish, it helps to know that back soonish is a more\nprobable sequence. Language models can also help in augmentative and alternative communication (Trnka et al. 2007, Kane et al. 2017). People can use AAC\nsystems if they are physically unable to speak or sign but can instead use eye gaze\nor other movements to select words from a menu. Word prediction can be used to\nsuggest likely words for the menu.\nIn this chapter we introduce the simplest kind of language model: the n-gram\n\n\f2\n\nC HAPTER 3\n\n•\n\nN- GRAM L ANGUAGE M ODELS\n\nlanguage model. An n-gram is a sequence of n words: a 2-gram (which we’ll call\nbigram) is a two-word sequence of words like The water, or water of, and a 3gram (a trigram) is a three-word sequence of words like The water of, or water\nof Walden. But we also (in a bit of terminological ambiguity) use the word ‘ngram’ to mean a probabilistic model that can estimate the probability of a word given\nthe n-1 previous words, and thereby also to assign probabilities to entire sequences.\nIn later chapters we will introduce the much more powerful neural large language models, based on the transformer architecture of Chapter 8. But because\nn-grams have a remarkably simple and clear formalization, we use them to introduce some major concepts of large language modeling, including training and test\nsets, perplexity, sampling, and interpolation.\n\n3.1\n\nN-Grams\nLet’s begin with the task of computing P(w|h), the probability of a word w given\nsome history h. Suppose the history h is “The water of Walden Pond is so\nbeautifully ” and we want to know the probability that the next word is blue:\nP(blue|The water of Walden Pond is so beautifully)\n\n(3.1)\n\nOne way to estimate this probability is directly from relative frequency counts: take a\nvery large corpus, count the number of times we see The water of Walden Pond\nis so beautifully, and count the number of times this is followed by blue. This\nwould be answering the question “Out of the times we saw the history h, how many\ntimes was it followed by the word w”, as follows:\nP(blue|The water of Walden Pond is so beautifully) =\nC(The water of Walden Pond is so beautifully blue)\nC(The water of Walden Pond is so beautifully)\n\n(3.2)\n\nIf we had a large enough corpus, we could compute these two counts and estimate\nthe probability from Eq. 3.2. But even the entire web isn’t big enough to give us\ngood estimates for counts of entire sentences. This is because language is creative;\nnew sentences are invented all the time, and we can’t expect to get accurate counts\nfor such large objects as entire sentences. For this reason, we’ll need more clever\nways to estimate the probability of a word w given a history h, or the probability of\nan entire word sequence W .\nLet’s start with some notation. First, throughout this chapter we’ll continue to\nrefer to words, although in practice we usually compute language models over tokens like the BPE tokens of page ??. To represent the probability of a particular\nrandom variable Xi taking on the value “the”, or P(Xi = “the”), we will use the\nsimplification P(the). We’ll represent a sequence of n words either as w1 . . . wn or\nw1:n . Thus the expression w1:n−1 means the string w1 , w2 , ..., wn−1 , but we’ll also\nbe using the equivalent notation w<n , which can be read as “all the elements of w\nfrom w1 up to and including wn−1 ”. For the joint probability of each word in a sequence having a particular value P(X1 = w1 , X2 = w2 , X3 = w3 , ..., Xn = wn ) we’ll\nuse P(w1 , w2 , ..., wn ).\nNow, how can we compute probabilities of entire sequences like P(w1 , w2 , ..., wn )?\nOne thing we can do is decompose this probability using the chain rule of proba-\n\n\f3.1\n\n•\n\nN-G RAMS\n\n3\n\nbility:\nP(X1 ...Xn ) = P(X1 )P(X2 |X1 )P(X3 |X1:2 ) . . . P(Xn |X1:n−1 )\nn\nY\n=\nP(Xk |X1:k−1 )\n\n(3.3)\n\nk=1\n\nApplying the chain rule to words, we get\nP(w1:n ) = P(w1 )P(w2 |w1 )P(w3 |w1:2 ) . . . P(wn |w1:n−1 )\nn\nY\n=\nP(wk |w1:k−1 )\n\n(3.4)\n\nk=1\n\nThe chain rule shows the link between computing the joint probability of a sequence\nand computing the conditional probability of a word given previous words. Equation 3.4 suggests that we could estimate the joint probability of an entire sequence of\nwords by multiplying together a number of conditional probabilities. But using the\nchain rule doesn’t really seem to help us! We don’t know any way to compute the\nexact probability of a word given a long sequence of preceding words, P(wn |w1:n−1 ).\nAs we said above, we can’t just estimate by counting the number of times every word\noccurs following every long string in some corpus, because language is creative and\nany particular context might have never occurred before!\n\n3.1.1\n\nbigram\n\nThe Markov assumption\n\nThe intuition of the n-gram model is that instead of computing the probability of a\nword given its entire history, we can approximate the history by just the last few\nwords.\nThe bigram model, for example, approximates the probability of a word given\nall the previous words P(wn |w1:n−1 ) by using only the conditional probability given\nthe preceding word P(wn |wn−1 ). In other words, instead of computing the probability\nP(blue|The water of Walden Pond is so beautifully)\n\n(3.5)\n\nwe approximate it with the probability\nP(blue|beautifully)\n\n(3.6)\n\nWhen we use a bigram model to predict the conditional probability of the next word,\nwe are thus making the following approximation:\nP(wn |w1:n−1 ) ≈ P(wn |wn−1 )\n\nMarkov\n\nn-gram\n\n(3.7)\n\nThe assumption that the probability of a word depends only on the previous word is\ncalled a Markov assumption. Markov models are the class of probabilistic models\nthat assume we can predict the probability of some future unit without looking too\nfar into the past. We can generalize the bigram (which looks one word into the past)\nto the trigram (which looks two words into the past) and thus to the n-gram (which\nlooks n − 1 words into the past).\nLet’s see a general equation for this n-gram approximation to the conditional\nprobability of the next word in a sequence. We’ll use N here to mean the n-gram\n\n\f4\n\nC HAPTER 3\n\n•\n\nN- GRAM L ANGUAGE M ODELS\n\nsize, so N = 2 means bigrams and N = 3 means trigrams. Then we approximate the\nprobability of a word given its entire context as follows:\nP(wn |w1:n−1 ) ≈ P(wn |wn−N+1:n−1 )\n\n(3.8)\n\nGiven the bigram assumption for the probability of an individual word, we can compute the probability of a complete word sequence by substituting Eq. 3.7 into Eq. 3.4:\nP(w1:n ) ≈\n\nn\nY\n\nP(wk |wk−1 )\n\n(3.9)\n\nk=1\n\n3.1.2\nmaximum\nlikelihood\nestimation\nnormalize\n\nHow to estimate probabilities\n\nHow do we estimate these bigram or n-gram probabilities? An intuitive way to\nestimate probabilities is called maximum likelihood estimation or MLE. We get\nthe MLE estimate for the parameters of an n-gram model by getting counts from\na corpus, and normalizing the counts so that they lie between 0 and 1. For probabilistic models, normalizing means dividing by some total count so that the resulting\nprobabilities fall between 0 and 1 and sum to 1.\nFor example, to compute a particular bigram probability of a word wn given a\nprevious word wn−1 , we’ll compute the count of the bigram C(wn−1 wn ) and normalize by the sum of all the bigrams that share the same first word wn−1 :\nC(wn−1 wn )\nP(wn |wn−1 ) = P\nw C(wn−1 w)\n\n(3.10)\n\nWe can simplify this equation, since the sum of all bigram counts that start with\na given word wn−1 must be equal to the unigram count for that word wn−1 (the reader\nshould take a moment to be convinced of this):\nP(wn |wn−1 ) =\n\nC(wn−1 wn )\nC(wn−1 )\n\n(3.11)\n\nLet’s work through an example using a mini-corpus of three sentences. We’ll\nfirst need to augment each sentence with a special symbol <s> at the beginning\nof the sentence, to give us the bigram context of the first word. We’ll also need a\nspecial end-symbol </s>.1\n<s> I am Sam </s>\n<s> Sam I am </s>\n<s> I do not like green eggs and ham </s>\nHere are the calculations for some of the bigram probabilities from this corpus\nP(I|<s>) = 23 = 0.67\n\nP(Sam|<s>) = 31 = 0.33\n\nP(am|I) = 23 = 0.67\n\nP(</s>|Sam) = 21 = 0.5\n\nP(Sam|am) = 12 = 0.5\n\nP(do|I) = 13 = 0.33\n\nFor the general case of MLE n-gram parameter estimation:\nP(wn |wn−N+1:n−1 ) =\n1\n\nC(wn−N+1:n−1 wn )\nC(wn−N+1:n−1 )\n\n(3.12)\n\nWe need the end-symbol to make the bigram grammar a true probability distribution. Without an endsymbol, instead of the sentence probabilities of all sentences summing to one, the sentence probabilities\nfor all sentences of a given length would sum to one. This model would define an infinite set of probability\ndistributions, with one distribution per sentence length. See Exercise 3.5.\n\n\f3.1\n\nrelative\nfrequency\n\n•\n\nN-G RAMS\n\n5\n\nEquation 3.12 (like Eq. 3.11) estimates the n-gram probability by dividing the\nobserved frequency of a particular sequence by the observed frequency of a prefix.\nThis ratio is called a relative frequency. We said above that this use of relative\nfrequencies as a way to estimate probabilities is an example of maximum likelihood\nestimation or MLE. In MLE, the resulting parameter set maximizes the likelihood of\nthe training set T given the model M (i.e., P(T |M)). For example, suppose the word\nChinese occurs 400 times in a corpus of a million words. What is the probability\nthat a random word selected from some other text of, say, a million words will be the\n400\nor 0.0004. Now 0.0004 is not\nword Chinese? The MLE of its probability is 1000000\nthe best possible estimate of the probability of Chinese occurring in all situations; it\nmight turn out that in some other corpus or context Chinese is a very unlikely word.\nBut it is the probability that makes it most likely that Chinese will occur 400 times\nin a million-word corpus. We present ways to modify the MLE estimates slightly to\nget better probability estimates in Section 3.6.\nLet’s move on to some examples from a real but tiny corpus, drawn from the\nnow-defunct Berkeley Restaurant Project, a dialogue system from the last century\nthat answered questions about a database of restaurants in Berkeley, California (Jurafsky et al., 1994). Here are some sample user queries (text-normalized, by lower\ncasing and with punctuation striped) (a sample of 9332 sentences is on the website):\ncan you tell me about any good cantonese restaurants close by\ntell me about chez panisse\ni’m looking for a good place to eat breakfast\nwhen is caffe venezia open during the day\n\nFigure 3.1 shows the bigram counts from part of a bigram grammar from textnormalized Berkeley Restaurant Project sentences. Note that the majority of the\nvalues are zero. In fact, we have chosen the sample words to cohere with each other;\na matrix selected from a random set of eight words would be even more sparse.\n\ni\nwant\nto\neat\nchinese\nfood\nlunch\nspend\n\ni\n5\n2\n2\n0\n1\n15\n2\n1\n\nwant\n827\n0\n0\n0\n0\n0\n0\n0\n\nto\n0\n608\n4\n2\n0\n15\n0\n1\n\neat\n9\n1\n686\n0\n0\n0\n0\n0\n\nchinese\n0\n6\n2\n16\n0\n1\n0\n0\n\nfood\n0\n6\n0\n2\n82\n4\n1\n0\n\nlunch\n0\n5\n6\n42\n1\n0\n0\n0\n\nspend\n2\n1\n211\n0\n0\n0\n0\n0\n\nFigure 3.1 Bigram counts for eight of the words (out of V = 1446) in the Berkeley Restaurant Project corpus of 9332 sentences. Zero counts are in gray. Each cell shows the count of\nthe column label word following the row label word. Thus the cell in row i and column want\nmeans that want followed i 827 times in the corpus.\n\nFigure 3.2 shows the bigram probabilities after normalization (dividing each cell\nin Fig. 3.1 by the appropriate unigram for its row, taken from the following set of\nunigram counts):\ni\nwant to\neat chinese food lunch spend\n2533 927 2417 746 158\n1093 341 278\n\n\f6\n\nC HAPTER 3\n\n•\n\nN- GRAM L ANGUAGE M ODELS\n\ni\nwant\nto\neat\nchinese\nfood\nlunch\nspend\n\ni\n0.002\n0.0022\n0.00083\n0\n0.0063\n0.014\n0.0059\n0.0036\n\nwant\n0.33\n0\n0\n0\n0\n0\n0\n0\n\nto\n0\n0.66\n0.0017\n0.0027\n0\n0.014\n0\n0.0036\n\neat\n0.0036\n0.0011\n0.28\n0\n0\n0\n0\n0\n\nchinese\n0\n0.0065\n0.00083\n0.021\n0\n0.00092\n0\n0\n\nfood\n0\n0.0065\n0\n0.0027\n0.52\n0.0037\n0.0029\n0\n\nlunch\n0\n0.0054\n0.0025\n0.056\n0.0063\n0\n0\n0\n\nspend\n0.00079\n0.0011\n0.087\n0\n0\n0\n0\n0\n\nFigure 3.2 Bigram probabilities for eight words in the Berkeley Restaurant Project corpus\nof 9332 sentences. Zero probabilities are in gray.\n\nHere are a few other useful probabilities:\nP(i|<s>) = 0.25\nP(food|english) = 0.5\n\nP(english|want) = 0.0011\nP(</s>|food) = 0.68\n\nNow we can compute the probability of sentences like I want English food or\nI want Chinese food by simply multiplying the appropriate bigram probabilities together, as follows:\nP(<s> i want english food </s>)\n= P(i|<s>)P(want|i)P(english|want)\nP(food|english)P(</s>|food)\n= 0.25 × 0.33 × 0.0011 × 0.5 × 0.68\n= 0.000031\nWe leave it as Exercise 3.2 to compute the probability of i want chinese food.\nWhat kinds of linguistic phenomena are captured in these bigram statistics?\nSome of the bigram probabilities above encode some facts that we think of as strictly\nsyntactic in nature, like the fact that what comes after eat is usually a noun or an\nadjective, or that what comes after to is usually a verb. Others might be a fact about\nthe personal assistant task, like the high probability of sentences beginning with\nthe words I. And some might even be cultural rather than linguistic, like the higher\nprobability that people are looking for Chinese versus English food.\n\n3.1.3\n\nDealing with scale in large n-gram models\n\nIn practice, language models can be very large, leading to practical issues.\nlog\nprobabilities\n\nLog probabilities Language model probabilities are always stored and computed\nin log space as log probabilities. This is because probabilities are (by definition)\nless than or equal to 1, and so the more probabilities we multiply together, the\nsmaller the product becomes. Multiplying enough n-grams together would result\nin numerical underflow. Adding in log space is equivalent to multiplying in linear\nspace, so we combine log probabilities by adding them. By adding log probabilities\ninstead of multiplying probabilities, we get results that are not as small. We do all\ncomputation and storage in log space, and just convert back into probabilities if we\nneed to report probabilities at the end by taking the exp of the logprob:\np1 × p2 × p3 × p4 = exp(log p1 + log p2 + log p3 + log p4 )\n\n(3.13)\n\nIn practice throughout this book, we’ll use log to mean natural log (ln) when the\nbase is not specified.\n\n\f3.2\n\ntrigram\n4-gram\n5-gram\n\n3.2\n\n•\n\nE VALUATING L ANGUAGE M ODELS : T RAINING AND T EST S ETS\n\n7\n\nLonger context Although for pedagogical purposes we have only described bigram models, when there is sufficient training data we use trigram models, which\ncondition on the previous two words, or 4-gram or 5-gram models. For these larger\nn-grams, we’ll need to assume extra contexts to the left and right of the sentence end.\nFor example, to compute trigram probabilities at the very beginning of the sentence,\nwe use two pseudo-words for the first trigram (i.e., P(I|<s><s>).\nSome large n-gram datasets have been created, like the million most frequent\nn-grams drawn from the Corpus of Contemporary American English (COCA), a\ncurated 1 billion word corpus of American English (Davies, 2020), Google’s Web\n5-gram corpus from 1 trillion words of English web text (Franz and Brants, 2006),\nor the Google Books Ngrams corpora (800 billion tokens from Chinese, English,\nFrench, German, Hebrew, Italian, Russian, and Spanish) (Lin et al., 2012)).\nIt’s even possible to use extremely long-range n-gram context. The infini-gram\n(∞-gram) project (Liu et al., 2024) allows n-grams of any length. Their idea is to\navoid the expensive (in space and time) pre-computation of huge n-gram count tables. Instead, n-gram probabilities with arbitrary n are computed quickly at inference\ntime by using an efficient representation called suffix arrays. This allows computing\nof n-grams of every length for enormous corpora of 5 trillion tokens.\nEfficiency considerations are important when building large n-gram language\nmodels. It is standard to quantize the probabilities using only 4-8 bits (instead of\n8-byte floats), store the word strings on disk and represent them in memory only as\na 64-bit hash, and represent n-grams in special data structures like ‘reverse tries’.\nIt is also common to prune n-gram language models, for example by only keeping\nn-grams with counts greater than some threshold or using entropy to prune lessimportant n-grams (Stolcke, 1998). Efficient language model toolkits like KenLM\n(Heafield 2011, Heafield et al. 2013) use sorted arrays and use merge sorts to efficiently build the probability tables in a minimal number of passes through a large\ncorpus.\n\nEvaluating Language Models: Training and Test Sets\n\nextrinsic\nevaluation\n\nintrinsic\nevaluation\n\ntraining set\ndevelopment\nset\ntest set\n\nThe best way to evaluate the performance of a language model is to embed it in\nan application and measure how much the application improves. Such end-to-end\nevaluation is called extrinsic evaluation. Extrinsic evaluation is the only way to\nknow if a particular improvement in the language model (or any component) is really\ngoing to help the task at hand. Thus for evaluating n-gram language models that are\na component of some task like speech recognition or machine translation, we can\ncompare the performance of two candidate language models by running the speech\nrecognizer or machine translator twice, once with each language model, and seeing\nwhich gives the more accurate transcription.\nUnfortunately, running big NLP systems end-to-end is often very expensive. Instead, it’s helpful to have a metric that can be used to quickly evaluate potential\nimprovements in a language model. An intrinsic evaluation metric is one that measures the quality of a model independent of any application. In the next section we’ll\nintroduce perplexity, which is the standard intrinsic metric for measuring language\nmodel performance, both for simple n-gram language models and for the more sophisticated neural large language models of Chapter 8.\nIn order to evaluate any machine learning model, we need to have at least three\ndistinct data sets: the training set, the development set, and the test set.\n\n\f8\n\nC HAPTER 3\n\ndata\ncontamination\n\ndevelopment\ntest\n\n•\n\nN- GRAM L ANGUAGE M ODELS\n\nThe training set is the data we use to learn the parameters of our model; for\nsimple n-gram language models it’s the corpus from which we get the counts that\nwe normalize into the probabilities of the n-gram language model.\nThe test set is a different, held-out set of data, not overlapping with the training\nset, that we use to evaluate the model. We need a separate test set to give us an\nunbiased estimate of how well the model we trained can generalize when we apply\nit to some new unknown dataset. A machine learning model that perfectly captured\nthe training data, but performed terribly on any other data, wouldn’t be much use\nwhen it comes time to apply it to any new data or problem! We thus measure the\nquality of an n-gram model by its performance on this unseen test set or test corpus.\nHow should we choose a training and test set? The test set should reflect the\nlanguage we want to use the model for. If we’re going to use our language model\nfor speech recognition of chemistry lectures, the test set should be text of chemistry\nlectures. If we’re going to use it as part of a system for translating hotel booking requests from Chinese to English, the test set should be text of hotel booking requests.\nIf we want our language model to be general purpose, then the test set should be\ndrawn from a wide variety of texts. In such cases we might collect a lot of texts\nfrom different sources, and then divide it up into a training set and a test set. It’s\nimportant to do the dividing carefully; if we’re building a general purpose model,\nwe don’t want the test set to consist of only text from one document, or one author,\nsince that wouldn’t be a good measure of general performance.\nThus if we are given a corpus of text and want to compare the performance of\ntwo different n-gram models, we divide the data into training and test sets, and train\nthe parameters of both models on the training set. We can then compare how well\nthe two trained models fit the test set.\nBut what does it mean to “fit the test set”? The standard answer is simple:\nwhichever language model assigns a higher probability to the test set—which\nmeans it more accurately predicts the test set—is a better model. Given two probabilistic models, the better model is the one that better predicts the details of the test\ndata, and hence will assign a higher probability to the test data.\nSince our evaluation metric is based on test set probability, it’s important not\nto let the test sentences into the training set. Suppose we are trying to compute\nthe probability of a particular “test” sentence. If our test sentence is part of the\ntraining corpus, we will mistakenly assign it an artificially high probability when\nit occurs in the test set. We call this situation training on the test set or also data\ncontamination. Training on the test set introduces a bias that makes the probabilities\nall look too high, and causes huge inaccuracies in perplexity, the probability-based\nmetric we introduce below.\nEven if we don’t train on the test set, if we test our language model on the\ntest set many times after making different changes, we might implicitly tune to its\ncharacteristics, by noticing which changes seem to make the model better. For this\nreason, we only want to run our model on the test set once, or a very few number of\ntimes, once we are sure our model is ready.\nFor this reason we normally instead have a third dataset called a development\ntest set or, devset. We do all our testing on this dataset until the very end, and then\nwe test on the test set once to see how good our model is.\nHow do we divide our data into training, development, and test sets? We want\nour test set to be as large as possible, since a small test set may be accidentally unrepresentative, but we also want as much training data as possible. At the minimum,\nwe would want to pick the smallest test set that gives us enough statistical power\n\n\f3.3\n\n•\n\nE VALUATING L ANGUAGE M ODELS : P ERPLEXITY\n\n9\n\nto measure a statistically significant difference between two potential models. It’s\nimportant that the devset be drawn from the same kind of text as the test set, since\nits goal is to measure how we would do on the test set.\n\n3.3\n\nEvaluating Language Models: Perplexity\n\nperplexity\n\nWe said above that we evaluate language models based on which one assigns a\nhigher probability to the test set. A better model is better at predicting upcoming\nwords, and so it will be less surprised by (i.e., assign a higher probability to) each\nword when it occurs in the test set. Indeed, a perfect language model would correctly\nguess each next word in a corpus, assigning it a probability of 1, and all the other\nwords a probability of zero. So given a test corpus, a better language model will\nassign a higher probability to it than a worse language model.\nBut in fact, we often do not use raw probability as our metric for evaluating\nlanguage models. The reason is that the probability of a test set (or any sequence)\ndepends on the number of words or tokens in it; the probability of a test set gets\nsmaller the longer the text. It’s useful to have a metric that is per-word, normalized\nby length, so we could compare across texts of different lengths. There is a such a\nmetric! It’s a function of probability called perplexity, and it is used for evaluating\nlarge language models as well as n-gram models.\nThe perplexity (sometimes abbreviated as PP or PPL) of a language model on a\ntest set is the inverse probability of the test set (one over the probability of the test\nset), normalized by the number of words (or tokens). For this reason it’s sometimes\ncalled the per-word or per-token perplexity. We normalize by the number of words\nN by taking the Nth root. For a test set W = w1 w2 . . . wN ,:\n1\n\nperplexity(W ) = P(w1 w2 . . . wN )− N\ns\n1\n= N\nP(w1 w2 . . . wN )\nOr we can use the chain rule to expand the probability of W :\nv\nuN\nuY\n1\nN\nperplexity(W ) = t\nP(wi |w1 . . . wi−1 )\n\n(3.14)\n\n(3.15)\n\ni=1\n\nNote that because of the inverse in Eq. 3.15, the higher the probability of the word\nsequence, the lower the perplexity. Thus the the lower the perplexity of a model on\nthe data, the better the model. Minimizing perplexity is equivalent to maximizing\nthe test set probability according to the language model. Why does perplexity use\nthe inverse probability? It turns out the inverse arises from the original definition\nof perplexity from cross-entropy rate in information theory; for those interested, the\nexplanation is in the advanced Section 3.7. Meanwhile, we just have to remember\nthat perplexity has an inverse relationship with probability.\nThe details of computing the perplexity of a test set W depends on which language model we use. Here’s the perplexity of W with a unigram language model\n(just the geometric mean of the inverse of the unigram probabilities):\nv\nuN\nuY 1\nN\n(3.16)\nperplexity(W ) = t\nP(wi )\ni=1\n\n\f10\n\nC HAPTER 3\n\n•\n\nN- GRAM L ANGUAGE M ODELS\n\nThe perplexity of W computed with a bigram language model is still a geometric\nmean, but now of the inverse of the bigram probabilities:\nv\nuN\nuY\nN\nperplexity(W ) = t\ni=1\n\n1\nP(wi |wi−1 )\n\n(3.17)\n\nWhat we generally use for word sequence in Eq. 3.15 or Eq. 3.17 is the entire\nsequence of words in some test set. Since this sequence will cross many sentence\nboundaries, if our vocabulary includes a between-sentence token <EOS> or separate\nbegin- and end-sentence markers <s> and </s> then we can include them in the\nprobability computation. If we do, then we also include one token per sentence in\nthe total count of word tokens N.2\nWe mentioned above that perplexity is a function of both the text and the language model: given a text W , different language models will have different perplexities. Because of this, perplexity can be used to compare different language models.\nFor example, here we trained unigram, bigram, and trigram models on 38 million\nwords from the Wall Street Journal newspaper. We then computed the perplexity of\neach of these models on a WSJ test set using Eq. 3.16 for unigrams, Eq. 3.17 for\nbigrams, and the corresponding equation for trigrams. The table below shows the\nperplexity of the 1.5 million word test set according to each of the language models.\nUnigram Bigram Trigram\nPerplexity 962\n170\n109\nAs we see above, the more information the n-gram gives us about the word\nsequence, the higher the probability the n-gram will assign to the string. A trigram\nmodel is less surprised than a unigram model because it has a better idea of what\nwords might come next, and so it assigns them a higher probability. And the higher\nthe probability, the lower the perplexity (since as Eq. 3.15 showed, perplexity is\nrelated inversely to the probability of the test sequence according to the model). So\na lower perplexity tells us that a language model is a better predictor of the test set.\nNote that in computing perplexities, the language model must be constructed\nwithout any knowledge of the test set, or else the perplexity will be artificially low.\nAnd the perplexity of two language models is only comparable if they use identical\nvocabularies.\nAn (intrinsic) improvement in perplexity does not guarantee an (extrinsic) improvement in the performance of a language processing task like speech recognition\nor machine translation. Nonetheless, because perplexity usually correlates with task\nimprovements, it is commonly used as a convenient evaluation metric. Still, when\npossible a model’s improvement in perplexity should be confirmed by an end-to-end\nevaluation on a real task.\n\n3.3.1\n\nPerplexity as Weighted Average Branching Factor\n\nIt turns out that perplexity can also be thought of as the weighted average branching factor of a language. The branching factor of a language is the number of\npossible next words that can follow any word. For example consider a mini artificial\n2\n\nFor example if we use both begin and end tokens, we would include the end-of-sentence marker </s>\nbut not the beginning-of-sentence marker <s> in our count of N; This is because the end-sentence token is\nfollowed directly by the begin-sentence token with probability almost 1, so we don’t want the probability\nof that fake transition to influence our perplexity.\n\n\f3.4\n\n•\n\nS AMPLING SENTENCES FROM A LANGUAGE MODEL\n\n11\n\nlanguage that is deterministic (no probabilities), any word can follow any word, and\nwhose vocabulary consists of only three colors:\nL = {red, blue, green}\n\n(3.18)\n\nThe branching factor of this language is 3.\nNow let’s make a probabilistic version of the same LM, let’s call it A, where each\nword follows each other with equal probability 13 (it was trained on a training set with\nequal counts for the 3 colors), and a test set T = “red red red red blue”.\nLet’s first convince ourselves that if we compute the perplexity of this artificial\ncolor language on this test set (or any such test set) we indeed get 3. By Eq. 3.15,\nthe perplexity of A on T is:\n1\n\nperplexityA (T ) = PA (red red red red blue)− 5\n\u0012 \u00135 !− 15\n1\n=\n3\n\u0012 \u0013−1\n1\n=3\n=\n3\n\n(3.19)\n\nBut now suppose red was very likely in the training set of a different LM B, and so\nB has the following probabilities:\nP(red) = 0.8 P(green) = 0.1 P(blue) = 0.1\n\n(3.20)\n\nWe should expect the perplexity of the same test set red red red red blue for\nlanguage model B to be lower since most of the time the next color will be red, which\nis very predictable, i.e. has a high probability. So the probability of the test set will\nbe higher, and since perplexity is inversely related to probability, the perplexity will\nbe lower. Thus, although the branching factor is still 3, the perplexity or weighted\nbranching factor is smaller:\nperplexityB (T ) = PB (red red red red blue)−1/5\n1\n\n= 0.04096− 5\n\n= 0.527−1 = 1.89\n\n3.4\n\n(3.21)\n\nSampling sentences from a language model\nsampling\n\nOne important way to visualize what kind of knowledge a language model embodies\nis to sample from it. Sampling from a distribution means to choose random points\naccording to their likelihood. Thus sampling from a language model—which represents a distribution over sentences—means to generate some sentences, choosing\neach sentence according to its likelihood as defined by the model. Thus we are more\nlikely to generate sentences that the model thinks have a high probability and less\nlikely to generate sentences that the model thinks have a low probability.\nThis technique of visualizing a language model by sampling was first suggested\nvery early on by Shannon (1948) and Miller and Selfridge (1950). It’s simplest to\nvisualize how this works for the unigram case. Imagine all the words of the English\nlanguage covering the number line between 0 and 1, each word covering an interval\n\n\f12\n\nC HAPTER 3\n\n•\n\nN- GRAM L ANGUAGE M ODELS\npolyphonic\nthe\n\nof\n\na\n\n0.06\n\n0.03\n\n0.02 0.02 0.02\n\nto in\n\nhowever\n\n…\n\np=0.0000018\n\n(p=0.0003)\n\n…\n0\n\n.06\n\n.09 .11 .13 .15\n\n…\n.66\n\n.99\n\n1\n\nFigure 3.3 A visualization of the sampling distribution for sampling sentences by repeatedly sampling unigrams. The blue bar represents the relative frequency of each word (we’ve\nordered them from most frequent to least frequent, but the choice of order is arbitrary). The\nnumber line shows the cumulative probabilities. If we choose a random number between 0\nand 1, it will fall in an interval corresponding to some word. The expectation for the random\nnumber to fall in the larger intervals of one of the frequent words (the, of, a) is much higher\nthan in the smaller interval of one of the rare words (polyphonic).\n\nproportional to its frequency. Fig. 3.3 shows a visualization, using a unigram LM\ncomputed from the text of this book. We choose a random value between 0 and 1,\nfind that point on the probability line, and print the word whose interval includes this\nchosen value. We continue choosing random numbers and generating words until\nwe randomly generate the sentence-final token </s>.\nWe can use the same technique to generate bigrams by first generating a random bigram that starts with <s> (according to its bigram probability). Let’s say the\nsecond word of that bigram is w. We next choose a random bigram starting with w\n(again, drawn according to its bigram probability), and so on.\n\n3.5\n\nGeneralizing vs. overfitting the training set\nThe n-gram model, like many statistical models, is dependent on the training corpus.\nOne implication of this is that the probabilities often encode specific facts about a\ngiven training corpus. Another implication is that n-grams do a better and better job\nof modeling the training corpus as we increase the value of n.\nWe can use the sampling method from the prior section to visualize both of\nthese facts! To give an intuition for the increasing power of higher-order n-grams,\nFig. 3.4 shows random sentences generated from unigram, bigram, trigram, and 4gram models trained on Shakespeare’s works.\nThe longer the context, the more coherent the sentences. The unigram sentences show no coherent relation between words nor any sentence-final punctuation. The bigram sentences have some local word-to-word coherence (especially\nconsidering punctuation as words). The trigram sentences are beginning to look a\nlot like Shakespeare. Indeed, the 4-gram sentences look a little too much like Shakespeare. The words It cannot be but so are directly from King John. This is because,\nnot to put the knock on Shakespeare, his oeuvre is not very large as corpora go\n(N = 884, 647,V = 29, 066), and our n-gram probability matrices are ridiculously\nsparse. There are V 2 = 844, 000, 000 possible bigrams alone, and the number of\npossible 4-grams is V 4 = 7 × 1017 . Thus, once the generator has chosen the first\n3-gram (It cannot be), there are only seven possible next words for the 4th element\n(but, I, that, thus, this, and the period).\nTo get an idea of the dependence on the training set, let’s look at LMs trained on a\ncompletely different corpus: the Wall Street Journal (WSJ) newspaper. Shakespeare\n\n\f3.5\n\n•\n\nG ENERALIZING VS . OVERFITTING THE TRAINING SET\n\n13\n\n1\ngram\n\n–To him swallowed confess hear both. Which. Of save on trail for are ay device and\nrote life have\n–Hill he late speaks; or! a more to leg less first you enter\n\n2\ngram\n\n–Why dost stand forth thy canopy, forsooth; he is this palpable hit the King Henry. Live\nking. Follow.\n–What means, sir. I confess she? then all sorts, he is trim, captain.\n\n3\ngram\n\n–Fly, and will rid me these news of price. Therefore the sadness of parting, as they say,\n’tis done.\n–This shall forbid it should be branded, if renown made it empty.\n\n4\ngram\n\n–King Henry. What! I will go seek the traitor Gloucester. Exeunt some of the watch. A\ngreat banquet serv’d in;\n–It cannot be but so.\n\nFigure 3.4 Eight sentences randomly generated from four n-gram models computed from Shakespeare’s\nworks. All characters were mapped to lower-case and punctuation marks were treated as words. Output is\nhand-corrected for capitalization to improve readability.\n\nand the WSJ are both English, so we might have expected some overlap between our\nn-grams for the two genres. Fig. 3.5 shows sentences generated by unigram, bigram,\nand trigram models trained on 40 million words from WSJ.\nMonths the my and issue of year foreign new exchange’s september\n1\nwere recession exchange new endorsed a acquire to six executives\ngram\nLast December through the way to preserve the Hudson corporation N.\nB. E. C. Taylor would seem to complete the major central planners one\n2\ngram\npoint five percent of U. S. E. has already old M. X. corporation of living\n\n3\ngram\n\non information such as more frequently fishing to keep her\nThey also point to ninety nine point six billion dollars from two hundred\nfour oh six three percent of the rates of interest stores as Mexico and\nBrazil on market conditions\n\nFigure 3.5 Three sentences randomly generated from three n-gram models computed from\n40 million words of the Wall Street Journal, lower-casing all characters and treating punctuation as words. Output was then hand-corrected for capitalization to improve readability.\n\nCompare these examples to the pseudo-Shakespeare in Fig. 3.4. While they both\nmodel “English-like sentences”, there is no overlap in the generated sentences, and\nlittle overlap even in small phrases. Statistical models are pretty useless as predictors\nif the training sets and the test sets are as different as Shakespeare and the WSJ.\nHow should we deal with this problem when we build n-gram models? One step\nis to be sure to use a training corpus that has a similar genre to whatever task we are\ntrying to accomplish. To build a language model for translating legal documents,\nwe need a training corpus of legal documents. To build a language model for a\nquestion-answering system, we need a training corpus of questions.\nIt is equally important to get training data in the appropriate dialect or variety,\nespecially when processing social media posts or spoken transcripts. For example some tweets will use features of African American English (AAE)— the name\nfor the many variations of language used in African American communities (King,\n2020). Such features can include words like finna—an auxiliary verb that marks\nimmediate future tense —that don’t occur in other varieties, or spellings like den for\nthen, in tweets like this one (Blodgett and O’Connor, 2017):\n\n\f14\n\nC HAPTER 3\n\n•\n\nN- GRAM L ANGUAGE M ODELS\n\n(3.22) Bored af den my phone finna die!!!\nwhile tweets from English-based languages like Nigerian Pidgin have markedly different vocabulary and n-gram patterns from American English (Jurgens et al., 2017):\n(3.23) @username R u a wizard or wat gan sef: in d mornin - u tweet, afternoon - u\ntweet, nyt gan u dey tweet. beta get ur IT placement wiv twitter\nIs it possible for the testset nonetheless to have a word we have never seen before? What happens if the word Jurafsky never occurs in our training set, but pops up\nin the test set? The answer is that although words might be unseen, we normally run\nour NLP algorithms not on words but on subword tokens. With subword tokenization (like the BPE algorithm of Chapter 2) any word can be modeled as a sequence\nof known smaller subwords, if necessary by a sequence of tokens corresponding to\nindividual letters. So although for convenience we’ve been referring to words in\nthis chapter, the language model vocabulary is normally the set of tokens rather than\nwords, and in this way the test set can never contain unseen tokens.\n\n3.6\n\nSmoothing, Interpolation, and Backoff\n\nzeros\n\nsmoothing\ndiscounting\n\nThere is a problem with using maximum likelihood estimates for probabilities: any\nfinite training corpus will be missing some perfectly acceptable English word sequences. That is, cases where a particular n-gram never occurs in the training data\nbut appears in the test set. Perhaps our training corpus has the words ruby and\nslippers in it but just happens not to have the phrase ruby slippers.\nThese unseen sequences or zeros—sequences that don’t occur in the training set\nbut do occur in the test set—are a problem for two reasons. First, their presence\nmeans we are underestimating the probability of word sequences that might occur,\nwhich hurts the performance of any application we want to run on this data. Second,\nif the probability of any word in the test set is 0, the probability of the whole test\nset is 0. Perplexity is defined based on the inverse probability of the test set. Thus\nif some words in context have zero probability, we can’t compute perplexity at all,\nsince we can’t divide by zero!\nThe standard way to deal with putative “zero probability n-grams” that should really have some non-zero probability is called smoothing or discounting. Smoothing\nalgorithms shave off a bit of probability mass from some more frequent events and\ngive it to unseen events. Here we’ll introduce some simple smoothing algorithms:\nLaplace (add-one) smoothing, stupid backoff, and n-gram interpolation.\n\n3.6.1\n\nLaplace\nsmoothing\n\nLaplace Smoothing\n\nThe simplest way to do smoothing is to add one to all the n-gram counts, before\nwe normalize them into probabilities. All the counts that used to be zero will now\nhave a count of 1, the counts of 1 will be 2, and so on. This algorithm is called\nLaplace smoothing. Laplace smoothing does not perform well enough to be used\nin modern n-gram models, but it usefully introduces many of the concepts that we\nsee in other smoothing algorithms, gives a useful baseline, and is also a practical\nsmoothing algorithm for other tasks like text classification (Appendix K).\nLet’s start with the application of Laplace smoothing to unigram probabilities.\nRecall that the unsmoothed maximum likelihood estimate of the unigram probability\n\n\f3.6\n\nadd-one\n\n•\n\nS MOOTHING , I NTERPOLATION , AND BACKOFF\n\n15\n\nof the word wi is its count ci normalized by the total number of word tokens N:\nci\nP(wi ) =\nN\nLaplace smoothing merely adds one to each count (hence its alternate name addone smoothing). Since there are V words in the vocabulary and each one was incremented, we also need to adjust the denominator to take into account the extra V\nobservations. (What happens to our P values if we don’t increase the denominator?)\nPLaplace (wi ) =\n\nci + 1\nN +V\n\n(3.24)\n\nNow that we have the intuition for the unigram case, let’s smooth our Berkeley\nRestaurant Project bigrams. Figure 3.6 shows the add-one smoothed counts for the\nbigrams in Fig. 3.1.\n\ni\nwant\nto\neat\nchinese\nfood\nlunch\nspend\n\ni\n6\n3\n3\n1\n2\n16\n3\n2\n\nwant\n828\n1\n1\n1\n1\n1\n1\n1\n\nto\n1\n609\n5\n3\n1\n16\n1\n2\n\neat\n10\n2\n687\n1\n1\n1\n1\n1\n\nchinese\n1\n7\n3\n17\n1\n2\n1\n1\n\nfood\n1\n7\n1\n3\n83\n5\n2\n1\n\nlunch\n1\n6\n7\n43\n2\n1\n1\n1\n\nspend\n3\n2\n212\n1\n1\n1\n1\n1\n\nFigure 3.6 Add-one smoothed bigram counts for eight of the words (out of V = 1446) in\nthe Berkeley Restaurant Project corpus of 9332 sentences. Previously-zero counts are in gray.\n\nFigure 3.7 shows the add-one smoothed probabilities for the bigrams in Fig. 3.2,\ncomputed by Eq. 3.26 below. Recall that normal bigram probabilities are computed\nby normalizing each row of counts by the unigram count:\nPMLE (wn |wn−1 ) =\n\nC(wn−1 wn )\nC(wn−1 )\n\n(3.25)\n\nFor add-one smoothed bigram counts, we need to augment the unigram count in the\ndenominator by the number of total word types in the vocabulary V . We can see\nwhy this is in the following equation, which makes it explicit that the unigram count\nin the denominator is really the sum over all the bigrams that start with wn−1 . Since\nwe add one to each of these, and there are V of them, we add a total of V to the\ndenominator:\nC(wn−1 wn ) + 1\nC(wn−1 wn ) + 1\nPLaplace (wn |wn−1 ) = P\n=\n(C(w\nw)\n+\n1)\nC(wn−1 ) +V\nn−1\nw\n\n(3.26)\n\nThus, each of the unigram counts given on page 5 will need to be augmented by V =\n1446. The result, using Eq. 3.26, is the smoothed bigram probabilities in Fig. 3.7.\nOne useful visualization technique is to reconstruct an adjusted count matrix\nso we can see how much a smoothing algorithm has changed the original counts.\nThis adjusted count C∗ is the count that, if divided by C(wn−1 ), would result in\nthe smoothed probability. This adjusted count is easier to compare directly with\nthe MLE counts. That is, the Laplace probability can equally be expressed as the\nadjusted count divided by the (non-smoothed) denominator from Eq. 3.25:\nPLaplace (wn |wn−1 ) =\n\nC(wn−1 wn ) + 1 C∗ (wn−1 wn )\n=\nC(wn−1 ) +V\nC(wn−1 )\n\n\f16\n\nC HAPTER 3\n\ni\nwant\nto\neat\nchinese\nfood\nlunch\nspend\n\n•\n\ni\n0.0015\n0.0013\n0.00078\n0.00046\n0.0012\n0.0063\n0.0017\n0.0012\n\nN- GRAM L ANGUAGE M ODELS\nwant\n0.21\n0.00042\n0.00026\n0.00046\n0.00062\n0.00039\n0.00056\n0.00058\n\nto\n0.00025\n0.26\n0.0013\n0.0014\n0.00062\n0.0063\n0.00056\n0.0012\n\neat\n0.0025\n0.00084\n0.18\n0.00046\n0.00062\n0.00039\n0.00056\n0.00058\n\nchinese\n0.00025\n0.0029\n0.00078\n0.0078\n0.00062\n0.00079\n0.00056\n0.00058\n\nfood\n0.00025\n0.0029\n0.00026\n0.0014\n0.052\n0.002\n0.0011\n0.00058\n\nlunch\n0.00025\n0.0025\n0.0018\n0.02\n0.0012\n0.00039\n0.00056\n0.00058\n\nspend\n0.00075\n0.00084\n0.055\n0.00046\n0.00062\n0.00039\n0.00056\n0.00058\n\nFigure 3.7 Add-one smoothed bigram probabilities for eight of the words (out of V = 1446) in the BeRP\ncorpus of 9332 sentences computed by Eq. 3.26. Previously-zero probabilities are in gray.\n\nRearranging terms, we can solve for C∗ (wn−1 wn ) :\nC∗ (wn−1 wn ) =\n\n[C(wn−1 wn ) + 1] ×C(wn−1 )\nC(wn−1 ) +V\n\n(3.27)\n\nFigure 3.8 shows the reconstructed counts, computed by Eq. 3.27.\ni\nwant\nto\neat\nchinese\nfood\nlunch\nspend\n\ni\n3.8\n1.2\n1.9\n0.34\n0.2\n6.9\n0.57\n0.32\n\nwant\n527\n0.39\n0.63\n0.34\n0.098\n0.43\n0.19\n0.16\n\nto\n0.64\n238\n3.1\n1\n0.098\n6.9\n0.19\n0.32\n\neat\n6.4\n0.78\n430\n0.34\n0.098\n0.43\n0.19\n0.16\n\nchinese\n0.64\n2.7\n1.9\n5.8\n0.098\n0.86\n0.19\n0.16\n\nfood\n0.64\n2.7\n0.63\n1\n8.2\n2.2\n0.38\n0.16\n\nlunch\n0.64\n2.3\n4.4\n15\n0.2\n0.43\n0.19\n0.16\n\nspend\n1.9\n0.78\n133\n0.34\n0.098\n0.43\n0.19\n0.16\n\nFigure 3.8 Add-one reconstituted counts for eight words (of V = 1446) in the BeRP corpus\nof 9332 sentences, computed by Eq. 3.27. Previously-zero counts are in gray.\n\nNote that add-one smoothing has made a very big change to the counts. Comparing Fig. 3.8 to the original counts in Fig. 3.1, we can see that C(want to) changed\nfrom 608 to 238. We can see this in probability space as well: P(to|want) decreases\nfrom 0.66 in the unsmoothed case to 0.26 in the smoothed case. Looking at the discount d, defined as the ratio between new and old counts, shows us how strikingly\nthe counts for each prefix word have been reduced; the discount for the bigram want\nto is 0.39, while the discount for Chinese food is 0.10, a factor of 10. The sharp\nchange occurs because too much probability mass is moved to all the zeros.\n\n3.6.2\n\nadd-k\n\nAdd-k smoothing\n\nOne alternative to add-one smoothing is to move a bit less of the probability mass\nfrom the seen to the unseen events. Instead of adding 1 to each count, we add a\nfractional count k (0.5? 0.01?). This algorithm is therefore called add-k smoothing.\n∗\nPAdd-k\n(wn |wn−1 ) =\n\nC(wn−1 wn ) + k\nC(wn−1 ) + kV\n\n(3.28)\n\nAdd-k smoothing requires that we have a method for choosing k; this can be\ndone, for example, by optimizing on a devset. Although add-k is useful for some\ntasks (including text classification), it turns out that it still doesn’t work well for\n\n\f3.6\n\n•\n\nS MOOTHING , I NTERPOLATION , AND BACKOFF\n\n17\n\nlanguage modeling, generating counts with poor variances and often inappropriate\ndiscounts (Gale and Church, 1994).\n\n3.6.3\n\ninterpolation\n\nLanguage Model Interpolation\n\nThere is an alternative source of knowledge we can draw on to solve the problem\nof zero frequency n-grams. If we are trying to compute P(wn |wn−2 wn−1 ) but we\nhave no examples of a particular trigram wn−2 wn−1 wn , we can instead estimate its\nprobability by using the bigram probability P(wn |wn−1 ). Similarly, if we don’t have\ncounts to compute P(wn |wn−1 ), we can look to the unigram P(wn ). In other words,\nsometimes using less context can help us generalize more for contexts that the model\nhasn’t learned much about.\nThe most common way to use this n-gram hierarchy is called interpolation:\ncomputing a new probability by interpolating (weighting and combining) the trigram, bigram, and unigram probabilities. In simple linear interpolation, we combine different order n-grams by linearly interpolating them. Thus, we estimate the\ntrigram probability P(wn |wn−2 wn−1 ) by mixing together the unigram, bigram, and\ntrigram probabilities, each weighted by a λ :\nP̂(wn |wn−2 wn−1 ) = λ1 P(wn )\n+λ2 P(wn |wn−1 )\n+λ3 P(wn |wn−2 wn−1 )\n\n(3.29)\n\nThe λ s must sum to 1, making Eq. 3.29 equivalent to a weighted average. In a\nslightly more sophisticated version of linear interpolation, each λ weight is computed by conditioning on the context. This way, if we have particularly accurate\ncounts for a particular bigram, we assume that the counts of the trigrams based on\nthis bigram will be more trustworthy, so we can make the λ s for those trigrams\nhigher and thus give that trigram more weight in the interpolation. Equation 3.30\nshows the equation for interpolation with context-conditioned weights, where each\nlambda takes an argument that is the two prior word context:\nP̂(wn |wn−2 wn−1 ) = λ1 (wn−2:n−1 )P(wn )\n+λ2 (wn−2:n−1 )P(wn |wn−1 )\n+ λ3 (wn−2:n−1 )P(wn |wn−2 wn−1 )\nheld-out\n\nHow are these λ values set? Both the simple interpolation and conditional interpolation λ s are learned from a held-out corpus. A held-out corpus is an additional\ntraining corpus, so-called because we hold it out from the training data, that we use\nto set these λ values.3 We do so by choosing the λ values that maximize the likelihood of the held-out corpus. That is, we fix the n-gram probabilities and then search\nfor the λ values that—when plugged into Eq. 3.29—give us the highest probability\nof the held-out set. There are various ways to find this optimal set of λ s. One way\nis to use the EM algorithm, an iterative learning algorithm that converges on locally\noptimal λ s (Jelinek and Mercer, 1980).\n\n3.6.4\nbackoff\n\n(3.30)\n\nStupid Backoff\n\nAn alternative to interpolation is backoff. In a backoff model, if the n-gram we need\n3\n\nHeld-out corpora are generally used to set hyperparameters, which are special parameters, unlike\nregular counts that are learned from the training data; we’ll discuss hyperparameters in Chapter 6.\n\n\f18\n\nC HAPTER 3\n\ndiscount\n\nstupid backoff\n\n•\n\nN- GRAM L ANGUAGE M ODELS\n\nhas zero counts, we approximate it by backing off to the (n-1)-gram. We continue\nbacking off until we reach a history that has some counts. For a backoff model to\ngive a correct probability distribution, we have to discount the higher-order n-grams\nto save some probability mass for the lower order n-grams. In practice, instead of\ndiscounting, it’s common to use a much simpler non-discounted backoff algorithm\ncalled stupid backoff (Brants et al., 2007).\nStupid backoff gives up the idea of trying to make the language model a true\nprobability distribution. There is no discounting of the higher-order probabilities. If\na higher-order n-gram has a zero count, we simply backoff to a lower order n-gram,\nweighed by a fixed (context-independent) weight. This algorithm does not produce\na probability distribution, so we’ll follow Brants et al. (2007) in referring to it as S:\n\nS(wi |wi−N+1 : i−1 ) =\n\n\n\n\ncount(wi−N+1 : i )\ncount(wi−N+1 : i−1 ) if count(wi−N+1 : i ) > 0\n\nλ S(wi |wi−N+2 : i−1 ) otherwise\n\n(3.31)\n\n. Brants et al.\nThe backoff terminates in the unigram, which has score S(w) = count(w)\nN\n(2007) find that a value of 0.4 worked well for λ .\n\n3.7\n\nAdvanced: Perplexity’s Relation to Entropy\n\nEntropy\n\nWe introduced perplexity in Section 3.3 as a way to evaluate n-gram models on\na test set. A better n-gram model is one that assigns a higher probability to the\ntest data, and perplexity is a normalized version of the probability of the test set.\nThe perplexity measure actually arises from the information-theoretic concept of\ncross-entropy, which explains otherwise mysterious properties of perplexity (why\nthe inverse probability, for example?) and its relationship to entropy. Entropy is a\nmeasure of information. Given a random variable X ranging over whatever we are\npredicting (words, letters, parts of speech), the set of which we’ll call χ, and with a\nparticular probability function, call it p(x), the entropy of the random variable X is:\nH(X) = −\n\nX\n\np(x) log2 p(x)\n\n(3.32)\n\nx∈χ\n\nThe log can, in principle, be computed in any base. If we use log base 2, the\nresulting value of entropy will be measured in bits.\nOne intuitive way to think about entropy is as a lower bound on the number of\nbits it would take to encode a certain decision or piece of information in the optimal\ncoding scheme. Consider an example from the standard information theory textbook\nCover and Thomas (1991). Imagine that we want to place a bet on a horse race but\nit is too far to go all the way to Yonkers Racetrack, so we’d like to send a short\nmessage to the bookie to tell him which of the eight horses to bet on. One way to\nencode this message is just to use the binary representation of the horse’s number\nas the code; thus, horse 1 would be 001, horse 2 010, horse 3 011, and so on, with\nhorse 8 coded as 000. If we spend the whole day betting and each horse is coded\nwith 3 bits, on average we would be sending 3 bits per race.\nCan we do better? Suppose that the spread is the actual distribution of the bets\nplaced and that we represent it as the prior probability of each horse as follows:\n\n\f3.7\n\n•\n\nA DVANCED : P ERPLEXITY ’ S R ELATION TO E NTROPY\nHorse 1\nHorse 2\nHorse 3\nHorse 4\n\n1\n2\n1\n4\n1\n8\n1\n16\n\nHorse 5\nHorse 6\nHorse 7\nHorse 8\n\n19\n\n1\n64\n1\n64\n1\n64\n1\n64\n\nThe entropy of the random variable X that ranges over horses gives us a lower\nbound on the number of bits and is\nH(X) = −\n\ni=8\nX\n\np(i) log2 p(i)\n\ni=1\n\n= − 12 log2 12 − 14 log2 41 − 18 log2 18 − 161 log2 161 −4( 641 log2 641 )\n= 2 bits\n\n(3.33)\n\nA code that averages 2 bits per race can be built with short encodings for more\nprobable horses, and longer encodings for less probable horses. For example, we\ncould encode the most likely horse with the code 0, and the remaining horses as 10,\nthen 110, 1110, 111100, 111101, 111110, and 111111.\nWhat if the horses are equally likely? We saw above that if we used an equallength binary code for the horse numbers, each horse took 3 bits to code, so the\naverage was 3. Is the entropy the same? In this case each horse would have a\nprobability of 18 . The entropy of the choice of horses is then\nH(X) = −\n\ni=8\nX\n1\ni=1\n\n8\n\nlog2\n\n1\n1\n= − log2 = 3 bits\n8\n8\n\n(3.34)\n\nUntil now we have been computing the entropy of a single variable. But most of\nwhat we will use entropy for involves sequences. For a grammar, for example, we\nwill be computing the entropy of some sequence of words W = {w1 , w2 , . . . , wn }.\nOne way to do this is to have a variable that ranges over sequences of words. For\nexample we can compute the entropy of a random variable that ranges over all sequences of words of length n in some language L as follows:\nX\nH(w1 , w2 , . . . , wn ) = −\np(w1 : n ) log p(w1 : n )\n(3.35)\nw1 : n ∈L\nentropy rate\n\nWe could define the entropy rate (we could also think of this as the per-word\nentropy) as the entropy of this sequence divided by the number of words:\n1\n1 X\nH(w1 : n ) = −\np(w1 : n ) log p(w1 : n )\nn\nn\n\n(3.36)\n\nw1 : n ∈L\n\nBut to measure the true entropy of a language, we need to consider sequences of\ninfinite length. If we think of a language as a stochastic process L that produces a\nsequence of words, and allow W to represent the sequence of words w1 , . . . , wn , then\nL’s entropy rate H(L) is defined as\nH(L) = lim\n\n1\n\nn→∞ n\n\nH(w1 : n )\n\n1X\np(w1 : n ) log p(w1 : n )\nn→∞ n\n\n= − lim\n\nW ∈L\n\n(3.37)\n\n\f20\n\nC HAPTER 3\n\n•\n\nN- GRAM L ANGUAGE M ODELS\n\nThe Shannon-McMillan-Breiman theorem (Algoet and Cover 1988, Cover and Thomas\n1991) states that if the language is regular in certain ways (to be exact, if it is both\nstationary and ergodic),\n1\nH(L) = lim − log p(w1 : n )\nn→∞ n\n\nStationary\n\ncross-entropy\n\n(3.38)\n\nThat is, we can take a single sequence that is long enough instead of summing over\nall possible sequences. The intuition of the Shannon-McMillan-Breiman theorem\nis that a long-enough sequence of words will contain in it many other shorter sequences and that each of these shorter sequences will reoccur in the longer sequence\naccording to their probabilities.\nA stochastic process is said to be stationary if the probabilities it assigns to a\nsequence are invariant with respect to shifts in the time index. In other words, the\nprobability distribution for words at time t is the same as the probability distribution\nat time t + 1. Markov models, and hence n-grams, are stationary. For example, in\na bigram, Pi is dependent only on Pi−1 . So if we shift our time index by x, Pi+x is\nstill dependent on Pi+x−1 . But natural language is not stationary, since as we show\nin Appendix D, the probability of upcoming words can be dependent on events that\nwere arbitrarily distant and time dependent. Thus, our statistical models only give\nan approximation to the correct distributions and entropies of natural language.\nTo summarize, by making some incorrect but convenient simplifying assumptions, we can compute the entropy of some stochastic process by taking a very long\nsample of the output and computing its average log probability.\nNow we are ready to introduce cross-entropy. The cross-entropy is useful when\nwe don’t know the actual probability distribution p that generated some data. It\nallows us to use some m, which is a model of p (i.e., an approximation to p). The\ncross-entropy of m on p is defined by\nH(p, m) = lim −\nn→∞\n\n1X\np(w1 , . . . , wn ) log m(w1 , . . . , wn )\nn\n\n(3.39)\n\nW ∈L\n\nThat is, we draw sequences according to the probability distribution p, but sum the\nlog of their probabilities according to m.\nAgain, following the Shannon-McMillan-Breiman theorem, for a stationary ergodic process:\n1\nH(p, m) = lim − log m(w1 w2 . . . wn )\nn→∞ n\n\n(3.40)\n\nThis means that, as for entropy, we can estimate the cross-entropy of a model m\non some distribution p by taking a single sequence that is long enough instead of\nsumming over all possible sequences.\nWhat makes the cross-entropy useful is that the cross-entropy H(p, m) is an upper bound on the entropy H(p). For any model m:\nH(p) ≤ H(p, m)\n\n(3.41)\n\nThis means that we can use some simplified model m to help estimate the true entropy of a sequence of symbols drawn according to probability p. The more accurate\nm is, the closer the cross-entropy H(p, m) will be to the true entropy H(p). Thus,\nthe difference between H(p, m) and H(p) is a measure of how accurate a model is.\nBetween two models m1 and m2 , the more accurate model will be the one with the\n\n\f3.8\n\n•\n\nS UMMARY\n\n21\n\nlower cross-entropy. (The cross-entropy can never be lower than the true entropy, so\na model cannot err by underestimating the true entropy.)\nWe are finally ready to see the relation between perplexity and cross-entropy\nas we saw it in Eq. 3.40. Cross-entropy is defined in the limit as the length of the\nobserved word sequence goes to infinity. We approximate this cross-entropy by\nrelying on a (sufficiently long) sequence of fixed length. This approximation to the\ncross-entropy of a model M = P(wi |wi−N+1 : i−1 ) on a sequence of words W is\nH(W ) = −\nperplexity\n\n1\nlog P(w1 w2 . . . wN )\nN\n\n(3.42)\n\nThe perplexity of a model P on a sequence of words W is now formally defined as\n2 raised to the power of this cross-entropy:\nPerplexity(W ) = 2H(W )\n1\n\n= P(w1 w2 . . . wN )− N\ns\n1\n= N\nP(w1 w2 . . . wN )\n\n3.8\n\nSummary\nThis chapter introduced language modeling via the n-gram model, a classic model\nthat allows us to introduce many of the basic concepts in language modeling.\n• Language models offer a way to assign a probability to a sentence or other\nsequence of words or tokens, and to predict a word or token from preceding\nwords or tokens.\n• N-grams are perhaps the simplest kind of language model. They are Markov\nmodels that estimate words from a fixed window of previous words. N-gram\nmodels can be trained by counting in a training corpus and normalizing the\ncounts (the maximum likelihood estimate).\n• N-gram language models can be evaluated on a test set using perplexity.\n• The perplexity of a test set according to a language model is a function of\nthe probability of the test set: the inverse test set probability according to the\nmodel, normalized by the length.\n• Sampling from a language model means to generate some sentences, choosing each sentence according to its likelihood as defined by the model.\n• Smoothing algorithms provide a way to estimate probabilities for events that\nwere unseen in training. Commonly used smoothing algorithms for n-grams\ninclude add-1 smoothing, or rely on lower-order n-gram counts through interpolation.\n\nHistorical Notes\nThe underlying mathematics of the n-gram was first proposed by Markov (1913),\nwho used what are now called Markov chains (bigrams and trigrams) to predict\nwhether an upcoming letter in Pushkin’s Eugene Onegin would be a vowel or a consonant. Markov classified 20,000 letters as V or C and computed the bigram and\n\n\f22\n\nC HAPTER 3\n\nclass-based\nn-gram\n\n•\n\nN- GRAM L ANGUAGE M ODELS\n\ntrigram probability that a given letter would be a vowel given the previous one or\ntwo letters. Shannon (1948) applied n-grams to compute approximations to English\nword sequences. Based on Shannon’s work, Markov models were commonly used in\nengineering, linguistic, and psychological work on modeling word sequences by the\n1950s. In a series of extremely influential papers starting with Chomsky (1956) and\nincluding Chomsky (1957) and Miller and Chomsky (1963), Noam Chomsky argued\nthat “finite-state Markov processes”, while a possibly useful engineering heuristic,\nwere incapable of being a complete cognitive model of human grammatical knowledge. These arguments led many linguists and computational linguists to ignore\nwork in statistical modeling for decades.\nThe resurgence of n-gram language models came from Fred Jelinek and colleagues at the IBM Thomas J. Watson Research Center, who were influenced by\nShannon, and James Baker at CMU, who was influenced by the prior, classified\nwork of Leonard Baum and colleagues on these topics at labs like the US Institute\nfor Defense Analyses (IDA) after they were declassified. Independently these two\nlabs successfully used n-grams in their speech recognition systems at the same time\n(Baker 1975b, Jelinek et al. 1975, Baker 1975a, Bahl et al. 1983, Jelinek 1990). The\nterms “language model” and “perplexity” were first used for this technology by the\nIBM group. Jelinek and his colleagues used the term language model in a pretty\nmodern way, to mean the entire set of linguistic influences on word sequence probabilities, including grammar, semantics, discourse, and even speaker characteristics,\nrather than just the particular n-gram model itself.\nAdd-one smoothing derives from Laplace’s 1812 law of succession and was first\napplied as an engineering solution to the zero frequency problem by Jeffreys (1948)\nbased on an earlier Add-K suggestion by Johnson (1932). Problems with the addone algorithm are summarized in Gale and Church (1994).\nA wide variety of different language modeling and smoothing techniques were\nproposed in the 80s and 90s, including Good-Turing discounting—first applied to the\nn-gram smoothing at IBM by Katz (Nádas 1984, Church and Gale 1991)— WittenBell discounting (Witten and Bell, 1991), and varieties of class-based n-gram models that used information about word classes. Starting in the late 1990s, Chen and\nGoodman performed a number of carefully controlled experiments comparing different algorithms and parameters (Chen and Goodman 1999, Goodman 2006, inter\nalia). They showed the advantages of Modified Interpolated Kneser-Ney, which\nbecame the standard baseline for n-gram language modeling around the turn of the\ncentury, especially because they showed that caches and class-based models provided only minor additional improvement. SRILM (Stolcke, 2002) and KenLM\n(Heafield 2011, Heafield et al. 2013) are publicly available toolkits for building ngram language models.\nLarge language models are based on neural networks rather than n-grams, enabling them to solve the two major problems with n-grams: (1) the number of parameters increases exponentially as the n-gram order increases, and (2) n-grams have no\nway to generalize from training examples to test set examples unless they use identical words. Neural language models instead project words into a continuous space\nin which words with similar contexts have similar representations. We’ll introduce\ntransformer-based large language models in Chapter 8, along the way introducing\nfeedforward language models (Bengio et al. 2006, Schwenk 2007) in Chapter 6 and\nrecurrent language models (Mikolov, 2012) in Chapter 13.\n\n\fE XERCISES\n\n23\n\nExercises\n3.1\n\nWrite out the equation for trigram probability estimation (modifying Eq. 3.11).\nNow write out all the non-zero trigram probabilities for the I am Sam corpus\non page 4.\n\n3.2\n\nCalculate the probability of the sentence i want chinese food. Give two\nprobabilities, one using Fig. 3.2 and the ‘useful probabilities’ just below it on\npage 6, and another using the add-1 smoothed table in Fig. 3.7. Assume the\nadditional add-1 smoothed probabilities P(i|<s>) = 0.19 and P(</s>|food) =\n0.40.\n\n3.3\n\nWhich of the two probabilities you computed in the previous exercise is higher,\nunsmoothed or smoothed? Explain why.\n\n3.4\n\nWe are given the following corpus, modified from the one in the chapter:\n<s> I am Sam </s>\n<s> Sam I am </s>\n<s> I am Sam </s>\n<s> I do not like green eggs and Sam </s>\nUsing a bigram language model with add-one smoothing, what is P(Sam |\nam)? Include <s> and </s> in your counts just like any other token.\n\n3.5\n\nSuppose we didn’t use the end-symbol </s>. Train an unsmoothed bigram\ngrammar on the following training corpus without using the end-symbol </s>:\n<s> a b\n<s> b b\n<s> b a\n<s> a a\nDemonstrate that your bigram model does not assign a single probability distribution across all sentence lengths by showing that the sum of the probability\nof the four possible 2 word sentences over the alphabet {a,b} is 1.0, and the\nsum of the probability of all possible 3 word sentences over the alphabet {a,b}\nis also 1.0.\n\n3.6\n\nSuppose we train a trigram language model with add-one smoothing on a\ngiven corpus. The corpus contains V word types. Express a formula for estimating P(w3|w1,w2), where w3 is a word which follows the bigram (w1,w2),\nin terms of various n-gram counts and V. Use the notation c(w1,w2,w3) to\ndenote the number of times that trigram (w1,w2,w3) occurs in the corpus, and\nso on for bigrams and unigrams.\n\n3.7\n\nWe are given the following corpus, modified from the one in the chapter:\n<s> I am Sam </s>\n<s> Sam I am </s>\n<s> I am Sam </s>\n<s> I do not like green eggs and Sam </s>\nIf we use linear interpolation smoothing between a maximum-likelihood bigram model and a maximum-likelihood unigram model with λ1 = 12 and λ2 =\n1\n2 , what is P(Sam|am)? Include <s> and </s> in your counts just like any\nother token.\n\n3.8\n\nWrite a program to compute unsmoothed unigrams and bigrams.\n\n\f24\n\nC HAPTER 3\n3.9\n\n•\n\nN- GRAM L ANGUAGE M ODELS\nRun your n-gram program on two different small corpora of your choice (you\nmight use email text or newsgroups). Now compare the statistics of the two\ncorpora. What are the differences in the most common unigrams between the\ntwo? How about interesting differences in bigrams?\n\n3.10 Add an option to your program to generate random sentences.\n3.11 Add an option to your program to compute the perplexity of a test set.\n3.12 You are given a training set of 100 numbers that consists of 91 zeros and 1\neach of the other digits 1-9. Now we see the following test set: 0 0 0 0 0 3 0 0\n0 0. What is the unigram perplexity?\n\n\fExercises\nAlgoet, P. H. and T. M. Cover. 1988. A sandwich proof of\nthe Shannon-McMillan-Breiman theorem. The Annals of\nProbability, 16(2):899–909.\nBahl, L. R., F. Jelinek, and R. L. Mercer. 1983. A maximum likelihood approach to continuous speech recognition. IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 5(2):179–190.\nBaker, J. K. 1975a. The DRAGON system – An overview.\nIEEE Transactions on ASSP, ASSP-23(1):24–29.\nBaker, J. K. 1975b. Stochastic modeling for automatic\nspeech understanding. In D. R. Reddy, ed., Speech Recognition. Academic Press.\nBengio, Y., H. Schwenk, J.-S. Senécal, F. Morin, and J.-L.\nGauvain. 2006. Neural probabilistic language models. In\nInnovations in Machine Learning, 137–186. Springer.\nBlodgett, S. L. and B. O’Connor. 2017. Racial disparity in\nnatural language processing: A case study of social media\nAfrican-American English. FAT/ML Workshop, KDD.\nBrants, T., A. C. Popat, P. Xu, F. J. Och, and J. Dean.\n2007. Large language models in machine translation.\nEMNLP/CoNLL.\nChen, S. F. and J. Goodman. 1999. An empirical study of\nsmoothing techniques for language modeling. Computer\nSpeech and Language, 13:359–394.\nChomsky, N. 1956. Three models for the description of\nlanguage. IRE Transactions on Information Theory,\n2(3):113–124.\nChomsky, N. 1957. Syntactic Structures. Mouton.\nChurch, K. W. and W. A. Gale. 1991. A comparison of the\nenhanced Good-Turing and deleted estimation methods\nfor estimating probabilities of English bigrams. Computer Speech and Language, 5:19–54.\nCover, T. M. and J. A. Thomas. 1991. Elements of Information Theory. Wiley.\nDavies, M. 2020. The Corpus of Contemporary American English (COCA): One billion words, 1990-2019.\nhttps://www.english-corpora.org/coca/.\nFranz, A. and T. Brants. 2006.\nAll our n-gram are\nbelong to you. https://research.google/blog/\nall-our-n-gram-are-belong-to-you/.\nGale, W. A. and K. W. Church. 1994. What is wrong with\nadding one? In N. Oostdijk and P. de Haan, eds, CorpusBased Research into Language, 189–198. Rodopi.\nGoodman, J. 2006. A bit of progress in language modeling: Extended version. Technical Report MSR-TR-200172, Machine Learning and Applied Statistics Group, Microsoft Research, Redmond, WA.\nHeafield, K. 2011. KenLM: Faster and smaller language\nmodel queries. Workshop on Statistical Machine Translation.\nHeafield, K., I. Pouzyrevsky, J. H. Clark, and P. Koehn. 2013.\nScalable modified Kneser-Ney language model estimation. ACL.\nJeffreys, H. 1948. Theory of Probability, 2nd edition. Clarendon Press. Section 3.23.\nJelinek, F. 1990. Self-organized language modeling for\nspeech recognition. In A. Waibel and K.-F. Lee, eds,\nReadings in Speech Recognition, 450–506. Morgan Kaufmann. Originally distributed as IBM technical report in\n1985.\n\n25\n\nJelinek, F. and R. L. Mercer. 1980. Interpolated estimation\nof Markov source parameters from sparse data. In E. S.\nGelsema and L. N. Kanal, eds, Proceedings, Workshop\non Pattern Recognition in Practice, 381–397. North Holland.\nJelinek, F., R. L. Mercer, and L. R. Bahl. 1975. Design of a\nlinguistic statistical decoder for the recognition of continuous speech. IEEE Transactions on Information Theory,\nIT-21(3):250–256.\nJohnson, W. E. 1932. Probability: deductive and inductive\nproblems (appendix to). Mind, 41(164):421–423.\nJurafsky, D., C. Wooters, G. Tajchman, J. Segal, A. Stolcke,\nE. Fosler, and N. Morgan. 1994. The Berkeley restaurant\nproject. ICSLP.\nJurgens, D., Y. Tsvetkov, and D. Jurafsky. 2017. Incorporating dialectal variability for socially equitable language\nidentification. ACL.\nKane, S. K., M. R. Morris, A. Paradiso, and J. Campbell.\n2017. “at times avuncular and cantankerous, with the\nreflexes of a mongoose”: Understanding self-expression\nthrough augmentative and alternative communication devices. CSCW.\nKing, S. 2020. From African American Vernacular English\nto African American Language: Rethinking the study of\nrace and language in African Americans’ speech. Annual\nReview of Linguistics, 6:285–300.\nLin, Y., J.-B. Michel, E. Aiden Lieberman, J. Orwant,\nW. Brockman, and S. Petrov. 2012. Syntactic annotations\nfor the Google books NGram corpus. ACL.\nLiu, J., S. Min, L. Zettlemoyer, Y. Choi, and H. Hajishirzi.\n2024. Infini-gram: Scaling unbounded n-gram language\nmodels to a trillion tokens. ArXiv preprint.\nMarkov, A. A. 1913. Essai d’une recherche statistique sur\nle texte du roman “Eugene Onegin” illustrant la liaison\ndes epreuve en chain (‘Example of a statistical investigation of the text of “Eugene Onegin” illustrating the dependence between samples in chain’). Izvistia Imperatorskoi Akademii Nauk (Bulletin de l’Académie Impériale\ndes Sciences de St.-Pétersbourg), 7:153–162.\nMikolov, T. 2012. Statistical language models based on neural networks. Ph.D. thesis, Brno University of Technology.\nMiller, G. A. and N. Chomsky. 1963. Finitary models of language users. In R. D. Luce, R. R. Bush, and E. Galanter,\neds, Handbook of Mathematical Psychology, volume II,\n419–491. John Wiley.\nMiller, G. A. and J. A. Selfridge. 1950. Verbal context and\nthe recall of meaningful material. American Journal of\nPsychology, 63:176–185.\nNádas, A. 1984. Estimation of probabilities in the language\nmodel of the IBM speech recognition system. IEEE\nTransactions on ASSP, 32(4):859–861.\nSchwenk, H. 2007. Continuous space language models.\nComputer Speech & Language, 21(3):492–518.\nShannon, C. E. 1948. A mathematical theory of communication. Bell System Technical Journal, 27(3):379–423.\nContinued in the following volume.\nStolcke, A. 1998. Entropy-based pruning of backoff language models. Proc. DARPA Broadcast News Transcription and Understanding Workshop.\n\n\f26\n\nChapter 3\n\n•\n\nN-gram Language Models\n\nStolcke, A. 2002. SRILM – an extensible language modeling\ntoolkit. ICSLP.\nTrnka, K., D. Yarrington, J. McCaw, K. F. McCoy, and\nC. Pennington. 2007. The effects of word prediction on\ncommunication rate for AAC. NAACL-HLT.\nWitten, I. H. and T. C. Bell. 1991. The zero-frequency problem: Estimating the probabilities of novel events in adaptive text compression. IEEE Transactions on Information\nTheory, 37(4):1085–1094.\n\n\f",
    "file_path": "/Users/colinsidberry/Downloads/NLP_Textbook/n-gram.txt",
    "file_size_kb": 74.14
  },
  {
    "id": "a173660b57ac607c",
    "source": "nlp_textbook",
    "chapter": "Neural Networks",
    "filename": "neural-networks.txt",
    "content": "Speech and Language Processing. Daniel Jurafsky & James H. Martin.\nrights reserved. Draft of August 24, 2025.\n\nCopyright © 2025.\n\nAll\n\nCHAPTER\n\n6\n\nNeural Networks\n\n“[M]achines of this character can behave in a very complicated manner when\nthe number of units is large.”\nAlan Turing (1948) “Intelligent Machines”, page 6\n\nfeedforward\ndeep learning\n\nNeural networks are a fundamental computational tool for language processing, and a very old one. They are called neural because their origins lie in the\nMcCulloch-Pitts neuron (McCulloch and Pitts, 1943), a simplified model of the\nbiological neuron as a kind of computing element that could be described in terms\nof propositional logic. But the modern use in language processing no longer draws\non these early biological inspirations.\nInstead, a modern neural network is a network of small computing units, each\nof which takes a vector of input values and produces a single output value. In this\nchapter we introduce the neural net applied to classification. The architecture we\nintroduce is called a feedforward network because the computation proceeds iteratively from one layer of units to the next. The use of modern neural nets is often\ncalled deep learning, because modern networks are often deep (have many layers).\nNeural networks share much of the same mathematics as logistic regression. But\nneural networks are a more powerful classifier than logistic regression, and indeed a\nminimal neural network (technically one with a single ‘hidden layer’) can be shown\nto learn any function.\nNeural net classifiers are different from logistic regression in another way. With\nlogistic regression, we applied the regression classifier to many different tasks by\ndeveloping many rich kinds of feature templates based on domain knowledge. When\nworking with neural networks, it is more common to avoid most uses of rich handderived features, instead building neural networks that take raw tokens as inputs\nand learn to induce features as part of the process of learning to classify. We saw\nexamples of this kind of representation learning for embeddings in Chapter 5, and\nwe’ll see lots of examples once we start studying deep transformers networks. Nets\nthat are very deep are particularly good at representation learning. For that reason\ndeep neural nets are the right tool for tasks that offer sufficient data to learn features\nautomatically.\nIn this chapter we’ll introduce feedforward networks as classifiers, first with\nhand-built features, and then using the embeddings that we studied in Chapter 5.\nIn subsequent chapters we’ll introduce many other kinds of neural models, most\nimportantly the transformer and attention, (Chapter 8), but also recurrent neural\nnetworks (Chapter 13) and convolutional neural networks (Chapter 15). And in\nthe next chapter we’ll introduce the paradigm of neural large language models.\n\n\f2\n\nC HAPTER 6\n\n6.1\n\n•\n\nN EURAL N ETWORKS\n\nUnits\n\nbias term\n\nThe building block of a neural network is a single computational unit. A unit takes\na set of real valued numbers as input, performs some computation on them, and\nproduces an output.\nAt its heart, a neural unit is taking a weighted sum of its inputs, with one additional term in the sum called a bias term. Given a set of inputs x1 ...xn , a unit has\na set of corresponding weights w1 ...wn and a bias b, so the weighted sum z can be\nrepresented as:\nX\nz = b+\nwi xi\n(6.1)\ni\n\nvector\n\nOften it’s more convenient to express this weighted sum using vector notation; recall\nfrom linear algebra that a vector is, at heart, just a list or array of numbers. Thus\nwe’ll talk about z in terms of a weight vector w, a scalar bias b, and an input vector\nx, and we’ll replace the sum with the convenient dot product:\nz = w·x+b\n\nactivation\n\n(6.2)\n\nAs defined in Eq. 6.2, z is just a real valued number.\nFinally, instead of using z, a linear function of x, as the output, neural units\napply a non-linear function f to z. We will refer to the output of this function as\nthe activation value for the unit, a. Since we are just modeling a single unit, the\nactivation for the node is in fact the final output of the network, which we’ll generally\ncall y. So the value y is defined as:\ny = a = f (z)\n\nsigmoid\n\nWe’ll discuss three popular non-linear functions f below (the sigmoid, the tanh, and\nthe rectified linear unit or ReLU) but it’s pedagogically convenient to start with the\nsigmoid function since we saw it in Chapter 4:\n1\n(6.3)\n1 + e−z\nThe sigmoid (shown in Fig. 6.1) has a number of advantages; it maps the output\ninto the range (0, 1), which is useful in squashing outliers toward 0 or 1. And it’s\ndifferentiable, which as we saw in Section ?? will be handy for learning.\ny = σ (z) =\n\nFigure 6.1 The sigmoid function takes a real value and maps it to the range (0, 1). It is\nnearly linear around 0 but outlier values get squashed toward 0 or 1.\n\nSubstituting Eq. 6.2 into Eq. 6.3 gives us the output of a neural unit:\ny = σ (w · x + b) =\n\n1\n1 + exp(−(w · x + b))\n\n(6.4)\n\n\f6.1\n\n•\n\nU NITS\n\n3\n\nFig. 6.2 shows a final schematic of a basic neural unit. In this example the unit\ntakes 3 input values x1 , x2 , and x3 , and computes a weighted sum, multiplying each\nvalue by a weight (w1 , w2 , and w3 , respectively), adds them to a bias term b, and then\npasses the resulting sum through a sigmoid function to result in a number between 0\nand 1.\n\nx1\nx2\nx3\n\nw1\nw2\n\n∑\n\nw3\n\nz\n\nσ\n\na\n\ny\n\nb\n\n+1\n\nFigure 6.2 A neural unit, taking 3 inputs x1 , x2 , and x3 (and a bias b that we represent as a\nweight for an input clamped at +1) and producing an output y. We include some convenient\nintermediate variables: the output of the summation, z, and the output of the sigmoid, a. In\nthis case the output of the unit y is the same as a, but in deeper networks we’ll reserve y to\nmean the final output of the entire network, leaving a as the activation of an individual node.\n\nLet’s walk through an example just to get an intuition. Let’s suppose we have a\nunit with the following weight vector and bias:\nw = [0.2, 0.3, 0.9]\nb = 0.5\nWhat would this unit do with the following input vector:\nx = [0.5, 0.6, 0.1]\nThe resulting output y would be:\ny = σ (w · x + b) =\n\ntanh\n\n1\n1 + e−(w·x+b)\n\n=\n\n1\n1 + e−(.5∗.2+.6∗.3+.1∗.9+.5)\n\n1\n= .70\n1 + e−0.87\n\nIn practice, the sigmoid is not commonly used as an activation function. A function\nthat is very similar but almost always better is the tanh function shown in Fig. 6.3a;\ntanh is a variant of the sigmoid that ranges from -1 to +1:\ny = tanh(z) =\n\nReLU\n\n=\n\nez − e−z\nez + e−z\n\n(6.5)\n\nThe simplest activation function, and perhaps the most commonly used, is the rectified linear unit, also called the ReLU, shown in Fig. 6.3b. It’s just the same as z\nwhen z is positive, and 0 otherwise:\ny = ReLU(z) = max(z, 0)\n\n(6.6)\n\nThese activation functions have different properties that make them useful for different language applications or network architectures. For example, the tanh function\nhas the nice properties of being smoothly differentiable and mapping outlier values\ntoward the mean. The rectifier function, on the other hand, has nice properties that\n\n\f4\n\nC HAPTER 6\n\n•\n\nN EURAL N ETWORKS\n\n(a)\nFigure 6.3\n\nThe tanh and ReLU activation functions.\n\nsaturated\n\nvanishing\ngradient\n\n6.2\n\n(b)\n\nresult from it being very close to linear. In the sigmoid or tanh functions, very high\nvalues of z result in values of y that are saturated, i.e., extremely close to 1, and have\nderivatives very close to 0. Zero derivatives cause problems for learning, because as\nwe’ll see in Section 6.6, we’ll train networks by propagating an error signal backwards, multiplying gradients (partial derivatives) from each layer of the network;\ngradients that are almost 0 cause the error signal to get smaller and smaller until it is\ntoo small to be used for training, a problem called the vanishing gradient problem.\nRectifiers don’t have this problem, since the derivative of ReLU for high values of z\nis 1 rather than very close to 0.\n\nThe XOR problem\nEarly in the history of neural networks it was realized that the power of neural networks, as with the real neurons that inspired them, comes from combining these\nunits into larger networks.\nOne of the most clever demonstrations of the need for multi-layer networks was\nthe proof by Minsky and Papert (1969) that a single neural unit cannot compute\nsome very simple functions of its input. Consider the task of computing elementary\nlogical functions of two inputs, like AND, OR, and XOR. As a reminder, here are\nthe truth tables for those functions:\nAND\nx1 x2 y\n0 0 0\n0 1 0\n1 0 0\n1 1 1\n\nperceptron\n\nOR\nx1 x2 y\n0 0 0\n0 1 1\n1 0 1\n1 1 1\n\nXOR\nx1 x2 y\n0 0 0\n0 1 1\n1 0 1\n1 1 0\n\nThis example was first shown for the perceptron, which is a very simple neural\nunit that has a binary output and has a very simple step function as its non-linear\nactivation function. The output y of a perceptron is 0 or 1, and is computed as\nfollows (using the same weight w, input x, and bias b as in Eq. 6.2):\n\u001a\n0, if w · x + b ≤ 0\ny=\n(6.7)\n1, if w · x + b > 0\n\n\f6.2\n\n•\n\nT HE XOR PROBLEM\n\n5\n\nIt’s very easy to build a perceptron that can compute the logical AND and OR\nfunctions of its binary inputs; Fig. 6.4 shows the necessary weights.\n\nx1\nx2\n+1\n\n1\n1\n-1\n\n(a)\n\nx1\nx2\n\n1\n1\n0\n\n+1\n(b)\n\nFigure 6.4 The weights w and bias b for perceptrons for computing logical functions. The\ninputs are shown as x1 and x2 and the bias as a special node with value +1 which is multiplied\nwith the bias weight b. (a) logical AND, with weights w1 = 1 and w2 = 1 and bias weight\nb = −1. (b) logical OR, with weights w1 = 1 and w2 = 1 and bias weight b = 0. These\nweights/biases are just one from an infinite number of possible sets of weights and biases that\nwould implement the functions.\n\ndecision\nboundary\n\nlinearly\nseparable\n\nIt turns out, however, that it’s not possible to build a perceptron to compute\nlogical XOR! (It’s worth spending a moment to give it a try!)\nThe intuition behind this important result relies on understanding that a perceptron is a linear classifier. For a two-dimensional input x1 and x2 , the perceptron\nequation, w1 x1 + w2 x2 + b = 0 is the equation of a line. (We can see this by putting\nit in the standard linear format: x2 = (−w1 /w2 )x1 + (−b/w2 ).) This line acts as a\ndecision boundary in two-dimensional space in which the output 0 is assigned to all\ninputs lying on one side of the line, and the output 1 to all input points lying on the\nother side of the line. If we had more than 2 inputs, the decision boundary becomes\na hyperplane instead of a line, but the idea is the same, separating the space into two\ncategories.\nFig. 6.5 shows the possible logical inputs (00, 01, 10, and 11) and the line drawn\nby one possible set of parameters for an AND and an OR classifier. Notice that there\nis simply no way to draw a line that separates the positive cases of XOR (01 and 10)\nfrom the negative cases (00 and 11). We say that XOR is not a linearly separable\nfunction. Of course we could draw a boundary with a curve, or some other function,\nbut not a single line.\n\n6.2.1\n\nThe solution: neural networks\n\nWhile the XOR function cannot be calculated by a single perceptron, it can be calculated by a layered network of perceptron units. Rather than see this with networks\nof simple perceptrons, however, let’s see how to compute XOR using two layers of\nReLU-based units following Goodfellow et al. (2016). Fig. 6.6 shows a figure with\nthe input being processed by two layers of neural units. The middle layer (called\nh) has two units, and the output layer (called y) has one unit. A set of weights and\nbiases are shown that allows the network to correctly compute the XOR function.\nLet’s walk through what happens with the input x = [0, 0]. If we multiply each\ninput value by the appropriate weight, sum, and then add the bias b, we get the vector\n[0, -1], and we then apply the rectified linear transformation to give the output of the\nh layer as [0, 0]. Now we once again multiply by the weights, sum, and add the\nbias (0 in this case) resulting in the value 0. The reader should work through the\ncomputation of the remaining 3 possible input pairs to see that the resulting y values\nare 1 for the inputs [0, 1] and [1, 0] and 0 for [0, 0] and [1, 1].\n\n\f6\n\nC HAPTER 6\n\n•\n\nN EURAL N ETWORKS\n\nx2\n\nx2\n\n1\n\nx2\n\n1\n\n1\n\n?\n0\n0\n\n1\n\nx1\n\n0\n0\n\n1\n\na) x1 AND x2\n\n0\n\nx1\n\n0\n\nb) x1 OR x2\n\n1\n\nx1\n\nc) x1 XOR x2\n\nFigure 6.5 The functions AND, OR, and XOR, represented with input x1 on the x-axis and input x2 on the\ny-axis. Filled circles represent perceptron outputs of 1, and white circles perceptron outputs of 0. There is no\nway to draw a line that correctly separates the two categories for XOR. Figure styled after Russell and Norvig\n(2002).\n\nx1\n\n1\n\nh1\n1\n\n1\n\ny1\n\n1\n\nx2\n+1\n\n1\n0\n\n-2\nh2\n\n0\n\n-1\n\n+1\n\nFigure 6.6 XOR solution after Goodfellow et al. (2016). There are three ReLU units, in\ntwo layers; we’ve called them h1 , h2 (h for “hidden layer”) and y1 . As before, the numbers\non the arrows represent the weights w for each unit, and we represent the bias b as a weight\non a unit clamped to +1, with the bias weights/units in gray.\n\nIt’s also instructive to look at the intermediate results, the outputs of the two\nhidden nodes h1 and h2 . We showed in the previous paragraph that the h vector for\nthe inputs x = [0, 0] was [0, 0]. Fig. 6.7b shows the values of the h layer for all\n4 inputs. Notice that hidden representations of the two input points x = [0, 1] and\nx = [1, 0] (the two cases with XOR output = 1) are merged to the single point h =\n[1, 0]. The merger makes it easy to linearly separate the positive and negative cases\nof XOR. In other words, we can view the hidden layer of the network as forming a\nrepresentation of the input.\nIn this example we just stipulated the weights in Fig. 6.6. But for real examples\nthe weights for neural networks are learned automatically using the error backpropagation algorithm to be introduced in Section 6.6. That means the hidden layers will\nlearn to form useful representations. This intuition, that neural networks can automatically learn useful representations of the input, is one of their key advantages,\nand one that we will return to again and again in later chapters.\n\n\f6.3\n\n•\n\nF EEDFORWARD N EURAL N ETWORKS\n\nx2\n\n7\n\nh2\n\n1\n\n1\n\n0\n0\n\n1\n\nx1\n\na) The original x space\n\n0\n0\n\n1\n\n2\n\nh1\n\nb) The new (linearly separable) h space\n\nFigure 6.7 The hidden layer forming a new representation of the input. (b) shows the\nrepresentation of the hidden layer, h, compared to the original input representation x in (a).\nNotice that the input point [0, 1] has been collapsed with the input point [1, 0], making it\npossible to linearly separate the positive and negative cases of XOR. After Goodfellow et al.\n(2016).\n\n6.3\n\nFeedforward Neural Networks\n\nfeedforward\nnetwork\n\nmulti-layer\nperceptrons\nMLP\n\nhidden layer\n\nfully-connected\n\nLet’s now walk through a slightly more formal presentation of the simplest kind of\nneural network, the feedforward network. A feedforward network is a multilayer\nnetwork in which the units are connected with no cycles; the outputs from units in\neach layer are passed to units in the next higher layer, and no outputs are passed\nback to lower layers. (In Chapter 13 we’ll introduce networks with cycles, called\nrecurrent neural networks.)\nFor historical reasons multilayer networks, especially feedforward networks, are\nsometimes called multi-layer perceptrons (or MLPs); this is a technical misnomer,\nsince the units in modern multilayer networks aren’t perceptrons (perceptrons have a\nsimple step-function as their activation function, but modern networks are made up\nof units with many kinds of non-linearities like ReLUs and sigmoids), but at some\npoint the name stuck.\nSimple feedforward networks have three kinds of nodes: input units, hidden\nunits, and output units.\nFig. 6.8 shows a picture. The input layer x is a vector of simple scalar values just\nas we saw in Fig. 6.2.\nThe core of the neural network is the hidden layer h formed of hidden units hi ,\neach of which is a neural unit as described in Section 6.1, taking a weighted sum of\nits inputs and then applying a non-linearity. In the standard architecture, each layer\nis fully-connected, meaning that each unit in each layer takes as input the outputs\nfrom all the units in the previous layer, and there is a link between every pair of units\nfrom two adjacent layers. Thus each hidden unit sums over all the input units.\nRecall that a single hidden unit has as parameters a weight vector and a bias. We\nrepresent the parameters for the entire hidden layer by combining the weight vector\nand bias for each unit i into a single weight matrix W and a single bias vector b for\nthe whole layer (see Fig. 6.8). Each element W ji of the weight matrix W represents\nthe weight of the connection from the ith input unit xi to the jth hidden unit h j .\nThe advantage of using a single matrix W for the weights of the entire layer is\nthat now the hidden layer computation for a feedforward network can be done very\n\n\f8\n\nC HAPTER 6\n\n•\n\nN EURAL N ETWORKS\n\nx1\n\nW\n\nU\nh1\n\n…\n\nh3\n\nxn\n\nhn\n\n0\n\n+1\ninput layer\n\nb\n\n1\n\ny2\n\n…\n\nh2\n\n…\n\nx2\n\ny1\n\nyn\n\n2\n\nhidden layer\n\noutput layer\n\nFigure 6.8 A simple 2-layer feedforward network, with one hidden layer, one output layer,\nand one input layer (the input layer is usually not counted when enumerating layers).\n\nefficiently with simple matrix operations. In fact, the computation only has three\nsteps: multiplying the weight matrix by the input vector x, adding the bias vector b,\nand applying the activation function g (such as the sigmoid, tanh, or ReLU activation\nfunction defined above).\nThe output of the hidden layer, the vector h, is thus the following (for this example we’ll use the sigmoid function σ as our activation function):\nh = σ (Wx + b)\n\n(6.8)\n\nNotice that we’re applying the σ function here to a vector, while in Eq. 6.3 it was\napplied to a scalar. We’re thus allowing σ (·), and indeed any activation function\ng(·), to apply to a vector element-wise, so g[z1 , z2 , z3 ] = [g(z1 ), g(z2 ), g(z3 )].\nLet’s introduce some constants to represent the dimensionalities of these vectors\nand matrices. We’ll refer to the input layer as layer 0 of the network, and have\nn0 represent the number of inputs, so x is a vector of real numbers of dimension\nn0 , or more formally x ∈ Rn0 , a column vector of dimensionality [n0 × 1]. Let’s\ncall the hidden layer layer 1 and the output layer layer 2. The hidden layer has\ndimensionality n1 , so h ∈ Rn1 and also b ∈ Rn1 (since each hidden unit can take a\ndifferent bias value). And the weight matrix W has dimensionality W ∈ Rn1 ×n0 , i.e.\n[n1 × n0 ].\nTake a moment to convince yourself\n\u0001 multiplication in Eq. 6.8 will\nPn0 that the matrix\ncompute the value of each h j as σ\nW\nx\n+\nb\n.\nji\ni\nj\ni=1\nAs we saw in Section 6.2, the resulting value h (for hidden but also for hypothesis) forms a representation of the input. The role of the output layer is to take\nthis new representation h and compute a final output. This output could be a realvalued number, but in many cases the goal of the network is to make some sort of\nclassification decision, and so we will focus on the case of classification.\nIf we are doing a binary task like sentiment classification, we might have a single output node, and its scalar value y is the probability of positive versus negative\nsentiment. If we are doing multinomial classification, such as assigning a part-ofspeech tag, we might have one output node for each potential part-of-speech, whose\noutput value is the probability of that part-of-speech, and the values of all the output\nnodes must sum to one. The output layer is thus a vector y that gives a probability\ndistribution across the output nodes.\n\n\f6.3\n\n•\n\nF EEDFORWARD N EURAL N ETWORKS\n\n9\n\nLet’s see how this happens. Like the hidden layer, the output layer has a weight\nmatrix (let’s call it U), but some models don’t include a bias vector b in the output\nlayer, so we’ll simplify by eliminating the bias vector in this example. The weight\nmatrix is multiplied by its input vector (h) to produce the intermediate output z:\nz = Uh\n\nnormalizing\n\nsoftmax\n\nThere are n2 output nodes, so z ∈ Rn2 , weight matrix U has dimensionality U ∈\nRn2 ×n1 , and element Ui j is the weight from unit j in the hidden layer to unit i in the\noutput layer.\nHowever, z can’t be the output of the classifier, since it’s a vector of real-valued\nnumbers, while what we need for classification is a vector of probabilities. There is\na convenient function for normalizing a vector of real values, by which we mean\nconverting it to a vector that encodes a probability distribution (all the numbers lie\nbetween 0 and 1 and sum to 1): the softmax function that we saw on page ?? of\nChapter 4. More generally for any vector z of dimensionality d, the softmax is\ndefined as:\nexp(zi )\n1≤i≤d\n(6.9)\nsoftmax(zi ) = Pd\nj=1 exp(z j )\nThus for example given a vector\nz = [0.6, 1.1, −1.5, 1.2, 3.2, −1.1],\n\n(6.10)\n\nthe softmax function will normalize it to a probability distribution (shown rounded):\nsoftmax(z) = [0.055, 0.090, 0.0067, 0.10, 0.74, 0.010]\n\n(6.11)\n\nYou may recall that we used softmax to create a probability distribution from a\nvector of real-valued numbers (computed from summing weights times features) in\nthe multinomial version of logistic regression in Chapter 4.\nThat means we can think of a neural network classifier with one hidden layer\nas building a vector h which is a hidden layer representation of the input, and then\nrunning standard multinomial logistic regression on the features that the network\ndevelops in h. By contrast, in Chapter 4 the features were mainly designed by hand\nvia feature templates. So a neural network is like multinomial logistic regression,\nbut (a) with many layers, since a deep neural network is like layer after layer of logistic regression classifiers; (b) with those intermediate layers having many possible\nactivation functions (tanh, ReLU, sigmoid) instead of just sigmoid (although we’ll\ncontinue to use σ for convenience to mean any activation function); (c) rather than\nforming the features by feature templates, the prior layers of the network induce the\nfeature representations themselves.\nHere are the final equations for a feedforward network with a single hidden layer,\nwhich takes an input vector x, outputs a probability distribution y, and is parameterized by weight matrices W and U and a bias vector b:\nh = σ (Wx + b)\nz = Uh\ny = softmax(z)\n\n(6.12)\n\nAnd just to remember the shapes of all our variables, x ∈ Rn0 , h ∈ Rn1 , b ∈ Rn1 ,\nW ∈ Rn1 ×n0 , U ∈ Rn2 ×n1 , and the output vector y ∈ Rn2 . We’ll call this network a 2layer network (we traditionally don’t count the input layer when numbering layers,\nbut do count the output layer). So by this terminology logistic regression is a 1-layer\nnetwork.\n\n\f10\n\nC HAPTER 6\n\n•\n\n6.3.1\n\nN EURAL N ETWORKS\n\nMore details on feedforward networks\n\nLet’s now set up some notation to make it easier to talk about deeper networks of\ndepth more than 2. We’ll use superscripts in square brackets to mean layer numbers, starting at 0 for the input layer. So W[1] will mean the weight matrix for the\n(first) hidden layer, and b[1] will mean the bias vector for the (first) hidden layer. n j\nwill mean the number of units at layer j. We’ll use g(·) to stand for the activation\nfunction, which will tend to be ReLU or tanh for intermediate layers and softmax\nfor output layers. We’ll use a[i] to mean the output from layer i, and z[i] to mean the\ncombination of previous layer output, weights and biases W[i] a[i−1] + b[i] . The 0th\nlayer is for inputs, so we’ll refer to the inputs x more generally as a[0] .\nThus we can re-represent our 2-layer net from Eq. 6.12 as follows:\nz[1] = W[1] a[0] + b[1]\na[1] = g[1] (z[1] )\nz[2] = W[2] a[1] + b[2]\na[2] = g[2] (z[2] )\nŷ = a[2]\n\n(6.13)\n\nNote that with this notation, the equations for the computation done at each layer are\nthe same. The algorithm for computing the forward step in an n-layer feedforward\nnetwork, given the input vector a[0] is thus simply:\nfor i in 1,...,n\nz[i] = W[i] a[i−1] + b[i]\na[i] = g[i] (z[i] )\nŷ = a[n]\n\nlogits\n\nIt’s often useful to have a name for the final set of activations right before the final\nsoftmax. So however many layers we have, we’ll generally call the unnormalized\nvalues in the final vector z[n] , the vector of scores right before the final softmax, the\nlogits (see Eq. ??).\nThe need for non-linear activation functions One of the reasons we use nonlinear activation functions for each layer in a neural network is that if we did not, the\nresulting network is exactly equivalent to a single-layer network. Let’s see why this\nis true. Imagine the first two layers of such a network of purely linear layers:\nz[1] = W[1] x + b[1]\nz[2] = W[2] z[1] + b[2]\nWe can rewrite the function that the network is computing as:\nz[2] = W[2] z[1] + b[2]\n= W[2] (W[1] x + b[1] ) + b[2]\n= W[2] W[1] x + W[2] b[1] + b[2]\n= W 0 x + b0\n\n(6.14)\n\nThis generalizes to any number of layers. So without non-linear activation functions,\na multilayer network is just a notational variant of a single layer network with a\ndifferent set of weights, and we lose all the representational power of multilayer\nnetworks.\n\n\f6.4\n\n•\n\n11\n\nF EEDFORWARD NETWORKS FOR NLP: C LASSIFICATION\n\nReplacing the bias unit In describing networks, we will sometimes use a slightly\nsimplified notation that represents exactly the same function without referring to an\nexplicit bias node b. Instead, we add a dummy node a0 to each layer whose value\n[0]\nwill always be 1. Thus layer 0, the input layer, will have a dummy node a0 = 1,\n[1]\n\nlayer 1 will have a0 = 1, and so on. This dummy node still has an associated weight,\nand that weight represents the bias value b. For example instead of an equation like\nh = σ (Wx + b)\n\n(6.15)\n\nh = σ (Wx)\n\n(6.16)\n\nwe’ll use:\n\nBut now instead of our vector x having n0 values: x = x1 , . . . , xn0 , it will have n0 +\n1 values, with a new 0th dummy value x0 = 1: x = x0 , . . . , xn0 . And instead of\ncomputing each h j as follows:\n!\nn0\nX\nhj = σ\nWji xi + b j ,\n(6.17)\ni=1\n\nwe’ll instead use:\nn0\nX\n\nhj = σ\n\n!\nWji xi ,\n\n(6.18)\n\ni=0\n\nwhere the value Wj0 replaces what had been b j . Fig. 6.9 shows a visualization.\n\nW\n\nh2\n\nx2\n\n+1\n\n1\n\nb\n\ny2\n\nyn\n\n2\n\nW\nh1\n\ny1\n\nx1\n\nh2\n\ny2\n\nx2\n\nh3\n\nxn\n\nhn\n\n0\n\n(a)\nFigure 6.9\n\nU\n\n1\n\n…\n\nhn\n\n0\n\nx0=1\n\n…\n\nxn\n\ny1\n\n…\n\n…\n\n…\n\nh3\n\nU\n\n…\n\nx1\n\nh1\n\nyn\n\n2\n\n(b)\n\nReplacing the bias node (shown in a) with x0 (b).\n\nWe’ll continue showing the bias as b when we go over the learning algorithm\nin Section 6.6, but going forward in the book, for most figures and some equations\nwe’ll use this simplified notation without explicit bias terms.\n\n6.4\n\nFeedforward networks for NLP: Classification\nLet’s see how to apply feedforward networks to NLP classification tasks. In practice,\nsimple feedforward networks aren’t the way we do text classification; for real applications we would use more sophisticated architectures like the BERT transformers\n\n\f12\n\nC HAPTER 6\n\n•\n\nN EURAL N ETWORKS\n\nof Chapter 10. Nonetheless seeing a feedforward network text classifier will let us\nintroduce key ideas that will play a role throughout the rest of the book, including the ideas of the embedding matrix, representation pooling, and representation\nlearning.\nBut before introducing any of these ideas, let’s start with a classifier by making\nonly minimal change from the sentiment classifiers we saw in Chapter 4. Like them,\nwe’ll take hand-built features, pass them through a classifier, and produce a class\nprobability. The only difference is that we’ll use a neural network instead of logistic\nregression as the classifier.\n\n6.4.1\n\nNeural net classifiers with hand-built features\n\nLet’s begin with a simple 2-layer sentiment classifier by taking our logistic regression classifier from Chapter 4, which corresponds to a 1-layer network, and just\nadding a hidden layer. The input element xi can be scalar features like those in\nFig. ??, e.g., x1 = count(words ∈ doc), x2 = count(positive lexicon words ∈ doc),\nx3 = 1 if “no” ∈ doc, and so on, for a total of d features. And the output layer\nŷ could have two nodes (one each for positive and negative), or 3 nodes (positive,\nnegative, neutral), in which case ŷ1 would be the estimated probability of positive\nsentiment, ŷ2 the probability of negative and ŷ3 the probability of neutral. The resulting equations would be just what we saw above for a 2-layer network (as always,\nwe’ll continue to use the σ to stand for any non-linearity, whether sigmoid, ReLU\nor other).\nx = [x1 , x2 , ...xd ] (each xi is a hand-designed feature)\nh = σ (Wx + b)\nz = Uh\nŷ = softmax(z)\n\n(6.19)\n\nFig. 6.10 shows a sketch of this architecture. As we mentioned earlier, adding this\nhidden layer to our logistic regression classifier allows the network to represent the\nnon-linear interactions between features. This alone might give us a better sentiment\nclassifier.\n\nwas\ngreat\nInput words\n\nwordcount\n=3\n\nh1\n\nx1\n\npositive lexicon x\n2\nwords = 1\ncount of “no”\n=0\n\nx3\n\nh2\n\ny^1\n\np(+)\n\nh3\n\ny^ 2\n\np(-)\n\n…\n\ndessert\n\ny^3\n\np(neut)\n\nhdh\n\nx\n\nW\n\n[d⨉1]\n\n[dh⨉d]\n\nInput layer\nd=3 features\n\nh\n\n[dh⨉1]\n\nU\n\ny\n\n[3⨉dh]\n\n[3⨉1]\n\nHidden layer\n\nOutput layer\nsoftmax\n\nFigure 6.10 Feedforward network sentiment analysis using traditional hand-built features\nof the input text.\n\n\f6.5\n\n6.4.2\n\n•\n\nE MBEDDINGS AS THE INPUT TO NEURAL NET CLASSIFIERS\n\n13\n\nVectorizing for parallelizing inference\n\nWhile Eq. 6.19 shows how to classify a single example x, in practice we want to\nefficiently classify an entire test set of m examples. We do this by vectorizing the\nprocess, just as we saw with logistic regression; instead of using for-loops to go\nthrough each example, we’ll use matrix multiplication to do the entire computation\nof an entire test set at once. First, we pack all the input feature vectors for each input\nx into a single input matrix X, with each row i a row vector consisting of the features\nfor input example x(i) (i.e., the vector x(i) ). If the dimensionality of our input feature\nvector is d, X will be a matrix of shape [m × d].\nBecause we are now modeling each input as a row vector rather than a column\nvector, we also need to slightly modify Eq. 6.19. X is of shape [m × d] and W is of\nshape [dh × d], so we’ll reorder how we multiply X and W and transpose W so they\ncorrectly multiply to yield a matrix H of shape [m × dh ]. 1\nThe bias vector b from Eq. 6.19 of shape [1 × dh ] will now have to be replicated\ninto a matrix of shape [m × dh ]. We’ll need to similarly reorder the next step and\ntranspose U. Finally, our output matrix Ŷ will be of shape [m × 3] (or more generally [m × do ], where do is the number of output classes), with each row i of our\noutput matrix Ŷ consisting of the output vector ŷ(i) . Here are the final equations for\ncomputing the output class distribution for an entire test set:\nH = σ (XW| + b)\nZ = HU|\nŶ = softmax(Z)\n\n(6.20)\n\nIn this book, we’ll sometimes see orderings like WX + b and sometimes XW + b.\nThat’s why it’s always important to be very aware of the shapes of your weight\nmatrices participating in any given equation.\n\n6.5\n\nEmbeddings as the input to neural net classifiers\nWhile hand-built features are a traditional way to design classifiers, most applications of neural networks for NLP don’t use hand-built human-engineered features as\ninputs. Instead, we draw on deep learning’s ability to learn features from the data by\nrepresenting tokens as embeddings. For this section we’ll represent each token by\nits static word2vec or GloVe embeddings that we saw how to compute in Chapter 5.\nBy static embedding, we mean that each token is represented by a fixed vector that\nwe train once, and then just put into a big dictionary. When we want to refer to that\ntoken, we grab its embedding out of the dictionary.\nHowever when we apply neural models to the task of language modeling (as\nwe’ll see in Chapter 8) the situation is more complex, and we’ll use a more powerful kind of embedding called a contextual embedding. Contextual embeddings are\ndifferent for each time a word occurs in a different context. Furthermore, we’ll have\nthe network learn these embeddings as part of the task of word prediction.\nSo let’s explore the text classification domain above, but using static embeddings\nas features instead of the hand-designed features. Let’s focus on the inference stage,\n1\n\nNote that we could have kept the original order of our products if we had instead made our input\nmatrix X represent each input as a column vector instead of a row vector, making it of shape [d × m]. But\nrepresenting inputs as row vectors is convenient and common in neural network models.\n\n\f14\n\nC HAPTER 6\n\nembedding\nmatrix\n\none-hot vector\n\n•\n\nN EURAL N ETWORKS\n\nin which we have already learned embeddings for all the input tokens. An embedding is a vector of dimension d that represents the input token. The dictionary of\nstatic embeddings in which we store these embeddings is the embedding matrix\nE. Each row of the embedding matrix represents each token of the vocabulary V\nas a (row) vector of dimensionality d. Since E has a row for each of the |V | tokens in the vocabulary, E has shape [|V | × d]. This embedding matrix E plays a role\nwhenever we are using embeddings as input to neural NLP systems, including in the\ntransformer-based large language models we will introduce over the next chapters.\nGiven an input token string like dessert was great we first convert the tokens\ninto vocabulary indices (these were created when we first tokenized the input using\nBPE or SentencePiece). So the representation of dessert was great might be\nw = [3, 9824, 226]. Next we use indexing to select the corresponding rows from E\n(row 3, row 4000, row 10532).\nAnother way to think about selecting token embeddings from the embedding\nmatrix is to represent input tokens as one-hot vectors of shape [1 × |V |], i.e., with\none dimension for each word in the vocabulary. Recall that in a one-hot vector all\nthe elements are 0 except one, the element whose dimension is the word’s index\nin the vocabulary, which has value 1. So if the word “dessert” has index 3 in the\nvocabulary, x3 = 1, and xi = 0 ∀i 6= 3, as shown here:\n[0 0 1 0 0 0 0 ... 0 0 0 0]\n1 2 3 4 5 6 7 ... ... |V|\nMultiplying by a one-hot vector that has only one non-zero element xi = 1 simply\nselects out the relevant row vector for word i, resulting in the embedding for word i,\nas depicted in Fig. 6.11.\nd\n3\n1\n\n|V|\n\n0010000…0000\n\n3\n\n✕\n\n=\n\nE\n\n1\n\nd\n\n|V|\n\nFigure 6.11 Selecting the embedding vector for word V3 by multiplying the embedding\nmatrix E with a one-hot vector with a 1 in index 3.\n\nWe can extend this idea to represent the entire input token sequence as a matrix\nof one-hot vectors, one for each of the N input positions as shown in Fig. 6.12.\nd\n|V|\n0010000…0000\n0000000…0010\n1000000…0000\n\n…\n\nN\n\n0000100…0000\n\nd\n\nE\n\n✕\n\n=\nN\n\n| V|\n\nFigure 6.12 Selecting the embedding matrix for the input sequence of token ids W by multiplying a one-hot matrix corresponding to W by the embedding matrix E.\n\nWe now need to classify this input of N [1 × d] embeddings, representing a window of N tokens, into a single class (like positive or negative).\nThere are two common ways to to pass embeddings to a classifier: concatenation and pooling. First, we can take this input of shape [N × d] and reshape it\n\n\f6.5\n\npool\n\nmean-pooling\n\n•\n\nE MBEDDINGS AS THE INPUT TO NEURAL NET CLASSIFIERS\n\n15\n\nby concatenating all the input vectors into one very long vector of shape [1 × dN].\nThen we pass this input to our classifier and let it make its decision. This gives\nus lots of information, at the cost of using a pretty large network. Second, we can\npool the N embeddings into a single embedding and then pass that single pooled\nembedding to the classifier. Pooling gives us less information than would have been\npresent in all the original embeddings, but has the advantage of being small and efficient and is especially useful in tasks for which we don’t care as much about the\noriginal word order. Let’s give an example of each: pooling for the sentiment task,\nand concatenation for the language modeling task.\nPooling input embeddings for sentiment So let’s begin with seeing how pooling\ncan work for the sentiment classification task. The intuition of pooling is that for\nsentiment, the exact position of the input (is some word like great the first word?\nthe second word?) is less important than the identity of the word itself.\nA pooling function is a way to turn a set of embeddings into a single embedding.\nFor example, for a text with N input words/tokens w1 , ..., wN , we want to turn\nthe N row embeddings e(w1 ), ..., e(wN ) (each of dimensionality d) into a single\nembedding also of dimensionality d.\nThere are various ways to pool. The simplest is mean-pooling: taking the mean\nby summing the embeddings and then dividing by N:\nN\n\nxmean =\n\n1X\ne(wi )\nN\n\n(6.21)\n\ni=1\n\nHere are the equations for this classifier assuming mean pooling:\nx = mean(e(w1 ), e(w2 ), . . . , e(wn ))\nh = σ (xW + b)\nz = hU\nŷ = softmax(z)\n\nmax-pooling\n\n(6.22)\n\nThe architecture is sketched in Fig. 6.13, where we also give the shapes for all the\nrelevant matrices.\nThere are many other options for pooling, like max-pooling, in which case for\neach dimension we take the element-wise max over all the inputs. The element-wise\nmax of a set of N vectors is a new vector whose kth element is the max of the kth\nelements of all the N vectors.\nConcatenating input embeddings for language modeling For sentiment analysis we saw how to generate an output vector with probabilities over three classes:\npositive, negative, or neutral, given as input a window of N input tokens, by first\npooling those token embeddings into a single embedding vector.\nNow let’s consider language modeling: predicting upcoming words from prior\nwords. In this task we are given the same window of N input tokens, but our task\nnow is to predict the next token that should follow the window. We’ll sketch a\nsimple feedforward neural language model, drawing on an algorithm first introduced\nby Bengio et al. (2003). The feedforward language model introduces many of the\nimportant concepts of large language modeling that we will return to in Chapter 7\nand Chapter 8.\nNeural language models have many advantages over the n-gram language models of Chapter 3. Neural language models can handle much longer histories, can\n\n\f16\n\nC HAPTER 6\n\n•\n\nN EURAL N ETWORKS\n\np(+) p(-) p(neut)\n\ny^3\n\ny^2\n\ny^ 1\n\nOutput probabilities\n\ny [1⨉3]\n\n[dh⨉3]\n\nU\nh1\n\nh2\n\nweights\n\nh [1⨉dh] Hidden layer\n\n… h\n\nh3\n\nOutput layer softmax\n\ndh\n\nW [d⨉dh] weights\nx [1⨉d]\n+\n\npooling\n\nembedding for “dessert”\nembedding for “was”\nembedding for “great”\n\nE\n1\n\n3\n\n00\n\n1\n\nE\n|V|\n00\n\nE\n\n1\n\n524\n\n00\n\n0 1\n\nInput layer\npooled embedding\n\n|V|\n\n1\n\n902\n\nN⨉d\n\nembeddings\n\n|V|⨉d\n\nE matrix\nshared across words\n\n|V|\n\n0 0\n00\n\n0\n\n1\n\nN⨉|V|\n\none-hot vectors\n\n0 0\n\n“dessert” = V3\n\n“was” = V524\n\n“great” = V902\n\ndessert\n\nwas\n\ngreat\n\nInput words\n\nFigure 6.13 Feedforward network sentiment analysis using a pooled embedding of the input words. At each\ntimestep the network computes a d-dimensional embedding for each context word (by multiplying a one-hot\nvector by the embedding matrix E), and pools the resulting N embeddings to get a single embedding that\nrepresents the context window as the layer e.\n\ngeneralize better over contexts of similar words, and are far more accurate at wordprediction. On the other hand, neural net language models are slower, more complex, need vast amounts of energy to train, and are less interpretable than n-gram\nmodels, so for some smaller tasks an n-gram language model is still the right tool.\nA feedforward neural language model is a feedforward network that takes as\ninput at time t a representation of some number of previous words (wt−1 , wt−2 , etc.)\nand outputs a probability distribution over possible next words. Thus—like the ngram LM—the feedforward neural LM approximates the probability of a word given\nthe entire prior context P(wt |w1:t−1 ) by approximating based on the N − 1 previous\nwords:\nP(wt |w1 , . . . , wt−1 ) ≈ P(wt |wt−N+1 , . . . , wt−1 )\n\n(6.23)\n\nIn the following examples we’ll use a 4-gram example, so we’ll show a neural net to\nestimate the probability P(wt = i|wt−3 , wt−2 , wt−1 ).\nNeural language models represent words in this prior context by their embeddings, rather than just by their word identity as used in n-gram language models.\nUsing embeddings allows neural language models to generalize better to unseen\ndata. For example, suppose we’ve seen this sentence in training:\nI have to make sure that the cat gets fed.\n\n\f6.5\n\n•\n\nE MBEDDINGS AS THE INPUT TO NEURAL NET CLASSIFIERS\n\n17\n\nbut have never seen the words “gets fed” after the word “dog”. Our test set has the\nprefix “I forgot to make sure that the dog gets”. What’s the next word? An n-gram\nlanguage model will predict “fed” after “that the cat gets”, but not after “that the dog\ngets”. But a neural LM, knowing that “cat” and “dog” have similar embeddings, will\nbe able to generalize from the “cat” context to assign a high enough probability to\n“fed” even after seeing “dog”.\np(wt=aardvark|wt-3,wt-2,wt-1)\noutput layer y\nsoftmax\n\ny^1\n\np(wt=do|…) p(wt=fish|…)\n\n^y\n34\n\n…\n\np(wt=zebra|…)\n\n^y … ^y\n^\n42\n35102 … y|V|\n\n…\n\nU\nh2\n\nh1\n\nhidden layer h\n\ndh⨉|V|\nh3\n\n…\n\n1⨉dh\n\nhdh\n\nW\n\nNd⨉dh\n1⨉Nd\n\nembedding layer e\nE is shared\nacross words\nInput layer\none-hot\nvectors\n\n1⨉|V|\n\nE\n\nE\n1\n\n35\n\n00\n\n1\n\n|V|\n00\n\n“for” = V35\n\n1\n\n992\n\n00\n\n0 1\n\nE\n|V|\n0 0\n\n1\n\n|V|⨉d\n\n451\n\n00\n\n0\n\n1\n\n|V|\n\nN⨉|V|\n\n0 0\n\n“all” = V992 “the” = V451\n\n...\n\n…\n\nand thanks\n\nfor\n\nall\n\nthe\n\n?\n\nwt-3\n\nwt-2\n\nwt-1\n\nwt\n\n…\n\nFigure 6.14 Forward inference in a feedforward neural language model. At each timestep\nt the network computes a d-dimensional embedding for each of the N = 3 context tokens (by\nmultiplying a one-hot vector by the embedding matrix E), and concatenates the three to get\nthe embedding e. This embedding e is multiplied by weight matrix W and then an activation\nfunction is applied element-wise to produce the hidden layer h, which is then multiplied by\nanother weight matrix U. A softmax layer predicts at each output node i the probability that\nthe next word wt will be vocabulary word Vi . We show the context window size N as 3 just to\nfit on the page, but in practice language modeling requires a much longer context.\n\nThis prediction task requires an output vector that expresses |V | probabilities:\none probability value for each possible next token. We might have a vocabulary\nbetween 60,000 and 300,000 tokens, so the output vector for the task of language\nmodeling is much longer than 3. Another difference for language modeling is that\ninstead of pooling the embeddings of the N input tokens to create a single embedding, we concatenate the inputs into one very long input vector. To predict the next\ntoken, it helps to know each of the preceding tokens and what order they were in.\nFig. 6.14 shows the language modeling task, sketched with a very short context\nwindow of N = 3 just to fit on the page. These 3 embedding vectors are concatenated\nto produce e, the embedding layer. This is multiplied by a weight matrix W to produce a hidden layer, and another weight matrix U to produce an output layer whose\nsoftmax gives a probability distribution over words. For example y42 , the value of\noutput node 42, is the probability of the next word wt being V42 , the vocabulary word\nwith index 42 (which is the word ‘fish’ in our example).\nThe equations for a simple feedforward neural language model with a window\n\n\f18\n\nC HAPTER 6\n\n•\n\nN EURAL N ETWORKS\n\nsize of 3, given one-hot input vectors for each input context word, are:\ne = [Ext−3 ; Ext−2 ; Ext−1 ]\nh = σ (We + b)\nz = Uh\nŷ = softmax(z)\n\n(6.24)\n\nNote that we we use semicolons to mean concatenation of vectors, so we form the\nembedding layer e by concatenating the 3 embeddings for the three context vectors.\nWe’ll return to this idea of using neural networks to do language modeling in\nChapter 7 and Chapter 8 when we introduce transformer language models.\n\n6.6\n\nTraining Neural Nets\nA feedforward neural net is an instance of supervised machine learning in which we\nknow the correct output y for each observation x. What the system produces, via\nEq. 6.13, is ŷ, the system’s estimate of the true y. The goal of the training procedure\nis to learn parameters W[i] and b[i] for each layer i that make ŷ for each training\nobservation as close as possible to the true y.\nIn general, we do all this by drawing on the methods we introduced in Chapter 4\nfor logistic regression, so the reader should be comfortable with that chapter before\nproceeding. We’ll explore the algorithm on simple generic networks rather than\nnetworks designed for sentiment or language modeling.\nFirst, we’ll need a loss function that models the distance between the system\noutput and the gold output, and it’s common to use the loss function used for logistic\nregression, the cross-entropy loss.\nSecond, to find the parameters that minimize this loss function, we’ll use the\ngradient descent optimization algorithm introduced in Chapter 4.\nThird, gradient descent requires knowing the gradient of the loss function, the\nvector that contains the partial derivative of the loss function with respect to each\nof the parameters. In logistic regression, for each observation we could directly\ncompute the derivative of the loss function with respect to an individual w or b. But\nfor neural networks, with millions of parameters in many layers, it’s much harder to\nsee how to compute the partial derivative of some weight in layer 1 when the loss\nis attached to some much later layer. How do we partial out the loss over all those\nintermediate layers? The answer is the algorithm called error backpropagation or\nbackward differentiation.\n\n6.6.1\ncross-entropy\nloss\n\nLoss function\n\nThe cross-entropy loss that is used in neural networks is the same one we saw for\nlogistic regression. If the neural network is being used as a binary classifier, with\nthe sigmoid at the final layer, the loss function is the same logistic regression loss\nwe saw in Eq. ??:\nLCE (ŷ, y) = − log p(y|x) = − [y log ŷ + (1 − y) log(1 − ŷ)]\n\n(6.25)\n\nIf we are using the network to classify into 3 or more classes, the loss function is\nexactly the same as the loss for multinomial regression that we saw in Chapter 4 on\n\n\f6.6\n\n•\n\n19\n\nT RAINING N EURAL N ETS\n\npage ??. Let’s briefly summarize the explanation here for convenience. First, when\nwe have more than 2 classes we’ll need to represent both y and ŷ as vectors. Let’s\nassume we’re doing hard classification, where only one class is the correct one.\nThe true label y is then a vector with K elements, each corresponding to a class,\nwith yc = 1 if the correct class is c, with all other elements of y being 0. Recall that\na vector like this, with one value equal to 1 and the rest 0, is called a one-hot vector.\nAnd our classifier will produce an estimate vector with K elements ŷ, each element\nŷk of which represents the estimated probability p(yk = 1|x).\nThe loss function for a single example x is the negative sum of the logs of the K\noutput classes, each weighted by their probability yk :\nLCE (ŷ, y) = −\n\nK\nX\n\nyk log ŷk\n\n(6.26)\n\nk=1\n\nWe can simplify this equation further; let’s first rewrite the equation using the function 1{} which evaluates to 1 if the condition in the brackets is true and to 0 otherwise. This makes it more obvious that the terms in the sum in Eq. 6.26 will be 0\nexcept for the term corresponding to the true class for which yk = 1:\nLCE (ŷ, y) = −\n\nK\nX\n\n1{yk = 1} log ŷk\n\nk=1\n\nnegative log\nlikelihood loss\n\nIn other words, the cross-entropy loss is simply the negative log of the output probability corresponding to the correct class, and we therefore also call this the negative\nlog likelihood loss:\nLCE (ŷ, y) = − log ŷc\n\n(where c is the correct class)\n\n(6.27)\n\nPlugging in the softmax formula from Eq. 6.9, and with K the number of classes:\nexp(zc )\nLCE (ŷ, y) = − log PK\nj=1 exp(z j )\n\n(where c is the correct class)\n\n(6.28)\n\nLet’s think about the negative log probability as a loss function. A perfect classifier would assign the correct class i probability 1 and all the incorrect classes probability 0. That means the higher p(ŷi ) (the closer it is to 1), the better the classifier;\np(ŷi ) is (the closer it is to 0), the worse the classifier. The negative log of this probability is a beautiful loss metric since it goes from 0 (negative log of 1, no loss)\nto infinity (negative log of 0, infinite loss). This loss function also insures that as\nprobability of the correct answer is maximized, the probability of all the incorrect\nanswers is minimized; since they all sum to one, any increase in the probability of\nthe correct answer is coming at the expense of the incorrect answers.\nThe number K of classes of the output vector ŷ can be small or large. Perhaps\nour task is 3-way sentiment, and then the classes might be positive, negative, and\nneutral. Or if our task is deciding the part of speech of a word (i.e., whether it is a\nnoun or verb or adjective, etc.), then K is set of possible parts of speech in our tagset\n(of which there are 17 in the tagset we will define in Chapter 17). And if our task\nis language modeling, and our classifier is trying to predict which word is next, then\nour set of classes is the set of words, which might be 50,000 or 100,000.\n\n\f20\n\nC HAPTER 6\n\n•\n\n6.6.2\n\nN EURAL N ETWORKS\n\nComputing the Gradient\n\nHow do we compute the gradient of this loss function? Computing the gradient\nrequires the partial derivative of the loss function with respect to each parameter.\nFor a network with one weight layer and sigmoid output (which is what logistic\nregression is), we could simply use the derivative of the loss that we used for logistic\nregression in Eq. 6.29 (and derived in Section ??):\n∂ LCE (ŷ, y)\n= (ŷ − y) x j\n∂wj\n= (σ (w · x + b) − y) x j\n\n(6.29)\n\nOr for a network with one weight layer and softmax output (=multinomial logistic\nregression), we could use the derivative of the softmax loss from Eq. ??, shown for\na particular weight wk and input xi\n∂ LCE (ŷ, y)\n= −(yk − ŷk )xi\n∂ wk,i\n= −(yk − p(yk = 1|x))xi\nexp (wk · x + bk )\n\n= − yk − PK\n\nj=1 exp (w j · x + b j )\n\nerror backpropagation\n\n!\nxi\n\n(6.30)\n\nBut these derivatives only give correct updates for one weight layer: the last one!\nFor deep networks, computing the gradients for each weight is much more complex,\nsince we are computing the derivative with respect to weight parameters that appear\nall the way back in the very early layers of the network, even though the loss is\ncomputed only at the very end of the network.\nThe solution to computing this gradient is an algorithm called error backpropagation or backprop (Rumelhart et al., 1986). While backprop was invented specially for neural networks, it turns out to be the same as a more general procedure\ncalled backward differentiation, which depends on the notion of computation\ngraphs. Let’s see how that works in the next subsection.\n\n6.6.3\n\nComputation Graphs\n\nA computation graph is a representation of the process of computing a mathematical\nexpression, in which the computation is broken down into separate operations, each\nof which is modeled as a node in a graph.\nConsider computing the function L(a, b, c) = c(a + 2b). If we make each of the\ncomponent addition and multiplication operations explicit, and add names (d and e)\nfor the intermediate outputs, the resulting series of computations is:\nd = 2∗b\ne = a+d\nL = c∗e\nWe can now represent this as a graph, with nodes for each operation, and directed edges showing the outputs from each operation as the inputs to the next, as\nin Fig. 6.15. The simplest use of computation graphs is to compute the value of\nthe function with some given inputs. In the figure, we’ve assumed the inputs a = 3,\nb = 1, c = −2, and we’ve shown the result of the forward pass to compute the result L(3, 1, −2) = −10. In the forward pass of a computation graph, we apply each\n\n\f6.6\n\n•\n\nT RAINING N EURAL N ETS\n\n21\n\noperation left to right, passing the outputs of each computation as the input to the\nnext node.\nforward pass\na=3\n\na\nb\n\nb=1\n\nd=2\n\ne=a+d\n\ne=5\nL=ce\n\nd = 2b\n\nL=-10\n\nc=-2\nc\nFigure 6.15 Computation graph for the function L(a, b, c) = c(a+2b), with values for input\nnodes a = 3, b = 1, c = −2, showing the forward pass computation of L.\n\n6.6.4\n\nchain rule\n\nBackward differentiation on computation graphs\n\nThe importance of the computation graph comes from the backward pass, which\nis used to compute the derivatives that we’ll need for the weight update. In this\nexample our goal is to compute the derivative of the output function L with respect\nto each of the input variables, i.e., ∂∂ La , ∂∂ Lb , and ∂∂ Lc . The derivative ∂∂ La tells us how\nmuch a small change in a affects L.\nBackwards differentiation makes use of the chain rule in calculus, so let’s remind ourselves of that. Suppose we are computing the derivative of a composite\nfunction f (x) = u(v(x)). The derivative of f (x) is the derivative of u(x) with respect\nto v(x) times the derivative of v(x) with respect to x:\ndu dv\ndf\n=\n·\ndx\ndv dx\n\n(6.31)\n\nThe chain rule extends to more than two functions. If computing the derivative of a\ncomposite function f (x) = u(v(w(x))), the derivative of f (x) is:\ndf\ndu dv dw\n=\n·\n·\ndx\ndv dw dx\n\n(6.32)\n\nThe intuition of backward differentiation is to pass gradients back from the final\nnode to all the nodes in the graph. Fig. 6.16 shows part of the backward computation\nat one node e. Each node takes an upstream gradient that is passed in from its parent\nnode to the right, and for each of its inputs computes a local gradient (the gradient\nof its output with respect to its input), and uses the chain rule to multiply these two\nto compute a downstream gradient to be passed on to the next earlier node.\nLet’s now compute the 3 derivatives we need. Since in the computation graph\nL = ce, we can directly compute the derivative ∂∂ Lc :\n∂L\n=e\n∂c\n\n(6.33)\n\nFor the other two, we’ll need to use the chain rule:\n∂L\n∂L ∂e\n=\n∂a\n∂e ∂a\n∂L\n∂L ∂e ∂d\n=\n∂b\n∂e ∂d ∂b\n\n(6.34)\n\n\f22\n\nC HAPTER 6\n\n•\n\nN EURAL N ETWORKS\nd\n\ne\n\nd\n\ne\n\nL\n\n∂L = ∂L ∂e\n∂d ∂e ∂d\n\n∂e\n∂d\n\n∂L\n∂e\n\ndownstream\ngradient\n\nlocal\ngradient\n\nupstream\ngradient\n\nFigure 6.16 Each node (like e here) takes an upstream gradient, multiplies it by the local\ngradient (the gradient of its output with respect to its input), and uses the chain rule to compute\na downstream gradient to be passed on to a prior node. A node may have multiple local\ngradients if it has multiple inputs.\n\nEq. 6.34 and Eq. 6.33 thus require five intermediate derivatives: ∂∂ Le , ∂∂ Lc , ∂∂ ae , ∂∂ de , and\n∂d\n∂ b , which are as follows (making use of the fact that the derivative of a sum is the\nsum of the derivatives):\nL = ce :\ne = a+d :\nd = 2b :\n\n∂L\n∂L\n= c,\n=e\n∂e\n∂c\n∂e\n∂e\n= 1,\n=1\n∂a\n∂d\n∂d\n=2\n∂b\n\nIn the backward pass, we compute each of these partials along each edge of the\ngraph from right to left, using the chain rule just as we did above. Thus we begin by\ncomputing the downstream gradients from node L, which are ∂∂ Le and ∂∂ Lc . For node e,\nwe then multiply this upstream gradient ∂∂ Le by the local gradient (the gradient of the\noutput with respect to the input), ∂∂ de to get the output we send back to node d: ∂∂ Ld .\nAnd so on, until we have annotated the graph all the way to all the input variables.\nThe forward pass conveniently already will have computed the values of the forward\nintermediate variables we need (like d and e) to compute these derivatives. Fig. 6.17\nshows the backward pass.\n\na=3\n\na\n\n∂L = ∂L ∂e =-2\n∂a ∂e ∂a\n\nd=2\n\nb=1\n\nb\n\nd = 2b\n∂L = ∂L ∂d =-4\n∂b ∂d ∂b\n\n∂d\n=2\n∂b\n\n∂L = ∂L ∂e =-2\n∂d ∂e ∂d\n\ne=d+a\n\ne=5\n\n∂e\n∂e\n=1\n=1\n∂a\n∂d\n\n∂L\n=-2\n∂e\n\n∂L =5\n∂c\n\nL=-10\n\n∂L\n=-2\n∂e\n∂L\n=5\n∂c\n\nc=-2\nc\n\nL=ce\n\nbackward pass\n\nFigure 6.17 Computation graph for the function L(a, b, c) = c(a + 2b), showing the backward pass computation of ∂∂ La , ∂∂ Lb , and ∂∂ Lc .\n\n\f6.6\n\n•\n\n23\n\nT RAINING N EURAL N ETS\n\nBackward differentiation for a neural network\nOf course computation graphs for real neural networks are much more complex.\nFig. 6.18 shows a sample computation graph for a 2-layer neural network with n0 =\n2, n1 = 2, and n2 = 1, assuming binary classification and hence using a sigmoid\noutput unit for simplicity. The function that the computation graph is computing is:\nz[1] = W[1] x + b[1]\na[1] = ReLU(z[1] )\nz[2] = W[2] a[1] + b[2]\na[2] = σ (z[2] )\nŷ = a[2]\n\n(6.35)\n\nFor the backward pass we’ll also need to compute the loss L. The loss function\nfor binary sigmoid output from Eq. 6.25 is\nLCE (ŷ, y) = − [y log ŷ + (1 − y) log(1 − ŷ)]\nOur output ŷ = a[2] , so we can rephrase this as\nh\ni\nLCE (a[2] , y) = − y log a[2] + (1 − y) log(1 − a[2] )\n\n[1]\nw11\n\n(6.37)\n\n*\n\nw[1]\n12\n\nx1\n\n(6.36)\n\n*\n\nz[1]\n=\n1\n+\n\na1[1] =\nReLU\n*\n\nb[1]\n1\n\nz[2] =\n+\n\nw[2]\n11\n\nx2\n\na[2] = σ\n\nL (a[2],y)\n\n*\n*\nw[1]\n21\n\n*\n\nw[1]\n22\n\nb[1]\n\nz[1]\n2 =\n+\n\na[1]\n2 =\nReLU\n\nw[2]\n12\nb[2]\n1\n\n2\n\nFigure 6.18 Sample computation graph for a simple 2-layer neural net (= 1 hidden layer) with two input units\nand 2 hidden units. We’ve adjusted the notation a bit to avoid long equations in the nodes by just mentioning\n[1]\nthe function that is being computed, and the resulting variable name. Thus the * to the right of node w11 means\n[1]\n\nthat w11 is to be multiplied by x1 , and the node z[1] = + means that the value of z[1] is computed by summing\n[1]\n\nthe three nodes that feed into it (the two products, and the bias term bi ).\n\nThe weights that need updating (those for which we need to know the partial\nderivative of the loss function) are shown in teal. In order to do the backward pass,\nwe’ll need to know the derivatives of all the functions in the graph. We already saw\nin Section ?? the derivative of the sigmoid σ :\ndσ (z)\n= σ (z)(1 − σ (z))\ndz\n\n(6.38)\n\n\f24\n\nC HAPTER 6\n\n•\n\nN EURAL N ETWORKS\n\nWe’ll also need the derivatives of each of the other activation functions. The\nderivative of tanh is:\nd tanh(z)\n= 1 − tanh2 (z)\ndz\n\n(6.39)\n\nThe derivative of the ReLU is2\nd ReLU(z)\n=\ndz\n\n\u001a\n\n0 f or z < 0\n1 f or z ≥ 0\n\n(6.40)\n\nWe’ll give the start of the computation, computing the derivative of the loss function\nL with respect to z, or ∂∂Lz (and leaving the rest of the computation as an exercise for\nthe reader). By the chain rule:\n∂ L ∂ a[2]\n∂L\n= [2]\n∂z\n∂z\n∂a\n\n(6.41)\n\nSo let’s first compute ∂∂aL[2] , taking the derivative of Eq. 6.37, repeated here:\nh\ni\nLCE (a[2] , y) = − y log a[2] + (1 − y) log(1 − a[2] )\n!\n!\n∂L\n∂ log(a[2] )\n∂ log(1 − a[2] )\n= −\ny\n+ (1 − y)\n∂ a[2]\n∂ a[2]\n∂ a[2]\n\u0012\u0012\n\u0013\n\u0013\n1\n1\n= −\ny [2] + (1 − y)\n(−1)\na\n1 − a[2]\n\u0012\n\u0013\ny\ny−1\n= − [2] +\na\n1 − a[2]\n\n(6.42)\n\nNext, by the derivative of the sigmoid:\n∂ a[2]\n= a[2] (1 − a[2] )\n∂z\nFinally, we can use the chain rule:\n∂L\n∂ L ∂ a[2]\n=\n∂z\n∂ a\u0012[2] ∂ z\n\u0013\ny\ny−1\na[2] (1 − a[2] )\n= − [2] +\na\n1 − a[2]\n= a[2] − y\n\n(6.43)\n\nContinuing the backward computation of the gradients (next by passing the gra[2]\ndients over b1 and the two product nodes, and so on, back to all the teal nodes), is\nleft as an exercise for the reader.\n\n6.6.5\n\nMore details on learning\n\nOptimization in neural networks is a non-convex optimization problem, more complex than for logistic regression, and for that and other reasons there are many best\npractices for successful learning.\n2\n\nThe derivative is actually undefined at the point z = 0, but by convention we treat it as 1.\n\n\f6.7\n\ndropout\n\nhyperparameter\n\n6.7\n\n•\n\nS UMMARY\n\n25\n\nFor logistic regression we can initialize gradient descent with all the weights and\nbiases having the value 0. In neural networks, by contrast, we need to initialize the\nweights with small random numbers. It’s also helpful to normalize the input values\nto have 0 mean and unit variance.\nVarious forms of regularization are used to prevent overfitting. One of the most\nimportant is dropout: randomly dropping some units and their connections from\nthe network during training (Hinton et al. 2012, Srivastava et al. 2014). At each\niteration of training (whenever we update parameters, i.e. each mini-batch if we are\nusing mini-batch gradient descent), we repeatedly choose a probability p and for\neach unit we replace its output with zero with probability p (and renormalize the\nrest of the outputs from that layer).\nTuning of hyperparameters is also important. The parameters of a neural network are the weights W and biases b; those are learned by gradient descent. The\nhyperparameters are things that are chosen by the algorithm designer; optimal values are tuned on a devset rather than by gradient descent learning on the training\nset. Hyperparameters include the learning rate η, the mini-batch size, the model\narchitecture (the number of layers, the number of hidden nodes per layer, the choice\nof activation functions), how to regularize, and so on. Gradient descent itself also\nhas many architectural variants such as Adam (Kingma and Ba, 2015).\nFinally, most modern neural networks are built using computation graph formalisms that make it easy and natural to do gradient computation and parallelization\non vector-based GPUs (Graphic Processing Units). PyTorch (Paszke et al., 2017)\nand TensorFlow (Abadi et al., 2015) are two of the most popular. The interested\nreader should consult a neural network textbook for further details; some suggestions are at the end of the chapter.\n\nSummary\n• Neural networks are built out of neural units, originally inspired by biological\nneurons but now simply an abstract computational device.\n• Each neural unit multiplies input values by a weight vector, adds a bias, and\nthen applies a non-linear activation function like sigmoid, tanh, or rectified\nlinear unit.\n• In a fully-connected, feedforward network, each unit in layer i is connected\nto each unit in layer i + 1, and there are no cycles.\n• The power of neural networks comes from the ability of early layers to learn\nrepresentations that can be utilized by later layers in the network.\n• Neural networks are trained by optimization algorithms like gradient descent.\n• Error backpropagation, backward differentiation on a computation graph,\nis used to compute the gradients of the loss function for a network.\n• Neural language models use a neural network as a probabilistic classifier, to\ncompute the probability of the next word given the previous n words.\n• Neural language models can use pretrained embeddings, or can learn embeddings from scratch in the process of language modeling.\n\n\f26\n\nC HAPTER 6\n\n•\n\nN EURAL N ETWORKS\n\nHistorical Notes\n\nconnectionist\n\nThe origins of neural networks lie in the 1940s McCulloch-Pitts neuron (McCulloch and Pitts, 1943), a simplified model of the biological neuron as a kind of computing element that could be described in terms of propositional logic. By the late\n1950s and early 1960s, a number of labs (including Frank Rosenblatt at Cornell and\nBernard Widrow at Stanford) developed research into neural networks; this phase\nsaw the development of the perceptron (Rosenblatt, 1958), and the transformation\nof the threshold into a bias, a notation we still use (Widrow and Hoff, 1960).\nThe field of neural networks declined after it was shown that a single perceptron\nunit was unable to model functions as simple as XOR (Minsky and Papert, 1969).\nWhile some small amount of work continued during the next two decades, a major\nrevival for the field didn’t come until the 1980s, when practical tools for building\ndeeper networks like error backpropagation became widespread (Rumelhart et al.,\n1986). During the 1980s a wide variety of neural network and related architectures were developed, particularly for applications in psychology and cognitive science (Rumelhart and McClelland 1986b, McClelland and Elman 1986, Rumelhart\nand McClelland 1986a, Elman 1990), for which the term connectionist or parallel distributed processing was often used (Feldman and Ballard 1982, Smolensky\n1988). Many of the principles and techniques developed in this period are foundational to modern work, including the ideas of distributed representations (Hinton,\n1986), recurrent networks (Elman, 1990), and the use of tensors for compositionality\n(Smolensky, 1990).\nBy the 1990s larger neural networks began to be applied to many practical language processing tasks as well, like handwriting recognition (LeCun et al. 1989) and\nspeech recognition (Morgan and Bourlard 1990). By the early 2000s, improvements\nin computer hardware and advances in optimization and training techniques made it\npossible to train even larger and deeper networks, leading to the modern term deep\nlearning (Hinton et al. 2006, Bengio et al. 2007). We cover more related history in\nChapter 13 and Chapter 15.\nThere are a number of excellent books on neural networks, including Goodfellow\net al. (2016) and Nielsen (2015).\n\n\fHistorical Notes\n\n27\n\nAbadi, M., A. Agarwal, P. Barham, E. Brevdo, Z. Chen,\nC. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin,\nS. Ghemawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia, R. Jozefowicz, L. Kaiser, M. Kudlur, J. Levenberg, D. Mané, R. Monga, S. Moore, D. Murray, C. Olah,\nM. Schuster, J. Shlens, B. Steiner, I. Sutskever, K. Talwar, P. Tucker, V. Vanhoucke, V. Vasudevan, F. Viégas,\nO. Vinyals, P. Warden, M. Wattenberg, M. Wicke, Y. Yu,\nand X. Zheng. 2015. TensorFlow: Large-scale machine\nlearning on heterogeneous systems. Software available\nfrom tensorflow.org.\n\nRumelhart, D. E. and J. L. McClelland. 1986a. On learning\nthe past tense of English verbs. In D. E. Rumelhart and\nJ. L. McClelland, eds, Parallel Distributed Processing,\nvolume 2, 216–271. MIT Press.\n\nBengio, Y., R. Ducharme, P. Vincent, and C. Jauvin. 2003.\nA neural probabilistic language model. JMLR, 3:1137–\n1155.\n\nSmolensky, P. 1990. Tensor product variable binding and\nthe representation of symbolic structures in connectionist\nsystems. Artificial intelligence, 46(1-2):159–216.\n\nBengio, Y., P. Lamblin, D. Popovici, and H. Larochelle.\n2007. Greedy layer-wise training of deep networks.\nNeurIPS.\n\nSrivastava, N., G. E. Hinton, A. Krizhevsky, I. Sutskever,\nand R. R. Salakhutdinov. 2014. Dropout: a simple\nway to prevent neural networks from overfitting. JMLR,\n15(1):1929–1958.\n\nElman, J. L. 1990. Finding structure in time. Cognitive science, 14(2):179–211.\nFeldman, J. A. and D. H. Ballard. 1982. Connectionist models and their properties. Cognitive Science, 6:205–254.\nGoodfellow, I., Y. Bengio, and A. Courville. 2016. Deep\nLearning. MIT Press.\nHinton, G. E. 1986. Learning distributed representations of\nconcepts. COGSCI.\nHinton, G. E., S. Osindero, and Y.-W. Teh. 2006. A fast\nlearning algorithm for deep belief nets. Neural computation, 18(7):1527–1554.\nHinton, G. E., N. Srivastava, A. Krizhevsky, I. Sutskever, and\nR. R. Salakhutdinov. 2012. Improving neural networks\nby preventing co-adaptation of feature detectors. ArXiv\npreprint arXiv:1207.0580.\nKingma, D. and J. Ba. 2015. Adam: A method for stochastic\noptimization. ICLR 2015.\nLeCun, Y., B. Boser, J. S. Denker, D. Henderson, R. E.\nHoward, W. Hubbard, and L. D. Jackel. 1989. Backpropagation applied to handwritten zip code recognition. Neural computation, 1(4):541–551.\nMcClelland, J. L. and J. L. Elman. 1986. The TRACE model\nof speech perception. Cognitive Psychology, 18:1–86.\nMcCulloch, W. S. and W. Pitts. 1943. A logical calculus of\nideas immanent in nervous activity. Bulletin of Mathematical Biophysics, 5:115–133.\nMinsky, M. and S. Papert. 1969. Perceptrons. MIT Press.\nMorgan, N. and H. Bourlard. 1990. Continuous speech\nrecognition using multilayer perceptrons with hidden\nmarkov models. ICASSP.\nNielsen, M. A. 2015. Neural networks and Deep learning.\nDetermination Press USA.\nPaszke, A., S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer.\n2017. Automatic differentiation in pytorch. NIPS-W.\nRosenblatt, F. 1958. The perceptron: A probabilistic model\nfor information storage and organization in the brain. Psychological review, 65(6):386–408.\nRumelhart, D. E., G. E. Hinton, and R. J. Williams. 1986.\nLearning internal representations by error propagation. In\nD. E. Rumelhart and J. L. McClelland, eds, Parallel Distributed Processing, volume 2, 318–362. MIT Press.\n\nRumelhart, D. E. and J. L. McClelland, eds. 1986b. Parallel\nDistributed Processing. MIT Press.\nRussell, S. and P. Norvig. 2002. Artificial Intelligence: A\nModern Approach, 2nd edition. Prentice Hall.\nSmolensky, P. 1988. On the proper treatment of connectionism. Behavioral and brain sciences, 11(1):1–23.\n\nWidrow, B. and M. E. Hoff. 1960. Adaptive switching circuits. IRE WESCON Convention Record, volume 4.\n\n\f",
    "file_path": "/Users/colinsidberry/Downloads/NLP_Textbook/neural-networks.txt",
    "file_size_kb": 67.93
  },
  {
    "id": "f82a99d753da42ab",
    "source": "nlp_textbook",
    "chapter": "Transformers",
    "filename": "transformers.txt",
    "content": "Copyright © 2025.\n\nSpeech and Language Processing. Daniel Jurafsky & James H. Martin.\nrights reserved. Draft of August 24, 2025.\n\nAll\n\nCHAPTER\n\n8\n\nTransformers\n“The true art of memory is the art of attention ”\nSamuel Johnson, Idler #74, September 1759\nIn this chapter we introduce the transformer, the standard architecture for building large language models. As we discussed in the prior chapter, transformer-based\nlarge language models have completely changed the field of speech and language\nprocessing. Indeed, every subsequent chapter in this textbook will make use of them.\nAs with the previous chapter, we’ll focus for this chapter on the use of transformers\nto model left-to-right (sometimes called causal or autoregressive) language modeling, in which we are given a sequence of input tokens and predict output tokens one\nby one by conditioning on the prior context.\nThe transformer is a neural network with a specific structure that includes a\nmechanism called self-attention or multi-head attention.1 Attention can be thought\nof as a way to build contextual representations of a token’s meaning by attending to\nand integrating information from surrounding tokens, helping the model learn how\ntokens relate to each other over large spans.\nNext token\nLanguage\nModeling\nHead\n\nStacked\nTransformer\nBlocks\n\nlong\n\nand\n\nlogits\n\nthanks\n\nlogits\n\nlogits\n\nfor\n\nall\n\nlogits\n\n…\n\nlogits\n\nU\n\nU\n\nU\n\nU\n\nU\n\n…\n\n…\n\n…\n\n…\n\n…\n…\n\nx1\n+\n\nx2\n1\n\n+\n\nx3\n2\n\n+\n\nx4\n3\n\n+\n\n…\n\nx5\n4\n\n+\n\nInput\nEncoding\n\nE\n\nE\n\nE\n\nE\n\nE\n\nInput tokens\n\nSo\n\nlong\n\nand\n\nthanks\n\nfor\n\n5\n\n…\n\nFigure 8.1 The architecture of a (left-to-right) transformer, showing how each input token\nget encoded, passed through a set of stacked transformer blocks, and then a language model\nhead that predicts the next token.\n\nFig. 8.1 sketches the transformer architecture. A transformer has three major\ncomponents. At the center are columns of transformer blocks. Each block is a\nmultilayer network (a multi-head attention layer, feedforward networks and layer\nAlthough multi-head attention developed historically from the RNN attention mechanism (Chapter 13), we’ll define attention from scratch here.\n\n1\n\n\f2\n\nC HAPTER 8\n\n•\n\nT RANSFORMERS\n\nnormalization steps) that maps an input vector xi in column i (corresponding to input\ntoken i) to an output vector hi . The set of n blocks maps an entire context window\nof input vectors (x1 , ..., xn ) to a window of output vectors (h1 , ..., hn ) of the same\nlength. A column might contain from 12 to 96 or more stacked blocks.\nThe column of blocks is preceded by the input encoding component, which processes an input token (like the word thanks) into a contextual vector representation,\nusing an embedding matrix E and a mechanism for encoding token position. Each\ncolumn is followed by a language modeling head, which takes the embedding output by the final transformer block, passes it through an unembedding matrix U and\na softmax over the vocabulary to generate a single token for that column.\nTransformer-based language models are complex, and so the details will unfold over the next few chapters. Chapter 7 already discussed how language models\nare pretrained, and how tokens are generated via sampling. In the next sections\nwe’ll introduce multi-head attention, the rest of the transformer block, and the input\nencoding and language modeling head components of the transformer. Chapter 10\nintroduces masked language modeling and the BERT family of bidirectional transformer encoder models. Chapter 9 shows how to instruction-tune language models\nto perform NLP tasks, and how to align the model with human preferences. Chapter 12 will introduce machine translation with the encoder-decoder architecture.\nWe’ll see further use of the encoder-decoder architecture in Chapter 15.\n\n8.1\n\nAttention\nRecall from Chapter 5 that for word2vec and other static embeddings, the representation of a word’s meaning is always the same vector irrespective of the context:\nthe word chicken, for example, is always represented by the same fixed vector. So\na static vector for the word it might somehow encode that this is a pronoun used\nfor animals and inanimate entities. But in context it has a much richer meaning.\nConsider it in one of these two sentences:\n(8.1) The chicken didn’t cross the road because it was too tired.\n(8.2) The chicken didn’t cross the road because it was too wide.\nIn (8.1) it is the chicken (i.e., the reader knows that the chicken was tired), while\nin (8.2) it is the road (and the reader knows that the road was wide).2 That is, if\nwe are to compute the meaning of this sentence, we’ll need the meaning of it to be\nassociated with the chicken in the first sentence and associated with the road in\nthe second one, sensitive to the context.\nFurthermore, consider reading left to right like a causal language model, processing the sentence up to the word it:\n(8.3) The chicken didn’t cross the road because it\nAt this point we don’t yet know which thing it is going to end up referring to! So a\nrepresentation of it at this point might have aspects of both chicken and road as\nthe reader is trying to guess what happens next.\nThis fact that words have rich linguistic relationships with other words that may\nbe far away pervades language. Consider two more examples:\n(8.4) The keys to the cabinet are on the table.\n2 We say that in the first example it corefers with the chicken, and in the second it corefers with the\nroad; we’ll return to this in Chapter 23.\n\n\f8.1\n\nATTENTION\n\n3\n\n(8.5) I walked along the pond, and noticed one of the trees along the bank.\nIn (8.4), the phrase The keys is the subject of the sentence, and in English and many\nlanguages, must agree in grammatical number with the verb are; in this case both are\nplural. In English we can’t use a singular verb like is with a plural subject like keys\n(we’ll discuss agreement more in Chapter 18). In (8.5), we know that bank refers\nto the side of a pond or river and not a financial institution because of the context,\nincluding words like pond. (We’ll discuss word senses more in Chapter 10.)\nThe point of all these examples is that these contextual words that help us compute the meaning of words in context can be quite far away in the sentence or paragraph. Transformers can build contextual representations of word meaning, contextual embeddings, by integrating the meaning of these helpful contextual words. In a\ntransformer, layer by layer, we build up richer and richer contextualized representations of the meanings of input tokens. At each layer, we compute the representation\nof a token i by combining information about i from the previous layer with information about the neighboring tokens to produce a contextualized representation for\neach word at each position.\nAttention is the mechanism in the transformer that weighs and combines the\nrepresentations from appropriate other tokens in the context from layer k to build\nthe representation for tokens in layer k + 1.\n\ncross\n\nthe\n\nroad\n\nbecause\n\nit\n\nwas\n\ntoo\n\ntired\n\ncross\n\nthe\n\nroad\n\nbecause\n\nit\n\nwas\n\ntoo\n\ntired\n\ndidn’t\n\nchicken\n\nLayer k+1\n\nThe\n\ncolumns corresponding to input tokens\n\ndidn’t\n\nLayer k\n\nchicken\n\nself-attention distribution\n\nThe\n\ncontextual\nembeddings\n\n•\n\nFigure 8.2 The self-attention weight distribution α that is part of the computation of the\nrepresentation for the word it at layer k + 1. In computing the representation for it, we attend\ndifferently to the various words at layer k, with darker shades indicating higher self-attention\nvalues. Note that the transformer is attending highly to the columns corresponding to the\ntokens chicken and road , a sensible result, since at the point where it occurs, it could plausibly\ncorefer with the chicken or the road, and hence we’d like the representation for it to draw on\nthe representation for these earlier words. Figure adapted from Uszkoreit (2017).\n\nFig. 8.2 shows a schematic example simplified from a transformer (Uszkoreit,\n2017). The figure describes the situation when the current token is it and we need\nto compute a contextual representation for this token at layer k +1 of the transformer,\ndrawing on the representations (from layer k) of every prior token. The figure uses\ncolor to represent the attention distribution over the contextual words: the tokens\nchicken and road both have a high attention weight, meaning that as we are computing the representation for it, we will draw most heavily on the representation for\nchicken and road. This will be useful in building the final representation for it,\nsince it will end up coreferring with either chicken or road.\nLet’s now turn to how this attention distribution is represented and computed.\n\n\f4\n\nC HAPTER 8\n\n•\n\n8.1.1\n\nT RANSFORMERS\n\nAttention more formally\n\nAs we’ve said, the attention computation is a way to compute a vector representation\nfor a token at a particular layer of a transformer, by selectively attending to and\nintegrating information from prior tokens at the previous layer. Attention takes an\ninput representation xi corresponding to the input token at position i, and a context\nwindow of prior inputs x1 ..xi−1 , and produces an output ai .\nIn causal, left-to-right language models, the context is any of the prior words.\nThat is, when processing xi , the model has access to xi as well as the representations\nof all the prior tokens in the context window (context windows consist of thousands\nof tokens) but no tokens after i. (By contrast, in Chapter 10 we’ll generalize attention\nso it can also look ahead to future words.)\nFig. 8.3 illustrates this flow of information in an entire causal self-attention layer,\nin which this same attention computation happens in parallel at each token position\ni. Thus a self-attention layer maps input sequences (x1 , ..., xn ) to output sequences\nof the same length (a1 , ..., an ).\n\nSelf-Attention\nLayer\n\na1\n\na2\n\na3\n\na4\n\na5\n\nattention\n\nattention\n\nattention\n\nattention\n\nattention\n\nx1\n\nx2\n\nx3\n\nx4\n\nx5\n\nFigure 8.3 Information flow in causal self-attention. When processing each input xi , the\nmodel attends to all the inputs up to, and including xi .\n\nSimplified version of attention At its heart, attention is really just a weighted\nsum of context vectors, with a lot of complications added to how the weights are\ncomputed and what gets summed. For pedagogical purposes let’s first describe a\nsimplified intuition of attention, in which the attention output ai at token position i\nis simply the weighted sum of all the representations x j , for all j ≤ i; we’ll use αi j\nto mean how much x j should contribute to ai :\nSimplified version:\n\nai =\n\nX\n\nαi j x j\n\n(8.6)\n\nj≤i\n\nEach αi j is a scalar used for weighing the value of input x j when summing up\nthe inputs to compute ai . How shall we compute this α weighting? In attention we\nweight each prior embedding proportionally to how similar it is to the current token\ni. So the output of attention is a sum of the embeddings of prior tokens weighted\nby their similarity with the current token embedding. We compute similarity scores\nvia dot product, which maps two vectors into a scalar value ranging from −∞ to\n∞. The larger the score, the more similar the vectors that are being compared. We’ll\nnormalize these scores with a softmax to create the vector of weights αi j , j ≤ i.\nSimplified Version:\n\nscore(xi , x j ) = xi · x j\n\nαi j = softmax(score(xi , x j )) ∀ j ≤ i\n\n(8.7)\n(8.8)\n\nThus in Fig. 8.3 we compute a3 by computing three scores: x3 · x1 , x3 · x2 and x3 · x3 ,\nnormalizing them by a softmax, and using the resulting probabilities as weights\nindicating each of their proportional relevance to the current position i. Of course,\n\n\f8.1\n\n•\n\nATTENTION\n\n5\n\nthe softmax weight will likely be highest for xi , since xi is very similar to itself,\nresulting in a high dot product. But other context words may also be similar to i, and\nthe softmax will also assign some weight to those words. Then we use these weights\nas the α values in Eq. 8.6 to compute the weighted sum that is our a3 .\nThe simplified attention in equations 8.6 – 8.8 demonstrates the attention-based\napproach to computing ai : compare the xi to prior vectors, normalize those scores\ninto a probability distribution used to weight the sum of the prior vector. But now\nwe’re ready to remove the simplifications.\nattention head\nhead\n\nquery\nkey\nvalue\n\nA single attention head using query, key, and value matrices Now that we’ve\nseen a simple intuition of attention, let’s introduce the actual attention head, the\nversion of attention that’s used in transformers. (The word head is often used in\ntransformers to refer to specific structured layers). The attention head allows us to\ndistinctly represent three different roles that each input embedding plays during the\ncourse of the attention process:\n• As the current element being compared to the preceding inputs. We’ll refer to\nthis role as a query.\n• In its role as a preceding input that is being compared to the current element\nto determine a similarity weight. We’ll refer to this role as a key.\n• And finally, as a value of a preceding element that gets weighted and summed\nup to compute the output for the current element.\nTo capture these three different roles, transformers introduce weight matrices\nWQ , WK , and WV . These weights will project each input vector xi into a representation of its role as a query, key, or value:\nqi = xi WQ ;\n\nki = xi WK ;\n\nvi = xi WV\n\n(8.9)\n\nGiven these projections, when we are computing the similarity of the current element xi with some prior element x j , we’ll use the dot product between the current\nelement’s query vector qi and the preceding element’s key vector k j . Furthermore,\nthe result of a dot product can be an arbitrarily large (positive or negative) value, and\nexponentiating large values can lead to numerical issues and loss of gradients during\ntraining. To avoid this, we scale the dot product by a factor related to the size of the\nembeddings, via dividing by the square root of the dimensionality of the query and\nkey vectors (dk ). We thus replace the simplified Eq. 8.7 with Eq. 8.11. The ensuing\nsoftmax calculation resulting in αi j remains the same, but the output calculation for\nheadi is now based on a weighted sum over the value vectors v (Eq. 8.13).\nHere’s a final set of equations for computing self-attention for a single selfattention output vector ai from a single input vector xi . This version of attention\ncomputes ai by summing the values of the prior elements, each weighted by the\nsimilarity of its key to the query from the current element:\nqi = xi WQ ; k j = x j WK ; v j = x j WV\nqi · k j\nscore(xi , x j ) = √\ndk\nαi j = softmax(score(xi , x j )) ∀ j ≤ i\nX\nheadi =\nαi j v j\n\n(8.10)\n(8.11)\n(8.12)\n(8.13)\n\nj≤i\n\nai = headi WO\n\n(8.14)\n\n\f6\n\nC HAPTER 8\n\n•\n\nT RANSFORMERS\n\n8. Output of self-attention a3\n\n[1 × d]\nWO\n\n7. Reshape to [1 x d]\n\n[dv × d]\n[1 × dv]\n\n6. Sum the weighted\nvalue vectors\n\n3. Divide scalar score by √dk √d ÷\nk\n\n√dk\n\n2. Compare x3’s query with\nthe keys for x1, x2, and x3\n1. Generate\nkey, query, value\nvectors\n\n𝛼3,2\n\n[1 × dv]\n\n𝛼3,3\n\n×\n\n4. Turn into 𝛼i,j weights via softmax\n\n×\n\n𝛼3,1\n\n5. Weigh each value vector\n\n[1 × dv]\n\n÷\n\n√dk\n\n[1 × dv]\n\n[1 × dv]\n\n÷\n\n[1 × dv]\n\n[1 x dv]\n\nk\n\nq\n\nv\n\nk\n\nq\n\nv\n\nk\n\nq\n\nv\n\nWK\n\nWQ\n\nWV\n\nWK\n\nWQ\n\nWV\n\nWK\n\nWQ\n\nWV\n\nx1\n[1 × d]\n\nx2\n[1 × d]\n\nx3\n[1 × d]\n\nFigure 8.4 Calculating the value of a3 , the third element of a sequence using causal (leftto-right) self-attention.\n\nWe illustrate this in Fig. 8.4 for the case of calculating the value of the third output\na3 in a sequence.\nNote that we’ve also introduced one more matrix, WO , which is right-multiplied\nby the attention head. This is necessary to reshape the output of the head. The input\nto attention xi and the output from attention ai both have the same dimensionality\n[1 × d]. We often call d the model dimensionality, and indeed as we’ll discuss in\nSection 8.2 the output hi of each transformer block, as well as the intermediate vectors inside the transformer block also have the same dimensionality [1 × d]. Having\neverything be the same dimensionality makes the transformer very modular.\nSo let’s talk shapes. How do we get from [1 × d] at the input to [1 × d] at the\noutput? Let’s look at all the internal shapes. We’ll have a dimension dk for the\nquery and key vectors. The query vector and the key vector are both dimensionality\n[1 × dk ], so we can take their dot product qi · k j to produce a scalar. We’ll have a\nseparate dimension dv for the value vectors. The transform matrix WQ has shape\n[d × dk ], WK is [d × dk ], and WV is [d × dv ]. So the output of headi in equation\nEq. 8.13 is of shape [1 × dv ]. To get the desired output shape [1 × d] we’ll need to\nreshape the head output, and so WO is of shape [dv × d]. In the original transformer\nwork (Vaswani et al., 2017), d was 512, dk and dv were both 64.\n\nmulti-head\nattention\n\nMulti-head Attention Equations 8.11-8.13 describe a single attention head. But\nactually, transformers use multiple attention heads. The intuition is that each head\nmight be attending to the context for different purposes: heads might be specialized to represent different linguistic relationships between context elements and the\ncurrent token, or to look for particular kinds of patterns in the context.\nSo in multi-head attention we have A separate attention heads that reside in\nparallel layers at the same depth in a model, each with its own set of parameters that\nallows the head to model different aspects of the relationships among inputs. Thus\n\n\f8.2\n\n•\n\nT RANSFORMER B LOCKS\n\n7\n\neach head i in a self-attention layer has its own set of query, key, and value matrices:\nWQi , WKi , and WVi . These are used to project the inputs into separate query, key,\nand value embeddings for each head.\nWhen using multiple heads the model dimension d is still used for the input\nand output, the query and key embeddings have dimensionality dk , and the value\nembeddings are of dimensionality dv (again, in the original transformer paper dk =\ndv = 64, A = 8, and d = 512). Thus for each head i, we have weight layers WQi of\nshape [d × dk ], WKi of shape [d × dk ], and WVi of shape [d × dv ].\nBelow are the equations for attention augmented with multiple heads; Fig. 8.5\nshows an intuition.\nqci = xi WQc ; kcj = x j WKc ; vcj = x j WVc ; ∀ c 1 ≤ c ≤ A\nqci · kcj\nscorec (xi , x j ) = √\ndk\nαicj = softmax(scorec (xi , x j )) ∀ j ≤ i\nX\nheadci =\nαicj vcj\n\n(8.15)\n(8.16)\n(8.17)\n(8.18)\n\nj≤i\n\nai = (head1 ⊕ head2 ... ⊕ headA )WO (8.19)\n\nMultiHeadAttention(xi , [x1 , · · · , xi−1 ]) = ai\n\n(8.20)\n\nNote in Eq. 8.20 that MultiHeadAttention is a function of the current input xi , as\nwell as all the other inputs. For the causal or left-to-right attention that we use in\nthis chapter, the other inputs are only to the left, but we’ll also see a version of\nattention in Chapter 10 where attention is a function of the tokens to the right as\nwell. We’ll return to this idea about causal inputs in Eq. 8.34 when we introduce the\nidea of masking the right context.\nThe output of each of the A heads is of shape [1 × dv ], and so the output of the\nmulti-head layer with A heads consists of A vectors of shape [1 × dv ]. These are\nconcatenated to produce a single output with dimensionality [1 × Adv ]. Then we use\nyet another linear projection WO ∈ RAdv ×d to reshape it, resulting in the multi-head\nattention vector ai with the correct output shape [1 × d] at each input i.\n\n8.2\n\nTransformer Blocks\n\nresidual stream\n\nThe self-attention calculation lies at the core of what’s called a transformer block,\nwhich, in addition to the self-attention layer, includes three other kinds of layers: (1)\na feedforward layer, (2) residual connections, and (3) normalizing layers (colloquially called “layer norm”).\nFig. 8.6 illustrates a transformer block, sketching a common way of thinking\nabout the block that is called the residual stream (Elhage et al., 2021). In the residual stream viewpoint, we consider the processing of an individual token i through\nthe transformer block as a single stream of d-dimensional representations for token\nposition i. This residual stream starts with the original input vector, and the various\ncomponents read their input from the residual stream and add their output back into\nthe stream.\nThe input at the bottom of the stream is an embedding for a token, which has\ndimensionality d. This initial embedding gets passed up (by residual connections),\nand is progressively added to by the other components of the transformer: the at-\n\n\f8\n\nC HAPTER 8\n\n•\n\nT RANSFORMERS\n\nai\nProject down to d\n\nWO [Adv x d]\n\n…\n\nConcatenate Outputs\n[1 x dv ]\n\nEach head\nattends diﬀerently\nto context\n\n[1 x d]\n\nHead 1\nWK1 WV1 WQ1\n\n[1 x Adv ]\n\n[1 x dv ]\n\nHead 2\nWK2 WV2 WQ2\n\nxi\n\n… xi-3 xi-2 xi-1\n\n…\n\nHead 8\nWK8 WV8 WQ8\n\n[1 ax d]\n\ni\n\nFigure 8.5 The multi-head attention computation for input xi , producing output ai . A multi-head attention\nlayer has A heads, each with its own query, key, and value weight matrices. The outputs from each of the heads\nare concatenated and then projected down to d, thus producing an output of the same size as the input.\n\nhi-1\n\nhi\n\nResidual\nStream\n\nhi+1\n\n+\nFeedforward\nLayer Norm\n\n…\n\n…\n\n+\nMultiHead\nAttention\nLayer Norm\n\nxi-1\n\nxi\n\nxi+1\n\nFigure 8.6 The architecture of a transformer block showing the residual stream. This\nfigure shows the prenorm version of the architecture, in which the layer norms happen before\nthe attention and feedforward layers rather than after.\n\ntention layer that we have seen, and the feedforward layer that we will introduce.\nBefore the attention and feedforward layer is a computation called the layer norm.\nThus the initial vector is passed through a layer norm and attention layer, and\nthe result is added back into the stream, in this case to the original input vector\nxi . And then this summed vector is again passed through another layer norm and a\nfeedforward layer, and the output of those is added back into the residual, and we’ll\nuse hi to refer to the resulting output of the transformer block for token i. (In earlier\ndescriptions the residual stream was often described using a different metaphor as\nresidual connections that add the input of a component to its output, but the residual\nstream is a more perspicuous way of visualizing the transformer.)\n\n\f8.2\n\n•\n\nT RANSFORMER B LOCKS\n\n9\n\nWe’ve already seen the attention layer, so let’s now introduce the feedforward\nand layer norm computations in the context of processing a single input xi at token\nposition i.\nFeedforward layer The feedforward layer is a fully-connected 2-layer network,\ni.e., one hidden layer, two weight matrices, as introduced in Chapter 6. The weights\nare the same for each token position i, but are different from layer to layer. It is common to make the dimensionality dff of the hidden layer of the feedforward network\nbe larger than the model dimensionality d. (For example in the original transformer\nmodel, d = 512 and dff = 2048.)\nFFN(xi ) = ReLU(xi W1 + b1 )W2 + b2\n\nlayer norm\n\n(8.21)\n\nLayer Norm At two stages in the transformer block we normalize the vector (Ba\net al., 2016). This process, called layer norm (short for layer normalization), is one\nof many forms of normalization that can be used to improve training performance\nin deep neural networks by keeping the values of a hidden layer in a range that\nfacilitates gradient-based training.\nLayer norm is a variation of the z-score from statistics, applied to a single vector in a hidden layer. That is, the term layer norm is a bit confusing; layer norm\nis not applied to an entire transformer layer, but just to the embedding vector of a\nsingle token. Thus the input to layer norm is a single vector of dimensionality d\nand the output is that vector normalized, again of dimensionality d. The first step in\nlayer normalization is to calculate the mean, µ, and standard deviation, σ , over the\nelements of the vector to be normalized. Given an embedding vector x of dimensionality d, these values are calculated as follows.\nd\n\n1X\nxi\nd\ni=1\nv\nu d\nu1 X\n(xi − µ)2\nσ = t\nd\nµ =\n\n(8.22)\n\n(8.23)\n\ni=1\n\nGiven these values, the vector components are normalized by subtracting the mean\nfrom each and dividing by the standard deviation. The result of this computation is\na new vector with zero mean and a standard deviation of one.\nx̂ =\n\n(x − µ)\nσ\n\n(8.24)\n\nFinally, in the standard implementation of layer normalization, two learnable parameters, γ and β , representing gain and offset values, are introduced.\nLayerNorm(x) = γ\n\n(x − µ)\n+β\nσ\n\n(8.25)\n\nPutting it all together The function computed by a transformer block can be expressed by breaking it down with one equation for each component computation,\nusing t (of shape [1 × d]) to stand for transformer and superscripts to demarcate\n\n\f10\n\nC HAPTER 8\n\n•\n\nT RANSFORMERS\n\neach computation inside the block:\nt1i = LayerNorm(xi )\n2\n\nti\n\n3\n\n(8.26)\n\n\u0002\n\u0003\n= MultiHeadAttention(ti , t11 , · · · , t1N )\n1\n\n2\n\nti = ti + xi\n\n(8.27)\n(8.28)\n\nti = LayerNorm(ti )\n\n(8.29)\n\nt5i = FFN(t4i )\n\n(8.30)\n\n5\n\n(8.31)\n\n4\n\n3\n\n3\n\nhi = ti + ti\n\nNotice that the only component that takes as input information from other tokens\n(other residual streams) is multi-head attention, which (as we see from Eq. 8.27)\nlooks at all the neighboring tokens in the context. The output from attention, however, is then added into this token’s embedding stream. In fact, Elhage et al. (2021)\nshow that we can view attention heads as literally moving information from the\nresidual stream of a neighboring token into the current stream. The high-dimensional\nembedding space at each position thus contains information about the current token and about neighboring tokens, albeit in different subspaces of the vector space.\nFig. 8.7 shows a visualization of this movement.\n\nToken A\nresidual\nstream\n\nToken B\nresidual\nstream\n\nFigure 8.7 An attention head can move information from token A’s residual stream into\ntoken B’s residual stream.\n\nCrucially, the input and output dimensions of transformer blocks are matched so\nthey can be stacked. Each token vector xi at the input to the block has dimensionality\nd, and the output hi also has dimensionality d. Transformers for large language\nmodels stack many of these blocks, from 12 layers (used for the T5 or GPT-3-small\nlanguage models) to 96 layers (used for GPT-3 large), to even more for more recent\nmodels. We’ll come back to this issue of stacking in a bit.\nEquation 8.26 and following are just the equation for a single transformer block,\nbut the residual stream metaphor goes through all the transformer layers, from the\nfirst transformer blocks to the 12th, in a 12-layer transformer. At the earlier transformer blocks, the residual stream is representing the current token. At the highest\ntransformer blocks, the residual stream is usually representing the following token,\nsince at the very end it’s being trained to predict the next token.\nOnce we stack many blocks, there is one more requirement: at the very end of\nthe last (highest) transformer block, there is a single extra layer norm that is run on\nthe last hi of each token stream (just below the language model head layer that we\nwill define soon). 3\n3\n\nNote that we are using the most common current transformer architecture, which is called the prenorm\n\n\f8.3\n\n8.3\n\n•\n\n11\n\nPARALLELIZING COMPUTATION USING A SINGLE MATRIX X\n\nParallelizing computation using a single matrix X\nThis description of multi-head attention and the rest of the transformer block has\nbeen from the perspective of computing a single output at a single time step i in\na single residual stream. But as we pointed out earlier, the attention computation\nperformed for each token to compute ai is independent of the computation for each\nother token, and that’s also true for all the computation in the transformer block\ncomputing hi from the input xi . That means we can easily parallelize the entire\ncomputation, taking advantage of efficient matrix multiplication routines.\nWe do this by packing the input embeddings for the N tokens of the input sequence into a single matrix X of size [N × d]. Each row of X is the embedding of\none token of the input. Transformers for large language models commonly have an\ninput length N from 1K to 32K; much longer contexts of 128K or even up to millions\nof tokens can also be achieved with architectural changes like special long-context\nmechanisms that we don’t discuss here. So for vanilla transformers, we can think of\nX having between 1K and 32K rows, each of the dimensionality of the embedding\nd (the model dimension).\nParallelizing attention Let’s first see this for a single attention head and then turn\nto multiple heads, and then add in the rest of the components in the transformer\nblock. For one head we multiply X by the query, key, and value matrices WQ of\nshape [d × dk ], WK of shape [d × dk ], and WV of shape [d × dv ], to produce matrices\nQ of shape [N × dk ], K of shape [N × dk ], and V of shape [N × dv ], containing all the\nkey, query, and value vectors:\nQ = XWQ ; K = XWK ; V = XWV\n\n(8.32)\n\nGiven these matrices we can compute all the requisite query-key comparisons simultaneously by multiplying Q and K| in a single matrix multiplication. The product is\nof shape N × N, visualized in Fig. 8.8.\nq1•k1 q1•k2 q1•k3 q1•k4\nq2•k1 q2•k2 q2•k3 q2•k4\n\nN\nq3•k1 q3•k2 q3•k3 q3•k4\nq4•k1 q4•k2 q4•k3 q4•k4\n\nN\n\nFigure 8.8\nsingle matrix multiple.\n\nThe N × N QK| matrix showing how it computes all qi · k j comparisons in a\n\nOnce we have this QK| matrix, we can very efficiently scale these scores, take\nthe softmax, and then multiply the result by V resulting in a matrix of shape N × d:\na vector embedding representation for each token in the input. We’ve reduced the\nentire self-attention step for an entire sequence of N tokens for one head to the\narchitecture. The original definition of the transformer in Vaswani et al. (2017) used an alternative architecture called the postnorm transformer in which the layer norm happens after the attention and FFN\nlayers; it turns out moving the layer norm beforehand works better, but does require this one extra layer\nat the end.\n\n\f12\n\nC HAPTER 8\n\n•\n\nT RANSFORMERS\n\nfollowing computation:\nQK|\nhead = softmax mask √\ndk\n\u0012\n\n\u0012\n\nA = head WO\n\n\u0013\u0013\nV\n\n(8.33)\n(8.34)\n\nMasking out the future You may have noticed that we introduced a mask function\nin Eq. 8.34 above. This is because the self-attention computation as we’ve described\nit has a problem: the calculation of QK| results in a score for each query value to\nevery key value, including those that follow the query. This is inappropriate in the\nsetting of language modeling: guessing the next word is pretty simple if you already\nknow it! To fix this, the elements in the upper-triangular portion of the matrix are set\nto −∞, which the softmax will turn to zero, thus eliminating any knowledge of words\nthat follow in the sequence. This is done in practice by adding a mask matrix M in\nwhich Mi j = −∞ ∀ j > i (i.e. for the upper-triangular portion) and Mi j = 0 otherwise.\nFig. 8.9 shows the resulting masked QK| matrix. (we’ll see in Chapter 10 how to\nmake use of words in the future for tasks that need it).\n\nq1•k1\n\n−∞\n\nq2•k1 q2•k2\n\nN\n\n−∞\n\n−∞\n\n−∞ −∞\n\nq3•k1 q3•k2 q3•k3\n\n−∞\n\nq4•k1 q4•k2 q4•k3 q4•k4\n\nN\n\nFigure 8.9 The N × N QK| matrix showing the qi · k j values, with the upper-triangle portion of the comparisons matrix zeroed out (set to −∞, which the softmax will turn to zero).\n\nFig. 8.10 shows a schematic of all the computations for a single attention head\nparallelized in matrix form.\nFig. 8.8 and Fig. 8.9 also make it clear that attention is quadratic in the length\nof the input, since at each layer we need to compute dot products between each pair\nof tokens in the input. This makes it expensive to compute attention over very long\ndocuments (like entire novels). Nonetheless modern large language models manage\nto use quite long contexts of thousands or tens of thousands of tokens.\nParallelizing multi-head attention In multi-head attention, as with self-attention,\nthe input and output have the model dimension d, the key and query embeddings\nhave dimensionality dk , and the value embeddings are of dimensionality dv (again,\nin the original transformer paper dk = dv = 64, A = 8, and d = 512). Thus for\neach head c, we have weight layers WQ c of shape [d × dk ], WK c of shape [d × dk ],\nand WV c of shape [d × dv ], and these get multiplied by the inputs packed into X to\nproduce Q of shape [N × dk ], K of shape [N × dk ], and V of shape [N × dv ]. The\noutput of each of the A heads is of shape [N × dv ], and so the output of the multihead layer with A heads consists of A matrices of shape [N × dv ]. To make use\nof these matrices in further processing, they are concatenated to produce a single\noutput with dimensionality [N × Adv ]. Finally, we use a final linear projection WO\nof shape [Adv × d], that reshapes it to the original output dimension for each token.\nMultiplying the concatenated [N × Adv ] matrix output by WO of shape [Adv × d]\n\n\f8.3\nX\nWQ\n\nInput\nToken 1\nInput\nToken 2\nInput\nToken 3\n\n=\n\nx\n\nInput\nToken 4\n\nd x dk\n\nNxd\n\nQ\n\nX\n\nQuery\nToken 1\n\nInput\nToken 1\n\nQuery\nToken 2\n\nInput\nToken 2\n\nQuery\nToken 3\n\nInput\nToken 3\n\nQuery\nToken 4\n\nInput\nToken 4\n\nN x dk\n\nNxd\n\nK\nWK\n\n=\n\nx\nd x dk\n\n=\n\nV\n\nX\n\nKey\nToken 1\n\nInput\nToken 1\n\nKey\nToken 2\n\nInput\nToken 2\n\nKey\nToken 3\n\nInput\nToken 3\n\nKey\nToken 4\n\nInput\nToken 4\n\nN x dk\n\nNxd\n\nQKT\nk4\n\nk3\n\nk2\n\nk1\n\nx\n\nq1\n\n13\n\nPARALLELIZING COMPUTATION USING A SINGLE MATRIX X\n\nKT\n\nQ\n\nmask\n\n•\n\nWV\n\nq1•k1\nq1•k1\n\n−∞\n\n−∞\n\nValue\nToken 2\n\n=\n\nx\n\nValue\nToken 3\nValue\nToken 4\n\nd x dv\n\nN x dv\n\nQKT masked\n\nq1•k1 q1•k2 q1•k3 q1•k4\n\nValue\nToken 1\n\n−∞\n\nx\n\nV\n\nA\n\nv1\n\na1\n\n=\n\nq2\n\nq2•k1 q2•k2 q2•k3 q2•k4\n\nq3\n\nq3•k1 q3•k2 q3•k3 q3•k4\n\nq3•k1 q3•k2 q3•k3\n\n−∞\n\nv3\n\na3\n\nq4•k1 q4•k2 q4•k3 q4•k4\n\nq4•k1 q4•k2 q4•k3 q4•k4\n\nv4\n\na4\n\nNxN\n\nNxN\n\nN x dv\n\nN x dv\n\ndk x N\n\nq4\n\nN x dk\n\n=\n\nq2•k1 q2•k2\n\n−∞ −∞\n\nv2\n\na2\n\nFigure 8.10 Schematic of the attention computation for a single attention head in parallel. The first row shows\nthe computation of the Q, K, and V matrices. The second row shows the computation of QKT , the masking\n(the softmax computation and the normalizing by dimensionality are not shown) and then the weighted sum of\nthe value vectors to get the final attention vectors.\n\nyields the self-attention output A of shape [N × d].\nQi = XWQi ; Ki = XWKi ; Vi = XWVi\n\u0012\n\u0012 i i | \u0013\u0013\nQK\ni\ni\ni\nheadi = SelfAttention(Q , K , V ) = softmax mask √\nVi\ndk\n\n(8.35)\n(8.36)\n\nMultiHeadAttention(X) = (head1 ⊕ head2 ... ⊕ headA )WO (8.37)\nPutting it all together with the parallel input matrix X The function computed\nin parallel by an entire layer of N transformer blocks—each block over one of the N\ninput tokens—can be expressed as:\nO = X + MultiHeadAttention(LayerNorm(X))\n\n(8.38)\n\nH = O + FFN(LayerNorm(O))\n\n(8.39)\n\nNote that in Eq. 8.38 we are using X to mean the input to the layer, wherever it\ncomes from. For the first layer, as we will see in the next section, that input is the\ninitial word + positional embedding vectors that we have been describing by X. But\nfor subsequent layers k, the input is the output from the previous layer Hk−1 . We\ncan also break down the computation performed in a transformer layer, showing one\nequation for each component computation. We’ll use T (of shape [N × d]) to stand\nfor transformer and superscripts to demarcate each computation inside the block,\nand again use X to mean the input to the block from the previous layer or the initial\n\n\f14\n\nC HAPTER 8\n\n•\n\nT RANSFORMERS\n\nembedding:\nT1 = LayerNorm(X)\nT\n\n2\n\n(8.40)\n\n= MultiHeadAttention(T )\n1\n\n(8.41)\n\nT3 = T2 + X\nT4 = LayerNorm(T3 )\n\n(8.42)\n\n4\n\n= FFN(T )\n\n(8.44)\n\n5\n\n3\n\n(8.45)\n\nT\n\n5\n\nH = T +T\n\n(8.43)\n\nHere when we use a notation like FFN(T3 ) we mean that the same FFN is applied\nin parallel to each of the N embedding vectors in the window. Similarly, each of the\nN tokens is normed in parallel in the LayerNorm. Crucially, the input and output\ndimensions of transformer blocks are matched so they can be stacked. Since each\ntoken xi at the input to the block is represented by an embedding of dimensionality\n[1 × d], that means the input X and output H are both of shape [N × d].\n\n8.4\n\nThe input: embeddings for token and position\n\nembedding\n\none-hot vector\n\nLet’s talk about where the input X comes from. Given a sequence of N tokens (N is\nthe context length in tokens), the matrix X of shape [N × d] has an embedding for\neach word in the context. The transformer does this by separately computing two\nembeddings: an input token embedding, and an input positional embedding.\nA token embedding, introduced in Chapter 6, is a vector of dimension d that will\nbe our initial representation for the input token. (As we pass vectors up through the\ntransformer layers in the residual stream, this embedding representation will change\nand grow, incorporating context and playing a different role depending on the kind\nof language model we are building.) The set of initial embeddings are stored in the\nembedding matrix E, which has a row for each of the |V | tokens in the vocabulary.\n(Reminder that V here means the vocabulary of tokens, this V is not related to the\nvalue vector.) Thus each word is a row vector of d dimensions, and E has shape\n[|V | × d].\nGiven an input token string like Thanks for all the we first convert the tokens\ninto vocabulary indices (these were created when we first tokenized the input using\nBPE or SentencePiece). So the representation of thanks for all the might be w =\n[5, 4000, 10532, 2224]. Next we use indexing to select the corresponding rows from\nE, (row 5, row 4000, row 10532, row 2224).\nAnother way to think about selecting token embeddings from the embedding\nmatrix is to represent tokens as one-hot vectors of shape [1 × |V |], i.e., with one\ndimension for each word in the vocabulary. Recall that in a one-hot vector all the\nelements are 0 except one, the element whose dimension is the word’s index in the\nvocabulary, which has value 1. So if the word “thanks” has index 5 in the vocabulary,\nx5 = 1, and xi = 0 ∀i 6= 5, as shown here:\n[0 0 0 0 1 0 0 ... 0 0 0 0]\n1 2 3 4 5 6 7 ... ... |V|\nMultiplying by a one-hot vector that has only one non-zero element xi = 1 simply\nselects out the relevant row vector for word i, resulting in the embedding for word i,\nas depicted in Fig. 8.11.\n\n\f8.4\n\n•\n\nT HE INPUT: EMBEDDINGS FOR TOKEN AND POSITION\n\n15\n\nd\n5\n1\n\n|V|\n\n0000100…0000\n\n5\n\n✕\n\n=\n\nE\n\nd\n\n1\n\n|V|\n\nFigure 8.11 Selecting the embedding vector for word V5 by multiplying the embedding\nmatrix E with a one-hot vector with a 1 in index 5.\n\nWe can extend this idea to represent the entire token sequence as a matrix of onehot vectors, one for each of the N positions in the transformer’s context window, as\nshown in Fig. 8.12.\nd\nd\n\n|V|\n0000100…0000\n0000000…0010\n1000000…0000\n\n✕\n\n…\n\nN\n\n=\n\nE\n\nN\n\n0000100…0000\n\n| V|\n\nFigure 8.12 Selecting the embedding matrix for the input sequence of token ids W by multiplying a one-hot matrix corresponding to W by the embedding matrix E.\n\npositional\nembeddings\nabsolute\nposition\n\nThese token embeddings are not position-dependent. To represent the position\nof each token in the sequence, we combine these token embeddings with positional\nembeddings specific to each position in an input sequence.\nWhere do we get these positional embeddings? The simplest method, called\nabsolute position, is to start with randomly initialized embeddings corresponding\nto each possible input position up to some maximum length. For example, just as\nwe have an embedding for the word fish, we’ll have an embedding for the position 3.\nAs with word embeddings, these positional embeddings are learned along with other\nparameters during training. We can store them in a matrix Epos of shape [N × d].\nTo produce an input embedding that captures positional information, we just\nadd the word embedding for each input to its corresponding positional embedding.\nThe individual token and position embeddings are both of size [1×d], so their sum is\nalso [1×d], This new embedding serves as the input for further processing. Fig. 8.13\nshows the idea.\nTransformer Block\n\n+\n\n+\n\n+\n\n+\n\nbill\n\nthe\n\nback\n\nwill\n1\n\n2\n\n3\n\n4\n\n5\n\nPosition\nEmbeddings\n\nJanet\n\nWord\nEmbeddings\n\n+\n\nX = Composite\nEmbeddings\n(word + position)\n\nJanet\n\nwill\n\nback\n\nthe\n\nbill\n\nFigure 8.13 A simple way to model position: add an embedding of the absolute position to\nthe token embedding to produce a new embedding of the same dimensionality.\n\n\f16\n\nC HAPTER 8\n\nrelative\nposition\n\n8.5\n\n•\n\nT RANSFORMERS\n\nThe final representation of the input, the matrix X, is an [N × d] matrix in which\neach row i is the representation of the ith token in the input, computed by adding\nE[id(i)]—the embedding of the id of the token that occurred at position i—, to P[i],\nthe positional embedding of position i.\nA potential problem with the simple position embedding approach is that there\nwill be plenty of training examples for the initial positions in our inputs and correspondingly fewer at the outer length limits. These latter embeddings may be poorly\ntrained and may not generalize well during testing. An alternative is to choose a\nstatic function that maps integer inputs to real-valued vectors in a way that better\nhandles sequences of arbitrary length. A combination of sine and cosine functions\nwith differing frequencies was used in the original transformer work. Sinusoidal position embeddings may also help in capturing the inherent relationships among the\npositions, like the fact that position 4 in an input is more closely related to position\n5 than it is to position 17.\nA more complex style of positional embedding methods extend this idea of capturing relationships even further to directly represent relative position instead of\nabsolute position, often implemented in the attention mechanism at each layer rather\nthan being added once at the initial input.\n\nThe Language Modeling Head\n\nlanguage\nmodeling head\nhead\n\nThe last component of the transformer we must introduce is the language modeling\nhead. Here we are using the word head to mean the additional neural circuitry we\nadd on top of the basic transformer architecture when we apply pretrained transformer models to various tasks. The language modeling head is the circuitry we\nneed to do language modeling.\nRecall that language models, from the simple n-gram models of Chapter 3 through\nthe feedforward and RNN language models of Chapter 6 and Chapter 13, are word\npredictors. Given a context of words, they assign a probability to each possible next\nword. For example, if the preceding context is “Thanks for all the” and we want to\nknow how likely the next word is “fish” we would compute:\nP(fish|Thanks for all the)\nLanguage models give us the ability to assign such a conditional probability to every\npossible next word, giving us a distribution over the entire vocabulary. The n-gram\nlanguage models of Chapter 3 compute the probability of a word given counts of\nits occurrence with the n − 1 prior words. The context is thus of size n − 1. For\ntransformer language models, the context is the size of the transformer’s context\nwindow, which can be quite large, like 32K tokens for large models (and much larger\ncontexts of millions of words are possible with special long-context architectures).\nThe job of the language modeling head is to take the output of the final transformer layer from the last token N and use it to predict the upcoming word at position N + 1. Fig. 8.14 shows how to accomplish this task, taking the output of the last\ntoken at the last layer (the d-dimensional output embedding of shape [1 × d]) and\nproducing a probability distribution over words (from which we will choose one to\ngenerate).\nThe first module in Fig. 8.14 is a linear layer, whose job is to project from the\noutput hLN , which represents the output token embedding at position N from the final\n\n\f8.5\n\ny1\n\n•\n\n…\n\ny2\n\ny|V|\n\nLanguage Model Head\n\nhL1\n\nWord probabilities 1 x |V|\nSoftmax over vocabulary V\n\ntakes hLN and outputs a\ndistribution over vocabulary V\n\nu1\n\n…\n\nu2\n\nu|V|\n\nUnembedding layer\nU = ET\n\nhL2\n\nLayer L\nTransformer\nBlock\n\n17\n\nT HE L ANGUAGE M ODELING H EAD\n\nhLN\n\nLogits\n\n1 x |V|\n\nUnembedding layer d x |V|\n\n1xd\n\n…\nw1\n\nw2\n\nwN\n\nFigure 8.14 The language modeling head: the circuit at the top of a transformer that maps from the output\nembedding for token N from the last transformer layer (hLN ) to a probability distribution over words in the\nvocabulary V .\nlogit\n\nweight tying\n\nunembedding\n\ndecoder-only\nmodel\n\nblock L, (hence of shape [1 × d]) to the logit vector, or score vector, that will have a\nsingle score for each of the |V | possible words in the vocabulary V . The logit vector\nu is thus of dimensionality [1 × |V |].\nThis linear layer can be learned, but more commonly we tie this matrix to (the\ntranspose of) the embedding matrix E. Recall that in weight tying, we use the\nsame weights for two different matrices in the model. Thus at the input stage of the\ntransformer the embedding matrix (of shape [|V | × d]) is used to map from a one-hot\nvector over the vocabulary (of shape [1 × |V |]) to an embedding (of shape [1 × d]).\nAnd then in the language model head, ET , the transpose of the embedding matrix (of\nshape [d × |V |]) is used to map back from an embedding (shape [1 × d]) to a vector\nover the vocabulary (shape [1×|V |]). In the learning process, E will be optimized to\nbe good at doing both of these mappings. We therefore sometimes call the transpose\nET the unembedding layer because it is performing this reverse mapping.\nA softmax layer turns the logits u into the probabilities y over the vocabulary.\nu = hLN ET\n\n(8.46)\n\ny = softmax(u)\n\n(8.47)\n\nWe can use these probabilities to do things like help assign a probability to a\ngiven text. But the most important usage is to generate text, which we do by sampling a word from these probabilities y. We might sample the highest probability\nword (‘greedy’ decoding), or use another of the sampling methods from Section ??\nor Section 8.6.\nIn either case, whatever entry yk we choose from the probability vector y, we\ngenerate the word that has that index k.\nFig. 8.15 shows the total stacked architecture for one token i. Note that the input\nto each transformer layer xi` is the same as the output from the preceding layer hi`−1 .\nA terminological note before we conclude: You will sometimes see a transformer used for this kind of unidirectional causal language model called a decoderonly model. This is because this model constitutes roughly half of the encoderdecoder model for transformers that we’ll see how to apply to machine translation\nin Chapter 12. (Confusingly, the original introduction of the transformer had an\nencoder-decoder architecture, and it was only later that the standard paradigm for\n\n\f18\n\nC HAPTER 8\n\n•\n\nT RANSFORMERS\n\ny1\n\nToken probabilities\nLanguage\nModeling\nHead\n\n…\n\ny2\n\ny|V|\n\nsoftmax\nlogits\n\nwi+1\nSample token to\ngenerate at position i+1\n\nu1\n\n…\n\nu2\n\nu|V|\n\nU\nhLi\nfeedforward\n\nLayer L\n\nlayer norm\nattention\nlayer norm\nhL-1i = xLi\n\n…\n\nh2i = x3i\n\nfeedforward\n\nLayer 2\n\nlayer norm\nattention\nlayer norm\n\nh1i = x2i\nfeedforward\n\nLayer 1\n\nlayer norm\nattention\nlayer norm\n\nx1i\nInput\nEncoding\nInput token\n\n+\n\ni\n\nE\n\nwi\n\nFigure 8.15 A transformer language model (decoder-only), stacking transformer blocks\nand mapping from an input token wi to to a predicted next token wi+1 .\n\ncausal language model was defined by using only the decoder part of this original\narchitecture).\n\n8.6\n\nMore on Sampling\nThe sampling methods we introduce below each have parameters that enable trading off two important factors in generation: quality and diversity. Methods that\nemphasize the most probable words tend to produce generations that are rated by\npeople as more accurate, more coherent, and more factual, but also more boring\nand more repetitive. Methods that give a bit more weight to the middle-probability\nwords tend to be more creative and more diverse, but less factual and more likely to\nbe incoherent or otherwise low-quality.\n\n8.6.1\ntop-k sampling\n\nTop-k sampling\n\nTop-k sampling is a simple generalization of greedy decoding. Instead of choosing\nthe single most probable word to generate, we first truncate the distribution to the\n\n\f8.7\n\n•\n\nT RAINING\n\n19\n\ntop k most likely words, renormalize to produce a legitimate probability distribution,\nand then randomly sample from within these k words according to their renormalized\nprobabilities. More formally:\n1. Choose in advance a number of words k\n2. For each word in the vocabulary V , use the language model to compute the\nlikelihood of this word given the context p(wt |w<t )\n3. Sort the words by their likelihood, and throw away any word that is not one of\nthe top k most probable words.\n4. Renormalize the scores of the k words to be a legitimate probability distribution.\n5. Randomly sample a word from within these remaining k most-probable words\naccording to its probability.\nWhen k = 1, top-k sampling is identical to greedy decoding. Setting k to a larger\nnumber than 1 leads us to sometimes select a word which is not necessarily the most\nprobable, but is still probable enough, and whose choice results in generating more\ndiverse but still high-enough-quality text.\n\n8.6.2\n\ntop-p sampling\n\nNucleus or top-p sampling\n\nOne problem with top-k sampling is that k is fixed, but the shape of the probability\ndistribution over words differs in different contexts. If we set k = 10, sometimes\nthe top 10 words will be very likely and include most of the probability mass, but\nother times the probability distribution will be flatter and the top 10 words will only\ninclude a small part of the probability mass.\nAn alternative, called top-p sampling or nucleus sampling (Holtzman et al.,\n2020), is to keep not the top k words, but the top p percent of the probability mass.\nThe goal is the same; to truncate the distribution to remove the very unlikely words.\nBut by measuring probability rather than the number of words, the hope is that the\nmeasure will be more robust in very different contexts, dynamically increasing and\ndecreasing the pool of word candidates.\nGiven a distribution P(wt |w<t ), we sort the distribution from most probable, and\nthen the top-p vocabulary V (p) is the smallest set of words such that\nX\nP(w|w<t ) ≥ p.\n(8.48)\nw∈V (p)\n\n8.7\n\nTraining\nWe described the training process for language models in the prior chapter. Recall that large language models are trained with cross-entropy loss, also called the\nnegative log likelihood loss. At time t the cross-entropy loss is the negative log probability the model assigns to the next word in the training sequence, − log p(wt+1 ).\nFig. 8.16 illustrates the general training approach. At each step, given all the\npreceding words, the final transformer layer produces an output distribution over the\nentire vocabulary. During training, the probability assigned to the correct word by\nthe model is used to calculate the cross-entropy loss for each item in the sequence.\nThe loss for a training sequence is the average cross-entropy loss over the entire\nsequence. The weights in the network are adjusted to minimize the average CE loss\nover the training sequence via gradient descent.\n\n\f20\n\nC HAPTER 8\nNext token\n\n•\n\nT RANSFORMERS\n\nlong\n\nand\n<latexit sha1_base64=\"AovqpaL476UmJ1EU1xZPgDZ70tQ=\">AAAB9nicbVDLSsNAFL2pr1pfURcu3AwWwY0lEakui25cVrAPaEqYTCbt0EkmzEzEEvIrbkTcKPgZ/oJ/Y9Jm09YDA4dzznDvPV7MmdKW9WtU1tY3Nreq27Wd3b39A/PwqKtEIgntEMGF7HtYUc4i2tFMc9qPJcWhx2nPm9wXfu+ZSsVE9KSnMR2GeBSxgBGsc8k1Ty4dLkZo6qZOiPVYhimO/CyruWbdalgzoFVil6QOJdqu+eP4giQhjTThWKmBbcV6mGKpGeE0qzmJojEmEzyi6WztDJ3nko8CIfMXaTRTF3I4VGoaenmy2E0te4X4nzdIdHA7TFkUJ5pGZD4oSDjSAhUdIJ9JSjSf5gQTyfINERljiYnOmypOt5cPXSXdq4bdbDQfr+utu7KEKpzCGVyADTfQggdoQwcIZPAGn/BlvBivxrvxMY9WjPLPMSzA+P4DPEiSHA==</latexit>\n\nLoss\nLanguage\nModeling\nHead\n\nStacked\nTransformer\nBlocks\n\nlogits\n\nthanks\n\nlog yand\nlogits\n\n<latexit sha1_base64=\"q3ZgXDyG7qtkT7t8hT47RdlwYG4=\">AAAB+XicbVDLSsNAFJ3UV62vWHe6GVsEN5bERXUlBUVcVrAPaEqYTCft0MlMmJkIIQT8AT/CTRE3Cv6Ev+DfmLTdtPXAwOGcM9x7jxcyqrRl/RqFtfWNza3idmlnd2//wDwst5WIJCYtLJiQXQ8pwignLU01I91QEhR4jHS88W3ud56JVFTwJx2HpB+gIac+xUhnkmseXzhMDGHsJk6A9EgGiR4hPlZpWnLNqlWzpoCrxJ6TauP0tXw3qdw0XfPHGQgcBYRrzJBSPdsKdT9BUlPMSFpyIkVChMdoSJLp5ik8y6QB9IXMHtdwqi7kUKBUHHhZMl9PLXu5+J/Xi7R/3U8oDyNNOJ4N8iMGtYB5DXBAJcGaxRlBWNJsQ4hHSCKss7Ly0+3lQ1dJ+7Jm12v1x6yDezBDEZyACjgHNrgCDfAAmqAFMHgBE/AJvozEeDPejY9ZtGDM/xyBBRjff79pldo=</latexit>\n\nfor\n\n…\n\nlog ythanks\n\nlogits\n\nlogits\n\nU\n\nU\n\nU\n\nU\n\n…\n\n…\n\n…\n\n…\n\n…\n\n…\n\nx1\n\nx2\n\nx3\n\nx4\n\nx5\n\n…\n\n+\n\n1\n\n+\n\n+\n\n2\n\n+\n\n3\n\n4\n\n+\n\nE\n\nE\n\nE\n\nE\n\nE\n\nInput tokens\n\nSo\n\nlong\n\nand\n\nthanks\n\nfor\n\n=\n\n…\n\nlogits\n\nU\n\nInput\nEncoding\n\nFigure 8.16\n\n…\n\nall\n\n5\n\n…\n\nTraining a transformer as a language model.\n\nWith transformers, each training item can be processed in parallel since the output for each element in the sequence is computed separately.\nLarge models are generally trained by filling the full context window (for example 4096 tokens for GPT4 or 8192 for Llama 3) with text. If documents are shorter\nthan this, multiple documents are packed into the window with a special end-of-text\ntoken between them. The batch size for gradient descent is usually quite large (the\nlargest GPT-3 model uses a batch size of 3.2 million tokens).\n\n8.8\n\nDealing with Scale\nLarge language models are large. For example the Llama 3.1 405B Instruct model\nfrom Meta has 405 billion parameters (L=126 layers, a model dimensionality of\nd=16,384, A=128 attention heads) and was trained on 15.6 terabytes of text tokens\n(Llama Team, 2024), using a vocabulary of 128K tokens. So there is a lot of research\non understanding how LLMs scale, and especially how to implement them given\nlimited resources. In the next few sections we discuss how to think about scale (the\nconcept of scaling laws), and important techniques for getting language models to\nwork efficiently, such as the KV cache and parameter-efficient fine tuning.\n\n8.8.1\n\nscaling laws\n\nScaling laws\n\nThe performance of large language models has shown to be mainly determined by\n3 factors: model size (the number of parameters not counting embeddings), dataset\nsize (the amount of training data), and the amount of compute used for training. That\nis, we can improve a model by adding parameters (adding more layers or having\nwider contexts or both), by training on more data, or by training for more iterations.\nThe relationships between these factors and performance are known as scaling\nlaws. Roughly speaking, the performance of a large language model (the loss) scales\n\n\f8.8\n\n•\n\nD EALING WITH S CALE\n\n21\n\nas a power-law with each of these three properties of model training.\nFor example, Kaplan et al. (2020) found the following three relationships for\nloss L as a function of the number of non-embedding parameters N, the dataset size\nD, and the compute budget C, for models training with limited parameters, dataset,\nor compute budget, if in each case the other two properties are held constant:\nL(N) =\nL(D) =\nL(C) =\n\n\u0012\n\nNc\nN\n\n\u0013αN\n\n\u0012\n\nDc\nD\n\n\u0013 αD\n\n\u0012\n\nCc\nC\n\n\u0013αC\n\n(8.49)\n(8.50)\n(8.51)\n\nThe number of (non-embedding) parameters N can be roughly computed as follows (ignoring biases, and with d as the input and output dimensionality of the\nmodel, dattn as the self-attention layer size, and dff the size of the feedforward layer):\nN ≈ 2 d nlayer (2 dattn + dff )\n≈ 12 nlayer d 2\n\n(8.52)\n\n(assuming dattn = dff /4 = d)\n\nThus GPT-3, with n = 96 layers and dimensionality d = 12288, has 12 × 96 ×\n122882 ≈ 175 billion parameters.\nThe values of Nc , Dc , Cc , αN , αD , and αC depend on the exact transformer\narchitecture, tokenization, and vocabulary size, so rather than all the precise values,\nscaling laws focus on the relationship with loss.4\nScaling laws can be useful in deciding how to train a model to a particular performance, for example by looking at early in the training curve, or performance with\nsmaller amounts of data, to predict what the loss would be if we were to add more\ndata or increase model size. Other aspects of scaling laws can also tell us how much\ndata we need to add when scaling up a model.\n\n8.8.2\n\nKV Cache\n\nWe saw in Fig. 8.10 and in Eq. 8.34 (repeated below) how the attention vector can\nbe very efficiently computed in parallel for training, via two matrix multiplications:\nA = softmax\n\nKV cache\n\n\u0012\n\nQK|\n√\ndk\n\n\u0013\nV\n\n(8.53)\n\nUnfortunately we can’t do quite the same efficient computation in inference as\nin training. That’s because at inference time, we iteratively generate the next tokens\none at a time. For a new token that we have just generated, call it xi , we need to\ncompute its query, key, and values by multiplying by WQ , WK , and WV respectively. But it would be a waste of computation time to recompute the key and value\nvectors for all the prior tokens x<i ; at prior steps we already computed these key\nand value vectors! So instead of recomputing these, whenever we compute the key\nand value vectors we store them in memory in the KV cache, and then we can just\ngrab them from the cache when we need them. Fig. 8.17 modifies Fig. 8.10 to show\n\n\f22\n\nC HAPTER 8\n\n•\n\nT RANSFORMERS\nQKT\n\nQ\n\nV\n\nKT\n\nv1\nk4\n\nk3\n\nk2\n\nk1\n\nx\n\nA\n\n=\n\nx\n\nv2\n\n=\n\nv3\n\ndk x N\n\nq4\n\n1 x dk\n\nq4•k1 q4•k2 q4•k3 q4•k4\n\nv4\n\n1xN\n\nN x dv\n\na4\n\n1 x dv\n\nFigure 8.17 Parts of the attention computation (extracted from Fig. 8.10) showing, in black,\nthe vectors that can be stored in the cache rather than recomputed when computing the attention score for the 4th token.\n\nthe computation that takes place for a single new token, showing which values we\ncan take from the cache rather than recompute.\n\n8.8.3\n\nparameterefficient fine\ntuning\nPEFT\nLoRA\n\nParameter Efficient Fine Tuning\n\nAs we mentioned above, it’s very common to take a language model and give it more\ninformation about a new domain by finetuning it (continuing to train it to predict\nupcoming words) on some additional data.\nFine-tuning can be very difficult with very large language models, because there\nare enormous numbers of parameters to train; each pass of batch gradient descent\nhas to backpropagate through many many huge layers. This makes finetuning huge\nlanguage models extremely expensive in processing power, in memory, and in time.\nFor this reason, there are alternative methods that allow a model to be finetuned\nwithout changing all the parameters. Such methods are called parameter-efficient\nfine tuning or sometimes PEFT, because we efficiently select a subset of parameters\nto update when finetuning. For example we freeze some of the parameters (don’t\nchange them), and only update some particular subset of parameters.\nHere we describe one such model, called LoRA, for Low-Rank Adaptation. The\nintuition of LoRA is that transformers have many dense layers which perform matrix\nmultiplication (for example the WQ , WK , WV , WO layers in the attention computation). Instead of updating these layers during finetuning, with LoRA we freeze these\nlayers and instead update a low-rank approximation that has fewer parameters.\nConsider a matrix W of dimensionality [N × d] that needs to be updated during\nfinetuning via gradient descent. Normally this matrix would get updates ∆W of\ndimensionality [N × d], for updating the N × d parameters after gradient descent. In\nLoRA, we freeze W and update instead a low-rank decomposition of W. We create\ntwo matrices A and B, where A has size [N ×r] and B has size [r ×d], and we choose\nr to be quite small, r << min(d, N). During finetuning we update A and B instead\nof W. That is, we replace W + ∆W with W + AB. Fig. 8.18 shows the intuition.\nFor replacing the forward pass h = xW, the new forward pass is instead:\nh = xW + xAB\n\n(8.54)\n\nLoRA has a number of advantages. It dramatically reduces hardware requirements,\nsince gradients don’t have to be calculated for most parameters. The weight updates\ncan be simply added in to the pretrained weights, since AB is the same size as W).\nFor the initial experiment in Kaplan et al. (2020) the precise values were αN = 0.076, Nc = 8.8 ×1013\n(parameters), αD = 0.095, Dc = 5.4 ×1013 (tokens), αC = 0.050, Cc = 3.1 ×108 (petaflop-days).\n\n4\n\n\f8.9\n\nh 1\n\n•\n\nI NTERPRETING THE T RANSFORMER\n\nd\n\n× r\nN\n\n23\n\nPretrained\nWeights\n\nd\n\nB\n\nN A\n\nW\nd\n\nr\n\nx 1\nd\n\nFigure 8.18 The intuition of LoRA. We freeze W to its pretrained values, and instead finetune by training a pair of matrices A and B, updating those instead of W, and just sum W and\nthe updated AB.\n\nThat means it doesn’t add any time during inference. And it also means it’s possible\nto build LoRA modules for different domains and just swap them in and out by\nadding them in or subtracting them from W.\nIn its original version LoRA was applied just to the matrices in the attention\ncomputation (the WQ , WK , WV , and WO layers). Many variants of LoRA exist.\n\n8.9\n\nInterpreting the Transformer\n\ninterpretability\n\nHow does a transformer-based language model manage to do so well at language\ntasks? The subfield of interpretability, sometimes called mechanistic interpretability, focuses on ways to understand mechanistically what is going on inside the\ntransformer. In the next two subsections we discuss two well-studied aspects of\ntransformer interpretability.\n\n8.9.1\n\nin-context\nlearning\n\nIn-Context Learning and Induction Heads\n\nAs a way of getting a model to do what we want, we can think of prompting as being\nfundamentally different than pretraining. Learning via pretraining means updating\nthe model’s parameters by using gradient descent according to some loss function.\nBut prompting with demonstrations can teach a model to do a new task. The model\nis learning something about the task from those demonstrations as it processes the\nprompt.\nEven without demonstrations, we can think of the process of prompting as a kind\nof learning. For example, the further a model gets in a prompt, the better it tends\nto get at predicting the upcoming tokens. The information in the context is helping\ngive the model more predictive power.\nThe term in-context learning was first proposed by Brown et al. (2020) in their\nintroduction of the GPT3 system, to refer to either of these kinds of learning that lan-\n\n\f24\n\nC HAPTER 8\n\ninduction heads\n\n•\n\nT RANSFORMERS\n\nguage models do from their prompts. In-context learning means language models\nlearning to do new tasks, better predict tokens, or generally reduce their loss during the forward-pass at inference-time, without any gradient-based updates to the\nmodel’s parameters.\nHow does in-context learning work? While we don’t know for sure, there are\nsome intriguing ideas. One hypothesis is based on the idea of induction heads\n(Elhage et al., 2021; Olsson et al., 2022). Induction heads are the name for a circuit,\nwhich is a kind of abstract component of a network. The induction head circuit\nis part of the attention computation in transformers, discovered by looking at mini\nlanguage models with only 1-2 attention heads.\nThe function of the induction head is to predict repeated sequences. For example\nif it sees the pattern AB...A in an input sequence, it predicts that B will follow,\ninstantiating the pattern completion rule AB...A→ B. It does this by having a prefix\nmatching component of the attention computation that, when looking at the current\ntoken A, searches back over the context to find a prior instance of A. If it finds one,\nthe induction head has a copying mechanism that “copies” the token B that followed\nthe earlier A, by increasing the probability the B will occur next. Fig. 8.19 shows an\nexample.\n\nFigure\nAn induction\nhead\nlooking\nuses\nthe prefix\nmatching\nmechanism\nFigure 1:8.19\nIn the sequence\n“...vintage\ncars ...\nvintage”,atanvintage\ninduction head\nidentifies\nthe initial\noccurrence\nof “vintage”,to\nattends\nto theinstance\nsubsequentofword\n“cars” forand\nprefixthe\nmatching,\nand mechanism\npredicts “cars” to\nas the\nnext word\ncopying\nfind\na prior\nvintage,\ncopying\npredict\nthathrough\ncars the\nwill\noccur\nmechanism.\nagain.\nFigure from Crosbie and Shutova (2022).\n\nablating\n\ndetermines each head’s independent output for the\n4.2 Identifying Induction Heads\nOlsson et al. (2022) propose that a generalized\nfuzzy version of this pattern comcurrent token.\nTo\nidentify\nmodels,\nweB\nmeapletion\nrule,\nimplementing\na\nrule\nlike\nA*B*...A→\nB,induction\nwhere heads\nA* ≈within\nA and\nB* ≈\n(by\nLeveraging this decomposition, Elhage et al. sure the ability\nof all attention heads to perform\n≈(2021)\nwe mean\ntheyathey\narebehaviour\nsemantically\nin\nsome\nway),\nmight\nbe\nresponsible\n4\ndiscovered\ndistinct\nin certainsimilar\nprefix matching on random input sequences. We\nheads, which\nthey named\ninduction evidence\nheads. follow\nforattention\nin-context\nlearning.\nSuggestive\nfor the\ntheir\nhypothesis\ncomes\nfrom Crostask-agnostic\napproach\nto computing\npreThis\nbehaviour\nemerges\nwhen who\nthese heads\nfix matching\nscores outlined\nBansal etin-context\nal. (2023).\nbie\nand\nShutova\n(2022),\nshowprocess\nthat ablating\ninduction\nheadsbycauses\nsequences of the form \"[A] [B] ... [A] → \". In\nWeis\nargue\nthat focusing\nsolely on term\nprefix matching\nlearning\nperformance to decrease. Ablation\noriginally\na medical\nmeaning\nthese heads, the QK circuit directs attention to- scores is sufficient for our analysis, as high prethe\nremoval\nofappears\nsomething.\nWetheuse\nit in NLP\ninterpretability\nstudies\nas\na\ntool for\nwards\n[B], which\ndirectly after\nprevious\nfix matching cores specifically indicate induction\noccurrence\nof theeffects;\ncurrent token\n[A].knock\nThis behaviour\ntesting\ncausal\nif we\nout a hypothesized\ncause,\nwould\nthe\nheads, while less\nrelevantwe\nheads\ntend toexpect\nshow high\nis termed\nprefix matching.\nThe OV\ncircuit\nsubse- (2022)\ncopying\ncapabilities\n(Bansalheads\net al., 2023).\nWe findgeneffect\nto disappear.\nCrosbie\nand\nShutova\nablate\ninduction\nby first\nquently increases the output logit of the [B] token, erate a sequence of 50 random tokens, excluding\ning\nattention heads that perform as induction heads on random input sequences, and\ntermed copying. An overview of this echanism is\nthe 4% most common and least common tokens.\nthen\nzeroing\nout\nsetting certain terms of the output mashown\nin Figure\n1. the output of these heads by\nThis sequence is repeated four times to form the\n\ntrix WO to zero. Indeed they find that ablated\nare The\nmuch\nworse\nat in-context\ninputmodels\nto the model.\nprefix\nmatching\nscore is cal4 Methods\nculated\nby averaging\nthe demonstrations\nattention values fromin\neach\nlearning:\nthey have much worse performance\nat learning\nfrom\nthe\ntoken to the tokens that directly followed the same\nprompts.\n4.1 Models\n\nlogit lens\n\ntoken in earlier repeats. The final prefix matching\nWe utilise two recently developed open-source\nscores are averaged over five random sequences.\nmodels, namely\nLlama-3-8B\nThe prefix matching scores for Llama-3-8B are\n8.9.2\nLogit\nLens 2 and InternLM2-20B\n(Cai et al., 2024), both of which are based on the\nshown in Figure 2. For IntermLM2-20B, we refer\noriginal Llama\net al., 2023a)\narchitecto Figure\nin Appendix A.1. Both\nmodelsoffers\nexhibit a\nAnother\nuseful(Touvron\ninterpretability\ntool,\nthe logit\nlens 8(Nostalgebraist,\n2020),\nture. These models feature grouped-query atten- heads with notably high prefix matching scores,\nway\nto visualize what the internal layers of the transformer might be representing.\ntion mechanisms (Ainslie et al., 2023) to enhance\ndistributed across various layers. In the Llama-3The idea\nis that we\ntake any\nvector\nlayer\ntheheads\ntransformer\npreefficiency.\nLlama-3-8B,\ncomprises\n32 layers,\neachfrom\n8Bany\nmodel,\n~3% of\nof the\nhave a prefixand,\nmatching\ntending\nthat it is\ntheand\nprefinal\nit indicating\nby the unembedding\nwith 32 attention\nheads\nit uses aembedding,\nquery group simply\nscore of multiply\n0.3 or higher,\na degree of spesize ofto4 get\nattention\nheads.\nhas shown a\nsuperior\nin prefix\nmatching, and\nsomewords\nheads have\nlayer\nlogits,\nandIt compute\nsoftmaxcialisation\nto see the\ndistribution\nover\nthat\nperformance compared to its predecessors, even\nhigh scores of up to 0.98.\nthat\nvector\nmight\nbe\nrepresenting.\nThis\ncan\nbe\na\nuseful\nwindow\ninto\nthe\ninternal\nthe larger Llama-2 models.\nrepresentations\nthe model.\nSince\nwasn’t\ntrained to make the internal\n4.3 Head\nAblations\nInternLM2-20B,ofeaturing\n48 layers\nwith the\n48 at-network\ntention heads each, uses a query group size of 6\nattention heads. We selected InternLM2-20B for\nits exemplary performance on the Needle-in-theHaystack3 task, which assesses LLMs’ ability to\nretrieve a single critical piece of information embedded within a lengthy text. This mirrors the\nfunctionality of induction heads, which scan the\n\nTo investigate the significance of induction heads\nfor a specific ICL task, we conduct zero-ablations\nof 1% and 3% of the heads with the highest prefix\nmatching scores. This ablation process involves\nmasking the corresponding partition of the output\nmatrix, denoted as Woh in Eq. 1, by setting it to\nzero. This effectively renders the heads inactive\n\n\f8.10\n\n•\n\nS UMMARY\n\n25\n\nrepresentations function in this way, the logit lens doesn’t always work perfectly, but\nthis can still be a useful trick to help us visualize the internal layers of a transformer.\n\n8.10\n\nSummary\nThis chapter has introduced the transformer and its components for the language\nmodeling task introduced in the previous chapter. Here’s a summary of the main\npoints that we covered:\n• Transformers are non-recurrent networks based on multi-head attention, a\nkind of self-attention. A multi-head attention computation takes an input\nvector xi and maps it to an output ai by adding in vectors from prior tokens,\nweighted by how relevant they are for the processing of the current word.\n• A transformer block consists of a residual stream in which the input from\nthe prior layer is passed up to the next layer, with the output of different components added to it. These components include a multi-head attention layer\nfollowed by a feedforward layer, each preceded by layer normalizations.\nTransformer blocks are stacked to make deeper and more powerful networks.\n• The input to a transformer is computed by adding an embedding (computed\nwith an embedding matrix) to a positional encoding that represents the sequential position of the token in the window.\n• Language models can be built out of stacks of transformer blocks, with a\nlanguage model head at the top, which applies an unembedding matrix to\nthe output H of the top layer to generate the logits, which are then passed\nthrough a softmax to generate word probabilities.\n• Transformer-based language models have a wide context window (200K tokens or even more for very large models with special mechanisms) allowing\nthem to draw on enormous amounts of context to predict upcoming words.\n• There are various computational tricks for making large language models\nmore efficient, such as the KV cache and parameter-efficient finetuning.\n\nHistorical Notes\nThe transformer (Vaswani et al., 2017) was developed drawing on two lines of prior\nresearch: self-attention and memory networks.\nEncoder-decoder attention, the idea of using a soft weighting over the encodings\nof input words to inform a generative decoder (see Chapter 12) was developed by\nGraves (2013) in the context of handwriting generation, and Bahdanau et al. (2015)\nfor MT. This idea was extended to self-attention by dropping the need for separate\nencoding and decoding sequences and instead seeing attention as a way of weighting\nthe tokens in collecting information passed from lower layers to higher layers (Ling\net al., 2015; Cheng et al., 2016; Liu et al., 2016).\nOther aspects of the transformer, including the terminology of key, query, and\nvalue, came from memory networks, a mechanism for adding an external readwrite memory to networks, by using an embedding of a query to match keys rep-\n\n\f26\n\nC HAPTER 8\n\n•\n\nT RANSFORMERS\n\nresenting content in an associative memory (Sukhbaatar et al., 2015; Weston et al.,\n2015; Graves et al., 2014).\nMORE HISTORY TBD IN NEXT DRAFT.\n\n\fHistorical Notes\nBa, J. L., J. R. Kiros, and G. E. Hinton. 2016. Layer normalization. NeurIPS workshop.\nBahdanau, D., K. H. Cho, and Y. Bengio. 2015. Neural machine translation by jointly learning to align and translate.\nICLR 2015.\nBrown, T., B. Mann, N. Ryder, M. Subbiah, J. Kaplan,\nP. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry,\nA. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger,\nT. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu,\nC. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin,\nS. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish,\nA. Radford, I. Sutskever, and D. Amodei. 2020. Language\nmodels are few-shot learners. NeurIPS, volume 33.\nCheng, J., L. Dong, and M. Lapata. 2016. Long short-term\nmemory-networks for machine reading. EMNLP.\nCrosbie, J. and E. Shutova. 2022. Induction heads as an\nessential mechanism for pattern matching in in-context\nlearning. ArXiv preprint.\nElhage, N., N. Nanda, C. Olsson, T. Henighan, N. Joseph,\nB. Mann, A. Askell, Y. Bai, A. Chen, T. Conerly, N. DasSarma, D. Drain, D. Ganguli, Z. Hatfield-Dodds, D. Hernandez, A. Jones, J. Kernion, L. Lovitt, K. Ndousse,\nD. Amodei, T. Brown, J. Clark, J. Kaplan, S. McCandlish, and C. Olah. 2021. A mathematical framework for\ntransformer circuits. White paper.\nGraves, A. 2013. Generating sequences with recurrent neural\nnetworks. ArXiv.\nGraves, A., G. Wayne, and I. Danihelka. 2014. Neural Turing machines. ArXiv.\nHoltzman, A., J. Buys, L. Du, M. Forbes, and Y. Choi. 2020.\nThe curious case of neural text degeneration. ICLR.\nKaplan, J., S. McCandlish, T. Henighan, T. B. Brown,\nB. Chess, R. Child, S. Gray, A. Radford, J. Wu, and\nD. Amodei. 2020. Scaling laws for neural language models. ArXiv preprint.\nLing, W., C. Dyer, A. W. Black, I. Trancoso, R. Fermandez,\nS. Amir, L. Marujo, and T. Luı́s. 2015. Finding function\nin form: Compositional character models for open vocabulary word representation. EMNLP.\nLiu, Y., C. Sun, L. Lin, and X. Wang. 2016. Learning natural\nlanguage inference using bidirectional LSTM model and\ninner-attention. ArXiv.\nLlama Team. 2024. The llama 3 herd of models.\nNostalgebraist. 2020. Interpreting gpt: the logit lens. White\npaper.\nOlsson, C., N. Elhage, N. Nanda, N. Joseph, N. DasSarma,\nT. Henighan, B. Mann, A. Askell, Y. Bai, A. Chen, et al.\n2022. In-context learning and induction heads. ArXiv\npreprint.\nSukhbaatar, S., A. Szlam, J. Weston, and R. Fergus. 2015.\nEnd-to-end memory networks. NeurIPS.\nUszkoreit, J. 2017. Transformer: A novel neural network architecture for language understanding. Google Research\nblog post, Thursday August 31, 2017.\nVaswani, A., N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\nA. N. Gomez, Ł. Kaiser, and I. Polosukhin. 2017. Attention is all you need. NeurIPS.\nWeston, J., S. Chopra, and A. Bordes. 2015. Memory networks. ICLR 2015.\n\n27\n\n\f",
    "file_path": "/Users/colinsidberry/Downloads/NLP_Textbook/transformers.txt",
    "file_size_kb": 73.46
  },
  {
    "id": "b91f23bed4a1b1b0",
    "source": "nlp_textbook",
    "chapter": "Words and Tokens",
    "filename": "words-and-tokens.txt",
    "content": "Speech and Language Processing. Daniel Jurafsky & James H. Martin.\nrights reserved. Draft of August 24, 2025.\n\nCopyright © 2025.\n\nAll\n\nCHAPTER\n\nWords and Tokens\n\n2\n\nUser: I need some help, that much seems certain.\nELIZA: WHAT WOULD IT MEAN TO YOU IF YOU GOT SOME HELP\nUser: Perhaps I could learn to get along with my mother.\nELIZA: TELL ME MORE ABOUT YOUR FAMILY\nUser: My mother takes care of me.\nELIZA: WHO ELSE IN YOU FAMILY TAKES CARE OF YOU\nUser: My father.\nELIZA: YOUR FATHER\nUser: You are like my father in some ways.\n\nWeizenbaum (1966)\n\nELIZA\n\ntokenization\n\nBPE\n\nregular\nexpressions\n\nThe dialogue above is from ELIZA, an early natural language processing system\nthat could carry on a limited conversation with a user by imitating the responses of\na Rogerian psychotherapist (Weizenbaum, 1966). ELIZA is a surprisingly simple\nprogram that uses pattern matching on words to recognize phrases like “I need X”\nand change the words into suitable outputs like “What would it mean to you if you\ngot X?”. ELIZA’s mimicry of human conversation, while very crude by modern\nstandards, was remarkably successful: many people who interacted with ELIZA\ncame to believe that it really understood them. As a result, this work led researchers\nto first think about the impacts of chatbots on their users (Weizenbaum, 1976).\nOf course modern chatbots don’t use the simple pattern-based mimicry that\nELIZA pioneered. Yet the pattern-based approach to words instantiated in ELIZA\nis still relevant today in the context of tokenization, the task of separating out or\ntokenizing words and word parts from running text. Tokenization, the first step in\nmodern NLP, includes pattern-based approaches that date back to ELIZA.\nTo understand tokenization we first need to ask: What is a word? Is um a word?\nWhat about New York? Is the nature of words similar across languages? Some\nlanguages, like Vietnamese or Cantonese, have very short words while others, like\nTurkish, have very long words. We also need to think about how to represent words\nin terms of characters. We’ll introduce Unicode, the modern system for representing characters, and the UTF-8 text encoding. And we’ll introduce the morpheme,\nthe meaningful subpart of words (like the morpheme -er in the word longer)\nThe standard way to tokenize text is to use the input characters to guide us.\nSo once we’ve understand the possible subparts of words, we’ll introduce the standard Byte-Pair Encoding (BPE) algorithm that automatically breaks up input text\ninto tokens. This algorithm uses simple statistics of letter sequences to induce a\nvocabulary of subword tokens. All tokenization systems also depend on regular\nexpressions as a processing step. The regular expression is a language for formally\nspecifying and manipulating text strings, an important tool in all modern NLP systems. We’ll introduce regular expressions and show examples of their use\nFinally, we’ll introduce a metric called edit distance that measures how similar\ntwo words or strings are based on the number of edits (insertions, deletions, substitutions) it takes to change one string into the other. Edit distance plays a role in NLP\nwhenever we need compare two words or strings, for example in the crucial word\nerror rate metric for automatic speech recognition.\n\n\f2\n\nC HAPTER 2\n\n2.1\n\n•\n\nW ORDS AND T OKENS\n\nWords\nHow many words are in the following sentence?\nThey picnicked by the pool, then lay back on the grass and\nlooked at the stars.\n\nutterance\n\nThis sentence has 16 words if we don’t count punctuation as words, 18 if we\ncount punctuation. Whether we treat period (“.”), comma (“,”), and so on as words\ndepends on the task. Punctuation is critical for finding boundaries of things (commas, periods, colons) and for identifying some aspects of meaning (question marks,\nexclamation marks, quotation marks). Large language models generally count punctuation as separate words.\nSpoken language introduces other complications with regard to defining words.\nWhat about this utterance from a spoken conversation? (Utterance is the technical\nlinguistic term for the spoken correlate of a sentence).\nI do uh main- mainly business data processing\n\ndisfluency\nfragment\nfilled pause\n\nword type\n\nword instance\n\nThis utterance has two kinds of disfluencies. The broken-off word main- is\ncalled a fragment. Words like uh and um are called fillers or filled pauses. Should\nwe consider these to be words? Again, it depends on the application. If we are\nbuilding a speech transcription system, we might want to eventually strip out the\ndisfluencies. But we also sometimes keep disfluencies around. Disfluencies like uh\nor um are actually helpful in speech recognition in predicting the upcoming word,\nbecause they may signal that the speaker is restarting the clause or idea, and so for\nspeech recognition they are treated as regular words. Because different people use\ndifferent disfluencies they can also be a cue to speaker identification. In fact Clark\nand Fox Tree (2002) showed that uh and um have different meanings in English.\nWhat do you think they are?\nPerhaps most important, in thinking about what is a word, we need to distinguish\ntwo ways of talking about words that will be useful throughout the book. Word types\nare the number of distinct words in a corpus; if the set of words in the vocabulary\nis V , the number of types is the vocabulary size |V |. Word instances are the total\nnumber N of running words.1 If we ignore punctuation, the picnic sentence has 14\ntypes and 16 instances:\nThey picnicked by the pool, then lay back on the grass and\nlooked at the stars.\nWe still have decisions to make! For example, should we consider a capitalized\nstring (like They) and one that is uncapitalized (like they) to be the same word type?\nThe answer is that it depends on the task! They and they might be lumped together as\nthe same type in some tasks where we care less about the formatting, while for other\ntasks, capitalization is a useful feature and is retained. Sometimes we keep around\ntwo versions of a particular NLP model, one with capitalization and one without\ncapitalization.\nSo far we have been talking about orthographic words: words based on our\nEnglish writing system. But there are many other possible ways to define words.\nFor example, while orthographically I’m is one word, grammatically it functions as\ntwo words: the subject pronoun I and the verb ’m, short for am.\n1 In earlier tradition, and occasionally still, you might see word instances referred to as word tokens, but\nwe now try to reserve the word token instead to mean the output of subword tokenization algorithms.\n\n\f2.1\nCorpus\nShakespeare\nBrown corpus\nSwitchboard telephone conversations\nCOCA\nGoogle n-grams\n\n•\n\nW ORDS\n\n3\n\nTypes = |V | Instances = N\n31 thousand 884 thousand\n38 thousand\n1 million\n20 thousand\n2.4 million\n2 million\n440 million\n13 million\n1 trillion\n\nFigure 2.1 Rough numbers of wordform types and instances for some English language\ncorpora. The largest, the Google n-grams corpus, contains 13 million types, but this count\nonly includes types appearing 40 or more times, so the true number would be much larger.\n\nhanzi\n\nThe distinctions get even harder to make once we start to think about other languages. For example the writing systems of languages like Chinese, Japanese, and\nThai simply don’t have orthographic words at all! That is, they don’t use spaces to\nmark potential word-boundaries. In Chinese, for example, words are composed of\ncharacters (called hanzi in Chinese). Each character generally represents a single\nunit of meaning (called a morpheme, introduced below) and is pronounceable as a\nsingle syllable. Words are about 2.4 characters long on average. But since Chinese\nhas no orthographic words, deciding what counts as a word in Chinese is complex.\nFor example, consider the following sentence:\n(2.1)\n\n姚明进入总决赛\nyáo mı́ng jı̀n rù zǒng jué sài\n“Yao Ming reaches the finals”\n\nAs Chen et al. (2017) point out, this could be treated as 3 words (a definition of\nwords called the ‘Chinese Treebank’ definition, in which Chinese names (family\nname followed by personal names) are treated as a single word):\n(2.2)\n\n姚明\n进入 总决赛\nYaoMing reaches finals\n\nBut the same sentence could be treated as 5 words (‘Peking University’ standard),\nin which names are separated into their own units and some adjectives appear as\ndistinct words:\n(2.3)\n\n姚 明 进入 总\n决赛\nYao Ming reaches overall finals\n\nFinally, it is possible in Chinese simply to ignore words altogether and use characters\nas the basic elements, treating the sentence as a series of 7 characters, which works\npretty well for Chinese since characters are at a reasonable semantic level for most\napplications (Li et al., 2019):\n(2.4)\n\n姚 明 进 入 总\n决\n赛\nYao Ming enter enter overall decision game\n\nBut that method doesn’t work for Japanese and Thai, where the individual character\nis too small a unit.\nThese issues with defining words makes it hard to use words as the basis for\ntokenizing text in NLP across languages.\nBut there’s another problem with words. There are too many of them!!! How\nmany words are there in English? When we speak about the number of words in\nthe language, we are generally referring to word types. Fig. 2.1 shows the rough\nnumbers of types and instances computed from some English corpora.\nYou will notice that the larger the corpora we look at, the more word types we\nfind! That suggests that there is not a clear answer to how many words there are;\nthe answer keeps growing as we see more data! We can see this fact mathematically\n\n\f4\n\nC HAPTER 2\n\nHerdan’s Law\nHeaps’ Law\n\n•\n\nW ORDS AND T OKENS\n\nbecause the relationship between the number of types |V | and number of instances\nN is called Herdan’s Law (Herdan, 1960) or Heaps’ Law (Heaps, 1978) after its\ndiscoverers (in linguistics and information retrieval respectively). It is shown in\nEq. 2.5, where k and β are positive constants, and 0 < β < 1.\n|V | = kN β\n\nfunction words\ncontent words\n\n(2.5)\n\nThe value of β depends on the corpus size and the genre; numbers from 0.44 to 0.56\nor even higher have often been reported. Roughly we can say that the vocabulary\nsize for a text goes up a little faster than the square root of its length in words.\nThere are also variants of the law, which capture the fact that we can distinguish\nroughly two classes of words. One is function words, the grammatical words like\nEnglish a and of, that tend not to grow indefinitely (a language tends to have a fixed\nnumber of these). The other is content words: nouns, adjectives and verbs that tend\nto have meanings about people and places and events. Nouns, and especially particular nouns like names and technical terms do tend to grow indefinitely. So models\nthat are sensitive to this difference between function words and content words have\none value of β for the initial part of the corpus where all words are still appearing,\nand then a second β afterwords for when only the content words are still appearing.\nFig. 2.2 shows an example from Tria et al. (2018) showing two values of β for Heaps\nlaw computed on the Gutenberg corpus of books.\n\nEntropy 2018, 20, 752\n\n4 of 19\n\nFigure\n2.2. Growth\nVocabulary\nsize asofa distinct\nfunction\nof text\nlength,oncomputed\non the\nGutenberg\ncorpus\nFigure\nof the number\nwords\ncomputed\nthe Gutenberg\ncorpus\nof texts [15].\nof publicly\navailable\nFigure\nfrom Tria\net al. (2018).\nThe position\nof texts books.\nin the corpus\nis chosen\nat random.\nIn this case g ' 0.44. Similar behaviours are\nobserved in many other systems.\n\nThe fact that words grow without end leads to a problem for any computational\nmodel. No matter how big our vocabulary, we will never have a vocabulary that\nIn this section\ncomparewords\nthe twothat\nlawsmight\njust observed,\nfor the\nfrequencies\nof occurrence\ncaptures\nall thewe\npossible\noccur! Zipf’s\nThat law\nmeans\nthat\nour computational\nof the elements in a system and Heaps’ law for their temporal appearance. It has often been claimed that\nmodel will constantly see unknown words: words that it has never seen before.\nHeaps’ and Zipf’s law are trivially related and that one can derive Heaps’s law once the Zipf’s is known.\nThis is a huge problem for machine learning models.\nThis is not true in general. It turns out to be true only under the specific hypothesis of random-sampling\nBecause\nof these\ntwo problems\n(first, that\nmany languages\ndon’t have\northoas follows.\nSuppose\nthe existence\nof a strict power-law\nbehaviour\nof the frequency-rank\ndistribution,\na , and construct\nand defining\nthem\nchallenging\nand from\nsecond,\nthat distribution\nthe numf graphic\n( R) ⇠ R words,\na sequence\nof post-hoc\nelements byisrandomly\nsampling\nthis Zipf\nf ber\n( R). of\nThrough\nprocedure,\nonebound),\nrecovers alanguage\nHeaps’ law\nwith theand\nfunctional\nform D\n(t) ⇠ tg don’t\n[23,24]\nwordsthis\ngrows\nwithout\nmodels\nother NLP\nmodels\nwith\ng\n=\n1/a.\nIn\norder\nto\ndo\nthat\nwe\nneed\nto\nconsider\nthe\ncorrect\nexpression\nfor\nf\n(\nR\n)\nthat\nincludes\nthe\ntend to use words as their unit of processing. Instead, they use smaller units\ncalled\nnormalisation\nfactor,\nwhose\nexpression\ncan\nbe\nderived\nthrough\nthe\nfollowing\napproximated\nintegral:\nsubwords that can be recombined to model new words that our model has never\nZ Rmax\nseen before. To think about defining\nsubwords, we first need to talk about units that\nf ( R̃)d R̃ = 1 .\n(3)\nare smaller than words; morphemes\n1 and characters.\n2.3. Zipf’s vs. Heaps’ Laws\n\nLet us now distinguish the two cases. For a 6= 1 one has\nf ( R) =\nwhile for a = 1 one obtains:\nf ( R) =\n\n1\n\na\n\nR1maxa\n1\n\n1\n\nR a.\n\n(4)\n\nR 1.\n\n(5)\n\n\f2.2\n\n2.2\n\n•\n\nM ORPHEMES : PARTS OF W ORDS\n\n5\n\nMorphemes: Parts of Words\n\nmorphology\nmorpheme\n\nWords have parts. At the level of characters, this is obvious. The word cats is composed of four characters, ‘c’, ‘a’, ‘t’, ‘s’. But this is also true at a more subtle level:\nwords have components that themselves have coherent meanings. These components are called morphemes, and the study of morphemes is called morphology. A\nmorpheme is a minimal meaning-bearing unit in a language. So, for example, the\nword fox consists of one morpheme (the morpheme fox) while the word cats consists\nof two: the morpheme cat and the morpheme -s that indicates plural.\nHere’s a sentence in English segmented into morphemes with hyphens:\n(2.6) Doc work-ed care-ful-ly wash-ing the glass-es\nAs we mentioned above, in Chinese, conveniently, the writing system is set up\nso that each character mainly describes a morpheme. Here’s a sentence in Mandarin\nChinese with each morpheme character glossed, followed by the translation:\n(2.7)\n\nroot\naffix\n\ninflectional\nmorphemes\n\nderivational\nmorphemes\n\nclitic\n\nmorphological\ntypology\n\n梅 干 菜\n用 清 水 泡 软 ，捞\n出 后 ，沥 干\nplum dry vegetable use clear water soak soft , remove out after , drip dry\n切 碎\nchop fragment\nSoak the preserved vegetable in water until soft, remove, drain, and chop\n\nWe generally distinguish two broad classes of morphemes: roots—the central\nmorpheme of the word, supplying the main meaning—and affixes—adding “additional” meanings of various kinds. In the English example above, for the word\nworked, work is a root and -ed is an affix; similarly for glasses, glass is a root and\n-es an affix.\nAffixes themselves fall into two classes, or more correctly a continuum between\ntwo poles. At one end, inflectional morphemes are grammatical morphemes that\ntend to play a syntactic role, such as marking agreement. For example, English has\nthe inflectional morpheme -s (or -es) for marking the plural on nouns and the inflectional morpheme -ed for marking the past tense on verbs. Inflectional morphemes\ntend to be productive and often obligatory and their meanings tend to be predictable.\nDerivational morphemes are more idiosyncratic in their application and meaning.\nUsually they apply only to a specific subclass of words and result in a word of a different grammatical class than the root, often with a meaning hard to predict exactly.\nIn the example above, the word care (a noun) can be combined with the derivational\naffix -full to produce an adjective (careful), and another derivational affix -ly to result\nin an adverb (carefully).\nThere is another class of morphemes: clitics. A clitic is a morpheme that acts\nsyntactically like a word but is reduced in form and attached (phonologically and\nsometimes orthographically) to another word. For example the English morpheme\n’ve in the word I’ve is a clitic; it has the grammatical meaning of the word have, but\nin form in cannot appear alone (you can’t just say the sentence “’ve”). The English\npossessive morpheme ’s in the phrase the teacher’s book is a clitic. French definite\narticle l’ in the word l’opera is a clitic, as are prepositions in Arabic like b ‘by/with’\nand conjunctions like w ‘and’.\nThe study of how languages vary in their morphology, i.e., how words break\nup into their parts, is called morphological typology. While morphologies of languages can differ along many dimensions, two dimensions are particularly relevant\nfor computational word tokenization.\n\n\f6\n\nC HAPTER 2\n\nisolating\n\n•\n\nW ORDS AND T OKENS\n\nThe first dimension is the number of morphemes per word. In some languages,\nlike Vietnamese and Cantonese, each word on average has just over one morpheme.\nWe call languages at this end of the scale isolating languages. For example each\nword in the following Cantonese sentence has one morpheme (and one syllable):\n(2.8)\n\nkeoi5 waa6 cyun4 gwok3 zeoi3 daai6 gaan1 uk1 hai6 ni1 gaan1\nhe say entire country most big building house is this building\n“He said the biggest house in the country was this one”\n\nsynthetic\npolysynthetic\n\nAlternatively, in languages like Koryak, a Chukotko-Kamchatkan language spoken in the northern part of the Kamchatka peninsula in Russia, a single word may\nhave very many morphemes, corresponding to a whole sentence in English (Arkadiev,\n2020; Kurebito, 2017). We call languages toward this end of the scale synthetic languages, and the very end of the scale polysynthetic languages.\n(2.9)\n\nt-@-nk’e-mejN-@-jetem@-nni-k\n1SG.S-E-midnight-big-E-yurt.cover-E-sew-1SG.S[PFV]\n“I sewed a lot of yurt covers in the middle of a night.”\n(Koryak, Chukotko-Kamchatkan, Russia; Kurebito (2017, 844))\n\nFig. 2.3 shows an early computation of morphemes per words on a few languages\nby the linguistic typologist Joseph Greenberg (1960).\n\nam\n\ne\nes\n\nn\n\net\nVi\n1.1\n\ni\n\nrs\n\nFa\n1.5\n\nAnalytic\n\nl\n\nish\n\ng\nEn\n1.7\n\nsh\ngli\nt\nn\nili kri\nE t\nd aku wah ans\nl\nO Y\nS S\n\n2.1 2.2\n\n2.5 2.6\n\nSynthetic\n\nic\nnd\na\nl\nn )\nee uit\nGr (In\n3.7\n\nPolysynthetic\n\nMorphemes per Word\n\nFigure 2.3\n\nagglutinative\nfusion\n\nAn early estimate of morphemes per word by Joseph Greenberg (1960).\n\nThe second dimension is the degree to which morphemes are easily segmentable,\nranging from agglutinative languages like Turkish, in which morphemes have relatively clean boundaries, to fusion languages like Russian, in which a single affix\nmay conflate multiple morphemes, like -om in the word stolom (table-SG-INSTRDECL 1), which fuses the distinct morphological categories instrumental, singular,\nand first declension.\nThe English -s suffix in She reads the article is an example of fusion, since the\nsuffix means both third person singular but also means present tense, and there’s no\nway to divide up the meaning to different parts of the -s.\nAlthough we have loosely talked about these properties (analytic, polysynthetic,\nfusional, agglutinative) as if they are properties of languages, in fact languages can\nmake use of different morphological systems so it would be more accurate to talk\nabout these as general tendencies.\nNonetheless, the fact morphemes can be hard to define, and that many languages\ncan have complex morphemes that aren’t easy to break up into pieces makes it very\ndifficult to use morphemes as a standard for tokenization cross-lingually.\n\n\f2.3\n\n2.3\n\n•\n\nU NICODE\n\n7\n\nUnicode\n\nUnicode\n\nASCII\n\nAnother option we could consider for tokenization is the level of the individual character. How do we even represent characters across languages and writing system?\nThe Unicode standard is a method for representing text written using any character\nin any script of the languages of the world (including dead languages like Sumerian\ncuneiform, and invented languages like Klingon).\nLet’s start with a brief historical note about an English-specific subset of Unicode\n(technically called ‘Basic Latin’ in Unicode, and commonly referred to as ASCII).\nStarting in the 1960s, the Latin characters used to write English (like the ones used\nin this sentence), were represented with a code called ASCII (American Standard\nCode for Information Interchange). ASCII represented each character with a single\nbyte. A byte can represent 256 different characters, but ASCII only used 127 of\nthem; the high-order bit of ASCII bytes is always set to 0. (Actually it only used 95\nof them and the rest were control codes for an obsolete machine called a teletype).\nHere’s a few ASCII characters with their representation in hex and decimal:\nCh Hex Dec\n< 3C 60\n= 3D 61\n> 3E 62\n? 3F 63\n\nCh Hex Dec\n@ 40 64\nA 41 65\nB 42 66\nC 43 67\n\n...\n...\n...\n...\n\nCh Hex Dec\n\\ 5C 92\n[ 5D 93\nˆ 5E 94\n_ 5F 95\n\nCh Hex Dec\n` 60 96\na 61 97\nb 62 98\nc 63 99\n\nFigure 2.4 Some selected ASCII codes for some English letters, with the codes shown both\nin hexadecimal and decimal.\n\nBut ASCII is of course insufficient since there are lots of other characters in the\nworld’s writing systems! Even for scripts that use Latin characters, there are many\nmore than the 95 in ASCII. For example, this Spanish phrase (meaning “Sir, replied\nSancho”) has two non-ASCII characters, ñ and ó:\n(2.10) Señor- respondió SanchoDevanagari\n\nAnd lots of languages aren’t based on Latin characters at all! The Devanagari\nscript is used for 120 languages (including Hindi, Marathi, Nepali, Sindhi, and Sanskrit). Here’s a Devanagari example from the Hindi text of the Universal Declaration\nof Human Rights:\n\nChinese has about 100,000 Chinese characters in Unicode (including overlapping and non-overlapping variants used in Chinese, Japanese, Korean, and Vietnamese, collectively referred to as CJKV).\nAll in all there are more than 150,000 characters and 168 different scripts supported in Unicode 16.0. Even though many scripts from around the world have\nyet to be added to Unicode, there are so many there, from scripts used by modern languages (Chinese, Arabic, Hindi, Cherokee, Ethiopic, Khmer, N’Ko, Turkish,\nSpanish) to scripts of ancient languages (Cuneiform, Ugaritic, Egyptian Hieroglyph,\nPahlavi), as well as mathematical symbols, emojis, currency symbols, and more.\n\n2.3.1\ncode point\n\nCode Points\n\nHow does it work? Unicode assigns a unique id, called a code point, for each one\n\n\f8\n\nC HAPTER 2\n\n•\n\nW ORDS AND T OKENS\n\nof these 150,000 characters.\nThe code point is an abstract representation of the character, and each code point\nis represented by a number, traditionally written in hexadecimal, from number 0\nthrough 0x10FFFF (which is 1,114,111 decimal). Having over a million code points\nmeans there is a lot of room for new characters. It is traditional to represent these\ncode points with the prefix “U+” (which just means “the following is a Unicode hex\nrepresentation of a code point”). So the code point for the character a is U+0061\nwhich is the same as 0x0061. (Note that Unicode was designed to be backwards\ncompatible with ASCII, which means that the first 127 code points, including the\ncode for a, are identical with ASCII.) Here are some sample code points; some (but\nnot all) come with descriptions:\nU+0061 a LATIN SMALL LETTER A\nU+0062 b LATIN SMALL LETTER B\nU+0063 c LATIN SMALL LETTER C\nU+00F9 ù LATIN SMALL LETTER U WITH GRAVE\nU+00FA ú LATIN SMALL LETTER U WITH ACUTE\nU+00FB û LATIN SMALL LETTER U WITH CIRCUMFLEX\nU+00FC ü LATIN SMALL LETTER U WITH DIAERESIS\nU+8FDB 5/23/25,\n进 5:26 PM\nU+8FDC 远\n🀎\nU+8FDD 违\nU+8FDE 连\n5/23/25, 5:26 PM\nU+1F600\nGRINNING FACE\nU+1F00E 🀎 MAHJONG TILE EIGHT OF CHARACTERS\nglyph\n\nNote that a code point does not specifiy the glyph, the visual representation\nof a character. Glyphs are stored in fonts. The code point U+0061 is an abstract\nrepresentation of a. There can be an indefinite number of visual representations,\nfor example in different fonts like Times Roman (a) or Courier (a), or different font\nstyles like boldface (a) or italic (a). But all of them are represented by the same code\npoint U+0061.\n\n2.3.2\n\nencoding\n\nUTF-8 Encoding\n\nWhile the code point (the unique id) is the abstract Unicode representation of the\ncharacter, we don’t just stick that id in a text file.\nInstead, whenever we need to represent a character in a text string, we write an\nencoding of the character. There are many different possible encoding methods, but\nthe encoding method called UTF-8 is by far the most frequent (for example almost\nthe entire web is encoded in UTF-8).\nLet’s talk about encodings. The Unicode representation of the word hello consists of the following sequence of 5 code points:\nU+0068\n\nU+0065 U+006C U+006C U+006F\n\nWe can imagine a very simple encoding method: just write the code point id in\na file. Since there are more than 1 million characters, 16 bits (2 bytes) isn’t enough,\nso we’ll need to use 4 bytes (32 bit) to capture the 21 bits we need to represent 1.1\nmillion characters. (We could fit it in 3 bytes but it’s inconvenient to use multiples\nof 3 for bytes.)\nWith this 4-byte representation the word hello would be encoded as the following set of bytes:\n\nx.htm\n\nx.htm\n\n\f2.3\n\nUTF-8\n\nvariable-length\nencoding\n\n•\n\nU NICODE\n\n9\n\n00 00 00 68 00 00 00 65 00 00 00 6C 00 00 00 6C 00 00 00 6F\nBut we don’t use this encoding (which is technically called UTF-32) because it\nmakes every file 4 times longer than it would have been in ASCII, making files really\nbig and full of zeros. Also those zeros cause another problem: it turns out that having\nany byte that is completely zero messes things up for backwards compatibility for\nASCII-based systems that historically used a 0 byte as an end-of-string marker.\nInstead, the most common encoding standard is UTF-8 (Unicode Transformation Format 8), which represents characters efficiently (using fewer bytes on average) by writing some characters using fewer bytes and some using more bytes.\nUTF-8 is thus a variable-length encoding.\nFor some characters (the first 127 code points, i.e. the set of ASCII characters),\nUTF-8 encodes them as a single byte, so the UTF-8 encoding of hello is :\n68 65 6C 6C 6F\nThis conveniently means that files encoded in ASCII are also valid UTF-8 encodings!\nBut UTF-8 is a variable length encoding, meaning that code points ≥128 are\nencoded as a sequence of two, three, or four bytes. Each of these bytes are between\n128 and 255, so they won’t be confused with ASCII, and each byte indicates in the\nfirst few bits whether it’s a 2-byte, 3-byte, or 4-byte encoding.\n\nCode Points\nUTF-8 Encoding\nFrom - To\nBit Value\nByte 1\nByte 2\nByte 3\nByte 4\nU+0000-U+007F\n0xxxxxxx\nxxxxxxxx\nU+0080-U+07FF\n00000yyy yyxxxxxx\n110yyyyy\n10xxxxxx\nU+0800-U+FFFF\nzzzzyyyy yyxxxxxx\n1110zzzz\n10yyyyyy 10xxxxxx\nU+010000-U+10FFFF 000uuuuu zzzzyyyy yyxxxxxx\n11110uuu\n10uuzzzz 10yyyyyy 10xxxxxx\nFigure 2.5 Mapping from Unicode code point to the variable length UTF-8 encoding. For a given code point\nin the From-To range, the bit value in column 2 is packed into 1, 2, 3, or 4 bytes. Figure adapted from Unicode\n16.0 Core Spec Chapter 3 Table 3-6.\n\nFig. 2.5 shows how this mapping occurs. For example these rules explain how\nthe character ñ, which has code point U+00F1, or bit sequence 00000000 11110001,\n(where blue indicates the sequence yyyyy and red the sequence xxxxxx) is encoded\ninto to the two-byte bit sequence 11000011 10110001 or 0xC3B1. As a result of\nthese rules, the first 127 characters (ASCII) are mapped to one byte, most remaining characters in European, Middle Eastern, and African scripts map to two bytes,\nmost Chinese, Japanese, and Korean characters map to three bytes, and rarer CJKV\ncharacters and emojis and some symbols map to 4 bytes.\nUTF-8 has a number of advantages. It’s relatively efficient, using fewer bytes for\ncommonly-encountered characters, it doesn’t use zero bytes (except when literally\nrepresenting the NULL character which is U+0000), it’s backwards compatible with\nASCII, and it’s self-synchronizing, meaning that if a file is corrupted, it’s always\npossible to find the start of the next or prior character just by moving up to 3 bytes\nleft or right.\nUnicode and Python: Starting with Python 3, all Python strings are stored internally as Unicode, each string a sequence of Unicode code points. Thus string\nfunctions and regular expressions all apply natively to code points. For example,\nfunctions like len() of a string return its length in characters, i.e., code points, not\nits length in bytes.\nWhen reading or writing from a file, however, the code points need to be encoded\nand decoding using a method like UTF-8. That is, every file is encoded in some\n\n\f10\n\nC HAPTER 2\n\n•\n\nW ORDS AND T OKENS\n\nencoding. If it’s not UTF-8, it’s an older encoding method like ASCII or Latin-1\n(iso 8859 1). There is no such thing as a text file without an encoding. The encoding\nmethod is specified in Python when opening a file for reading and writing.\n\n2.4\n\nSubword Tokenization: Byte-Pair Encoding\n\ntokenization\ntokens\n\nsubwords\n\nBPE\n\nTokenization, the first stage of natural language processing, is the process of segmenting the running input text into tokens.\nWe’ve seen three candidates for tokens: words, morphemes and characters. But\neach has problems as a unit. Words and morphemes seem approximately at the right\nlevel for NLP processing, since they tend to have consistent meanings, but they are\nchallenging to define formally. Characters are clearer to define, but seem too small\na unit to choose for tokens.\nIn this section we introduce what we do in practice for NLP: use a data-driven\napproach to define tokens that will generally result in units about the size of morphemes or words, but occasionally use units as small as characters.\nWhy tokenize the input? One reason is that converting an input to a deterministic\nfixed set of units means that different algorithms and systems can agree on simple\nquestions. For example, How long is this text? (How many units are in it?). Or:\nIs don’t or New York one token or two? Standardizing is thus essential for replicability in NLP experiments, and many algorithms that we introduce in this book\n(like the perplexity metric for language models) assume that all texts have a fixed\ntokenization.\nTokenization algorithms that include smaller tokens for morphemes and letters\nalso eliminate the problem of unknown words. What are these? As we will see\nin the next chapter, NLP algorithms often learn some facts about language from\none corpus (a training corpus) and then use these facts to make decisions about a\nseparate test corpus and its language. Thus if our training corpus contains, say the\nwords low, new, and newer, but not lower, then if the word lower appears in our test\ncorpus, our system will not know what to do with it.\nTo deal with this unknown word problem, modern tokenizers automatically induce sets of tokens that include tokens smaller than words, called subwords. Subwords can be arbitrary substrings, or they can be meaning-bearing units like the\nmorphemes -est or -er. In modern tokenization schemes, many tokens are words,\nbut other tokens are frequently occurring morphemes or other subwords like -er.\nEvery unseen word can thus be represented by some sequence of known subword\nunits. For example, if we had happened not to ever see the word lower, when it appears we could segment it successfully into low and er which we had already seen.\nIn the worst case, a really unusual word (perhaps an acronym like GRPO) could be\ntokenized as a sequence of individual letters if necessary.\nTwo tokenization algorithms are widely used in modern language models: bytepair encoding (BPE) (Sennrich et al., 2016), and unigram language modeling\n(ULM) (Kudo, 2018).2 In this section we introduce the byte-pair encoding or BPE\nalgorithm (Sennrich et al., 2016; Gage, 1994); see Fig. 2.6.\nLike most tokenization schemes, the BPE algorithm has two parts: a trainer,\nand an encoder. In general in the token training phase we take a raw training corpus\n2\n\nThe SentencePiece library includes implementations of both of these (Kudo and Richardson, 2018a),\nand people sometimes use the name SentencePiece to simply mean ULM tokenization.\n\n\f2.4\n\n•\n\nS UBWORD T OKENIZATION : B YTE -PAIR E NCODING\n\n11\n\n(usually roughly pre-separated into words, for example by whitespace) and induce\na vocabulary, a set of tokens. Then a token encoder take a raw test sentence and\nencodes it into the tokens in the vocabulary that were learned in training.\n\n2.4.1\n\nBPE training\n\nThe BPE training algorithm iteratively merges frequent neighboring tokens to create\nlonger and longer tokens. The algorithm begins with a vocabulary that is just the\nset of all individual characters. It then examines the training corpus, and finds the\ntwo characters that are most frequently adjacent. Imagine our original corpus is 10\ncharacters long, using a vocabulary of 5 characters, {A, B, C, D, E}:\nA B D C A B E C A B\nThe most frequent neighboring pair of characters is “A B” so we merge those,\nadd a new merged token ‘AB’ to the vocabulary, and replace every adjacent ‘A’ ‘B’\nin the corpus with the new ‘AB’:\nAB D C AB E C AB\nNow we have a vocabulary of 6 possible tokens {A, B, C, D, E, AB}, and the\ncorpus has length 7. And now the most frequent pair of tokens is “C AB”, so we\nmerge those, leading to a vocabulary with 7 tokens {A, B, C, D, E, AB, CAB}, and the\ncorpus has length 5.\nAB D CAB F CAB\nThe algorithm continues to count and merge, creating new longer and longer\ncharacter strings, until k merges have been done creating k novel tokens; k is thus a\nparameter of the algorithm. The resulting vocabulary consists of the original set of\ncharacters plus k new symbols. That’s the core of the algorithm.\nThe only additional complication is that in practice, instead of running on the\nraw sequence of characters, the algorithm is usually run only inside words. That is,\nthe algorithm does not merge across word boundaries. To do this, the input corpus\nis often first separated at white space and punctuation (using the regular expressions\nthat we define later in the chapter). This gives a starting set of strings, each corresponding to the characters of a word, (with the white space usually attached to the\nstart of the word), together with the counts of the words. Then while counts come\nfrom a corpus, merges are only allowed within the strings.\nLet’s see how the full algorithm thus works on this tiny synthetic corpus, where\nwe’ve explicitly marked the spaces between words:3\n(2.11) set new new renew reset renew\nFirst, we’ll break up the corpus into words, with leading whitespace, together\nwith their counts; no merges will be allowed to go beyond these word boundaries.\nThe result looks like the following list of 4 words and a starting vocabulary of 7\ncharacters:\ncorpus\n2\nn e w\n2\nr e n e w\n1\ns e t\n1\nr e s e t\n3\n\nvocabulary\n, e, n, r, s, t, w\n\nYes, we realize this isn’t a particularly likely or exciting sentence.\n\n\f12\n\nC HAPTER 2\n\n•\n\nW ORDS AND T OKENS\n\nThe BPE training algorithm first counts all pairs of adjacent symbols: the most\nfrequent is the pair n e because it occurs in new (frequency of 2) and renew (frequency of 2) for a total of 4 occurrences. We then merge these symbols, treating ne\nas one symbol, and count again:\ncorpus\nne w\n2\nr e ne w\n2\n1\ns e t\n1\nr e s e t\n\nvocabulary\n, e, n, r, s, t, w, ne\n\nNow the most frequent pair is ne w (total count=4), which we merge.\ncorpus\n2\nnew\n2\nr e new\n1\ns e t\nr e s e t\n1\n\nvocabulary\n, e, n, r, s, t, w, ne, new\n\nNext r (total count of 3) get merged to r, and then r e (total count 3) gets\nmerged to re. The system has essentially induced that there is a word-initial prefix\nre-:\ncorpus\nvocabulary\n2\nnew\n, e, n, r, s, t, w, ne, new, r, re\n2\nre new\n1\ns e t\n1\nre s e t\nIf we continue, the next merges are:\nmerge\ncurrent vocabulary\n, e, n, r, s, t, w, ne, new,\n( , new)\n( re, new) , e, n, r, s, t, w, ne, new,\n, e, n, r, s, t, w, ne, new,\n(s, e)\n(se, t)\n, e, n, r, s, t, w, ne, new,\n\nr,\nr,\nr,\nr,\n\nre,\nre,\nre,\nre,\n\nnew\nnew, renew\nnew, renew, se\nnew, renew, se, set\n\nfunction B YTE - PAIR ENCODING(strings C, number of merges k) returns vocab V\nV ← all unique characters in C\n# initial set of tokens is characters\nfor i = 1 to k do\n# merge tokens k times\ntL , tR ← Most frequent pair of adjacent tokens in C\ntNEW ← tL + tR\n# make new token by concatenating\nV ← V + tNEW\n# update the vocabulary\nReplace each occurrence of tL , tR in C with tNEW\n# and update the corpus\nreturn V\nFigure 2.6 The training part of the BPE algorithm for taking a corpus broken up into individual characters or bytes, and learning a vocabulary by iteratively merging tokens. Figure\nadapted from Bostrom and Durrett (2020).\n\n2.4.2\n\nBPE encoder\n\nOnce we’ve learned our vocabulary, the BPE encoder is used to tokenize a test\nsentence. The encoder just runs on the test data the merges we have learned from\n\n\f2.4\n\n•\n\nS UBWORD T OKENIZATION : B YTE -PAIR E NCODING\n\n13\n\nthe training data. It runs them greedily, in the order we learned them. (Thus the\nfrequencies in the test data don’t play a role, just the frequencies in the training\ndata). So first we segment each test sentence word into characters. Then we apply\nthe first rule: replace every instance of n e in the test corpus with ne, and then the\nsecond rule: replace every instance of ne w in the test corpus with new, and so on.\nBy the end of course many of the merges simple recreated words in the training\nset. But the merges also created knowledge of morphemes like the re- prefix (that\nmight appear in perhaps unseen combinations like revisit or rearrange), or the\nmorpheme new without an initial space (hence word-internal) that might appear at\nthe start of sentences or in words unseen in training like anew.\nOf course in real settings BPE is run with tens of thousands of merges on a very\nlarge input corpus, to produce vocabulary sizes of 50,000, 100,000, or even 200,000\ntokens. The result is that most words can be represented as single tokens, and only\nthe rarer words (and unknown words) will have to be represented by multiple tokens.\nAt least for English. For multilingual systems, the tokens can be dominated by\nEnglish, leaving fewer tokens for other languages, as we’ll discuss below.\n\n2.4.3\n\nBPE in practice\n\nThe example above just showed simple BPE learning from sequences of ASCII\nbytes. How does BPE work with Unicode input? We normally run BPE on the\nindividual bytes of UTF-8-encoded text. That is, we take a Unicode representations\nof text as a series of code points, encode it in bytes using UTF-8, and we treat each of\nthese individual bytes as the input to BPE. Thus BPE likely begins by rediscovering\nthe 2-byte and common 3-byte sequences that UTF-8 uses to encode various code\npoints. Again, running BPE only inside presegmented words helps avoid problems.\nBecause there are only 256 possible values of a byte, there will be no unknown tokens, although it’s possible that BPE will learn some illegal UTF-8 sequences across\ncharacter boundaries. These will be very rare, and can be eliminated with a filter.\nLet’s see some examples of the industrial application of the BPE tokenizer used\nin large systems like OpenAI GPT4o. This tokenizer has 200K tokens, which is a\ncomparatively large number. We can use Tat Dat Duong’s Tiktokenizer visualizer\n(https://tiktokenizer.vercel.app/) to see the number of tokens in a given\nsentence. For example here’s the tokenization of a nonsense sentence we made up;\nthe visualizer uses a center dot to indicate a space:\n\npretokenization\n\nThe visualization shows colors to separate out words, but of course the true output of the tokenizer is simply a sequence of unique token ids. (In case you’re interested, they were the following 13 tokens: 11865, 8923, 11, 31211, 6177, 23919,\n885, 220, 19427, 7633, 18887, 147065, 0)\nNotice that most words are their own token, usually including the leading space.\nClitics like ’s are segmented off when they appear on proper nouns like Jane, but\nare counted as part of a word for frequent words like she’s. Numbers tend to be\nsegmented into chunks of 3 digits. And some words (like anyhow) are segmented\ndifferently if they appear capitalized sentence-initially (two tokens, Any and how),\nthen if they appear after a space, lower case (one token anyhow).\nSome of these are related to preprocessing steps. As we mentioned briefly above,\nlanguage models usually create their tokens in a pretokenization stage that first segments the input using regular expressions, for example breaking the input at spaces\nand punctuation, stripping off clitics, and breaking numbers into sets of 3 digits.\n\n\f14\n\nC HAPTER 2\n\nSuperBPE\n\n•\n\nW ORDS AND T OKENS\n\nWe’ll see how to use regular expressions in Section 2.7.\nIt’s possible to change this pretokenization to allow BPE tokens to span multiple\nwords. For example the SuperBPE algorithm first induces regular BPE subword\ntokens by enforcing pretokenization. It then runs a second stage of BPE allowing\nmerges across spaces and punctuation. The result is a large set of tokens that can be\nPreprint\nmore efficient.\nSee Fig. 2.7.\n\nFigure 2.7 The SuperBPE algorithm creating larger tokens by allowing a second stage of\nmerging across spaces. Figure from Liu et al. (2025).\n\nMany of the tokenizers used in practice for large language models are multilingual, trained on many languages. But because the training data for large language\nmodels is vastly dominated by English text, these multilingual BPE tokenizers tend\nto use most of the tokens for English, leaving fewer of them for other languages. The\nresult is that they do a better job of tokenizing English, and the other languages tend\nto get their words split up into shorter tokens. For example let’s look at a Spanish\nsentence from a recipe for plantains, together with an English translation.\nThe English has 18 tokens; each of the 14 words is a token (none of the words\nare split into Figure\nmultiple\ntokens):\n1: SuperBPE tokenizers encode text much more efficiently than BPE, and the\n\ngap grows with larger vocabulary size. Encoding efficiency (y-axis) is measured with\nbytes-per-token, the number of bytes encoded per token on average over a large corpus of text.\nIn the above text with 40 bytes, SuperBPE uses 7 tokens and BPE uses 13, so the methods’\nefficiencies are 40/7 = 5.7 and 40/13 = 3.1 bytes-per-token, respectively. In the graph,\nthe encoding efficiency of BPE plateaus early due to exhausting the valuable whitespacedelimited words in the training data. In fact, it is bounded above by the gray dotted line,\nwhich shows the maximum achievable encoding efficiency with BPE, if every whitespaceBy contrast,\nthe original\nwords\nin Spanish\nhave\nbeen\nencoded\ninto 33 tokens,\ndelimited\nword were 16\nin the\nvocabulary.\nOn the other\nhand,\nSuperBPE\nhas dramatically\nencoding\nefficiency\nthat many\ncontinues\nto improve\nwithave\nincreased\nvocabulary\nsize,\na much largerbetter\nnumber.\nNotice\nthat\nbasic\nwords\nbeen\nbroken\nintoas pieces.\nit can continue to add common word sequences to treat as tokens to the vocabulary. The\ngradient\nlines show\npoints from\nsubword\nto superword\nFor example different\nhondo,\n‘deep’,\nhasdifferent\nbeen transition\nsegmented\nintolearning\nh and\nondo.\nSimilarly for\ntokens, which always gives an immediate improvement. SuperBPE also has better encoding\njugo, ‘juice’,efficiency\nnuez,than\n‘nut’\nandvariant\njenjibre\n‘ginger’):\na naive\nof BPE that\ndoes not use whitespace pretokenization at all.\nperforming well on these languages. Including multi-word tokens promises to be beneficial\nin several ways: it can lead to shorter token sequences, lowering the computational costs of\nLM training and inference, and may also offer representational advantages by segmenting\ntext into more semantically cohesive units (Salehi et al., 2015; Otani et al., 2020; Hofmann\net al., 2021).\nIn this work, we introduce a superword tokenization algorithm that produces a vocabulary of\n\nSpanish isboth\nnot\na particularly\nlow-resource\nthis\noversegmenting\nsubword\nand “superword”\ntokens, whichlanguage;\nwe use to refer\nto tokens\nthat bridge more can be\nthan one word. Our method, SuperBPE, introduces a pretokenization curriculum to the popueven more serious\nin lower\nresource\nlanguages,\ndown\nto individual\ncharacters.\nlar byte-pair\nencoding\n(BPE) algorithm\n(Sennrich etoften\nal., 2016):\nwhitespace\npretokenization\nis\ninitially\nused these\nto enforce\nlearning\nof subword\ntokens\nonly\n(as done in\nconventionalfor\nBPE),the\nbut downOversegmenting\ninto\ntiny\ntokens\ncan\ncause\nvarious\nproblems\nis disabled in a second stage, where the tokenizer transitions to learning superword tokens.\nNotably, of\nSuperBPE\ntokenizers scale\nwith vocabulary\nBPEwe\nquickly\nstream processing\nthe language.\nAsmuch\nwillbetter\nbecome\nmore size—while\nclear once\nintroduce\nhits a point of diminishing returns and begins adding increasingly rare subwords to the\ntransformer models\nChapter\nsuchtofragmentation\ncansequences\nlead to\npoor\nrepresentavocabulary,in\nSuperBPE\ncan 8,\ncontinue\ndiscover common word\nto treat\nas single\ntokens and improve encoding efficiency (see Figure 1).\ntions of meaning,\nthe need for longer contexts, and higher costs to train models\nIn our main experiments, we pretrain English LMs at 8B scale from scratch. When fixing the\n(Rust et al., 2021;\nAhia\net al.,size,\n2023).\nmodel size, vocabulary\nand training compute—varying only the algorithm for learning\nthe vocabulary—we find that models trained with SuperBPE tokenizers consistently and\nsignificantly improve over counterparts trained with a BPE tokenizer, while also being 27–\n33% more efficient at inference time. Our best SuperBPE model achieves an average +4.0%\n\n2.5\n\nRule-based tokenization\n\n2\n\nWhile data-based tokenization like BPE is the most common way of doing tokenization, there are also situations where we want to constrain our tokens to be words and\nnot subwords. This might be useful if we are running parsing algorithms for English\nwhere the parser might need grammatical words as input. Or it can be useful for\nany linguistic application where we have some a prior definition of the token that we\n\n\f2.5\n\nclitic\n\nPenn Treebank\ntokenization\n\n•\n\nRULE - BASED TOKENIZATION\n\n15\n\nare interested in studying. Or it can be useful for social science applications where\northographic words are useful domains of study.\nIn rule-based tokenization, we pre-define a standard and implement rules to implement that kind of tokenization. Let’s explore this for English word tokenization.\nWe have some desiderata for English. We often want to break off punctuation as a separate token; commas are a useful piece of information for parsers,\nand periods help indicate sentence boundaries. But we’ll often want to keep the\npunctuation that occurs word internally, in examples like m.p.h., Ph.D., AT&T, and\ncap’n. Special characters and numbers will need to be kept in prices ($45.55) and\ndates (01/02/06); we don’t want to segment that price into separate tokens of “45”\nand “55”. And there are URLs (https://www.stanford.edu), Twitter hashtags\n(#nlproc), or email addresses (someone@cs.colorado.edu).\nNumber expressions introduce complications; in addition to appearing at word\nboundaries, commas appear inside numbers in English, every three digits: 555,500.50.\nTokenization differs by language; languages like Spanish, French, and German, for\nexample, use a comma to mark the decimal point, and spaces (or sometimes periods)\nwhere English puts commas, for example, 555 500,50.\nA rule-based tokenizer can also be used to expand clitic contractions that are\nmarked by apostrophes, converting what’re to the two tokens what are, and we’re\nto we are. A clitic is a part of a word that can’t stand on its own, and can only occur when it is attached to another word. Such contractions occur in other alphabetic\nlanguages, including French pronouns (j’ai and articles l’homme).\nDepending on the application, tokenization algorithms may also tokenize multiword expressions like New York or rock ’n’ roll as a single token, which requires a multiword expression dictionary of some sort. Rule-based tokenization is\nthus intimately tied up with named entity recognition, the task of detecting names,\ndates, and organizations (Chapter 17).\nOne commonly used tokenization standard is known as the Penn Treebank tokenization standard, used for the parsed corpora (treebanks) released by the Linguistic Data Consortium (LDC), the source of many useful datasets. This standard\nseparates out clitics (doesn’t becomes does plus n’t), keeps hyphenated words together, and separates out all punctuation (to save space we’re showing visible spaces\n‘ ’ between tokens, although newlines is a more common output):\nInput:\n\n\"The San Francisco-based restaurant,\" they said,\n\"doesn’t charge $10\".\nOutput: \" The San Francisco-based restaurant , \" they said ,\n\" does n’t charge $ 10 \" .\n\nIn practice, since tokenization is run before any other language processing, it\nneeds to be very fast. For rule-based word tokenization we generally use deterministic algorithms based on regular expressions compiled into efficient finite state\nautomata. For example, Fig. 2.8 shows a basic regular expression that can be used\nto tokenize English with the nltk.regexp tokenize function of the Python-based\nNatural Language Toolkit (NLTK) (Bird et al. 2009; https://www.nltk.org).\nCarefully designed deterministic algorithms can deal with the ambiguities that\narise, such as the fact that the apostrophe needs to be tokenized differently when used\nas a genitive marker (as in the book’s cover), a quotative as in ‘The other class’, she\nsaid, or in clitics like they’re.\n\n\f16\n\nC HAPTER 2\n\n•\n\nW ORDS AND T OKENS\n\n>>> text = 'That U.S.A. poster-print costs $12.40...'\n>>> pattern = r'''(?x)\n# set flag to allow verbose regexps\n...\n(?:[A-Z]\\.)+\n# abbreviations, e.g. U.S.A.\n...\n| \\w+(?:-\\w+)*\n# words with optional internal hyphens\n...\n| \\$?\\d+(?:\\.\\d+)?%? # currency, percentages, e.g. $12.40, 82%\n...\n| \\.\\.\\.\n# ellipsis\n...\n| [][.,;\"'?():_`-] # these are separate tokens; includes ], [\n... '''\n>>> nltk.regexp_tokenize(text, pattern)\n['That', 'U.S.A.', 'poster-print', 'costs', '$12.40', '...']\nFigure 2.8 A Python trace of regular expression tokenization in the NLTK Python-based\nnatural language processing toolkit (Bird et al., 2009), commented for readability; the (?x)\nverbose flag tells Python to strip comments and whitespace. Figure from Chapter 3 of Bird\net al. (2009).\n\n2.5.1\nsentence\nsegmentation\n\n2.6\n\nSentence Segmentation\n\nRule-based segmentation is commonly used for another kind of tokenization process: the sentence. Sentence segmentation is a step that is can be optionally applied\nin text processing. It is especially important when applying NLP algorithms to tasks\nof detecting structure, like parse structure.\nSentence segmentation depends on the language and the genre. The most useful\ncues for segmenting a text into sentences in English written text tend to be punctuation, like periods, question marks, and exclamation points. Question marks and\nexclamation points are relatively unambiguous markers of sentence boundaries, and\nsimple rules can segment sentences when they appear.\nThe period character “.”, on the other hand, is ambiguous between a sentence\nboundary marker and a marker of abbreviations like Dr. or Inc. The previous sentence that you just read showed an even more complex case of this ambiguity, in\nwhich the final period of Inc. marked both an abbreviation and the sentence boundary marker. For this reason, sentence tokenization and word tokenization can be\naddressed jointly.\nMany English sentence tokenization methods work by first deciding (often based\non deterministic rules, but sometimes via machine learning) whether a period is part\nof the word or is a sentence-boundary marker. An abbreviation dictionary can help\ndetermine whether the period is part of a commonly used abbreviation; the dictionaries can be hand-built or machine-learned (Kiss and Strunk, 2006), as can the final\nsentence splitter. In the Stanford CoreNLP toolkit (Manning et al., 2014), for example sentence splitting is rule-based, a deterministic consequence of tokenization; a\nsentence ends when a sentence-ending punctuation (., !, or ?) is not already grouped\nwith other characters into a token (such as for an abbreviation or number), optionally\nfollowed by additional final quotes or brackets.\n\nCorpora\nWords don’t appear out of nowhere. Any particular piece of text that we study\nis produced by one or more specific speakers or writers, in a specific dialect of a\nspecific language, at a specific time, in a specific place, for a specific function.\nPerhaps the most important dimension of variation is the language. NLP algo-\n\n\f2.6\n\nAAE\n\nMAE\n\ncode switching\n\n•\n\nC ORPORA\n\n17\n\nrithms are most useful when they apply across many languages. The world has 7097\nlanguages at the time of this writing, according to the online Ethnologue catalog\n(Simons and Fennig, 2018). It is important to test algorithms on more than one language, and particularly on languages with different properties; by contrast there is\nan unfortunate current tendency for NLP algorithms to be developed or tested just on\nEnglish (Bender, 2019). Even when algorithms are developed beyond English, they\ntend to be developed for the official languages of large industrialized nations (Chinese, Spanish, Japanese, German etc.), but we don’t want to limit tools to just these\nfew languages. Furthermore, most languages also have multiple varieties, often spoken in different regions or by different social groups. Thus, for example, if we’re\nprocessing text that uses features of African American English (AAE) or African\nAmerican Vernacular English (AAVE)—the variations of English that can be used\nby millions of people in African American communities (King 2020)—we must use\nNLP tools that function with features of those varieties. Twitter posts might use features often used by speakers of African American English, such as constructions like\niont (I don’t in Mainstream American English (MAE)), or talmbout corresponding\nto MAE talking about, both examples that influence word segmentation (Blodgett\net al. 2016, Jones 2015).\nIt’s also quite common for speakers or writers to use multiple languages in a single utterance, a phenomenon called code switching. Code switching is enormously\ncommon across the world; here are examples showing Spanish and (transliterated)\nHindi code switching with English (Solorio et al. 2014, Jurgens et al. 2017):\n(2.12) Por primera vez veo a @username actually being hateful! it was beautiful:)\n[For the first time I get to see @username actually being hateful! it was\nbeautiful:) ]\n(2.13) dost tha or ra- hega ... dont wory ... but dherya rakhe\n[“he was and will remain a friend ... don’t worry ... but have faith”]\n\ndatasheet\n\nAnother dimension of variation is the genre. The text that our algorithms must\nprocess might come from newswire, fiction or non-fiction books, scientific articles,\nWikipedia, or religious texts. It might come from spoken genres like telephone\nconversations, business meetings, police body-worn cameras, medical interviews,\nor transcripts of television shows or movies. It might come from work situations\nlike doctors’ notes, legal text, or parliamentary or congressional proceedings.\nText also reflects the demographic characteristics of the writer (or speaker): their\nage, gender, race, socioeconomic class can all influence the linguistic properties of\nthe text we are processing.\nAnd finally, time matters too. Language changes over time, and for some languages we have good corpora of texts from different historical periods.\nBecause language is so situated, when developing computational models for language processing from a corpus, it’s important to consider who produced the language, in what context, for what purpose. How can a user of a dataset know all these\ndetails? The best way is for the corpus creator to build a datasheet (Gebru et al.,\n2020) or data statement (Bender et al., 2021) for each corpus. A datasheet specifies\nproperties of a dataset like:\nMotivation: Why was the corpus collected, by whom, and who funded it?\nSituation: When and in what situation was the text written/spoken? For example,\nwas there a task? Was the language originally spoken conversation, edited\ntext, social media communication, monologue vs. dialogue?\nLanguage variety: What language (including dialect/region) was the corpus in?\n\n\f18\n\nC HAPTER 2\n\n•\n\nW ORDS AND T OKENS\n\nSpeaker demographics: What was, e.g., the age or gender of the text’s authors?\nCollection process: How big is the data? If it is a subsample how was it sampled?\nWas the data collected with consent? How was the data pre-processed, and\nwhat metadata is available?\nAnnotation process: What are the annotations, what are the demographics of the\nannotators, how were they trained, how was the data annotated?\nDistribution: Are there copyright or other intellectual property restrictions?\n\n2.7\n\nRegular Expressions\n\nregular\nexpression\n\nstring\n\nOne of the most useful tools for text processing in computer science is the regular\nexpression (or regex), a language for specifying text strings. Regexes are used in\nevery computer language, in text processing tools like Unix grep, and in editors\nlike vim or Emacs. And they play an important role in the pre-tokenization step\nfor tokenization algorithms like BPE. Formally, a regular expression is an algebraic\nnotation for characterizing a set of strings. Practically, we can use a regex to search\nfor a string in a text and to specify how to change the string, both of which are key\nto tokenization.\nWe use regular expressions to search for a pattern in a string which can be a\nsingle line or a longer text. For example, the Python function\nre.search(pattern,string)\nscans through the string and returns the first match inside it for the pattern. In the\nfollowing examples we generally highlight the exact string that matches the regular\nexpression and show only the first match. We’ll use Python syntax, expressing the\nregex as a raw string delimited by double quotes: r\"regex\". Raw strings treat\nbackslashes as literal characters, which will be important since many regex patterns\nwe’ll introduce use backslashes.\nRegular expressions come in different variants, so using an online regex tester\ncan help make sure your regex does what you think it’s doing.\n\n2.7.1\n\ncharacter\ndisjunction\n\nCharacter Disjunction: The Square Bracket\n\nThe simplest kind of regular expression is a sequence of simple characters. The pattern r\"Buttercup\" matches the substring Buttercup in any string (like the string\nI’m called little Buttercup). But often we need to use special characters.\nFor example, we might want to match either some character or another. For example, regular expressions are generally case sensitive: r\"s\" matches a lower case s\nbut not an upper case S. To match both s and S we can use the character disjunction operator, the square braces [ and ]. The string of characters inside the braces\nspecifies a disjunction of characters to match. For example, Fig. 2.9 shows that the\npattern r\"[mM]\" matches patterns containing either m or M.\nPattern\nr\"[mM]ary\"\nr\"[abc]\"\nr\"[1234567890]\"\nFigure 2.9\n\nMatch\nMary or mary\n‘a’, ‘b’, or ‘c’\nany one digit\n\nString\n“Mary Ann stopped by Mona’s”\n“In uomini, in soldati”\n“plenty of 7 to 5”\n\nThe use of the brackets [] to specify a disjunction of characters.\n\nThe regular expression r\"[1234567890]\" specifies any single digit. This can\n\n\f2.7\n\nrange\n\n•\n\nR EGULAR E XPRESSIONS\n\n19\n\nget awkward (imagine typing r\"[ABCDEFGHIJKLMNOPQRSTUVWXYZ]\" to mean an\nuppercase letter) so the brackets can also be used with a dash (-) to specify any one\ncharacter in a range. The pattern r\"[2-5]\" specifies any one of the characters 2, 3,\n4, or 5. The pattern r\"[b-g]\" specifies one of the characters b, c, d, e, f, or g. Some\nother examples are shown in Fig. 2.10.\nRegex\nr\"[A-Z]\"\nr\"[a-z]\"\nr\"[0-9]\"\nFigure 2.10\n\nMatch\nan upper case letter\na lower case letter\na single digit\n\nExample Patterns Matched\n“we should call it ‘Drenched Blossoms’ ”\n“my beans were impatient to be hoed!”\n“Chapter 1: Down the Rabbit Hole”\n\nThe use of the brackets [] plus the dash - to specify a range.\n\nThe square braces can also be used to specify what a single character cannot be,\nby use of the caret ˆ. If the caret ˆ is the first symbol after the open square brace\n[, the resulting pattern is negated. For example, the pattern r\"[ˆa]\" matches any\nsingle character (including special characters) except a. This is only true when the\ncaret is the first symbol after the open square brace. If it occurs anywhere else, it\nusually stands for a caret; Fig. 2.11 shows some examples.\nRegex\nr\"[ˆA-Z]\"\nr\"[ˆSs]\"\nr\"[ˆ.]\"\nr\"[eˆ]\"\nr\"aˆb\"\nFigure 2.11\n\nMatch (single characters)\nnot an upper case letter\nneither ‘S’ nor ‘s’\nnot a period\neither ‘e’ or ‘ˆ’\nthe pattern ‘aˆb’\n\nExample Patterns Matched\n“Oyfn pripetchik”\n“I have no exquisite reason for’t”\n“our resident Djinn”\n“look up ˆ now”\n“look up aˆ b now”\n\nThe caret ˆ for negation or just to mean ˆ. See below re: the backslash for escaping the period.\n\n2.7.2\n\nCounting, Optionality, and Wildcards\n\nHow can we talk about optional elements, like an optional s if we want to match both\nkoala and koalas? We can’t use the square brackets, because while they allow us to\nsay “s or S”, they don’t allow us to say “s or nothing”. For this we use the question\nmark r\"?\", which means “the preceding character or nothing”,. So r\"colou?r\"\nmatches both color and colour, and r\"koala?\" matches koala or koalas.\nThere’s another way to talk about elements that may or may not occur. Consider\nthe language of certain sheep, which consists of strings that look like the following:\nbaa!\nbaaa!\nbaaaa!\n...\n\nKleene *\n\nThis sheep language consists of strings with a b, followed by at least two (and\narbitrarily more) a’s, followed by an exclamation point. To represent this language,\nwe’ll use a useful operator that is represented by the asterisk or *, called the Kleene\n* (generally pronounced “cleany star”). The Kleene star means “zero or more occurrences of the immediately previous character or regular expression”. So r\"a*\"\nmeans “any string of zero or more as”.\nCould r\"ba*\" represent the sheep language? It will correctly match ba or\nbaaaaaa, but there’s a problem! It will also match b, with no a, or ba with only one\na. That’s because Kleene star means “zero or more occurrences”. Instead, for the\n\n\f20\n\nC HAPTER 2\n\nKleene +\n\nperiod\n\n•\n\nW ORDS AND T OKENS\n\nsheep language we’ll want r\"baaa*\", meaning b followed by aa followed by zero\nor more additional as. More complex patterns can also be repeated. So r\"[ab]*\"\nmeans “zero or more a’s or b’s” (not “zero or more right square braces”). This will\nmatch strings like aaaa or ababab or bbbb, as well as the empty string. For specifying an integer (a string of digits) we can use r\"[0-9][0-9]*\". (Why isn’t it just\nr\"[0-9]*\"?)\nThere is a slightly shorter way to specify “at least one” of some character: the\nKleene +, which means “one or more occurrences of the immediately preceding\ncharacter or regular expression”. So r\"[0-9]+\" is the normal way to specify “a\nsequence of digits”, and we could also specify the sheep language as r\"baa+!\".\nBesides the Kleene * and Kleene + we can also use explicit numbers as counters, by enclosing them in curly brackets. The operator r\"{3}\" means “exactly 3\noccurrences of the previous character or expression”. So r\"ax{10}z\" will match a\nfollowed by exactly 10 x’s followed by z.\nAn important special character is the period (r\".\"), a wildcard expression that\nmatches any single character (except a newline).\nThe wildcard is often used together with the Kleene star to mean “any string\nof characters”. For example, suppose we want to find any line in which a particular word, for example, rose, appears twice. We can specify this with the regular\nexpression r\"rose.*rose\", meaning two roses, with a sequence of zero or more\ncharacters (of any kind) between them. Fig. 2.12 summarizes.\nRegex\n*\n+\n?\n{n}\n.\n.*\nFigure 2.12\n\n2.7.3\nanchors\n\nMatch\nzero or more occurrences of the previous char or expression\none or more occurrences of the previous char or expression\nzero or one occurrence of the previous char or expression\nexactly n occurrences of the previous char or expression\nany single char\nany string of zero or more chars\nCounting and wildcards.\n\nAnchors and Boundaries\n\nAnchors are special characters that anchor regular expressions to particular places\nin a string. The most common anchors are the caret ˆ and the dollar sign $. The\ncaret ˆ matches the start of a line. The pattern r\"ˆThe\" matches the word The only\nat the start of a line. Thus, the caret ˆ has three uses: to match the start of a line,\nto indicate a negation inside of square brackets, and just to mean a caret. (What are\nthe contexts that allow the system to know which function a given caret is supposed\nto have?) The dollar sign $ matches the end of a line. So the pattern $ is a useful\npattern for matching a space at the end of a line, and r\"ˆThe dog\\.$\" matches a\nline that contains only the phrase The dog. with a final period.\nNote that we have to use the backslash in the prior example since we want\nthe . to mean “period” and not the wildcard. By contrast, the regular expression\nr\"ˆThe dog.$\" would match The dog. but also The dog! and The dogo. As\nwe’ll discuss below, all the special characters we’ve defined so far (* + ? . [\n]) need to be backslashed when we mean to use them literally.\nThere are other anchors: \\b matches a word boundary, and \\B matches a non\nword-boundary. Thus, r\"\\bthe\\b\" matches the word the but not the word other.\nA “word” for the purposes of a regex is defined (based on words in programming\n\n\f2.7\nRegex\nˆ\n$\n\\b\n\\B\nFigure 2.13\n\n•\n\nR EGULAR E XPRESSIONS\n\n21\n\nMatch\nstart of line\nend of line\nword boundary\nnon-word boundary\n\nAnchors in regular expressions.\n\nlanguages) as a sequence of digits, underscores, or letters. Thus r\"\\b99\\b\" will\nmatch the string 99 in There are 99 bottles of beer on the wall (because\n99 follows a space) but not 99 in There are 299 bottles of beer on the\nwall (since 99 follows a number). But it will match 99 in $99 (since 99 follows a\ndollar sign ($), which is not a digit, underscore, or letter).\nNote that all these anchors and boundary operators technically match the empty\nstring, meaning that they don’t eat up any characters of the string. The carat in the\npattern r\"ˆThe\" matches the start of \"The\" but doesn’t actually advance over the\nfirst character T. And the pattern r\"the\\b the matches the the; the \\b is aware\nof the fact that the space is a boundary, but it matches the empty string right before\nthe space, not the space, so that the space character is available to be matched.\n\n2.7.4\n\ndisjunction\n\nprecedence\n\noperator\nprecedence\n\nDisjunction, Grouping, and Precedence\n\nSuppose we need to search for texts about pets; perhaps we are particularly interested\nin cats and dogs. In such a case, we might want to search for either the string\ncat or the string dog. Since we can’t use the square brackets to search for “cat or\ndog” (why wouldn’t r\"[catdog]\" do the right thing?), we need a new operator,\nthe disjunction operator, also called the pipe symbol |. The pattern r\"cat|dog\"\nmatches either the string cat or the string dog.\nSometimes we need to use this disjunction operator in the midst of a larger sequence. For example, suppose I want to search for mentions of pet fish. How can\nI specify both guppy and guppies? We cannot simply say r\"guppy|ies\", because\nthat would match only the strings guppy and ies. This is because sequences like\nguppy take precedence over the disjunction operator |. To make the disjunction\noperator apply only to a specific pattern, we need to use the parenthesis operators (\nand ). Enclosing a pattern in parentheses makes it act like a single character for the\npurposes of neighboring operators like the pipe | and the Kleene*. So the pattern\nr\"gupp(y|ies)\" would specify that we meant the disjunction only to apply to the\nsuffixes y and ies.\nThe parenthesis operator ( is also useful when we are using counters like the\nKleene*. Unlike the | operator, the Kleene* operator applies by default only to\na single character, not to a whole sequence. Suppose we want to match repeated\ninstances of a string. Perhaps we have a line that has column labels of the form\nColumn 1 Column 2 Column 3. The expression r\"Column [0-9]+ *\" will not\nmatch any number of columns; instead, it will match a single column followed by\nany number of spaces! The star here applies only to the space that precedes it,\nnot to the whole sequence. With the parentheses, we could write the expression\nr\"(Column [0-9]+ +)*\" to match the word Column, followed by a number and\noptional spaces, the whole pattern repeated zero or more times.\nThis idea that one operator may take precedence over another, requiring us to\nsometimes use parentheses to specify what we mean, is formalized by the operator\nprecedence hierarchy for regular expressions. The following table gives the order\nof operator precedence, from highest precedence to lowest precedence.\n\n\f22\n\nC HAPTER 2\n\n•\n\nW ORDS AND T OKENS\nParenthesis\n()\nCounters\n* + ? {}\nSequences and anchors the ˆmy end$\nDisjunction\n|\n\ngreedy\nnon-greedy\n*?\n+?\n\nThus, because counters have a higher precedence than sequences,\nr\"the*\" matches theeeee but not thethe. Because sequences have a higher precedence than disjunction, r\"the|any\" matches the or any but not thany or theny.\nPatterns can be ambiguous in another way. Consider the expression r\"[a-z]*\"\nwhen matching against the text once upon a time. Since r\"[a-z]*r\" matches zero\nor more letters, this expression could match nothing, or just the first letter o, on, onc,\nor once. In these cases regular expressions always match the largest string they can;\nwe say that patterns are greedy, expanding to cover as much of a string as they can.\nThere are, however, ways to enforce non-greedy matching, using another meaning of the ? qualifier. The operator *? is a Kleene star that matches as little text as\npossible. The operator +? is a Kleene plus that matches as little text as possible.\n\n2.7.5\n\nA Simple Example\n\nSuppose we wanted to write a regex to find cases of the English article the. A simple\n(but incorrect) pattern might be:\nr\"the\"\n\n(2.14)\n\nOne problem is that this pattern will miss the word when it begins a sentence and\nhence is capitalized (i.e., The). This might lead us to the following pattern:\nr\"[tT]he\"\n\n(2.15)\n\nBut we will still overgeneralize, incorrectly return texts with the embedded in other\nwords (e.g., other or there). So we need to specify that we want instances with a\nword boundary on both sides:\nr\"\\b[tT]he\\b\"\nfalse positives\nfalse negatives\n\nThe simple process we just went through was based on fixing two kinds of errors:\nfalse positives, strings that we incorrectly matched like other or there, and false\nnegatives, strings that we incorrectly missed, like The. Addressing these two kinds\nof errors comes up again and again in language processing. Reducing the overall\nerror rate for an application thus involves two antagonistic efforts:\n• Increasing precision (minimizing false positives)\n• Increasing recall (minimizing false negatives)\nWe’ll come back to precision and recall with more precise definitions in Chapter 4.\n\n2.7.6\n\nnewline\n\n(2.16)\n\nMore Operators\n\nFigure 2.14 shows some useful aliases for common ranges:\nFinally, certain special characters are referred to by special notation based on the\nbackslash (\\) (see Fig. 2.15). The most common of these are the newline character\n\\n and the tab character \\t.\nHow do we refer to characters that are special themselves (like ., *, -, [, and\n\\) when we mean them literally, not in their special usage? That is, if we are trying\nto match a period, or a star, or a bracket or paren? To get the literal meaning of a\nspecial character, we need to precede them with a backslash, (i.e., r\"\\.\", r\"\\*\",\nr\"\\[\", and r\"\\\\\").\n\n\f2.7\nRegex\n\\d\n\\D\n\\w\n\\W\n\\s\n\\S\nFigure 2.14\n\nRegex\n\\*\n\\.\n\\?\n\\n\n\\t\nFigure 2.15\n\n2.7.7\nsubstitution\n\nExpansion\n[0-9]\n[ˆ0-9]\n[a-zA-Z0-9_]\n[ˆ\\w]\n[ \\r\\t\\n\\f]\n[ˆ\\s]\n\n•\n\nR EGULAR E XPRESSIONS\n\nMatch\nany digit\nany non-digit\nany alphanumeric/underscore\na non-alphanumeric\nwhitespace (space, tab)\nNon-whitespace\n\n23\n\nFirst Matches\nParty of 5\nBlue moon\nDaiyu\n!!!!\nin Concord\nin Concord\n\nAliases for common sets of characters.\n\nMatch\nan asterisk “*”\na period “.”\na question mark\na newline\na tab\n\nFirst Patterns Matched\n“K*A*P*L*A*N”\n“Dr. Livingston, I presume”\n“Why don’t they come and lend a hand?”\n\nSome characters that need to be escaped (via backslash).\n\nSubstitutions and Capture Groups\n\nAn important use of regular expressions is in substitutions, where we want to replace one string with another. Regular expression can help us specify the string to\nbe replaced as well as the replacement. In Python we use the function re.sub()\n(similar functions exist in other languages and environments).\nre.sub(pattern, repl, string) takes three arguments: a pattern to search for, a\nreplacement to replace it with, and a string in which to do the search and replacing\nWe could for example change every instance of cherry to apricot in string:\nre.sub(r\"cherry\", r\"apricot\", string)\nOr we could convert to upper case all the instances of a particular name:\nre.sub(r\"janet\", r\"Janet\", string)\n\ncapture group\n\nMore often, however, the substitution depends in a more complex way on the\nstring that matched the pattern. For example, suppose we have a document in\nwhich all the dates are in US format (mm/dd/yyyy) and we want to change them\ninto the format used in the EU and many other regions: (dd-mm-yyyy). The pattern r\"\\d{2}/\\d{2}/\\d{4}\" will match a date. But how do we specify in the\nreplacement that we want to swap the date and month values?\nThe tool in regular expression for this is the capture group. A capture group\nuses parentheses to capture (store) the values that we matched in the search, so we\ncan reuse them in the replacement. We put a set of parentheses around the part of\nthe pattern we want to capture, and it will get stored in a numbered group (groups\nare numbered from left to right). Then in the repl, we refer back to that group with\na number command.\nConsider the following expression:\nre.sub(r\"(\\d{2})/(\\d{2})/(\\d{4})\", r\"\\2-\\1-\\3\", string)}\nWe’ve put parentheses ( and ) around the two month digits, the two day digits,\nand the four year digits, thus storing the first 2 digits in group 1, the second 2 digits\nin group 2, and the final digits in group 3. Then in the repl string, we use number\noperators \\1, \\2, and \\3, to refer back to the first, second, and third registers. The\nresult would take a string like\nThe date is 10/15/2011\n\n\f24\n\nC HAPTER 2\n\n•\n\nW ORDS AND T OKENS\n\nand convert it to\nThe date is 15-10-2011\nCapture groups can be useful even if we are not doing substitutions. For example\nwe can use them to find repetitions, something we often need in text processing. For\nexample, to find a repeated word in a string, we can use this pattern which searches\nfor a word, captures it in a group, and then refers back to it after whitespace:\nr\"\\b([A-Za-z]+)\\s+\\1\\b\"\n\nnon-capturing\ngroup\n\nParentheses thus have a double function in regular expressions; they are used to\ngroup terms for specifying the order in which operators should apply, and they are\nused to capture the match. Occasionally we need parentheses for grouping, but don’t\nwant to capture the resulting pattern. In that case we use a non-capturing group,\nwhich is specified by putting the special commands ?: after the open parenthesis,\nin the form (?: pattern ). Non-capture groups are usually used when we are\ntrying to capture only part of a long or complex pattern. Perhaps we are matching\na sequence of dates (\\d\\d/\\d\\d/\\d\\d\\d\\d) separated by spaces and we want to\nextract only the 15th one. We need to use parenthesis in order to use the counting\noperator on the first 14, but we don’t want to store all the useless information. The\nfollowing pattern only stores the 15th date in group 1:\nr\"(?:\\d\\d/\\d\\d/\\d\\d\\d\\d\\s+){14}(\\d\\d/\\d\\d/\\d\\d\\d\\d)\"\n\n(2.17)\n\nSubstitutions and capture groups are also useful for implementing historically\nimportant chatbots like ELIZA (Weizenbaum, 1966). Recall that ELIZA simulates\na Rogerian psychologist by carrying on conversations like the following:\nUser2 :\nThey’re always bugging us about something or other.\nELIZA2 : CAN YOU THINK OF A SPECIFIC EXAMPLE\nUser3 :\nWell, my boyfriend made me come here.\nELIZA3 : YOUR BOYFRIEND MADE YOU COME HERE\nUser4 :\nHe says I’m depressed much of the time.\nELIZA4 : I AM SORRY TO HEAR YOU ARE DEPRESSED\nELIZA works by having a series or cascade of regex substitutions each of which\nmatches and changes some part of the input lines. After the input is uppercased,\nsubstitutions change all instances of MY to YOUR, and I’M to YOU ARE, and so on.\nThat way when ELIZA repeats back part of the user utterance, it will seem to be\nreferring correctly to the user. The next set of substitutions matches and replaces\nother patterns in the input, turning the input into a complete response. Here are\nsome examples:\nre.sub(r\".* YOU ARE (DEPRESSED|SAD) .*\",r\"I AM SORRY TO HEAR YOU ARE \\1\",input)\nre.sub(r\".* YOU ARE (DEPRESSED|SAD) .*\",r\"WHY DO YOU THINK YOU ARE \\1\",input)\nre.sub(r\".* ALWAYS .*\",r\"CAN YOU THINK OF A SPECIFIC EXAMPLE\",input)\n\n2.7.8\n\nLookahead Assertions\n\nFinally, there will be times when we need to predict the future: look ahead in the\ntext to see if some pattern matches, but not yet advance the pointer we always keep\nto where we are in the text, so that we can then deal with the pattern if it occurs, but\nif it doesn’t we can check for something else instead.\n\n\f2.8\nlookahead\nzero-width\n\n•\n\nS IMPLE U NIX T OOLS FOR W ORD T OKENIZATION\n\n25\n\nThese lookahead assertions make use of the (? syntax that we saw in the previous section for non-capture groups. The operator (?= pattern) is true if pattern\noccurs, but is zero-width, i.e. the match pointer doesn’t advance, just as we saw\nwith anchors and boundary markers like \\b. The operator (?! pattern) only returns true if a pattern does not match, but again is zero-width and doesn’t advance\nthe pointer. Negative lookahead is commonly used when we are parsing some complex pattern but want to rule out a special case. For example suppose we want to\ncapture the first word on the line, but only if it doesn’t start with the letter T. We can\nuse negative lookahead to do this:\nr\"ˆ(?![tT])(\\w+)\\b\"\n\n(2.18)\n\nThe first negative lookahead says that the line must not start with a t or T, but\nmatches the empty string, not moving the match pointer. Then the capture group\ncaptures the first word.\n\n2.8\n\nSimple Unix Tools for Word Tokenization\nFor English it is possible to do simple naive word tokenization and frequency computation in a single Unix command-line. As Church (1994) points out, this can be\nuseful when we need quick information about a text corpus. We’ll make use of some\nUnix commands: tr, used to systematically change particular characters in the input; sort, which sorts input lines in alphabetical order; and uniq, which collapses\nand counts adjacent identical lines.\nFor example let’s begin with the ‘complete words’ of Shakespeare in one file,\nsh.txt. We can use tr to tokenize the words by changing every sequence of nonalphabetic characters to a newline (’A-Za-z’ means alphabetic and the -c option\ncomplements to non-alphabet, so together they mean to change every non-alphabetic\ncharacter into a newline. The -s (‘squeeze’) option is used to replace the result\nof multiple consecutive changes into a single output, so a series of non-alphabetic\ncharacters in a row would all be ‘squeezed’ into a single newline):\ntr -sc 'A-Za-z' '\\n' < sh.txt\nThe output of this command will be:\nTHE\nSONNETS\nby\nWilliam\nShakespeare\nFrom\nfairest\ncreatures\n...\nNow that there is one word per line, we can sort the lines, and pass them to uniq\n-c which will collapse and count them:\ntr -sc ’A-Za-z’ ’\\n’ < sh.txt | sort | uniq -c\nwith the following output:\n\n\f26\n\nC HAPTER 2\n\n•\n\nW ORDS AND T OKENS\n\n1945 A\n72 AARON\n19 ABBESS\n25 Aaron\n6 Abate\n1 Abates\n...\nAlternatively, we can collapse all the upper case to lower case:\ntr -sc 'A-Za-z' '\\n' < sh.txt | tr A-Z a-z | sort | uniq -c\nwhose output is\n14725 a\n97 aaron\n1 abaissiez\n10 abandon\n2 abandoned\n2 abase\n1 abash\n14 abate\n...\nNow we can sort again to find the frequent words. The -n option to sort means\nto sort numerically rather than alphabetically, and the -r option means to sort in\nreverse order (highest-to-lowest):\ntr -sc 'A-Za-z' '\\n' < sh.txt | tr A-Z a-z | sort | uniq -c | sort -n -r\n\nThe results show that the most frequent words in Shakespeare, as in any other\ncorpus, are the short function words like articles, pronouns, prepositions:\n27378 the\n26084 and\n22538 i\n19771 to\n17481 of\n14725 a\n13826 you\n...\nUnix tools of this sort can be very handy in building quick word count statistics\nfor any corpus in English. For anything more complex, we generally turn to the\nmore sophisticated tokenization algorithms we’ve discussed above.\n\n2.9\n\nMinimum Edit Distance\n\nminimum edit\ndistance\n\nWe often need a way to compare how similar two words or strings are. As we’ll\nsee in later chapters, this comes up most commonly in tasks like automatic speech\nrecognition or machine translation, where we want to know how similar the sequence\nof words is to some reference sequence of words.\nEdit distance gives us a way to quantify these intuitions about string similarity.\nMore formally, the minimum edit distance between two strings is defined as the\n\n\f2.9\n\nalignment\n\n•\n\nM INIMUM E DIT D ISTANCE\n\n27\n\nminimum number of editing operations (operations like insertion, deletion, substitution) needed to transform one string into another. In this section we’ll introduce edit\ndistance for single words, but the algorithm applies equally to entire strings.\nThe gap between intention and execution, for example, is 5 (delete an i, substitute e for n, substitute x for t, insert c, substitute u for n). It’s much easier to see\nthis by looking at the most important visualization for string distances, an alignment\nbetween the two strings, shown in Fig. 2.16. Given two sequences, an alignment is\na correspondence between substrings of the two sequences. Thus, we say I aligns\nwith the empty string, N with E, and so on. Beneath the aligned strings is another\nrepresentation; a series of symbols expressing an operation list for converting the\ntop string into the bottom string: d for deletion, s for substitution, i for insertion.\n\nINTE*NTION\n| | | | | | | | | |\n*EXECUTION\nd s s\n\ni s\n\nFigure 2.16 Representing the minimum edit distance between two strings as an alignment.\nThe final row gives the operation list for converting the top string into the bottom string: d for\ndeletion, s for substitution, i for insertion.\n\nWe can also assign a particular cost or weight to each of these operations. The\nLevenshtein distance between two sequences is the simplest weighting factor in\nwhich each of the three operations has a cost of 1 (Levenshtein, 1966)—we assume\nthat the substitution of a letter for itself, for example, t for t, has zero cost. The Levenshtein distance between intention and execution is 5. Levenshtein also proposed\nan alternative version of his metric in which each insertion or deletion has a cost of\n1 and substitutions are not allowed. (This is equivalent to allowing substitution, but\ngiving each substitution a cost of 2 since any substitution can be represented by one\ninsertion and one deletion). Using this version, the Levenshtein distance between\nintention and execution is 8.\n\n2.9.1\n\nThe Minimum Edit Distance Algorithm\n\nHow do we find the minimum edit distance? We can think of this as a search task, in\nwhich we are searching for the shortest path—a sequence of edits—from one string\nto another.\ni n t e n t i o n\ndel\n\nn t e n t i o n\n\nFigure 2.17\n\ndynamic\nprogramming\n\nins\n\nsubst\n\ni n t e c n t i o n\n\ni n x e n t i o n\n\nFinding the edit distance viewed as a search problem\n\nThe space of all possible edits is enormous, so we can’t search naively. However,\nlots of distinct edit paths will end up in the same state (string), so rather than recomputing all those paths, we could just remember the shortest path to a state each time\nwe saw it. We can do this by using dynamic programming. Dynamic programming\nis the name for a class of algorithms, first introduced by Bellman (1957), that apply\n\n\f28\n\nC HAPTER 2\n\n•\n\nW ORDS AND T OKENS\n\na table-driven method to solve problems by combining solutions to subproblems.\nSome of the most commonly used algorithms in natural language processing make\nuse of dynamic programming, such as the Viterbi algorithm (Chapter 17) and the\nCKY algorithm for parsing (Chapter 18).\nThe intuition of a dynamic programming problem is that a large problem can\nbe solved by properly combining the solutions to various subproblems. Consider\nthe shortest path of transformed words that represents the minimum edit distance\nbetween the strings intention and execution shown in Fig. 2.18.\ni n t e n t i o n\nn t e n t i o n\ne t e n t i o n\ne x e n t i o n\ne x e n u t i o n\n\ndelete i\nsubstitute n by e\nsubstitute t by x\ninsert u\nsubstitute n by c\n\ne x e c u t i o n\nFigure 2.18\n\nminimum edit\ndistance\nalgorithm\n\nPath from intention to execution.\n\nImagine some string (perhaps it is exention) that is in this optimal path (whatever\nit is). The intuition of dynamic programming is that if exention is in the optimal\noperation list, then the optimal sequence must also include the optimal path from\nintention to exention. Why? If there were a shorter path from intention to exention,\nthen we could use it instead, resulting in a shorter overall path, and the optimal\nsequence wouldn’t be optimal, thus leading to a contradiction.\nThe minimum edit distance algorithm was named by Wagner and Fischer\n(1974) but independently discovered by many people (see the Historical Notes section of Chapter 17).\nLet’s first define the minimum edit distance between two strings. Given two\nstrings, the source string X of length n, and target string Y of length m, we’ll define\nD[i, j] as the edit distance between X[1..i] and Y [1.. j], i.e., the first i characters of X\nand the first j characters of Y . The edit distance between X and Y is thus D[n, m].\nWe’ll use dynamic programming to compute D[n, m] bottom up, combining solutions to subproblems. In the base case, with a source substring of length i but an\nempty target string, going from i characters to 0 requires i deletes. With a target\nsubstring of length j but an empty source going from 0 characters to j characters\nrequires j inserts. Having computed D[i, j] for small i, j we then compute larger\nD[i, j] based on previously computed smaller values. The value of D[i, j] is computed by taking the minimum of the three possible paths through the matrix which\narrive there:\n\n D[i − 1, j] + del-cost(source[i])\nD[i, j] = min D[i, j − 1] + ins-cost(target[ j])\n(2.19)\n\nD[i − 1, j − 1] + sub-cost(source[i], target[ j])\nWe mentioned above two versions of Levenshtein distance, one in which substitutions cost 1 and one in which substitutions cost 2 (i.e., are equivalent to an insertion\nplus a deletion). Let’s here use that second version of Levenshtein distance in which\nthe insertions and deletions each have a cost of 1 (ins-cost(·) = del-cost(·) = 1), and\nsubstitutions have a cost of 2 (except substitution of identical letters has zero cost).\n\n\f2.9\n\n•\n\nM INIMUM E DIT D ISTANCE\n\nUnder this version of Levenshtein, the computation for D[i, j] becomes:\n\nD[i − 1, j] + 1\n\n\n\nD[i, j − 1] + 1 \u001a\nD[i, j] = min\n2; if source[i] 6= target[ j]\n\n\n D[i − 1, j − 1] +\n0; if source[i] = target[ j]\n\n29\n\n(2.20)\n\nThe algorithm is summarized in Fig. 2.19; Fig. 2.20 shows the results of applying\nthe algorithm to the distance between intention and execution with the version of\nLevenshtein in Eq. 2.20.\n\nfunction M IN -E DIT-D ISTANCE(source, target) returns min-distance\nn ← L ENGTH(source)\nm ← L ENGTH(target)\nCreate a distance matrix D[n+1,m+1]\n# Initialization: the zeroth row and column is the distance from the empty string\nD[0,0] = 0\nfor each row i from 1 to n do\nD[i,0] ← D[i-1,0] + del-cost(source[i])\nfor each column j from 1 to m do\nD[0,j] ← D[0, j-1] + ins-cost(target[j])\n# Recurrence relation:\nfor each row i from 1 to n do\nfor each column j from 1 to m do\nD[i, j] ← M IN( D[i−1, j] + del-cost(source[i]),\nD[i−1, j−1] + sub-cost(source[i], target[j]),\nD[i, j−1] + ins-cost(target[j]))\n# Termination\nreturn D[n,m]\nFigure 2.19 The minimum edit distance algorithm, an example of the class of dynamic\nprogramming algorithms. The various costs can either be fixed (e.g., ∀x, ins-cost(x) = 1)\nor can be specific to the letter (to model the fact that some letters are more likely to be inserted than others). We assume that there is no cost for substituting a letter for itself (i.e.,\nsub-cost(x, x) = 0).\n\nAlignment Knowing the minimum edit distance is useful for algorithms like finding potential spelling error corrections. But the edit distance algorithm is important\nin another way; with a small change, it can also provide the minimum cost alignment between two strings. Aligning two strings is useful throughout speech and\nlanguage processing. In speech recognition, minimum edit distance alignment is\nused to compute the word error rate (Chapter 15). Alignment plays a role in machine translation, in which sentences in a parallel corpus (a corpus with a text in two\nlanguages) need to be matched to each other.\nTo extend the edit distance algorithm to produce an alignment, we can start by\nvisualizing an alignment as a path through the edit distance matrix. Figure 2.21\nshows this path with boldfaced cells. Each boldfaced cell represents an alignment\nof a pair of letters in the two strings. If two boldfaced cells occur in the same row,\nthere will be an insertion in going from the source to the target; two boldfaced cells\nin the same column indicate a deletion.\n\n\f30\n\nC HAPTER 2\n\n•\n\nW ORDS AND T OKENS\n\nSrc\\Tar\n#\ni\nn\nt\ne\nn\nt\ni\no\nn\n\n#\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\ne\n1\n2\n3\n4\n3\n4\n5\n6\n7\n8\n\nx\n2\n3\n4\n5\n4\n5\n6\n7\n8\n9\n\ne\n3\n4\n5\n6\n5\n6\n7\n8\n9\n10\n\nc\n4\n5\n6\n7\n6\n7\n8\n9\n10\n11\n\nu\n5\n6\n7\n8\n7\n8\n9\n10\n11\n12\n\nt\n6\n7\n8\n7\n8\n9\n8\n9\n10\n11\n\ni\n7\n6\n7\n8\n9\n10\n9\n8\n9\n10\n\no\n8\n7\n8\n9\n10\n11\n10\n9\n8\n9\n\nn\n9\n8\n7\n8\n9\n10\n11\n10\n9\n8\n\nFigure 2.20 Computation of minimum edit distance between intention and execution with\nthe algorithm of Fig. 2.19, using Levenshtein distance with cost of 1 for insertions or deletions, 2 for substitutions.\n\nbacktrace\n\nFigure 2.21 also shows the intuition of how to compute this alignment path. The\ncomputation proceeds in two steps. In the first step, we augment the minimum edit\ndistance algorithm to store backpointers in each cell. The backpointer from a cell\npoints to the previous cell (or cells) that we came from in entering the current cell.\nWe’ve shown a schematic of these backpointers in Fig. 2.21. Some cells have multiple backpointers because the minimum extension could have come from multiple\nprevious cells. In the second step, we perform a backtrace. In a backtrace, we start\nfrom the last cell (at the final row and column), and follow the pointers back through\nthe dynamic programming matrix. Each complete path between the final cell and the\ninitial cell is a minimum distance alignment. Exercise 2.7 asks you to modify the\nminimum edit distance algorithm to store the pointers and compute the backtrace to\noutput an alignment.\n\n#\ni\nn\nt\ne\nn\nt\ni\no\nn\n\n#\ne\nx\ne\nc\nu\nt\ni\no\nn\n0\n←1\n←2\n←3\n←4\n←5\n←6\n←7\n←8 ←9\n↑ 1 -←↑ 2 -←↑ 3\n-←↑ 4\n-←↑ 5\n-←↑ 6 -←↑ 7\n-6\n←7 ←8\n↑ 2 -←↑ 3 -←↑ 4\n-←↑ 5\n-←↑ 6\n-←↑ 7 -←↑ 8\n↑7\n-←↑ 8 - 7\n↑ 3 -←↑ 4 -←↑ 5\n-←↑ 6\n-←↑ 7\n-←↑ 8\n-7\n←↑ 8\n-←↑ 9\n↑8\n↑4\n-3\n←4\n-← 5\n←6\n←7\n←↑ 8\n-←↑ 9 -←↑ 10\n↑9\n↑5\n↑ 4 -←↑ 5\n-←↑ 6\n-←↑ 7\n-←↑ 8 -←↑ 9 -←↑ 10 -←↑ 11 -↑ 10\n↑6\n↑ 5 -←↑ 6\n-←↑ 7\n-←↑ 8\n-←↑ 9\n-8\n←9\n← 10 ←↑ 11\n↑7\n↑ 6 -←↑ 7\n-←↑ 8\n-←↑ 9 -←↑ 10\n↑9\n-8\n← 9 ← 10\n↑8\n↑ 7 -←↑ 8\n-←↑ 9 -←↑ 10 -←↑ 11\n↑ 10\n↑9\n-8 ←9\n↑9\n↑ 8 -←↑ 9 -←↑ 10 -←↑ 11 -←↑ 12\n↑ 11\n↑ 10\n↑9 -8\n\nFigure 2.21 When entering a value in each cell, we mark which of the three neighboring\ncells we came from with up to three arrows. After the table is full we compute an alignment\n(minimum edit path) by using a backtrace, starting at the 8 in the lower-right corner and\nfollowing the arrows back. The sequence of bold cells represents one possible minimum\ncost alignment between the two strings, again using Levenshtein distance with cost of 1 for\ninsertions or deletions, 2 for substitutions. Diagram design after Gusfield (1997).\n\nWhile we worked our example with simple Levenshtein distance, the algorithm\nin Fig. 2.19 allows arbitrary weights on the operations. For spelling correction, for\nexample, substitutions are more likely to happen between letters that are next to\neach other on the keyboard. The Viterbi algorithm is a probabilistic extension of\nminimum edit distance. Instead of computing the “minimum edit distance” between\n\n\f2.10\n\n•\n\nS UMMARY\n\n31\n\ntwo strings, Viterbi computes the “maximum probability alignment” of one string\nwith another. We’ll discuss this more in Chapter 17.\n\n2.10\n\nSummary\nThis chapter introduced the fundamental concepts of tokens and tokenization in language processing. We discussed the linguistic levels of words, morphemes, and\ncharacters, introduced Unicode code points and the UTF-8 encoding, introduced\nthe BPE algorithm for tokenization, and introduced the regular expression and the\nminimum edit distance algorithm for comparing strings. Here’s a summary of the\nmain points we covered about these ideas:\n• Words and morphemes are useful units of representation, but difficult to define\nformally.\n• Unicode is a system for representing characters in the many scripts used to\nwrite the languages of the world.\n• Each character is represented internally with a unique id called a code point,\nand can be encoded in a file via encoding methods like UTF-8, which is a\nvariable-length encoding.\n• Byte-Pair Encoding or BPE is the standard way to induce tokens in a datadriven way. It is the first step in most large language models.\n• BPE tokens are often roughly word or morpheme-sized, although they can be\nas small as single characters.\n• The regular expression language is a powerful tool for pattern-matching.\n• Basic operations in regular expressions include disjunction of symbols ([],\n|), counters (*, +, and {n,m}), anchors (ˆ, $), capture groups ((,)), and\nsubstitutions.\n• The minimum edit distance between two strings is the minimum number of\noperations it takes to edit one into the other. Minimum edit distance can be\ncomputed by dynamic programming, which also results in an alignment of\nthe two strings.\n\nHistorical Notes\nFor more on Herdan’s law and Heaps’ Law, see Herdan (1960, p. 28), Heaps (1978),\nEgghe (2007) and Baayen (2001);\nUnicode drew on ASCII and ISO character encoding standards. Early drafts\nwere worked out in discussions between engineers from Xerox and Apple. An early\ndraft standard was published in 1988, with a more formal release of the Unicode\nStanford in 1991. What became UTF-8 began with ISO drafts in 1989, with various\nextensions. The self-synchronizing aspects were famously outlined on a placemat in\na New Jersey dinner in 1992 by Ken Thompson.\nWord tokenization and other text normalization algorithms have been applied\nsince the beginning of the field. This include stemming, like the widely used stemmer of Lovins (1968), and applications to the digital humanities like those of by\nPackard (1973), who built an affix-stripping morphological parser for Ancient Greek.\nBPE, originally a text compression method proposed by Gage (1994), was applied\n\n\f32\n\nC HAPTER 2\n\n•\n\nW ORDS AND T OKENS\n\nto subword tokenization in the context of early neural machine translation by Sennrich et al. (2016). It was then taken up in OpenAI’s GPT-2 (Radford et al., 2019)\nas the default tokenization method, and also included in the open-source SentencePiece library (Kudo and Richardson, 2018b). There is a nice a public implementation, minbpe, https://github.com/karpathy/minbpe, by Andrej Karpathy,\nwho also has a popular lecture introducing BPE (https://www.youtube.com/\nwatch?v=zduSFxRajkE).\nKleene 1951; 1956 first defined regular expressions and the finite automaton,\nbased on the McCulloch-Pitts neuron. Ken Thompson was one of the first to build\nregular expressions compilers into editors for text searching (Thompson, 1968). His\neditor ed included a command “g/regular expression/p”, or Global Regular Expression Print, which later became the Unix grep utility.\nNLTK is an essential tool that offers both useful Python libraries (https://\nwww.nltk.org) and textbook descriptions (Bird et al., 2009) of many algorithms\nincluding text normalization and corpus interfaces.\nFor more on edit distance, see Gusfield (1997). Our example measuring the edit\ndistance from ‘intention’ to ‘execution’ was adapted from Kruskal (1983). There are\nvarious publicly available packages to compute edit distance, including Unix diff\nand the NIST sclite program (NIST, 2005).\nIn his autobiography Bellman (1984) explains how he originally came up with\nthe term dynamic programming:\n“...The 1950s were not good years for mathematical research. [the]\nSecretary of Defense ...had a pathological fear and hatred of the word,\nresearch... I decided therefore to use the word, “programming”. I\nwanted to get across the idea that this was dynamic, this was multistage... I thought, let’s ... take a word that has an absolutely precise\nmeaning, namely dynamic... it’s impossible to use the word, dynamic,\nin a pejorative sense. Try thinking of some combination that will possibly give it a pejorative meaning. It’s impossible. Thus, I thought\ndynamic programming was a good name. It was something not even a\nCongressman could object to.”\n\nExercises\n2.1\n\nWrite regular expressions for the following languages.\n1. the set of all alphabetic strings;\n2. the set of all lower case alphabetic strings ending in a b;\n3. the set of all strings from the alphabet a, b such that each a is immediately preceded by and immediately followed by a b;\n\n2.2\n\nWrite regular expressions for the following languages. By “word”, we mean\nan alphabetic string separated from other words by whitespace, any relevant\npunctuation, line breaks, and so forth.\n1. the set of all strings with two consecutive repeated words (e.g., “Humbert Humbert” and “the the” but not “the bug” or “the big bug”);\n2. all strings that start at the beginning of the line with an integer and that\nend at the end of the line with a word;\n\n\fE XERCISES\n\n33\n\n3. all strings that have both the word grotto and the word raven in them\n(but not, e.g., words like grottos that merely contain the word grotto);\n4. write a pattern that places the first word of an English sentence in a\nregister. Deal with punctuation.\n2.3\n\nImplement an ELIZA-like program, using substitutions such as those described\non page 24. You might want to choose a different domain than a Rogerian psychologist, although keep in mind that you would need a domain in which your\nprogram can legitimately engage in a lot of simple repetition.\n\n2.4\n\nCompute the edit distance (using insertion cost 1, deletion cost 1, substitution\ncost 1) of “leda” to “deal”. Show your work (using the edit distance grid).\n\n2.5\n\nFigure out whether drive is closer to brief or to divers and what the edit distance is to each. You may use any version of distance that you like.\n\n2.6\n\nNow implement a minimum edit distance algorithm and use your hand-computed\nresults to check your code.\n\n2.7\n\nAugment the minimum edit distance algorithm to output an alignment; you\nwill need to store pointers and add a stage to compute the backtrace.\n\n\f34\n\nChapter 2\n\n•\n\nWords and Tokens\n\nAhia, O., S. Kumar, H. Gonen, J. Kasai, D. Mortensen,\nN. Smith, and Y. Tsvetkov. 2023. Do all languages cost\nthe same? tokenization in the era of commercial language\nmodels. EMNLP.\nArkadiev, P. M. 2020. Morphology in typology: Historical\nretrospect, state of the art, and prospects. Oxford.\nBaayen, R. H. 2001. Word frequency distributions. Springer.\nBellman, R. 1957. Dynamic Programming. Princeton University Press.\nBellman, R. 1984. Eye of the Hurricane: an autobiography.\nWorld Scientific Singapore.\nBender, E. M. 2019. The #BenderRule: On naming the languages we study and why it matters. Blog post.\nBender, E. M., B. Friedman, and A. McMillan-Major. 2021.\nA guide for writing data statements for natural language processing. http://techpolicylab.uw.edu/\ndata-statements/.\nBird, S., E. Klein, and E. Loper. 2009. Natural Language\nProcessing with Python. O’Reilly.\nBlodgett, S. L., L. Green, and B. O’Connor. 2016. Demographic dialectal variation in social media: A case study\nof African-American English. EMNLP.\nBostrom, K. and G. Durrett. 2020. Byte pair encoding is\nsuboptimal for language model pretraining. EMNLP.\nChen, X., Z. Shi, X. Qiu, and X. Huang. 2017. Adversarial multi-criteria learning for Chinese word segmentation.\nACL.\nChurch, K. W. 1994. Unix for Poets. Slides from 2nd ELSNET Summer School and unpublished paper ms.\n\nKiss, T. and J. Strunk. 2006. Unsupervised multilingual\nsentence boundary detection. Computational Linguistics,\n32(4):485–525.\nKleene, S. C. 1951. Representation of events in nerve nets\nand finite automata. Technical Report RM-704, RAND\nCorporation. RAND Research Memorandum.\nKleene, S. C. 1956. Representation of events in nerve nets\nand finite automata. In C. Shannon and J. McCarthy, eds,\nAutomata Studies, 3–41. Princeton University Press.\nKruskal, J. B. 1983. An overview of sequence comparison.\nIn D. Sankoff and J. B. Kruskal, eds, Time Warps, String\nEdits, and Macromolecules: The Theory and Practice of\nSequence Comparison, 1–44. Addison-Wesley.\nKudo, T. 2018. Subword regularization: Improving neural\nnetwork translation models with multiple subword candidates. ACL.\nKudo, T. and J. Richardson. 2018a. SentencePiece: A simple\nand language independent subword tokenizer and detokenizer for neural text processing. EMNLP.\nKudo, T. and J. Richardson. 2018b. SentencePiece: A simple\nand language independent subword tokenizer and detokenizer for neural text processing. EMNLP.\nKurebito, M. 2017. Koryak. In M. Fortescue, M. Mithun,\nand N. Evans, eds, Oxford Handbook of Polysynthesis.\nOxford.\nLevenshtein, V. I. 1966. Binary codes capable of correcting deletions, insertions, and reversals. Cybernetics and\nControl Theory, 10(8):707–710. Original in Doklady\nAkademii Nauk SSSR 163(4): 845–848 (1965).\n\nClark, H. H. and J. E. Fox Tree. 2002. Using uh and um in\nspontaneous speaking. Cognition, 84:73–111.\n\nLi, X., Y. Meng, X. Sun, Q. Han, A. Yuan, and J. Li. 2019.\nIs word segmentation necessary for deep learning of Chinese representations? ACL.\n\nEgghe, L. 2007. Untangling Herdan’s law and Heaps’\nlaw: Mathematical and informetric arguments. JASIST,\n58(5):702–709.\n\nLiu, A., J. Hayase, V. Hofmann, S. Oh, N. A. Smith, and\nY. Choi. 2025. SuperBPE: Space travel for language models. ArXiv preprint.\n\nGage, P. 1994. A new algorithm for data compression. The\nC Users Journal, 12(2):23–38.\n\nLovins, J. B. 1968. Development of a stemming algorithm.\nMechanical Translation and Computational Linguistics,\n11(1–2):9–13.\n\nGebru, T., J. Morgenstern, B. Vecchione, J. W. Vaughan,\nH. Wallach, H. Daumé III, and K. Crawford. 2020.\nDatasheets for datasets. ArXiv.\nGreenberg, J. H. 1960. A quantitative approach to the morphological typology of language. International journal of\nAmerican linguistics, 26(3):178–194.\n\nManning, C. D., M. Surdeanu, J. Bauer, J. Finkel, S. Bethard,\nand D. McClosky. 2014. The Stanford CoreNLP natural\nlanguage processing toolkit. ACL.\nNIST. 2005. Speech recognition scoring toolkit (sctk) version 2.1. http://www.nist.gov/speech/tools/.\n\nGusfield, D. 1997. Algorithms on Strings, Trees, and Sequences. Cambridge University Press.\n\nPackard, D. W. 1973. Computer-assisted morphological\nanalysis of ancient Greek. COLING.\n\nHeaps, H. S. 1978. Information retrieval. Computational and\ntheoretical aspects. Academic Press.\nHerdan, G. 1960. Type-token mathematics. Mouton.\n\nRadford, A., J. Wu, R. Child, D. Luan, D. Amodei, and\nI. Sutskever. 2019. Language models are unsupervised\nmultitask learners. OpenAI tech report.\n\nJones, T. 2015. Toward a description of African American\nVernacular English dialect regions using “Black Twitter”.\nAmerican Speech, 90(4):403–440.\n\nRust, P., J. Pfeiffer, I. Vulić, S. Ruder, and I. Gurevych. 2021.\nHow good is your tokenizer? on the monolingual performance of multilingual language models. ACL.\n\nJurgens, D., Y. Tsvetkov, and D. Jurafsky. 2017. Incorporating dialectal variability for socially equitable language\nidentification. ACL.\n\nSennrich, R., B. Haddow, and A. Birch. 2016. Neural machine translation of rare words with subword units. ACL.\n\nKing, S. 2020. From African American Vernacular English\nto African American Language: Rethinking the study of\nrace and language in African Americans’ speech. Annual\nReview of Linguistics, 6:285–300.\n\nSimons, G. F. and C. D. Fennig. 2018. Ethnologue: Languages of the world, 21st edition. SIL International.\n\n\fExercises\nSolorio, T., E. Blair, S. Maharjan, S. Bethard, M. Diab,\nM. Ghoneim, A. Hawwari, F. AlGhamdi, J. Hirschberg,\nA. Chang, and P. Fung. 2014. Overview for the first\nshared task on language identification in code-switched\ndata. Workshop on Computational Approaches to Code\nSwitching.\nThompson, K. 1968. Regular expression search algorithm.\nCACM, 11(6):419–422.\nTria, F., V. Loreto, and V. D. Servedio. 2018. Zipf’s, heaps’\nand taylor’s laws are determined by the expansion into the\nadjacent possible. Entropy, 20(10):752.\nWagner, R. A. and M. J. Fischer. 1974. The string-to-string\ncorrection problem. Journal of the ACM, 21:168–173.\nWeizenbaum, J. 1966. ELIZA – A computer program for the\nstudy of natural language communication between man\nand machine. CACM, 9(1):36–45.\nWeizenbaum, J. 1976. Computer Power and Human Reason:\nFrom Judgement to Calculation. W.H. Freeman & Co.\n\n35\n\n\f",
    "file_path": "/Users/colinsidberry/Downloads/NLP_Textbook/words-and-tokens.txt",
    "file_size_kb": 105.03
  }
]