rights reserved. Draft of August 24, 2025.

CHAPTER

 Context-Free Grammars and
18 Constituency Parsing
 Because the Night by Bruce Springsteen and Patti Smith
 The Fire Next Time by James Baldwin
 If on a winterâ€™s night a traveler by Italo Calvino
 Love Actually by Richard Curtis
 Suddenly Last Summer by Tennessee Williams
 A Scanner Darkly by Philip K. Dick
 Six titles that are not constituents, from Geoffrey K. Pullum on
 Language Log (who was pointing out their incredible rarity).

 One morning I shot an elephant in my pajamas.
 How he got into my pajamas I donâ€™t know.
 Groucho Marx, Animal Crackers, 1930

 The study of grammar has an ancient pedigree. The grammar of Sanskrit was
 described by the Indian grammarian PaÌ„n.ini sometime between the 7th and 4th censyntax turies BCE, in his famous treatise the As.t.aÌ„dhyaÌ„yÄ±Ì„ (â€˜8 booksâ€™). And our word syntax
 comes from the Greek syÌntaxis, meaning â€œsetting out together or arrangementâ€, and
 refers to the way words are arranged together. We have seen syntactic notions in previous chapters like the use of part-of-speech categories (Chapter 17). In this chapter
 and the next one we introduce formal models for capturing more sophisticated notions of grammatical structure and algorithms for parsing these structures.
 Our focus in this chapter is context-free grammars and the CKY algorithm
 for parsing them. Context-free grammars are the backbone of many formal models of the syntax of natural language (and, for that matter, of computer languages).
 Syntactic parsing is the task of assigning a syntactic structure to a sentence. Parse
 trees (whether for context-free grammars or for the dependency or CCG formalisms
 we introduce in following chapters) can be used in applications such as grammar
 checking: sentence that cannot be parsed may have grammatical errors (or at least
 be hard to read). Parse trees can be an intermediate stage of representation for formal semantic analysis. And parsers and the grammatical structure they assign a
 sentence are a useful text analysis tool for text data science applications that require
 modeling the relationship of elements in sentences.
 In this chapter we introduce context-free grammars, give a small sample grammar of English, introduce more formal definitions of context-free grammars and
 grammar normal form, and talk about treebanks: corpora that have been annotated with syntactic structure. We then discuss parse ambiguity and the problems
 it presents, and turn to parsing itself, giving the famous Cocke-Kasami-Younger
 (CKY) algorithm (Kasami 1965, Younger 1967), the standard dynamic programming approach to syntactic parsing. The CKY algorithm returns an efficient representation of the set of parse trees for a sentence, but doesnâ€™t tell us which parse tree
 is the right one. For that, we need to augment CKY with scores for each possible
 constituent. Weâ€™ll see how to do this with neural span-based parsers. Finally, weâ€™ll
 introduce the standard set of metrics for evaluating parser accuracy.
2 C HAPTER 18 â€¢ C ONTEXT-F REE G RAMMARS AND C ONSTITUENCY PARSING

 Syntactic constituency is the idea that groups of words can behave as single units,
 or constituents. Part of developing a grammar involves building an inventory of the
 constituents in the language. How do words group together in English? Consider
 noun phrase the noun phrase, a sequence of words surrounding at least one noun. Here are some
 examples of noun phrases (thanks to Damon Runyon):

 Harry the Horse a high-class spot such as Mindyâ€™s
 the Broadway coppers the reason he comes into the Hot Box
 they three parties from Brooklyn

 What evidence do we have that these words group together (or â€œform constituentsâ€)?
 One piece of evidence is that they can all appear in similar syntactic environments,
 for example, before a verb.

 three parties from Brooklyn arrive. . .
 a high-class spot such as Mindyâ€™s attracts. . .
 the Broadway coppers love. . .
 they sit

 But while the whole noun phrase can occur before a verb, this is not true of each
 of the individual words that make up a noun phrase. The following are not grammatical sentences of English (recall that we use an asterisk (*) to mark fragments that
 are not grammatical English sentences):

 *from arrive. . . *as attracts. . .
 *the is. . . *spot sat. . .

 Thus, to correctly describe facts about the ordering of these words in English, we
 must be able to say things like â€œNoun Phrases can occur before verbsâ€. Letâ€™s now
 see how to do this in a more formal way!

 A widely used formal system for modeling constituent structure in natural lan-
CFG guage is the context-free grammar, or CFG. Context-free grammars are also called
 phrase-structure grammars, and the formalism is equivalent to Backus-Naur form,
 or BNF. The idea of basing a grammar on constituent structure dates back to the psychologist Wilhelm Wundt (1900) but was not formalized until Chomsky (1956) and,
 independently, Backus (1959).
 rules A context-free grammar consists of a set of rules or productions, each of which
 expresses the ways that symbols of the language can be grouped and ordered tolexicon gether, and a lexicon of words and symbols. For example, the following productions
 NP express that an NP (or noun phrase) can be composed of either a ProperNoun or
 a determiner (Det) followed by a Nominal; a Nominal in turn can consist of one or
 18.2 â€¢ C ONTEXT-F REE G RAMMARS 3

 more Nouns.1
 NP â†’ Det Nominal
 NP â†’ ProperNoun
 Nominal â†’ Noun | Nominal Noun
 Context-free rules can be hierarchically embedded, so we can combine the previous
 rules with others, like the following, that express facts about the lexicon:
 Det â†’ a
 Det â†’ the
 Noun â†’ flight
 The symbols that are used in a CFG are divided into two classes. The symbols
 terminal that correspond to words in the language (â€œtheâ€, â€œnightclubâ€) are called terminal
 symbols; the lexicon is the set of rules that introduce these terminal symbols. The
non-terminal symbols that express abstractions over these terminals are called non-terminals. In
 each context-free rule, the item to the right of the arrow (â†’) is an ordered list of one
 or more terminals and non-terminals; to the left of the arrow is a single non-terminal
 symbol expressing some cluster or generalization. The non-terminal associated with
 each word in the lexicon is its lexical category, or part of speech.
 A CFG can be thought of in two ways: as a device for generating sentences
 and as a device for assigning a structure to a given sentence. Viewing a CFG as a
 generator, we can read the â†’ arrow as â€œrewrite the symbol on the left with the string
 of symbols on the rightâ€.
 So starting from the symbol: NP
 we can use our first rule to rewrite NP as: Det Nominal
 and then rewrite Nominal as: Noun
 and finally rewrite these parts-of-speech as: a flight
 We say the string a flight can be derived from the non-terminal NP. Thus, a CFG
 can be used to generate a set of strings. This sequence of rule expansions is called a
 derivation derivation of the string of words. It is common to represent a derivation by a parse
 parse tree tree (commonly shown inverted with the root at the top). Figure 18.1 shows the tree
 representation of this derivation.

 NP

 Det Nom

 a Noun

 flight

 dominates In the parse tree shown in Fig. 18.1, we can say that the node NP dominates
 all the nodes in the tree (Det, Nom, Noun, a, flight). We can say further that it
 immediately dominates the nodes Det and Nom.
 The formal language defined by a CFG is the set of strings that are derivable
start symbol from the designated start symbol. Each grammar must have one designated start
 1 When talking about these rules we can pronounce the rightarrow â†’ as â€œgoes toâ€, and so we might
 read the first rule above as â€œNP goes to Det Nominalâ€.
4 C HAPTER 18 â€¢ C ONTEXT-F REE G RAMMARS AND C ONSTITUENCY PARSING

 symbol, which is often called S. Since context-free grammars are often used to define
 sentences, S is usually interpreted as the â€œsentenceâ€ node, and the set of strings that
 are derivable from S is the set of sentences in some simplified version of English.
 Letâ€™s add a few additional rules to our inventory. The following rule expresses
 verb phrase the fact that a sentence can consist of a noun phrase followed by a verb phrase:

 S â†’ NP VP I prefer a morning flight

 A verb phrase in English consists of a verb followed by assorted other things;
 for example, one kind of verb phrase consists of a verb followed by a noun phrase:

 VP â†’ Verb NP prefer a morning flight

 Or the verb may be followed by a noun phrase and a prepositional phrase:

 VP â†’ Verb NP PP leave Boston in the morning

 Or the verb phrase may have a verb followed by a prepositional phrase alone:

 VP â†’ Verb PP leaving on Thursday

 A prepositional phrase generally has a preposition followed by a noun phrase.
 For example, a common type of prepositional phrase in the ATIS corpus is used to
 indicate location or direction:

 PP â†’ Preposition NP from Los Angeles

 The NP inside a PP need not be a location; PPs are often used with times and
 dates, and with other nouns as well; they can be arbitrarily complex. Here are ten
 examples from the ATIS corpus:
 to Seattle on these flights
 in Minneapolis about the ground transportation in Chicago
 on Wednesday of the round trip flight on United Airlines
 in the evening of the AP fifty seven flight
 on the ninth of July with a stopover in Nashville
 weâ€™ve seen so far, which weâ€™ll call L0 . Note that we can use the or-symbol | to
 indicate that a non-terminal has alternate possible expansions.

 Noun â†’ flights | flight | breeze | trip | morning
 Verb â†’ is | prefer | like | need | want | fly | do
 Adjective â†’ cheapest | non-stop | first | latest
 | other | direct
 Pronoun â†’ me | I | you | it
 Proper-Noun â†’ Alaska | Baltimore | Los Angeles
 | Chicago | United | American
 Determiner â†’ the | a | an | this | these | that
 Preposition â†’ from | to | on | near | in
 Conjunction â†’ and | or | but

 We can use this grammar to generate sentences of this â€œATIS-languageâ€. We
 start with S, expand it to NP VP, then choose a random expansion of NP (letâ€™s say, to
 18.2 â€¢ C ONTEXT-F REE G RAMMARS 5

 Grammar Rules Examples
 S â†’ NP VP I + want a morning flight

 NP â†’ Pronoun I
 | Proper-Noun Los Angeles
 | Det Nominal a + flight
 Nominal â†’ Nominal Noun morning + flight
 | Noun flights

 VP â†’ Verb do
 | Verb NP want + a flight
 | Verb NP PP leave + Boston + in the morning
 | Verb PP leaving + on Thursday

 PP â†’ Preposition NP from + Los Angeles

 S

 NP VP

 Pro Verb NP

 I prefer Det Nom

 a Nom Noun

 Noun flight

 morning

 I), and a random expansion of VP (letâ€™s say, to Verb NP), and so on until we generate
 the string I prefer a morning flight. Figure 18.4 shows a parse tree that represents a
 complete derivation of I prefer a morning flight.
 We can also represent a parse tree in a more compact format called bracketed
 bracketed notation; here is the bracketed representation of the parse tree of Fig. 18.4:
 notation
 (18.1) [S [NP [Pro I]] [VP [V prefer] [NP [Det a] [Nom [N morning] [Nom [N flight]]]]]]
 A CFG like that of L0 defines a formal language. Sentences (strings of words)
 that can be derived by a grammar are in the formal language defined by that gramgrammatical mar, and are called grammatical sentences. Sentences that cannot be derived by
 a given formal grammar are not in the language defined by that grammar and are
ungrammatical referred to as ungrammatical. This hard line between â€œinâ€ and â€œoutâ€ characterizes
 all formal languages but is only a very simplified model of how natural languages
 really work. This is because determining whether a given sentence is part of a given
 natural language (say, English) often depends on the context. In linguistics, the use
 generative
 grammar of formal languages to model natural languages is called generative grammar since
 the language is defined by the set of possible sentences â€œgeneratedâ€ by the grammar.
 (Note that this is a different sense of the word â€˜generateâ€™ than when we talk about
6 C HAPTER 18 â€¢ C ONTEXT-F REE G RAMMARS AND C ONSTITUENCY PARSING

 language models generating text.)

 18.2.1 Formal Definition of Context-Free Grammar
 We conclude this section with a quick, formal description of a context-free grammar and the language it generates. A context-free grammar G is defined by four
 parameters: N, Î£, R, S (technically it is a â€œ4-tupleâ€).

 N a set of non-terminal symbols (or variables)
 Î£ a set of terminal symbols (disjoint from N)
 R a set of rules or productions, each of the form A â†’ Î² ,
 where A is a non-terminal,
 Î² is a string of symbols from the infinite set of strings (Î£ âˆª N)âˆ—
 S a designated start symbol and a member of N

 For the remainder of the book we adhere to the following conventions when discussing the formal properties of context-free grammars (as opposed to explaining
 particular facts about English or other languages).
 Capital letters like A, B, and S Non-terminals
 S The start symbol
 Lower-case Greek letters like Î±, Î² , and Î³ Strings drawn from (Î£ âˆª N)âˆ—
 Lower-case Roman letters like u, v, and w Strings of terminals

 A language is defined through the concept of derivation. One string derives another one if it can be rewritten as the second one by some series of rule applications.
 More formally, following Hopcroft and Ullman (1979),
 if A â†’ Î² is a production of R and Î± and Î³ are any strings in the set
directly derives (Î£ âˆª N)âˆ— , then we say that Î±AÎ³ directly derives Î±Î² Î³, or Î±AÎ³ â‡’ Î±Î² Î³.
 Derivation is then a generalization of direct derivation:
 Let Î±1 , Î±2 , . . . , Î±m be strings in (Î£ âˆª N)âˆ— , m â‰¥ 1, such that

 Î±1 â‡’ Î±2 , Î±2 â‡’ Î±3 , . . . , Î±mâˆ’1 â‡’ Î±m
 âˆ—
 derives We say that Î±1 derives Î±m , or Î±1 â‡’ Î±m .
 We can then formally define the language LG generated by a grammar G as the
 set of strings composed of terminal symbols that can be derived from the designated
 start symbol S.
 âˆ—
 LG = {w|w is in Î£âˆ— and S â‡’ w}
 The problem of mapping from a string of words to its parse tree is called synsyntactic
 parsing tactic parsing, as weâ€™ll see in Section 18.6.

 treebank A corpus in which every sentence is annotated with a parse tree is called a treebank.
 18.3 â€¢ T REEBANKS 7

 Treebanks play an important role in parsing as well as in linguistic investigations of
 syntactic phenomena.
 Treebanks are generally made by running a parser over each sentence and then
 having the resulting parse hand-corrected by human linguists. Figure 18.5 shows
Penn Treebank sentences from the Penn Treebank project, which includes various treebanks in
 English, Arabic, and Chinese. The Penn Treebank part-of-speech tagset was defined
 in Chapter 17, but weâ€™ll see minor formatting differences across treebanks. The use
 of LISP-style parenthesized notation for trees is extremely common and resembles
 the bracketed notation we saw earlier in (18.1). For those who are not familiar with
 it we show a standard node-and-line tree representation in Fig. 18.6.

 ((S
 (NP-SBJ (DT That) ((S
 (JJ cold) (, ,) (NP-SBJ The/DT flight/NN )
 (JJ empty) (NN sky) ) (VP should/MD
 (VP (VBD was) (VP arrive/VB
 (ADJP-PRD (JJ full) (PP-TMP at/IN
 (PP (IN of) (NP eleven/CD a.m/RB ))
 (NP (NN fire) (NP-TMP tomorrow/NN )))))
 (CC and)
 (NN light) ))))
 (. .) ))
 (a) (b)

 S

 NP-SBJ VP .

 DT JJ , JJ NN VBD ADJP-PRD .

 That cold , empty sky was JJ PP

 full IN NP

 of NN CC NN

 fire and light

 The sentences in a treebank implicitly constitute a grammar of the language. For
 example, from the parsed sentences in Fig. 18.5 we can extract the CFG rules shown
 in Fig. 18.7 (with rule suffixes (-SBJ) stripped for simplicity). The grammar used
 to parse the Penn Treebank is very flat, resulting in very many rules. For example,
8 C HAPTER 18 â€¢ C ONTEXT-F REE G RAMMARS AND C ONSTITUENCY PARSING

 Grammar Lexicon
 S â†’ NP VP . DT â†’ the | that
 S â†’ NP VP JJ â†’ cold | empty | full
 NP â†’ CD RB NN â†’ sky | fire | light | flight | tomorrow
 NP â†’ DT NN CC â†’ and
 NP â†’ NN CC NN IN â†’ of | at
 NP â†’ DT JJ , JJ NN CD â†’ eleven
 NP â†’ NN RB â†’ a.m.
 VP â†’ MD VP VB â†’ arrive
 VP â†’ VBD ADJP VBD â†’ was | said
 VP â†’ MD VP MD â†’ should | would
 VP â†’ VB PP NP
 ADJP â†’ JJ PP
 PP â†’ IN NP

 among the approximately 4,500 different rules for expanding VPs are separate rules
 for PP sequences of any length and every possible arrangement of verb arguments:

 VP â†’ VBD PP
 VP â†’ VBD PP PP
 VP â†’ VBD PP PP PP
 VP â†’ VBD PP PP PP PP
 VP â†’ VB ADVP PP
 VP â†’ VB PP ADVP
 VP â†’ ADVP VB PP

 A formal language is defined as a (possibly infinite) set of strings of words. This suggests that we could ask if two grammars are equivalent by asking if they generate the
 same set of strings. In fact, it is possible to have two distinct context-free grammars
 strongly
 equivalent generate the same language. We say that two grammars are strongly equivalent if
 they generate the same set of strings and if they assign the same phrase structure
 to each sentence (allowing merely for renaming of the non-terminal symbols). Two
 weakly
 equivalent grammars are weakly equivalent if they generate the same set of strings but do not
 assign the same phrase structure to each sentence.
 normal form It is sometimes useful to have a normal form for grammars, in which each of
 the productions takes a particular form. For example, a context-free grammar is in
 Chomsky Chomsky normal form (CNF) (Chomsky, 1963) if it is -free and if in addition
 normal form
 each production is either of the form A â†’ B C or A â†’ a. That is, the right-hand side
 of each rule either has two non-terminal symbols or one terminal symbol. Chomsky
 binary
 branching normal form grammars are binary branching, that is they have binary trees (down
 to the prelexical nodes). We make use of this binary branching property in the CKY
 parsing algorithm in Section 18.6.
 Any context-free grammar can be converted into a weakly equivalent Chomsky
 normal form grammar. For example, a rule of the form

 A â†’ B C D

 can be converted into the following two CNF rules (Exercise 18.1 asks the reader to
 18.5 â€¢ A MBIGUITY 9

 Grammar Lexicon
 S â†’ NP VP Det â†’ that | this | the | a
 S â†’ Aux NP VP Noun â†’ book | flight | meal | money
 S â†’ VP Verb â†’ book | include | prefer
 NP â†’ Pronoun Pronoun â†’ I | she | me
 NP â†’ Proper-Noun Proper-Noun â†’ Houston | United
 NP â†’ Det Nominal Aux â†’ does
 Nominal â†’ Noun Preposition â†’ from | to | on | near | through
 Nominal â†’ Nominal Noun
 Nominal â†’ Nominal PP
 VP â†’ Verb
 VP â†’ Verb NP
 VP â†’ Verb NP PP
 VP â†’ Verb PP
 VP â†’ VP PP
 PP â†’ Preposition NP

 formulate the complete algorithm):

 A â†’ B X
 X â†’ C D

 Sometimes using binary branching can actually produce smaller grammars. For
 example, the sentences that might be characterized as
 VP -> VBD NP PP*
 are represented in the Penn Treebank by this series of rules:
 VP â†’ VBD NP PP
 VP â†’ VBD NP PP PP
 VP â†’ VBD NP PP PP PP
 VP â†’ VBD NP PP PP PP PP
 ...
 but could also be generated by the following two-rule grammar:
 VP â†’ VBD NP PP
 VP â†’ VP PP
 The generation of a symbol A with a potentially infinite sequence of symbols B with
 Chomskyadjunction a rule of the form A â†’ A B is known as Chomsky-adjunction.

 Ambiguity is the most serious problem faced by syntactic parsers. Chapter 17 introduced the notions of part-of-speech ambiguity and part-of-speech disambiguastructural
 ambiguity tion. Here, we introduce a new kind of ambiguity, called structural ambiguity,
 illustrated with a new toy grammar L1 , shown in Figure 18.8, which adds a few
 rules to the L0 grammar.
 Structural ambiguity occurs when the grammar can assign more than one parse
 to a sentence. Groucho Marxâ€™s well-known line as Captain Spaulding in Animal
10 C HAPTER 18 â€¢ C ONTEXT-F REE G RAMMARS AND C ONSTITUENCY PARSING

 S S

 NP VP NP VP

 Pronoun Verb NP Pronoun VP PP

 I shot Det Nominal I Verb NP in my pajamas

 an Nominal PP shot Det Nominal

 Noun in my pajamas an Noun

 elephant elephant

reading in which the elephant is in the pajamas, the parse on the right corresponds to the reading in which
Captain Spaulding did the shooting in his pajamas.

 Crackers is ambiguous because the phrase in my pajamas can be part of the NP
 headed by elephant or a part of the verb phrase headed by shot. Figure 18.9 illustrates these two analyses of Marxâ€™s line using rules from L1 .
 Structural ambiguity, appropriately enough, comes in many forms. Two common
 kinds of ambiguity are attachment ambiguity and coordination ambiguity. A
 attachment
 ambiguity sentence has an attachment ambiguity if a particular constituent can be attached to
 the parse tree at more than one place. The Groucho Marx sentence is an example
 PP-attachment
 ambiguity of PP-attachment ambiguity: the preposition phrase can be attached either as part
 of the NP or as part of the VP. Various kinds of adverbial phrases are also subject
 to this kind of ambiguity. For instance, in the following example the gerundive-VP
 flying to Paris can be part of a gerundive sentence whose subject is the Eiffel Tower
 or it can be an adjunct modifying the VP headed by saw:
 (18.2) We saw the Eiffel Tower flying to Paris.
 coordination
 ambiguity In coordination ambiguity phrases can be conjoined by a conjunction like and.
 For example, the phrase old men and women can be bracketed as [old [men and
 women]], referring to old men and old women, or as [old men] and [women], in
 which case it is only the men who are old. These ambiguities combine in complex
 ways in real sentences, like the following news sentence from the Brown corpus:
 (18.3) President Kennedy today pushed aside other White House business to
 devote all his time and attention to working on the Berlin crisis address he
 will deliver tomorrow night to the American people over nationwide
 television and radio.
 This sentence has a number of ambiguities, although since they are semantically
 unreasonable, it requires a careful reading to see them. The last noun phrase could be
 parsed [nationwide [television and radio]] or [[nationwide television] and radio].
 The direct object of pushed aside should be other White House business but could
 also be the bizarre phrase [other White House business to devote all his time and
 attention to working] (i.e., a structure like Kennedy affirmed [his intention to propose
 a new budget to address the deficit]). Then the phrase on the Berlin crisis address he
 18.6 â€¢ CKY PARSING : A DYNAMIC P ROGRAMMING A PPROACH 11

 will deliver tomorrow night to the American people could be an adjunct modifying
 the verb pushed. A PP like over nationwide television and radio could be attached
 to any of the higher VPs or NPs (e.g., it could modify people or night).
 The fact that there are many grammatically correct but semantically unreasonable parses for naturally occurring sentences is an irksome problem that affects all
 parsers. Fortunately, the CKY algorithm below is designed to efficiently handle
 structural ambiguities. And as weâ€™ll see in the following section, we can augment
 CKY with neural methods to choose a single correct parse by syntactic disambiguasyntactic
disambiguation tion.

 Dynamic programming provides a powerful framework for addressing the problems caused by ambiguity in grammars. Recall that a dynamic programming approach systematically fills in a table of solutions to subproblems. The complete
 table has the solution to all the subproblems needed to solve the problem as a whole.
 In the case of syntactic parsing, these subproblems represent parse trees for all the
 constituents detected in the input.
 The dynamic programming advantage arises from the context-free nature of our
 grammar rulesâ€”once a constituent has been discovered in a segment of the input we
 can record its presence and make it available for use in any subsequent derivation
 that might require it. This provides both time and storage efficiencies since subtrees
 can be looked up in a table, not reanalyzed. This section presents the Cocke-Kasami-
Younger (CKY) algorithm, the most widely used dynamic-programming based approach to parsing. Chart parsing (Kaplan 1973, Kay 1982) is a related approach,
 chart parsing and dynamic programming methods are often referred to as chart parsing methods.

 18.6.1 Conversion to Chomsky Normal Form
 The CKY algorithm requires grammars to first be in Chomsky Normal Form (CNF).
 Recall from Section 18.4 that grammars in CNF are restricted to rules of the form
 A â†’ B C or A â†’ w. That is, the right-hand side of each rule must expand either to
 two non-terminals or to a single terminal. Restricting a grammar to CNF does not
 lead to any loss in expressiveness, since any context-free grammar can be converted
 into a corresponding CNF grammar that accepts exactly the same set of strings as
 the original grammar.
 Letâ€™s start with the process of converting a generic CFG into one represented in
 CNF. Assuming weâ€™re dealing with an -free grammar, there are three situations we
 need to address in any generic grammar: rules that mix terminals with non-terminals
 on the right-hand side, rules that have a single non-terminal on the right-hand side,
 and rules in which the length of the right-hand side is greater than 2.
 The remedy for rules that mix terminals and non-terminals is to simply introduce
 a new dummy non-terminal that covers only the original terminal. For example, a
 rule for an infinitive verb phrase such as INF-VP â†’ to VP would be replaced by the
 two rules INF-VP â†’ TO VP and TO â†’ to.
 Unit
 productions Rules with a single non-terminal on the right are called unit productions. We
 can eliminate unit productions by rewriting the right-hand side of the original rules
 with the right-hand side of all the non-unit production rules that they ultimately lead
 âˆ—
 to. More formally, if A â‡’ B by a chain of one or more unit productions and B â†’ Î³
12 C HAPTER 18 â€¢ C ONTEXT-F REE G RAMMARS AND C ONSTITUENCY PARSING

 is a non-unit production in our grammar, then we add A â†’ Î³ for each such rule in
 the grammar and discard all the intervening unit productions. As we demonstrate
 with our toy grammar, this can lead to a substantial flattening of the grammar and a
 consequent promotion of terminals to fairly high levels in the resulting trees.
 Rules with right-hand sides longer than 2 are normalized through the introduction of new non-terminals that spread the longer sequences over several new rules.
 Formally, if we have a rule like

 A â†’ BCÎ³

 we replace the leftmost pair of non-terminals with a new non-terminal and introduce
 a new production, resulting in the following new rules:

 A â†’ X1 Î³
 X1 â†’ B C

 In the case of longer right-hand sides, we simply iterate this process until the offending rule has been replaced by rules of length 2. The choice of replacing the
 leftmost pair of non-terminals is purely arbitrary; any systematic scheme that results
 in binary rules would suffice.
 In our current grammar, the rule S â†’ Aux NP VP would be replaced by the two
 rules S â†’ X1 VP and X1 â†’ Aux NP.
 The entire conversion process can be summarized as follows:
 1. Copy all conforming rules to the new grammar unchanged.
 2. Convert terminals within rules to dummy non-terminals.
 3. Convert unit productions.
 4. Make all rules binary and add them to new grammar.
 the L1 grammar introduced earlier on page 9. Note that this figure doesnâ€™t show
 the original lexical rules; since these original lexical rules are already in CNF, they
 all carry over unchanged to the new grammar. Figure 18.10 does, however, show
 the various places where the process of eliminating unit productions has, in effect,
 created new lexical rules. For example, all the original verbs have been promoted to
 both VPs and to Ss in the converted grammar.

 18.6.2 CKY Recognition
 With our grammar now in CNF, each non-terminal node above the part-of-speech
 level in a parse tree will have exactly two daughters. A two-dimensional matrix can
 be used to encode the structure of an entire tree. For a sentence of length n, we will
 work with the upper-triangular portion of an (n + 1) Ã— (n + 1) matrix. Each cell [i, j]
 in this matrix contains the set of non-terminals that represent all the constituents that
 span positions i through j of the input. Since our indexing scheme begins with 0, itâ€™s
 natural to think of the indexes as pointing at the gaps between the input words (as in
 fenceposts 0 Book 1 that 2 flight 3 ). These gaps are often called fenceposts, on the metaphor of
 the posts between segments of fencing. It follows then that the cell that represents
 the entire input resides in position [0, n] in the matrix.
 Since each non-terminal entry in our table has two daughters in the parse, it follows that for each constituent represented by an entry [i, j], there must be a position
 in the input, k, where it can be split into two parts such that i < k < j. Given such
 18.6 â€¢ CKY PARSING : A DYNAMIC P ROGRAMMING A PPROACH 13

 L1 Grammar L1 in CNF
S â†’ NP VP S â†’ NP VP
S â†’ Aux NP VP S â†’ X1 VP
 X1 â†’ Aux NP
S â†’ VP S â†’ book | include | prefer
 S â†’ Verb NP
 S â†’ X2 PP
 S â†’ Verb PP
 S â†’ VP PP
NP â†’ Pronoun NP â†’ I | she | me
NP â†’ Proper-Noun NP â†’ United | Houston
NP â†’ Det Nominal NP â†’ Det Nominal
Nominal â†’ Noun Nominal â†’ book | flight | meal | money
Nominal â†’ Nominal Noun Nominal â†’ Nominal Noun
Nominal â†’ Nominal PP Nominal â†’ Nominal PP
VP â†’ Verb VP â†’ book | include | prefer
VP â†’ Verb NP VP â†’ Verb NP
VP â†’ Verb NP PP VP â†’ X2 PP
 X2 â†’ Verb NP
VP â†’ Verb PP VP â†’ Verb PP
VP â†’ VP PP VP â†’ VP PP
PP â†’ Preposition NP PP â†’ Preposition NP
here, all the original lexical entries from L1 carry over unchanged as well.

a position k, the first constituent [i, k] must lie to the left of entry [i, j] somewhere
along row i, and the second entry [k, j] must lie beneath it, along column j.
 To make this more concrete, consider the following example with its completed
parse matrix, shown in Fig. 18.11.
(18.4) Book the flight through Houston.
The superdiagonal row in the matrix contains the parts of speech for each word in
the input. The subsequent diagonals above that superdiagonal contain constituents
that cover all the spans of increasing length in the input.
 Given this setup, CKY recognition consists of filling the parse table in the right
way. To do this, weâ€™ll proceed in a bottom-up fashion so that at the point where we
are filling any cell [i, j], the cells containing the parts that could contribute to this
entry (i.e., the cells to the left and the cells below) have already been filled. The
algorithm given in Fig. 18.12 fills the upper-triangular matrix a column at a time
working from left to right, with each column filled from bottom to top, as the right
side of Fig. 18.11 illustrates. This scheme guarantees that at each point in time we
have all the information we need (to the left, since all the columns to the left have
already been filled, and below since weâ€™re filling bottom to top). It also mirrors online processing, since filling the columns from left to right corresponds to processing
each word one at a time.
 The outermost loop of the algorithm given in Fig. 18.12 iterates over the columns,
and the second loop iterates over the rows, from the bottom up. The purpose of the
innermost loop is to range over all the places where a substring spanning i to j in
the input might be split in two. As k ranges over the places where the string can be
split, the pairs of cells we consider move, in lockstep, to the right along row i and
down along column j. Figure 18.13 illustrates the general case of filling cell [i, j].
14 C HAPTER 18 â€¢ C ONTEXT-F REE G RAMMARS AND C ONSTITUENCY PARSING

 Book the flight through Houston

 S, VP, Verb, S,VP,X2 S,VP,X2
 Nominal,
 Noun
 [0,1] [0,2] [0,3] [0,4] [0,5]
 Det NP NP

 [1,2] [1,3] [1,4] [1,5]
 Nominal, Nominal
 Noun

 [2,3] [2,4] [2,5]
 Prep PP

 [3,4] [3,5]
 NP,
 Proper-
Noun
 [4,5]

 function CKY-PARSE(words, grammar) returns table

 for j â† from 1 to L ENGTH(words) do
 for all {A | A â†’ words[ j] âˆˆ grammar}
 table[ j âˆ’ 1, j] â† table[ j âˆ’ 1, j] âˆª A
 for i â† from j âˆ’ 2 down to 0 do
 for k â† i + 1 to j âˆ’ 1 do
 for all {A | A â†’ BC âˆˆ grammar and B âˆˆ table[i, k] and C âˆˆ table[k, j]}
 table[i,j] â† table[i,j] âˆª A

 At each such split, the algorithm considers whether the contents of the two cells can
 be combined in a way that is sanctioned by a rule in the grammar. If such a rule
 exists, the non-terminal on its left-hand side is entered into the table.
 word Houston is read. The arrows point out the two spans that are being used to add
 an entry to the table. Note that the action in cell [0, 5] indicates the presence of three
 alternative parses for this input, one where the PP modifies the flight, one where
 it modifies the booking, and one that captures the second argument in the original
 VP â†’ Verb NP PP rule, now captured indirectly with the VP â†’ X2 PP rule.

 18.6.3 CKY Parsing
 The algorithm given in Fig. 18.12 is a recognizer, not a parser. That is, it can tell
 us whether a valid parse exists for a given sentence based on whether or not if finds
 an S in cell [0, n], but it canâ€™t provide the derivation, which is the actual job for a
 parser. To turn it into a parser capable of returning all possible parses for a given
 input, we can make two simple changes to the algorithm: the first change is to
 augment the entries in the table so that each non-terminal is paired with pointers to
 the table entries from which it was derived (more or less as shown in Fig. 18.14), the
 second change is to permit multiple versions of the same non-terminal to be entered
 into the table (again as shown in Fig. 18.14). With these changes, the completed
 table contains all the possible parses for a given input. Returning an arbitrary single
 18.6 â€¢ CKY PARSING : A DYNAMIC P ROGRAMMING A PPROACH 15

 [0,1] [0,n]

 ...
 [i,j]

 [i,i+1] [i,i+2]
 ... [i,j-2] [i,j-1]

 [i+1,j]

 [i+2,j]

 [j-2,j]

 [j-1,j]

 ...

 [n-1, n]

parse consists of choosing an S from cell [0, n] and then recursively retrieving its
component constituents from the table. Of course, instead of returning every parse
for a sentence, we usually want just the best parse; weâ€™ll see how to do that in the
next section.

18.6.4 CKY in Practice
Finally, we should note that while the restriction to CNF does not pose a problem
theoretically, it does pose some non-trivial problems in practice. The returned CNF
trees may not be consistent with the original grammar built by the grammar developers, and will complicate any syntax-driven approach to semantic analysis.
 One approach to getting around these problems is to keep enough information
around to transform our trees back to the original grammar as a post-processing step
of the parse. This is trivial in the case of the transformation used for rules with length
greater than 2. Simply deleting the new dummy non-terminals and promoting their
daughters restores the original tree.
 In the case of unit productions, it turns out to be more convenient to alter the basic CKY algorithm to handle them directly than it is to store the information needed
to recover the correct trees. Exercise 18.3 asks you to make this change. Many of
the probabilistic parsers presented in Appendix C use the CKY algorithm altered in
16 C HAPTER 18 â€¢ C ONTEXT-F REE G RAMMARS AND C ONSTITUENCY PARSING

 Book the flight through Houston Book the flight through Houston

 S, VP, Verb, S,VP,X2 S, VP, Verb, S,VP,X2
 Nominal, Nominal,
 Noun Noun
 [0,1] [0,2] [0,3] [0,4] [0,5] [0,1] [0,2] [0,3] [0,4] [0,5]
 Det NP Det NP NP

 [1,2] [1,3] [1,4] [1,5] [1,2] [1,3] [1,4] [1,5]
 Nominal, Nominal Nominal,
 Noun Noun

 [2,3] [2,4] [2,5] [2,3] [2,4] [2,5]
 Prep Prep PP

 [3,4] [3,5] [3,4] [3,5]
 NP, NP,
 Proper- Proper-
Noun Noun
 [4,5] [4,5]

 Book the flight through Houston Book the flight through Houston

 S, VP, Verb, S,VP,X2 S, VP, Verb, S,VP,X2
 Nominal, Nominal,
 Noun Noun
 [0,1] [0,2] [0,3] [0,4] [0,5] [0,1] [0,2] [0,3] [0,4] [0,5]
 Det NP NP Det NP NP

 [1,2] [1,3] [1,4] [1,5] [1,2] [1,3] [1,4] [1,5]
 Nominal, Nominal Nominal, Nominal
 Noun Noun

 [2,3] [2,4] [2,5] [2,3] [2,4] [2,5]
 Prep PP Prep PP

 [3,4] [3,5] [3,4] [3,5]
 NP, NP,
 Proper- Proper-
Noun Noun
 [4,5] [4,5]

 Book the flight through Houston

 S, VP, Verb, S1,VP, X2
 Nominal, S,
 Noun VP, S2, VP
 X2 S3
 [0,1] [0,2] [0,3] [0,4]
 Det NP NP

 [1,2] [1,3] [1,4] [1,5]
 Nominal, Nominal
 Noun

 [2,3] [2,4] [2,5]
 Prep PP

 [3,4] [3,5]
 NP,
 Proper-
Noun
 [4,5]

 18.7 â€¢ S PAN -BASED N EURAL C ONSTITUENCY PARSING 17

 just this manner.

 While the CKY parsing algorithm weâ€™ve seen so far does great at enumerating all
 the possible parse trees for a sentence, it has a large problem: it doesnâ€™t tell us which
 parse is the correct one! That is, it doesnâ€™t disambiguate among the possible parses.
 To solve the disambiguation problem weâ€™ll use a simple neural extension of the
 CKY algorithm. The intuition of such parsing algorithms (often called span-based
 constituency parsing, or neural CKY), is to train a neural classifier to assign a
 score to each constituent, and then use a modified version of CKY to combine these
 constituent scores to find the best-scoring parse tree.
 Here weâ€™ll describe a version of the algorithm from Kitaev et al. (2019). This
 parser learns to map a span of words to a constituent, and, like CKY, hierarchically
 combines larger and larger spans to build the parse-tree bottom-up. But unlike classic CKY, this parser doesnâ€™t use the hand-written grammar to constrain what constituents can be combined, instead just relying on the learned neural representations
 of spans to encode likely combinations.

 18.7.1 Computing Scores for a Span
 span Letâ€™s begin by considering just the constituent (weâ€™ll call it a span) that lies between
 fencepost positions i and j with non-terminal symbol label l. Weâ€™ll build a system
 to assign a score s(i, j, l) to this constituent span.

 CKY for computing best parse NP

 Compute score for span MLP

 Represent span hj-hi
 i=1 j=3

 0 1 2 3 4 5

 postprocessing layers
 map back to words

 ENCODER
 map to subwords

 [START] Book the flight through Houston [END]
 the label NP.

 Fig. 18.15 sketches the architecture. The input word tokens are embedded by
18 C HAPTER 18 â€¢ C ONTEXT-F REE G RAMMARS AND C ONSTITUENCY PARSING

 passing them through a pretrained language model like BERT. Because BERT operates on the level of subword (wordpiece) tokens rather than words, weâ€™ll first need to
 convert the BERT outputs to word representations. One standard way of doing this
 is to simply use the first subword unit as the representation for the entire word; using the last subword unit, or the sum of all the subword units are also common. The
 embeddings can then be passed through some postprocessing layers; Kitaev et al.
 (2019), for example, use 8 Transformer layers.
 The resulting word encoder outputs yt are then used to compute a span score.
 First, we must map the word encodings (indexed by word positions) to span encodings (indexed by fenceposts). We do this by representing each fencepost with two
 separate values; the intuition is that a span endpoint to the right of a word represents
 different information than a span endpoint to the left of a word. We convert each
 word output yt into a (leftward-pointing) value for spans ending at this fencepost,
 â†âˆ’
 y t , and a (rightward-pointing) value â†’ âˆ’y t for spans beginning at this fencepost, by
 splitting yt into two halves. Each span then stretches from one double-vector fencepost to another, as in the following representation of the flight, which is span(1, 3):

 START 0 Book the flight through
 y0 â†’
 âˆ’
 y0 â†
 yâˆ’1 y â†’ âˆ’
 y â†
 yâˆ’ y2 â†’
 âˆ’
 y2 â†
 yâˆ’3 y â†’ âˆ’y â†
 yâˆ’ y â†’ âˆ’y â†
 yâˆ’ ...
 1 1 2 3 3 4 4 4 5
 0 1 2 3 4

 span(1,3)

 A traditional way to represent a span, developed originally for RNN-based models
 (Wang and Chang, 2016), but extended also to Transformers, is to take the difference between the embeddings of its start and end, i.e., representing span (i, j) by
 subtracting the embedding of i from the embedding of j. Here we represent a span
 by concatenating the difference of each of its fencepost components:

 v(i, j) = [â†’
 âˆ’
 yj âˆ’â†’
 âˆ’
 yi ; â† âˆ’âˆ’ âˆ’ â†
 y j+1 âˆ’âˆ’]
 yi+1 (18.5)

 The span vector v is then passed through an MLP span classifier, with two fullyconnected layers and one ReLU activation function, whose output dimensionality is
 the number of possible non-terminal labels:

 s(i, j, Â·) = W2 ReLU(LayerNorm(W1 v(i, j))) (18.6)

 The MLP then outputs a score for each possible non-terminal.

 18.7.2 Integrating Span Scores into a Parse
 Now we have a score for each labeled constituent span s(i, j, l). But we need a score
 for an entire parse tree. Formally a tree T is represented as a set of |T | such labeled
 spans, with the t th span starting at position it and ending at position jt , with label lt :

 T = {(it , jt , lt ) : t = 1, . . . , |T |} (18.7)

 Thus once we have a score for each span, the parser can compute a score for the
 whole tree s(T ) simply by summing over the scores of its constituent spans:
 X
 s(T ) = s(i, j, l) (18.8)
 (i, j,l)âˆˆT
 18.8 â€¢ E VALUATING PARSERS 19

 And we can choose the final parse tree as the tree with the maximum score:

 TÌ‚ = argmax s(T ) (18.9)
 T

 The simplest method to produce the most likely parse is to greedily choose the
 highest scoring label for each span. This greedy method is not guaranteed to produce
 a tree, since the best label for a span might not fit into a complete tree. In practice,
 however, the greedy method tends to find trees; in their experiments Gaddy et al.
 (2018) finds that 95% of predicted bracketings form valid trees.
 Nonetheless it is more common to use a variant of the CKY algorithm to find the
 full parse. The variant defined in Gaddy et al. (2018) works as follows. Letâ€™s define
 sbest (i, j) as the score of the best subtree spanning (i, j). For spans of length one, we
 choose the best label:

 sbest (i, i + 1) = max s(i, i + 1, l) (18.10)
 l

 For other spans (i, j), the recursion is:

 sbest (i, j) = max s(i, j, l)
 l
 + max[sbest (i, k) + sbest (k, j)] (18.11)
 k

 Note that the parser is using the max label for span (i, j) + the max labels for spans
 (i, k) and (k, j) without worrying about whether those decisions make sense given a
 grammar. The role of the grammar in classical parsing is to help constrain possible
 combinations of constituents (NPs like to be followed by VPs). By contrast, the
 neural model seems to learn these kinds of contextual constraints during its mapping
 from spans to non-terminals.
 For more details on span-based parsing, including the margin-based training algorithm, see Stern et al. (2017), Gaddy et al. (2018), Kitaev and Klein (2018), and
 Kitaev et al. (2019).

 The standard tool for evaluating parsers that assign a single parse tree to a sentence
 PARSEVAL is the PARSEVAL metrics (Black et al., 1991). The PARSEVAL metric measures
 how much the constituents in the hypothesis parse tree look like the constituents in a
 hand-labeled, reference parse. PARSEVAL thus requires a human-labeled reference
 (or â€œgold standardâ€) parse tree for each sentence in the test set; we generally draw
 these reference parses from a treebank like the Penn Treebank.
 A constituent in a hypothesis parse Ch of a sentence s is labeled correct if there
 is a constituent in the reference parse Cr with the same starting point, ending point,
 and non-terminal symbol. We can then measure the precision and recall just as for
 tasks weâ€™ve seen already like named entity tagging:

 # of correct constituents in hypothesis parse of s
 labeled recall: = # of total constituents in reference parse of s

 # of correct constituents in hypothesis parse of s
 labeled precision: = # of total constituents in hypothesis parse of s
20 C HAPTER 18 â€¢ C ONTEXT-F REE G RAMMARS AND C ONSTITUENCY PARSING

 S(dumped)

 NP(workers) VP(dumped)

 NNS(workers) VBD(dumped) NP(sacks) PP(into)

 workers dumped NNS(sacks) P NP(bin)

 sacks into DT(a) NN(bin)

 a bin

 As usual, we often report a combination of the two, F1 :
 2PR
 F1 = (18.12)
 P+R
 We additionally use a new metric, crossing brackets, for each sentence s:

 cross-brackets: the number of constituents for which the reference parse has a
 bracketing such as ((A B) C) but the hypothesis parse has a bracketing such
 as (A (B C)).
 For comparing parsers that use different grammars, the PARSEVAL metric includes a canonicalization algorithm for removing information likely to be grammarspecific (auxiliaries, pre-infinitival â€œtoâ€, etc.) and for computing a simplified score
 (Black et al., 1991). The canonical implementation of the PARSEVAL metrics is
 evalb called evalb (Sekine and Collins, 1997).

 Syntactic constituents can be associated with a lexical head; N is the head of an NP,
 V is the head of a VP. This idea of a head for each constituent dates back to Bloomfield 1914, and is central to the dependency grammars and dependency parsing weâ€™ll
 introduce in Chapter 19. Indeed, heads can be used as a way to map between constituency and dependency parses. Heads are also important in probabilistic parsing (Appendix C) and in constituent-based grammar formalisms like Head-Driven
 Phrase Structure Grammar (Pollard and Sag, 1994)..
 In one simple model of lexical heads, each context-free rule is associated with
 a head (Charniak 1997, Collins 1999). The head is the word in the phrase that is
 grammatically the most important. Heads are passed up the parse tree; thus, each
 non-terminal in a parse tree is annotated with a single word, which is its lexical head.
 non-terminal is annotated with its head.
 For the generation of such a tree, each CFG rule must be augmented to identify
 one right-side constituent to be the head child. The headword for a node is then set to
 the headword of its head child. Choosing these head children is simple for textbook
 examples (NN is the head of NP) but is complicated and indeed controversial for
 18.10 â€¢ S UMMARY 21

 most phrases. (Should the complementizer to or the verb be the head of an infinite
 verb phrase?) Modern linguistic theories of syntax generally include a component
 that defines heads (see, e.g., (Pollard and Sag, 1994)).
 An alternative approach to finding a head is used in most practical computational
 systems. Instead of specifying head rules in the grammar itself, heads are identified
 dynamically in the context of trees for specific sentences. In other words, once
 a sentence is parsed, the resulting tree is walked to decorate each node with the
 appropriate head. Most current systems rely on a simple set of handwritten rules,
 such as a practical one for Penn Treebank grammars given in Collins (1999) but
 developed originally by Magerman (1995). For example, the rule for finding the
 head of an NP is as follows (Collins, 1999, p. 238):

 â€¢ If the last word is tagged POS, return last-word.
 â€¢ Else search from right to left for the first child which is an NN, NNP, NNPS, NX, POS,
 or JJR.
 â€¢ Else search from left to right for the first child which is an NP.
 â€¢ Else search from right to left for the first child which is a $, ADJP, or PRN.
 â€¢ Else search from right to left for the first child which is a CD.
 â€¢ Else search from right to left for the first child which is a JJ, JJS, RB or QP.
 â€¢ Else return the last word

 Selected other rules from this set are shown in Fig. 18.17. For example, for VP
 rules of the form VP â†’ Y1 Â· Â· Â· Yn , the algorithm would start from the left of Y1 Â· Â· Â·
 Yn looking for the first Yi of type TO; if no TOs are found, it would search for the
 first Yi of type VBD; if no VBDs are found, it would search for a VBN, and so on.
 See Collins (1999) for more details.

Parent Direction Priority List
ADJP Left NNS QP NN $ ADVP JJ VBN VBG ADJP JJR NP JJS DT FW RBR RBS
 SBAR RB
ADVP Right RB RBR RBS FW ADVP TO CD JJR JJ IN NP JJS NN
PRN Left
PRT Right RP
QP Left $ IN NNS NN JJ RB DT CD NCD QP JJR JJS
S Left TO IN VP S SBAR ADJP UCP NP
SBAR Left WHNP WHPP WHADVP WHADJP IN DT S SQ SINV SBAR FRAG
VP Left TO VBD VBN MD VBZ VB VBG VBP VP ADJP NN NNS NP

 This chapter introduced constituency parsing. Hereâ€™s a summary of the main points:
 â€¢ In many languages, groups of consecutive words act as a group or a constituent, which can be modeled by context-free grammars (which are also
 known as phrase-structure grammars).
 â€¢ A context-free grammar consists of a set of rules or productions, expressed
 over a set of non-terminal symbols and a set of terminal symbols. Formally,
 a particular context-free language is the set of strings that can be derived
 from a particular context-free grammar.
22 C HAPTER 18 â€¢ C ONTEXT-F REE G RAMMARS AND C ONSTITUENCY PARSING

 â€¢ Structural ambiguity is a significant problem for parsers. Common sources
 of structural ambiguity include PP-attachment and coordination ambiguity.
 â€¢ Dynamic programming parsing algorithms, such as CKY, use a table of
 partial parses to efficiently parse ambiguous sentences.
 â€¢ CKY restricts the form of the grammar to Chomsky normal form (CNF).
 â€¢ The basic CKY algorithm compactly represents all possible parses of the sentence but doesnâ€™t choose a single best parse.
 â€¢ Choosing a single parse from all possible parses (disambiguation) can be
 done by neural constituency parsers.
 â€¢ Span-based neural constituency parses train a neural classifier to assign a score
 to each constituent, and then use a modified version of CKY to combine these
 constituent scores to find the best-scoring parse tree.
 â€¢ Parsers are evaluated with three metrics: labeled recall, labeled precision,
 and cross-brackets.
 â€¢ Partial parsing and chunking are methods for identifying shallow syntactic constituents in a text. They are solved by sequence models trained on
 syntactically-annotated data.

Historical Notes
 According to Percival (1976), the idea of breaking up a sentence into a hierarchy of
 constituents appeared in the VoÌˆlkerpsychologie of the groundbreaking psychologist
 Wilhelm Wundt (Wundt, 1900):
 ...den sprachlichen Ausdruck fuÌˆr die willkuÌˆrliche Gliederung einer Gesammtvorstellung in ihre in logische Beziehung zueinander gesetzten
 Bestandteile
 [the linguistic expression for the arbitrary division of a total idea
 into its constituent parts placed in logical relations to one another]
 Wundtâ€™s idea of constituency was taken up into linguistics by Leonard Bloomfield in his early book An Introduction to the Study of Language (Bloomfield, 1914).
 By the time of his later book, Language (Bloomfield, 1933), what was then called
 â€œimmediate-constituent analysisâ€ was a well-established method of syntactic study
 in the United States. By contrast, traditional European grammar, dating from the
 Classical period, defined relations between words rather than constituents, and European syntacticians retained this emphasis on such dependency grammars, the subject of Chapter 19. (And indeed, both dependency and constituency grammars have
 been in vogue in computational linguistics at different times).
 American Structuralism saw a number of specific definitions of the immediate
 constituent, couched in terms of their search for a â€œdiscovery procedureâ€: a methodological algorithm for describing the syntax of a language. In general, these attempt
 to capture the intuition that â€œThe primary criterion of the immediate constituent
 is the degree in which combinations behave as simple unitsâ€ (Bazell, 1952/1966, p.
 284). The most well known of the specific definitions is Harrisâ€™ idea of distributional
 similarity to individual units, with the substitutability test. Essentially, the method
 proceeded by breaking up a construction into constituents by attempting to substitute
 simple structures for possible constituentsâ€”if a substitution of a simple form, say,
 H ISTORICAL N OTES 23

 man, was substitutable in a construction for a more complex set (like intense young
 man), then the form intense young man was probably a constituent. Harrisâ€™s test was
 the beginning of the intuition that a constituent is a kind of equivalence class.
 The context-free grammar was a formalization of this idea of hierarchical
 constituency defined in Chomsky (1956) and further expanded upon (and argued
 against) in Chomsky (1957) and Chomsky (1956/1975). Shortly after Chomskyâ€™s
 initial work, the context-free grammar was reinvented by Backus (1959) and independently by Naur et al. (1960) in their descriptions of the ALGOL programming
 language; Backus (1996) noted that he was influenced by the productions of Emil
 Post and that Naurâ€™s work was independent of his (Backusâ€™) own. After this early
 work, a great number of computational models of natural language processing were
 based on context-free grammars because of the early development of efficient parsing algorithms.
 Dynamic programming parsing has a history of independent discovery. According to the late Martin Kay (personal communication), a dynamic programming
 parser containing the roots of the CKY algorithm was first implemented by John
 Cocke in 1960. Later work extended and formalized the algorithm, as well as proving its time complexity (Kay 1967, Younger 1967, Kasami 1965). The related well-
WFST formed substring table (WFST) seems to have been independently proposed by
 Kuno (1965) as a data structure that stores the results of all previous computations
 in the course of the parse. Based on a generalization of Cockeâ€™s work, a similar
 data structure had been independently described in Kay (1967) (and Kay 1973). The
 top-down application of dynamic programming to parsing was described in Earleyâ€™s
 Ph.D. dissertation (Earley 1968, Earley 1970). Sheil (1976) showed the equivalence
 of the WFST and the Earley algorithm. Norvig (1991) shows that the efficiency offered by dynamic programming can be captured in any language with a memoization
 function (such as in LISP) simply by wrapping the memoization operation around a
 simple top-down parser.
probabilistic
 The earliest disambiguation algorithms for parsing were based on probabilistic
 context-free context-free grammars, first worked out by Booth (1969) and Salomaa (1969); see
 grammars
 Appendix C for more history. Neural methods were first applied to parsing at around
 the same time as statistical parsing methods were developed (Henderson, 1994). In
 the earliest work neural networks were used to estimate some of the probabilities for
 statistical constituency parsers (Henderson, 2003, 2004; Emami and Jelinek, 2005)
 . The next decades saw a wide variety of neural parsing algorithms, including recursive neural architectures (Socher et al., 2011, 2013), encoder-decoder models
 (Vinyals et al., 2015; Choe and Charniak, 2016), and the idea of focusing on spans
 (Cross and Huang, 2016). For more on the span-based self-attention approach we
 describe in this chapter see Stern et al. (2017), Gaddy et al. (2018), Kitaev and Klein
 (2018), and Kitaev et al. (2019). See Chapter 19 for the parallel history of neural
 dependency parsing.
 The classic reference for parsing algorithms is Aho and Ullman (1972); although
 the focus of that book is on computer languages, most of the algorithms have been
 applied to natural language.
24 C HAPTER 18 â€¢ C ONTEXT-F REE G RAMMARS AND C ONSTITUENCY PARSING

Exercises
 Apply your program to the L1 grammar.
 grammars that contain unit productions.
 example, containing spelling errors or mistakes arising from automatic speech
 recognition.
 parser and a treebank, compare your metrics against a standard implementation. Analyze the errors in your approach.
 Exercises 25

Aho, A. V. and J. D. Ullman. 1972. The Theory of Parsing, Harris, Z. S. 1946. From morpheme to utterance. Language,
 Translation, and Compiling, volume 1. Prentice Hall. 22(3):161â€“183.
Backus, J. W. 1959. The syntax and semantics of the Henderson, J. 1994. Description Based Parsing in a Connecproposed international algebraic language of the Zurich tionist Network. Ph.D. thesis, University of Pennsylvania,
 ACM-GAMM Conference. Information Processing: Pro- Philadelphia, PA.
 ceedings of the International Conference on Information Henderson, J. 2003. Inducing history representations for
 Processing, Paris. UNESCO. broad coverage statistical parsing. HLT-NAACL-03.
Backus, J. W. 1996. Transcript of question and answer ses- Henderson, J. 2004. Discriminative training of a neural netsion. In R. L. Wexelblat, ed., History of Programming work statistical parser. ACL.
 Languages, page 162. Academic Press. Hopcroft, J. E. and J. D. Ullman. 1979. Introduction to Au-
Bazell, C. E. 1952/1966. The correspondence fallacy in tomata Theory, Languages, and Computation. Addisonstructural linguistics. In E. P. Hamp, F. W. Householder, Wesley.
 and R. Austerlitz, eds, Studies by Members of the En- Kaplan, R. M. 1973. A general syntactic processor. In
 glish Department, Istanbul University (3), reprinted in R. Rustin, ed., Natural Language Processing, 193â€“241.
 Readings in Linguistics II (1966), 271â€“298. University of Algorithmics Press.
 Chicago Press.
 Kasami, T. 1965. An efficient recognition and syntax anal-
Black, E., S. P. Abney, D. Flickinger, C. Gdaniec, R. Gr- ysis algorithm for context-free languages. Technical
 ishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek, Report AFCRL-65-758, Air Force Cambridge Research
 J. L. Klavans, M. Y. Liberman, M. P. Marcus, S. Roukos, Laboratory, Bedford, MA.
 B. Santorini, and T. Strzalkowski. 1991. A procedure for
 Kay, M. 1967. Experiments with a powerful parser. COLquantitatively comparing the syntactic coverage of En-
ING.
 glish grammars. Speech and Natural Language Workshop. Kay, M. 1973. The MIND system. In R. Rustin, ed., Natural
 Language Processing, 155â€“188. Algorithmics Press.
Bloomfield, L. 1914. An Introduction to the Study of Language. Henry Holt and Company. Kay, M. 1982. Algorithm schemata and data structures in
 syntactic processing. In S. AlleÌn, ed., Text Processing:
Bloomfield, L. 1933. Language. University of Chicago Text Analysis and Generation, Text Typology and Attribu-
Press. tion, 327â€“358. Almqvist and Wiksell, Stockholm.
Booth, T. L. 1969. Probabilistic representation of formal Kitaev, N., S. Cao, and D. Klein. 2019. Multilingual
 languages. IEEE Conference Record of the 1969 Tenth constituency parsing with self-attention and pre-training.
 Annual Symposium on Switching and Automata Theory. ACL.
Charniak, E. 1997. Statistical parsing with a context-free Kitaev, N. and D. Klein. 2018. Constituency parsing with a
 grammar and word statistics. AAAI. self-attentive encoder. ACL.
Choe, D. K. and E. Charniak. 2016. Parsing as language Kuno, S. 1965. The predictive analyzer and a path eliminamodeling. EMNLP. tion technique. CACM, 8(7):453â€“462.
Chomsky, N. 1956. Three models for the description of Magerman, D. M. 1995. Statistical decision-tree models for
 language. IRE Transactions on Information Theory, parsing. ACL.
 2(3):113â€“124. Naur, P., J. W. Backus, F. L. Bauer, J. Green, C. Katz,
Chomsky, N. 1956/1975. The Logical Structure of Linguistic J. McCarthy, A. J. Perlis, H. Rutishauser, K. Samelson,
 Theory. Plenum. B. Vauquois, J. H. Wegstein, A. van Wijnagaarden, and
Chomsky, N. 1957. Syntactic Structures. Mouton. M. Woodger. 1960. Report on the algorithmic language
 ALGOL 60. CACM, 3(5):299â€“314. Revised in CACM
Chomsky, N. 1963. Formal properties of grammars. In R. D. 6:1, 1-17, 1963.
 Luce, R. Bush, and E. Galanter, eds, Handbook of Mathematical Psychology, volume 2, 323â€“418. Wiley. Norvig, P. 1991. Techniques for automatic memoization with
 applications to context-free parsing. Computational Lin-
Collins, M. 1999. Head-Driven Statistical Models for Natu- guistics, 17(1):91â€“98.
 ral Language Parsing. Ph.D. thesis, University of Penn-
Percival, W. K. 1976. On the historical source of immedisylvania, Philadelphia.
 ate constituent analysis. In J. D. McCawley, ed., Syntax
Cross, J. and L. Huang. 2016. Span-based constituency pars- and Semantics Volume 7, Notes from the Linguistic Uning with a structure-label system and provably optimal derground, 229â€“242. Academic Press.
 dynamic oracles. EMNLP.
 Pollard, C. and I. A. Sag. 1994. Head-Driven Phrase Struc-
Earley, J. 1968. An Efficient Context-Free Parsing Algorithm. ture Grammar. University of Chicago Press.
 Ph.D. thesis, Carnegie Mellon University, Pittsburgh, PA. Salomaa, A. 1969. Probabilistic and weighted grammars.
Earley, J. 1970. An efficient context-free parsing algorithm. Information and Control, 15:529â€“544.
 CACM, 6(8):451â€“455. Sekine, S. and M. Collins. 1997. The evalb software. http:
Emami, A. and F. Jelinek. 2005. A neural syntactic language //cs.nyu.edu/cs/projects/proteus/evalb.
 model. Machine learning, 60(1):195â€“227. Sheil, B. A. 1976. Observations on context free parsing.
Gaddy, D., M. Stern, and D. Klein. 2018. Whatâ€™s going on SMIL: Statistical Methods in Linguistics, 1:71â€“109.
 in neural constituency parsers? an analysis. NAACL HLT. Socher, R., J. Bauer, C. D. Manning, and A. Y. Ng. 2013.
 Parsing with compositional vector grammars. ACL.
26 Chapter 18 â€¢ Context-Free Grammars and Constituency Parsing

Socher, R., C. C.-Y. Lin, A. Y. Ng, and C. D. Manning. 2011.
 Parsing natural scenes and natural language with recursive neural networks. ICML.
Stern, M., J. Andreas, and D. Klein. 2017. A minimal spanbased neural constituency parser. ACL.
Vinyals, O., Å. Kaiser, T. Koo, S. Petrov, I. Sutskever,
 and G. Hinton. 2015. Grammar as a foreign language.
 NeurIPS.
Wang, W. and B. Chang. 2016. Graph-based dependency
 parsing with bidirectional LSTM. ACL.
Wundt, W. 1900. VoÌˆlkerpsychologie: eine Untersuchung der
 Entwicklungsgesetze von Sprache, Mythus, und Sitte. W.
 Engelmann, Leipzig. Band II: Die Sprache, Zweiter Teil.
Younger, D. H. 1967. Recognition and parsing of contextfree languages in time n3 . Information and Control,
 10:189â€“208.
