rights reserved. Draft of August 24, 2025.

CHAPTER

10 Masked Language Models
 Larvatus prodeo [Masked, I go forward]
 Descartes

 In the previous two chapters we introduced the transformer and saw how to pretrain a transformer language model as a causal or left-to-right language model. In
 this chapter we’ll introduce a second paradigm for pretrained language models, the
 BERT
 masked
 bidirectional transformer encoder, and the most widely-used version, the BERT
 language model (Devlin et al., 2019). This model is trained via masked language modeling,
 modeling
 where instead of predicting the following word, we mask a word in the middle and
 ask the model to guess the word given the words on both sides. This method thus
 allows the model to see both the right and left context.
 finetuning We also introduced finetuning in the prior chapter. Here we describe a new
 kind of finetuning, in which we take the transformer network learned by these pretrained models, add a neural net classifier after the top layer of the network, and train
 it on some additional labeled data to perform some downstream task like named
 entity tagging or natural language inference. As before, the intuition is that the
 pretraining phase learns a language model that instantiates rich representations of
 word meaning, that thus enables the model to more easily learn (‘be finetuned to’)
 the requirements of a downstream language understanding task. This aspect of the
 transfer
 learning pretrain-finetune paradigm is an instance of what is called transfer learning in machine learning: the method of acquiring knowledge from one task or domain, and
 then applying it (transferring it) to solve a new task.
 The second idea that we introduce in this chapter is the idea of contextual embeddings: representations for words in context. The methods of Chapter 5 like
 word2vec or GloVe learned a single vector embedding for each unique word w in
 the vocabulary. By contrast, with contextual embeddings, such as those learned by
 masked language models like BERT, each word w will be represented by a different
 vector each time it appears in a different context. While the causal language models
 of Chapter 8 also use contextual embeddings, the embeddings created by masked
 language models seem to function particularly well as representations.

 Let’s begin by introducing the bidirectional transformer encoder that underlies models like BERT and its descendants like RoBERTa (Liu et al., 2019) or SpanBERT
 (Joshi et al., 2020). In Chapter 7 we introduced the idea of left-to-right language
 models that can be applied to autoregressive contextual generation problems like
 question answering or summarization, and in Chapter 8 we saw how to implement
 language models with causal (left-to-right) transformers. But this left-to-right nature of these models is also a limitation, because there are tasks for which it would
 be useful, when processing a token, to be able to peek at future tokens. This is es-
2 C HAPTER 10 • M ASKED L ANGUAGE M ODELS

 pecially true for sequence labeling tasks in which we want to tag each token with
 a label, such as the named entity tagging task we’ll introduce in Section 10.5, or
 tasks like part-of-speech tagging or parsing that come up in later chapters.
 The bidirectional encoders that we introduce here are a different kind of beast
 than causal models. The causal models of Chapter 8 are generative models, designed to easily generate the next token in a sequence. But the focus of bidirectional encoders is instead on computing contextualized representations of the input
 tokens. Bidirectional encoders use self-attention to map sequences of input embeddings (x1 , ..., xn ) to sequences of output embeddings of the same length (h1 , ..., hn ),
 where the output vectors have been contextualized using information from the entire input sequence. These output embeddings are contextualized representations of
 each input token that are useful across a range of applications where we need to do
 a classification or a decision based on the token in context.
 Remember that we said the models of Chapter 8 are sometimes called decoderonly, because they correspond to the decoder part of the encoder-decoder model we
 will introduce in Chapter 12. By contrast, the masked language models of this chapter are sometimes called encoder-only, because they produce an encoding for each
 input token but generally aren’t used to produce running text by decoding/sampling.
 That’s an important point: masked language models are not used for generation.
 They are generally instead used for interpretative tasks.

 10.1.1 The architecture for bidirectional masked models
 Let’s first discuss the overall architecture. Bidirectional transformer-based language
 models differ in two ways from the causal transformers in the previous chapters. The
 first is that the attention function isn’t causal; the attention for a token i can look at
 following tokens i + 1 and so on. The second is that the training is slightly different
 since we are predicting something in the middle of our text rather than at the end.
 We’ll discuss the first here and the second in the following section.
 Fig. 10.1a, reproduced here from Chapter 8, shows the information flow in the
 left-to-right approach of Chapter 8. The attention computation at each token is based
 on the preceding (and current) input tokens, ignoring potentially useful information
 located to the right of the token under consideration. Bidirectional encoders overcome this limitation by allowing the attention mechanism to range over the entire
 input, as shown in Fig. 10.1b.

 a1 a2 a3 a4 a5 a1 a2 a3 a4 a5

 attention attention attention attention attention attention attention attention attention attention

 x1 x2 x3 x4 x5 x1 x2 x3 x4 x5

 a) A causal self-attention layer b) A bidirectional self-attention layer

attention value at each token is computed using only information seen earlier in the context. (b) Information
flow in a bidirectional attention model. In processing each token, the model attends to all inputs, both before
and after the current one. So attention for token 3 can draw on information from following tokens.

 The implementation is very simple! We simply remove the attention masking
 step that we introduced in Eq. ??. Recall from Chapter 8 that we had to mask the
 QK| matrix for causal transformers so that attention couldn’t look at future tokens
 10.1 • B IDIRECTIONAL T RANSFORMER E NCODERS 3

(repeated from Eq. ?? for a single attention head):
 QK|
   
 head = softmax mask √ V
 dk
 (10.1)

 q1•k1 q1•k2 q1•k3 q1•k4
 q1•k1 −∞ −∞ −∞

 q2•k1 q2•k2 −∞ −∞ q2•k1 q2•k2 q2•k3 q2•k4

 N N
 q3•k1 q3•k2 q3•k3 −∞ q3•k1 q3•k2 q3•k3 q3•k4

 q4•k1 q4•k2 q4•k3 q4•k4 q4•k1 q4•k2 q4•k3 q4•k4

 N N

 (a) (b)
portion of the comparisons matrix zeroed out (set to −∞, which the softmax will turn to zero),
while (b) shows the unmasked version.

 Fig. 10.2 shows the masked version of QK| and the unmasked version. For bidirectional attention, we use the unmasked version of Fig. 10.2b. Thus the attention
computation for bidirectional attention is exactly the same as Eq. 10.1 but with the
mask removed:
 QK|
  
 head = softmax √ V (10.2)
 dk
Otherwise, the attention computation is identical to what we saw in Chapter 8, as is
the transformer block architecture (the feedforward layer, layer norm, and so on). As
in Chapter 8, the input is also a series of subword tokens, usually computed by one of
the 3 popular tokenization algorithms (including the BPE algorithm that we already
saw in Chapter 2 and two others, the WordPiece algorithm and the SentencePiece
Unigram LM algorithm). That means every input sentence first has to be tokenized,
and all further processing takes place on subword tokens rather than words. This will
require, as we’ll see in the third part of the textbook, that for some NLP tasks that
require notions of words (like parsing) we will occasionally need to map subwords
back to words.
 To make this more concrete, the original English-only bidirectional transformer
encoder model, BERT (Devlin et al., 2019), consisted of the following:
 • An English-only subword vocabulary consisting of 30,000 tokens generated
 using the WordPiece algorithm (Schuster and Nakajima, 2012).
 • Input context window N=512 tokens, and model dimensionality d=768
 • So X, the input to the model, is of shape [N × d] = [512 × 768].
 • L=12 layers of transformer blocks, each with A=12 (bidirectional) multihead
 attention layers.
 • The resulting model has about 100M parameters.
The larger multilingual XLM-RoBERTa model, trained on 100 languages, has
 • A multilingual subword vocabulary with 250,000 tokens generated using the
 SentencePiece Unigram LM algorithm (Kudo and Richardson, 2018).
4 C HAPTER 10 • M ASKED L ANGUAGE M ODELS

 • Input context window N=512 tokens, and model dimensionality d=1024, hence
 X, the input to the model, is of shape [N × d] = [512 × 1024].
 • L=24 layers of transformer blocks, with A=16 multihead attention layers each
 • The resulting model has about 550M parameters.
 Note that 550M parameters is relatively small as large language models go
 (Llama 3 has 405B parameters, so is 3 orders of magnitude bigger). Indeed, masked
 language models tend to be much smaller than causal language models.

 We trained causal transformer language models in Chapter 8 by making them iteratively predict the next word in a text. But eliminating the causal mask in attention makes the guess-the-next-word language modeling task trivial—the answer
 is directly available from the context—so we’re in need of a new training scheme.
 Instead of trying to predict the next word, the model learns to perform a fill-in-thecloze task blank task, technically called the cloze task (Taylor, 1953). To see this, let’s return
 to the motivating example from Chapter 3. Instead of predicting which words are
 likely to come next in this example:
 The water of Walden Pond is so beautifully
 we’re asked to predict a missing item given the rest of the sentence.
 The of Walden Pond is so beautifully ...
 That is, given an input sequence with one or more elements missing, the learning
 task is to predict the missing elements. More precisely, during training the model is
 deprived of one or more tokens of an input sequence and must generate a probability
 distribution over the vocabulary for each of the missing items. We then use the crossentropy loss from each of the model’s predictions to drive the learning process.
 This approach can be generalized to any of a variety of methods that corrupt the
 training input and then asks the model to recover the original input. Examples of the
 kinds of manipulations that have been used include masks, substitutions, reorderings, deletions, and extraneous insertions into the training text. The general name
 denoising for this kind of training is called denoising: we corrupt (add noise to) the input in
 some way (by masking a word, or putting in an incorrect word) and the goal of the
 system is to remove the noise.

 10.2.1 Masking Words
 Masked
 Language Let’s describe the Masked Language Modeling (MLM) approach to training bidi-
Modeling
 rectional encoders (Devlin et al., 2019). As with the language model training methods we’ve already seen, MLM uses unannotated text from a large corpus. In MLM
 training, the model is presented with a series of sentences from the training corpus
 in which a percentage of tokens (15% in the BERT model) have been randomly chosen to be manipulated by the masking procedure. Given an input sentence lunch
 was delicious and assume we randomly chose the 3rd token delicious to be
 manipulated,
 • 80% of the time: The token is replaced with the special vocabulary token
 named [MASK], e.g. lunch was delicious → lunch was [MASK].
 10.2 • T RAINING B IDIRECTIONAL E NCODERS 5

 • 10% of the time: The token is replaced with another token, randomly sampled
 from the vocabulary based on token unigram probabilities. e.g. lunch was
 delicious → lunch was gasp.
 • 10% of the time: the token is left unchanged. e.g. lunch was delicious
 → lunch was delicious.
 We then train the model to guess the correct token for the manipulated tokens. Why
 the three possible manipulations? Adding the [MASK] token creates a mismatch
 between pretraining and downstream finetuning or inference, since when we employ
 the MLM model to perform a downstream task, we don’t use any [MASK] tokens. If
 we just replaced tokens with the [MASK], the model might only predict tokens when
 it sees a [MASK], but we want the model to try to always predict the input token.
 To train the model to make the prediction, the original input sequence is tokenized using a subword model and tokens are sampled to be manipulated. Word
 embeddings for all of the tokens in the input are retrieved from the E embedding matrix and combined with positional embeddings to form the input to the transformer,
 passed through the stack of bidirectional transformer blocks, and then the language
 modeling head. The MLM training objective is to predict the original inputs for
 each of the masked tokens and the cross-entropy loss from these predictions drives
 the training process for all the parameters in the model. That is, all of the input
 tokens play a role in the self-attention process, but only the sampled tokens are used
 for learning.

 long thanks the

 CE Loss

 LM Head with Softmax
 over Vocabulary

 hL1 hL2 hL3 hL4 hL5 hL6 hL7 hL8

 Bidirectional Transformer Encoder

 Token + + + + + + + + +
 Positional
 Embeddings p1 p2 p3 p4 p5 p6 p7 p8

 So [mask] and [mask] for all apricot fish
 So long and thanks for all the fish
which are masked and the third is replaced with an unrelated word. The probabilities assigned by the model to
these three items are used as the training loss. The other 5 tokens don’t play a role in training loss.

 Fig. 10.3 illustrates this approach with a simple example. Here, long, thanks and
 the have been sampled from the training sequence, with the first two masked and the
 replaced with the randomly sampled token apricot. The resulting embeddings are
 passed through a stack of bidirectional transformer blocks. Recall from Section ??
 in Chapter 8 that to produce a probability distribution over the vocabulary for each
 of the masked tokens, the language modeling head takes the output vector hLi from
 the final transformer layer L for each masked token i, multiplies it by the unembedding layer ET to produce the logits u, and then uses softmax to turn the logits into
6 C HAPTER 10 • M ASKED L ANGUAGE M ODELS

 probabilities y over the vocabulary:

 ui = hLi ET (10.3)
 yi = softmax(ui ) (10.4)

 With a predicted probability distribution for each masked item, we can use crossentropy to compute the loss for each masked item—the negative log probability
 assigned to the actual masked word, as shown in Fig. 10.3. More formally, for a
 given vector of input tokens in a sentence or batch x, let the set of tokens that are
 masked be M, the version of that sentence with some tokens replaced by masks be
 xmask , and the sequence of output vectors be h. For a given input token xi , such as
 the word long in Fig. 10.3, the loss is the probability of the correct word long, given
 xmask (as summarized in the single output vector hLi ):

 LMLM (xi ) = − log P(xi |hLi )

 The gradients that form the basis for the weight updates are based on the average
 loss over the sampled learning items from a single training sequence (or batch of
 sequences).
 1 X
 LMLM = − log P(xi |hLi )
 i∈M

 Note that only the tokens in M play a role in learning; the other words play no role
 in the loss function, so in that sense BERT and its descendents are inefficient; only
 15% of the input samples in the training data are actually used for training weights.1

 10.2.2 Next Sentence Prediction
 The focus of mask-based learning is on predicting words from surrounding contexts
 with the goal of producing effective word-level representations. However, an important class of applications involves determining the relationship between pairs of
 sentences. These include tasks like paraphrase detection (detecting if two sentences
 have similar meanings), entailment (detecting if the meanings of two sentences entail or contradict each other) or discourse coherence (deciding if two neighboring
 sentences form a coherent discourse).
 To capture the kind of knowledge required for applications such as these, some
 models in the BERT family include a second learning objective called Next Sen-
Next Sentence tence Prediction (NSP). In this task, the model is presented with pairs of sentences
 Prediction
 and is asked to predict whether each pair consists of an actual pair of adjacent sentences from the training corpus or a pair of unrelated sentences. In BERT, 50% of
 the training pairs consisted of positive pairs, and in the other 50% the second sentence of a pair was randomly selected from elsewhere in the corpus. The NSP loss
 is based on how well the model can distinguish true pairs from random pairs.
 To facilitate NSP training, BERT introduces two special tokens to the input representation (tokens that will prove useful for finetuning as well). After tokenizing
 the input with the subword model, the token [CLS] is prepended to the input sentence pair, and the token [SEP] is placed between the sentences and after the final
 token of the second sentence. There are actually two more special tokens, a ‘First
 Segment’ token, and a ‘Second Segment’ token. These tokens are added in the input stage to the word and positional embeddings. That is, each token of the input
 1 ELECTRA, another BERT family member, does use all examples for training (Clark et al., 2020).
 10.2 • T RAINING B IDIRECTIONAL E NCODERS 7

 X is actually formed by summing 3 embeddings: word, position, and first/second
 segment embeddings.
 During training, the output vector hLCLS from the final layer associated with the
 [CLS] token represents the next sentence prediction. As with the MLM objective,
 we add a special head, in this case an NSP head, which consists of a learned set of
 classification weights WNSP ∈ Rd×2 that produces a two-class prediction from the
 raw [CLS] vector hLCLS :

 yi = softmax(hLCLS WNSP )

 Cross entropy is used to compute the NSP loss for each sentence pair presented
 to the model. Fig. 10.4 illustrates the overall NSP training setup. In BERT, the NSP
 loss was used in conjunction with the MLM training objective to form final loss.

 CE Loss

 NSP
 Head

 hCLS

 Bidirectional Transformer Encoder

 Token +
 Segment + + + + + + + + + + + + + + + + + + +
 Positional s1 p1 s1 p2 s1 p3 s1 p4 s1 p5 s2 p6 s2 p7 s2 p8 s2 p9
 Embeddings
 [CLS] Cancel my flight [SEP] And the hotel [SEP]

 10.2.3 Training Regimes
 BERT and other early transformer-based language models were trained on about
 called BooksCorpus (Zhu et al., 2015) that is no longer used for intellectual property
 reasons). Modern masked language models are now trained on much larger datasets
 of web text, filtered a bit, and augmented by higher-quality data like Wikipedia,
 the same as those we discussed for the causal large language models of Chapter 8.
 Multilingual models similarly use webtext and multilingual Wikipedia. For example
 the XLM-R model was trained on about 300 billion tokens in 100 languages, taken
 from the web via Common Crawl (https://commoncrawl.org/).
 To train the original BERT models, pairs of text segments were selected from the
 training corpus according to the next sentence prediction 50/50 scheme. Pairs were
 sampled so that their combined length was less than the 512 token input. Tokens
 within these sentence pairs were then masked using the MLM approach with the
 combined loss from the MLM and NSP objectives used for a final loss. Because this
 final loss is backpropagated through the entire transformer, the embeddings at each
 transformer layer will learn representations that are useful for predicting words from
 their neighbors. Since the [CLS] tokens are the direct input to the NSP classifier,
 their learned representations will tend to contain information about the sequence as
8 C HAPTER 10 • M ASKED L ANGUAGE M ODELS

 a whole. Approximately 40 passes (epochs) over the training data was required for
 the model to converge.
 Some models, like the RoBERTa model, drop the next sentence prediction objective, and therefore change the training regime a bit. Instead of sampling pairs of
 sentence, the input is simply a series of contiguous sentences, still beginning with
 the special [CLS] token. If the document runs out before 512 tokens are reached, an
 extra separator token is added, and sentences from the next document are packed in,
 until we reach a total of 512 tokens. Usually large batch sizes are used, between 8K
 and 32K tokens.
 Multilingual models have an additional decision to make: what data to use to
 build the vocabulary? Recall that all language models use subword tokenization
 (BPE or SentencePiece Unigram LM are the two most common algorithms). What
 text should be used to learn this multilingual tokenization, given that it’s easier to
 get much more text in some languages than others? One option would be to create this vocabulary-learning dataset by sampling sentences from our training data
 (perhaps web text from Common Crawl), randomly. In that case we will choose a
 lot of sentences from languages with lots of web representation like English, and
 the tokens will be biased toward rare English tokens instead of creating frequent
 tokens from languages with less data. Instead, it is common to divide the training
 data into subcorpora of N different languages, compute the number of sentences ni
 of each language i, and readjust these probabilities so as to upweight the probability
 of less-represented languages (Lample and Conneau, 2019). The new probability of
 selecting a sentence from each of the N languages (whose prior frequency is ni ) is
 {qi }i=1...N , where:

 pα ni
 qi = PN i α
 with pi = PN (10.5)
 j=1 p j k=1 nk

 Recall from Eq. ?? in Chapter 5 that an α value between 0 and 1 will give higher
 weight to lower probability samples. Conneau et al. (2020) show that α = 0.3 works
 well to give rare languages more inclusion in the tokenization, resulting in better
 multilingual performance overall.
 The result of this pretraining process consists of both learned word embeddings,
 as well as all the parameters of the bidirectional encoder that are used to produce
 contextual embeddings for novel inputs.
 For many purposes, a pretrained multilingual model is more practical than a
 monolingual model, since it avoids the need to build many (a hundred!) separate
 monolingual models. And multilingual models can improve performance on lowresourced languages by leveraging linguistic information from a similar language in
 the training data that happens to have more resources. Nonetheless, when the number of languages grows very large, multilingual models exhibit what has been called
 the curse of multilinguality (Conneau et al., 2020): the performance on each language degrades compared to a model training on fewer languages. Another problem
 with multilingual models is that they ‘have an accent’: grammatical structures in
 higher-resource languages (often English) bleed into lower-resource languages; the
 vast amount of English language in training makes the model’s representations for
 low-resource languages slightly more English-like (Papadimitriou et al., 2023).
 10.3 • C ONTEXTUAL E MBEDDINGS 9

 Given a pretrained language model and a novel input sentence, we can think of the
 contextual
 embeddings sequence of model outputs as constituting contextual embeddings for each token in
 the input. These contextual embeddings are vectors representing some aspect of the
 meaning of a token in context, and can be used for any task requiring the meaning
 of tokens or words. More formally, given a sequence of input tokens x1 , ..., xn , we
 can use the output vector hL i from the final layer L of the model as a representation
 of the meaning of token xi in the context of sentence x1 , ..., xn . Or instead of just
 using the vector hL i from the final layer of the model, it’s common to compute a
 representation for xi by averaging the output tokens hi from each of the last four
 layers of the model, i.e., hL i , hL−1 i , hL−2 i , and hL−3 i .

 hLCLS hL1 hL2 hL3 hL4 hL5 hL6

 + i + i + i + i + i + i + i
 E E E E E E E

 [CLS] So long and thanks for all
 each input token xi .

 Just as we used static embeddings like word2vec in Chapter 5 to represent the
 meaning of words, we can use contextual embeddings as representations of word
 meanings in context for any task that might require a model of word meaning. Where
 static embeddings represent the meaning of word types (vocabulary entries), contextual embeddings represent the meaning of word instances: instances of a particular
 word type in a particular context. Thus where word2vec had a single vector for each
 word type, contextual embeddings provide a single vector for each instance of that
 word type in its sentential context. Contextual embeddings can thus be used for
 tasks like measuring the semantic similarity of two words in context, and are useful
 in linguistic tasks that require models of word meaning.

 10.3.1 Contextual Embeddings and Word Sense
 ambiguous Words are ambiguous: the same word can be used to mean different things. In
 Chapter 5 we saw that the word “mouse” can mean (1) a small rodent, or (2) a handoperated device to control a cursor. The word “bank” can mean: (1) a financial
 institution or (2) a sloping mound. We say that the words ‘mouse’ or ‘bank’ are
10 C HAPTER 10 • M ASKED L ANGUAGE M ODELS

 polysemous (from Greek ‘many senses’, poly- ‘many’ + sema, ‘sign, mark’).2
 word sense A sense (or word sense) is a discrete representation of one aspect of the meaning
 of a word. We can represent each sense with a superscript: bank1 and bank2 ,
 mouse1 and mouse2 . These senses can be found listed in online thesauruses (or
 WordNet thesauri) like WordNet (Fellbaum, 1998), which has datasets in many languages
 listing the senses of many words. In context, it’s easy to see the different meanings:
 mouse1 : .... a mouse controlling a computer system in 1968.
 mouse2 : .... a quiet animal like a mouse
 bank1 : ...a bank can hold the investments in a custodial account ...
 bank2 : ...as agriculture burgeons on the east bank, the river ...
 This fact that context disambiguates the senses of mouse and bank above can
 also be visualized geometrically. Fig. 10.6 shows a two-dimensional projection of
 many instances of the BERT embeddings of the word die in English and German.
 Each point in the graph represents the use of die in one input sentence. We can
 clearly see at least two different English senses of die (the singular of dice and the
 verb to die, as well as the German article, in the BERT embedding space.

in English and German, projected into two dimensions with the UMAP algorithm. The German and English
meanings and the different English senses fall into different clusters. Some sample points are shown with the
contextual sentence they came from. Figure from Coenen et al. (2019).
 are annotated with corresponding sentences. Overall annotations (blue text) are added as a guide.
 Thus while thesauruses like WordNet give discrete lists of senses, embeddings
 (whether
 or contextual) offer a continuous high-dimensional model of meaning
 that, although it can be clustered, doesn’t divide up into fully discrete senses.
Our first experiment is an exploratory visualization of how word sense affects context embeddings.
 Word word
For data on different Sensesenses,
 Disambiguation
 we collected all sentences used in the introductions to Englishlanguage Wikipedia articles. (Text outside of introductions was frequently fragmentary.) We created
an interactive
 word sense
 The task of selecting
 application, which wethe plancorrect sense
 to make for a word
 public. A user entersword
 is called a word,sense
 anddisambiguathe system
 tion, WSD.
retrieves 1,000 sentences containing that word. It sends these sentences to BERT-basefixed
disambiguation or WSD algorithms take as input a word in context and a inventory
 as input, and
for eachWSD of potential
 one it retrieves word senses
 the context (like the
 embedding for ones in WordNet)
 the word from a layer andofoutputs thechoosing.
 the user’s correct word
 sense in context. Fig. 10.7 sketches out the task.
The system visualizes these 1,000 context embeddings using UMAP [15], generally showing clear
clusters relating2 toThe
 word
 wordsenses.
 polysemyDifferent senses of
 itself is ambiguous; youamay
 word
 see are typically
 it used spatially
 in a different way, toseparated,
 refer only to and
 cases
within the clusters there
 where is often
 a word’s sensesfurther structure
 are related in some related toway,
 structured finereserving
 shades theof word
 meaning. In Figure
 homonymy to mean4,sense
 for
example, we not only seewith
 ambiguities crisp, well-separated
 no relation between the clusters for three
 senses (Haber meanings
 and Poesio, of the
 2020). Here we word
 will use“die,” but
 ‘polysemy’
within one of these clusters
 to mean any kindthere is ambiguity,
 of sense a kind ofand quantitative scale, related
 ‘structured polysemy’ to thewith
 for polysemy number of people
 sense elations.
dying. See Appendix 6.4 for further examples. The apparent detail in the clusters we visualized raises
two immediate questions. First, is it possible to find quantitative corroboration that word senses are
well-represented? Second, how can we resolve a seeming contradiction: in the previous section, we
saw how position represented syntax; yet here we see position representing semantics.

 10.3 • C ONTEXTUAL E MBEDDINGS 11

 y5 y6
 y3
 stand1: side1:
 y1 bass1: y4 upright relative
 low range … region
 electric1: … player1: stand5: …
 using bass4: in game bear side3:
 electricity sea fish player2: … of body
 electric2: … musician stand10: …
 tense
 y2 bass7: player3: put side11:
 electric3: instrument actor upright slope
 thrilling guitar1 … … … …

 x1 x2 x3 x4 x5 x6
 an electric guitar and bass player stand oﬀ to one side

 (y). Figure inspired by Chaplot and Salakhutdinov (2018).

 WSD can be a useful analytic tool for text analysis in the humanities and social
 sciences, and word senses can play a role in model interpretability for word representations. Word senses also have interesting distributional properties. For example
 a word often is used in roughly the same sense through a discourse, an observation
one sense per
 discourse called the one sense per discourse rule (Gale et al., 1992).
 The best performing WSD algorithm is a simple 1-nearest-neighbor algorithm
 using contextual word embeddings, due to Melamud et al. (2016) and Peters et al.
 (2018). At training time we pass each sentence in some sense-labeled dataset (like
 the SemCore or SenseEval datasets in various languages) through any contextual
 embedding (e.g., BERT) resulting in a contextual embedding for each labeled token.
 (There are various ways to compute this contextual embedding vi for a token i; for
 BERT it is common to pool multiple layers by summing the vector representations
 of i from the last four BERT layers). Then for each sense s of any word in the corpus,
 for each of the n tokens of that sense, we average their n contextual representations
 vi to produce a contextual sense embedding vs for s:
 1X
 vs = vi ∀vi ∈ tokens(s) (10.6)
 n
 i

 At test time, given a token of a target word t in context, we compute its contextual
 embedding t and choose its nearest neighbor sense from the training set, i.e., the
 sense whose sense embedding has the highest cosine with t:

 sense(t) = argmax cosine(t, vs ) (10.7)
 s∈senses(t)

 Fig. 10.8 illustrates the model.

 10.3.2 Contextual Embeddings and Word Similarity
 In Chapter 5 we introduced the idea that we could measure the similarity of two
 words by considering how close they are geometrically, by using the cosine as a
 similarity function. The idea of meaning similarity is also clear geometrically in the
 meaning clusters in Fig. 10.6; the representation of a word which has a particular
 sense in a context is closer to other instances of the same sense of the word. Thus we
12 C HAPTER 10 • M ASKED L ANGUAGE M ODELS

 find5
 find4
 v
 v
 find1
 v
 find9
 v

 cI cfound cthe cjar cempty

 ENCODER
 I found the jar empty
 find. A contextual embedding is computed for the target word found, and then the nearest
 neighbor sense (in this case find9v ) is chosen. Figure inspired by Loureiro and Jorge (2019).

 often measure the similarity between two instances of two words in context (or two
 instances of the same word in two different contexts) by using the cosine between
 their contextual embeddings.
 Usually some transformations to the embeddings are required before computing
 cosine. This is because contextual embeddings (whether from masked language
 models or from autoregressive ones) have the property that the vectors for all words
 are extremely similar. If we look at the embeddings from the final layer of BERT
 or other models, embeddings for instances of any two randomly chosen words will
 have extremely high cosines that can be quite close to 1, meaning all word vectors
 tend to point in the same direction. The property of vectors in a system all tending
 to point in the same direction is known as anisotropy. Ethayarajh (2019) defines
 anisotropy the anisotropy of a model as the expected cosine similarity of any pair of words in
 a corpus. The word ‘isotropy’ means uniformity in all directions, so in an isotropic
 model, the collection of vectors should point in all directions and the expected cosine
 between a pair of random embeddings would be zero. Timkey and van Schijndel
 (2021) show that one cause of anisotropy is that cosine measures are dominated by
 a small number of dimensions of the contextual embedding whose values are very
 different than the others: these rogue dimensions have very large magnitudes and
 very high variance.
 Timkey and van Schijndel (2021) shows that we can make the embeddings more
 isotropic by standardizing (z-scoring) the vectors, i.e., subtracting the mean and
 dividing by the variance. Given a set C of all the embeddings in some corpus, each
 with dimensionality d (i.e., x ∈ Rd ), the mean vector µ ∈ Rd is:
 1 X
 µ= x (10.8)
 x∈C

 The standard deviation in each dimension σ ∈ Rd is:
 1 X
 s
 σ= (x − µ)2 (10.9)
 x∈C

 Then each word vector x is replaced by a standardized version z:
 x−µ
 z= (10.10)
 σ
 10.4 • F INE -T UNING FOR C LASSIFICATION 13

 One problem with cosine that is not solved by standardization is that cosine tends
 to underestimate human judgments on similarity of word meaning for very frequent
 words (Zhou et al., 2022).

 The power of pretrained language models lies in their ability to extract generalizations from large amounts of text—generalizations that are useful for myriad downstream applications. There are two ways to make practical use of the generalizations
 to solve downstream tasks. The most common way is to use natural language to
 prompt the model, putting it in a state where it contextually generates what we
 want.
 In this section we explore an alternative way to use pretrained language models
 finetuning for downstream applications: a version of the finetuning paradigm from Chapter 7.
 In the kind of finetuning used for masked language models, we add applicationspecific circuitry (often called a special head) on top of pretrained models, taking
 their output as its input. The finetuning process consists of using labeled data about
 the application to train these additional application-specific parameters. Typically,
 this training will either freeze or make only minimal adjustments to the pretrained
 language model parameters.
 The following sections introduce finetuning methods for the most common kinds
 of applications: sequence classification, sentence-pair classification, and sequence
 labeling.

 10.4.1 Sequence Classification
 The task of sequence classification is to classify an entire sequence of text with a
 single label. This set of tasks is commonly called text classification, like sentiment
 analysis or spam detection (Appendix K) in which we classify a text into two or
 three classes (like positive or negative), as well as classification tasks with a large
 number of categories, like document-level topic classification.
 For sequence classification we represent the entire input to be classified by a
 single vector. We can represent a sequence in various ways. One way is to take
 the sum or the mean of the last output vector from each token in the sequence.
 For BERT, we instead add a new unique token to the vocabulary called [CLS], and
 prepended it to the start of all input sequences, both during pretraining and encoding.
 The output vector in the final layer of the model for the [CLS] input represents
classifier head the entire input sequence and serves as the input to a classifier head, a logistic
 regression or neural network classifier that makes the relevant decision.
 As an example, let’s return to the problem of sentiment classification. Finetuning
 a classifier for this application involves learning a set of weights, WC , to map the
 output vector for the [CLS] token—hLCLS —to a set of scores over the possible sentiment classes. Assuming a three-way sentiment classification task (positive, negative,
 neutral) and dimensionality d as the model dimension, WC will be of size [d × 3]. To
 classify a document, we pass the input text through the pretrained language model to
 generate hLCLS , multiply it by WC , and pass the resulting vector through a softmax.

 y = softmax(hLCLS WC ) (10.11)

 Finetuning the values in WC requires supervised training data consisting of input
14 C HAPTER 10 • M ASKED L ANGUAGE M ODELS

 sequences labeled with the appropriate sentiment class. Training proceeds in the
 usual way; cross-entropy loss between the softmax output and the correct answer is
 used to drive the learning that produces WC .
 This loss can be used to not only learn the weights of the classifier, but also to
 update the weights for the pretrained language model itself. In practice, reasonable
 classification performance is typically achieved with only minimal changes to the
 language model parameters, often limited to updates over the final few layers of the
 transformer. Fig. 10.9 illustrates this overall approach to sequence classification.

 y

 sentiment
 classification
 head WC

 hCLS

 Bidirectional Transformer Encoder

 + i + i + i + i + i + i

 E E E E E E

 [CLS] entirely predictable and lacks energy
[CLS] token serves as input to a simple classifier.

 10.4.2 Sequence-Pair Classification
 As mentioned in Section 10.2.2, an important type of problem involves the classification of pairs of input sequences. Practical applications that fall into this class include
 paraphrase detection (are the two sentences paraphrases of each other?), logical entailment (does sentence A logically entail sentence B?), and discourse coherence
 (how coherent is sentence B as a follow-on to sentence A?).
 Fine-tuning an application for one of these tasks proceeds just as with pretraining using the NSP objective. During finetuning, pairs of labeled sentences from a
 supervised finetuning set are presented to the model, and run through all the layers
 of the model to produce the h outputs for each input token. As with sequence classification, the output vector associated with the prepended [CLS] token represents the
 model’s view of the input pair. And as with NSP training, the two inputs are separated by the [SEP] token. To perform classification, the [CLS] vector is multiplied
 by a set of learned classification weights and passed through a softmax to generate
 label predictions, which are then used to update the weights.
 As an example, let’s consider an entailment classification task with the Multi-
Genre Natural Language Inference (MultiNLI) dataset (Williams et al., 2018). In
 natural
 language the task of natural language inference or NLI, also called recognizing textual
 inference
 entailment, a model is presented with a pair of sentences and must classify the relationship between their meanings. For example in the MultiNLI corpus, pairs of
 sentences are given one of 3 labels: entails, contradicts and neutral. These labels
 10.5 • F INE -T UNING FOR S EQUENCE L ABELLING : NAMED E NTITY R ECOGNITION 15

 describe a relationship between the meaning of the first sentence (the premise) and
 the meaning of the second sentence (the hypothesis). Here are representative examples of each class from the corpus:
 • Neutral
 a: Jon walked back to the town to the smithy.
 b: Jon traveled back to his hometown.
 • Contradicts
 a: Tourist Information offices can be very helpful.
 b: Tourist Information offices are never of any help.
 • Entails
 a: I’m confused.
 b: Not all of it is very clear to me.
 A relationship of contradicts means that the premise contradicts the hypothesis; entails means that the premise entails the hypothesis; neutral means that neither is
 necessarily true. The meaning of these labels is looser than strict logical entailment
 or contradiction indicating that a typical human reading the sentences would most
 likely interpret the meanings in this way.
 To finetune a classifier for the MultiNLI task, we pass the premise/hypothesis
 pairs through a bidirectional encoder as described above and use the output vector
 for the [CLS] token as the input to the classification head. As with ordinary sequence
 classification, this head provides the input to a three-way classifier that can be trained
 on the MultiNLI training corpus.

 tity Recognition
 In sequence labeling, the network’s task is to assign a label chosen from a small
 fixed set of labels to each token in the sequence. One of the most common sequence
 labeling task is named entity recognition.

 10.5.1 Named Entities
 named entity A named entity is, roughly speaking, anything that can be referred to with a proper
 named entity
 recognition name: a person, a location, an organization. The task of named entity recognition
 NER (NER) is to find spans of text that constitute proper names and tag the type of the
 entity. Four entity tags are most common: PER (person), LOC (location), ORG
 (organization), or GPE (geo-political entity). However, the term named entity is
 commonly extended to include things that aren’t entities per se, including temporal
 expressions like dates and times, and even numerical expressions like prices. Here’s
 an example of the output of an NER tagger:
 Citing high fuel prices, [ORG United Airlines] said [TIME Friday] it
 has increased fares by [MONEY $6] per round trip on flights to some
 cities also served by lower-cost carriers. [ORG American Airlines], a
 unit of [ORG AMR Corp.], immediately matched the move, spokesman
 [PER Tim Wagner] said. [ORG United], a unit of [ORG UAL Corp.],
16 C HAPTER 10 • M ASKED L ANGUAGE M ODELS

 said the increase took effect [TIME Thursday] and applies to most
 routes where it competes against discount carriers, such as [LOC Chicago]
 to [LOC Dallas] and [LOC Denver] to [LOC San Francisco].
 The text contains 13 mentions of named entities including 5 organizations, 4 locations, 2 times, 1 person, and 1 mention of money. Figure 10.10 shows typical generic
 named entity types. Many applications will also need to use specific entity types like
 proteins, genes, commercial products, or works of art.

Type Tag Sample Categories Example sentences
People PER people, characters Turing is a giant of computer science.
Organization ORG companies, sports teams The IPCC warned about the cyclone.
Location LOC regions, mountains, seas Mt. Sanitas is in Sunshine Canyon.
Geo-Political Entity GPE countries, states Palo Alto is raising the fees for parking.

 Named entity recognition is a useful step in various natural language processing
 tasks, including linking text to information in structured knowledge sources like
 Wikipedia, measuring sentiment or attitudes toward a particular entity in text, or
 even as part of anonymizing text for privacy. The NER task is difficult because
 of the ambiguity of segmenting NER spans, figuring out which tokens are entities
 and which aren’t, since most words in a text will not be named entities. Another
 difficulty is caused by type ambiguity. The mention Washington can refer to a
 person, a sports team, a city, or the US government, as we see in Fig. 10.11.

 [PER Washington] was born into slavery on the farm of James Burroughs.
 [ORG Washington] went up 2 games to 1 in the four-game series.
 Blair arrived in [LOC Washington] for what may well be his last state visit.
 In June, [GPE Washington] passed a primary seatbelt law.

 10.5.2 BIO Tagging
 One standard approach to sequence labeling for a span-recognition problem like
 BIO tagging NER is BIO tagging (Ramshaw and Marcus, 1995). This is a method that allows us
 to treat NER like a word-by-word sequence labeling task, via tags that capture both
 the boundary and the named entity type. Consider the following sentence:
 [PER Jane Villanueva ] of [ORG United] , a unit of [ORG United Airlines
 Holding] , said the fare applies to the [LOC Chicago ] route.
 BIO Figure 10.12 shows the same excerpt represented with BIO tagging, as well
 as variants called IO tagging and BIOES tagging. In BIO tagging we label any
 token that begins a span of interest with the label B, tokens that occur inside a span
 are tagged with an I, and any tokens outside of any span of interest are labeled O.
 While there is only one O tag, we’ll have distinct B and I tags for each named entity
 class. The number of tags is thus 2n + 1, where n is the number of entity types. BIO
 tagging can represent exactly the same information as the bracketed notation, but has
 the advantage that we can represent the task in the same simple sequence modeling
 way as part-of-speech tagging: assigning a single label yi to each input word xi :
 We’ve also shown two variant tagging schemes: IO tagging, which loses some
 information by eliminating the B tag, and BIOES tagging, which adds an end tag E
 for the end of a span, and a span tag S for a span consisting of only one word.
 10.5 • F INE -T UNING FOR S EQUENCE L ABELLING : NAMED E NTITY R ECOGNITION 17

 Words IO Label BIO Label BIOES Label
 Jane I-PER B-PER B-PER
 Villanueva I-PER I-PER E-PER
 of O O O
 United I-ORG B-ORG B-ORG
 Airlines I-ORG I-ORG I-ORG
 Holding I-ORG I-ORG E-ORG
 discussed O O O
 the O O O
 Chicago I-LOC B-LOC S-LOC
 route O O O
 . O O O

 10.5.3 Sequence Labeling
 In sequence labeling, we pass the final output vector corresponding to each input
 token to a classifier that produces a softmax distribution over the possible set of
 tags. For a single feedforward layer classifier, the set of weights to be learned is
 WK of size [d × k], where k is the number of possible tags for the task. A greedy
 approach, where the argmax tag for each token is taken as a likely answer, can be
 used to generate the final output tag sequence. Fig. 10.13 illustrates an example of
 this approach, where yi is a vector of probabilities over tags, and k indexes the tags.

 yi = softmax(hLi WK ) (10.12)
 ti = argmaxk (yi ) (10.13)

 Alternatively, the distribution over labels provided by the softmax for each input
 token can be passed to a conditional random field (CRF) layer which can take global
 tag-level transitions into account (see Chapter 17 on CRFs).

 argmax B-PER I-PER O B-ORG I-ORG I-ORG O
 yi

 NER
 head WK WK WK WK WK WK WK

 hi

 Bidirectional Transformer Encoder

 + i + i + i + i + i + i + i + i

 E E E E E E E E

 [CLS] Jane Villanueva of United Airlines Holding discussed
output vector for each input token is passed to a simple k-way classifier.
18 C HAPTER 10 • M ASKED L ANGUAGE M ODELS

 Tokenization and NER
 Note that supervised training data for NER is typically in the form of BIO tags associated with text segmented at the word level. For example the following sentence
 containing two named entities:
 [LOC Mt. Sanitas ] is in [LOC Sunshine Canyon] .
 would have the following set of per-word BIO tags.
 (10.14) Mt. Sanitas is in Sunshine Canyon .
 B-LOC I-LOC O O B-LOC I-LOC O

 Unfortunately, the sequence of WordPiece tokens for this sentence doesn’t align
 directly with BIO tags in the annotation:
 ’Mt’, ’.’, ’San’, ’##itas’, ’is’, ’in’, ’Sunshine’, ’Canyon’ ’.’

 To deal with this misalignment, we need a way to assign BIO tags to subword
 tokens during training and a corresponding way to recover word-level tags from
 subwords during decoding. For training, we can just assign the gold-standard tag
 associated with each word to all of the subword tokens derived from it.
 For decoding, the simplest approach is to use the argmax BIO tag associated with
 the first subword token of a word. Thus, in our example, the BIO tag assigned to
 “Mt” would be assigned to “Mt.” and the tag assigned to “San” would be assigned
 to “Sanitas”, effectively ignoring the information in the tags assigned to “.” and
 “##itas”. More complex approaches combine the distribution of tag probabilities
 across the subwords in an attempt to find an optimal word-level tag.

 10.5.4 Evaluating Named Entity Recognition
 Named entity recognizers are evaluated by recall, precision, and F1 measure. Recall that recall is the ratio of the number of correctly labeled responses to the total
 that should have been labeled; precision is the ratio of the number of correctly labeled responses to the total labeled; and F1 measure is the harmonic mean of the
 two.
 To know if the difference between the F1 scores of two NER systems is a significant difference, we use the paired bootstrap test, or the similar randomization test
 (Section ??).
 For named entity tagging, the entity rather than the word is the unit of response.
 Thus in the example in Fig. 10.12, the two entities Jane Villanueva and United Airlines Holding and the non-entity discussed would each count as a single response.
 The fact that named entity tagging has a segmentation component which is not
 present in tasks like text categorization or part-of-speech tagging causes some problems with evaluation. For example, a system that labeled Jane but not Jane Villanueva as a person would cause two errors, a false positive for O and a false negative for I-PER. In addition, using entities as the unit of response but words as the unit
 of training means that there is a mismatch between the training and test conditions.

 This chapter has introduced the bidirectional encoder and the masked language
 model. Here’s a summary of the main points that we covered:
 H ISTORICAL N OTES 19

 • Bidirectional encoders can be used to generate contextualized representations
 of input embeddings using the entire input context.
 • Pretrained language models based on bidirectional encoders can be learned
 using a masked language model objective where a model is trained to guess
 the missing information from an input.
 • The vector output of each transformer block or component in a particular token column is a contextual embedding that represents some aspect of the
 meaning of a token in context.
 • A word sense is a discrete representation of one aspect of the meaning of a
 word. Contextual embeddings offer a continuous high-dimensional model of
 meaning that is richer than fully discrete senses.
 • The cosine between contextual embeddings can be used as one way to model
 the similarity between two words in context, although some transformations
 to the embeddings are required first.
 • Pretrained language models can be finetuned for specific applications by adding
 lightweight classifier layers on top of the outputs of the pretrained model.
 • These applications can include sequence classification tasks like sentiment
 analysis, sequence-pair classification tasks like natural language inference,
 or sequence labeling tasks like named entity recognition.

Historical Notes
 History TBD.
20 Chapter 10 • Masked Language Models

Chaplot, D. S. and R. Salakhutdinov. 2018. Knowledge- Timkey, W. and M. van Schijndel. 2021. All bark and no
 based word sense disambiguation using topic models. bite: Rogue dimensions in transformer language models
 AAAI. obscure representational quality. EMNLP.
Clark, K., M.-T. Luong, Q. V. Le, and C. D. Manning. Williams, A., N. Nangia, and S. Bowman. 2018. A broad-
2020. Electra: Pre-training text encoders as discrimina- coverage challenge corpus for sentence understanding
 tors rather than generators. ICLR. through inference. NAACL HLT.
Coenen, A., E. Reif, A. Yuan, B. Kim, A. Pearce, F. Viégas, Zhou, K., K. Ethayarajh, D. Card, and D. Jurafsky. 2022.
 and M. Wattenberg. 2019. Visualizing and measuring the Problems with cosine as a measure of embedding simigeometry of bert. NeurIPS. larity for high frequency words. ACL.
Conneau, A., K. Khandelwal, N. Goyal, V. Chaudhary, Zhu, Y., R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun,
 G. Wenzek, F. Guzmán, E. Grave, M. Ott, L. Zettlemoyer, A. Torralba, and S. Fidler. 2015. Aligning books and
 and V. Stoyanov. 2020. Unsupervised cross-lingual rep- movies: Towards story-like visual explanations by watchresentation learning at scale. ACL. ing movies and reading books. IEEE International Con-
Devlin, J., M.-W. Chang, K. Lee, and K. Toutanova. 2019. ference on Computer Vision.
 BERT: Pre-training of deep bidirectional transformers for
 language understanding. NAACL HLT.
Ethayarajh, K. 2019. How contextual are contextualized word representations? Comparing the geometry of
 BERT, ELMo, and GPT-2 embeddings. EMNLP.
Fellbaum, C., ed. 1998. WordNet: An Electronic Lexical
 Database. MIT Press.
Gale, W. A., K. W. Church, and D. Yarowsky. 1992. One
 sense per discourse. HLT.
Haber, J. and M. Poesio. 2020. Assessing polyseme sense
 similarity through co-predication acceptability and contextualised embedding distance. *SEM.
Joshi, M., D. Chen, Y. Liu, D. S. Weld, L. Zettlemoyer, and
 O. Levy. 2020. SpanBERT: Improving pre-training by
 representing and predicting spans. TACL, 8:64–77.
Kudo, T. and J. Richardson. 2018. SentencePiece: A simple
 and language independent subword tokenizer and detokenizer for neural text processing. EMNLP.
Lample, G. and A. Conneau. 2019. Cross-lingual language
 model pretraining. NeurIPS, volume 32.
Liu, Y., M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen,
 O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov.
 2019. RoBERTa: A robustly optimized BERT pretraining
 approach. ArXiv preprint arXiv:1907.11692.
Loureiro, D. and A. Jorge. 2019. Language modelling makes
 sense: Propagating representations through WordNet for
 full-coverage word sense disambiguation. ACL.
Melamud, O., J. Goldberger, and I. Dagan. 2016. context2vec: Learning generic context embedding with bidirectional LSTM. CoNLL.
Papadimitriou, I., K. Lopez, and D. Jurafsky. 2023. Multilingual BERT has an accent: Evaluating English influences
 on fluency in multilingual models. EACL Findings.
Peters, M., M. Neumann, M. Iyyer, M. Gardner, C. Clark,
 K. Lee, and L. Zettlemoyer. 2018. Deep contextualized
 word representations. NAACL HLT.
Ramshaw, L. A. and M. P. Marcus. 1995. Text chunking
 using transformation-based learning. Proceedings of the
 3rd Annual Workshop on Very Large Corpora.
Schuster, M. and K. Nakajima. 2012. Japanese and Korean
 voice search. ICASSP.
Taylor, W. L. 1953. Cloze procedure: A new tool for measuring readability. Journalism Quarterly, 30:415–433.
