rights reserved. Draft of August 24, 2025.

CHAPTER

 Hidden Markov Models
A Chapter 17 introduced the Hidden Markov Model and applied it to part of speech
 tagging. Part of speech tagging is a fully-supervised learning task, because we have
 a corpus of words labeled with the correct part-of-speech tag. But many applications
 don’t have labeled data. So in this chapter, we introduce the full set of algorithms for
 HMMs, including the key unsupervised learning algorithm for HMM, the Forward-
Backward algorithm. We’ll repeat some of the text from Chapter 17 for readers who
 want the whole story laid out in a single chapter.

A.1 Markov Chains
 Markov chain The HMM is based on augmenting the Markov chain. A Markov chain is a model
 that tells us something about the probabilities of sequences of random variables,
 states, each of which can take on values from some set. These sets can be words, or
 tags, or symbols representing anything, like the weather. A Markov chain makes a
 very strong assumption that if we want to predict the future in the sequence, all that
 matters is the current state. The states before the current state have no impact on the
 future except via the current state. It’s as if to predict tomorrow’s weather you could
 examine today’s weather but you weren’t allowed to look at yesterday’s weather.

 .8
 are .2
 .1 COLD2 .1 .4 .5
 .1 .5
 .1
 .3 uniformly charming
 HOT1 WARM3 .5

 .6 .3 .6 .1 .2
 .6
 (a) (b)
 Figure A.1 A Markov chain for weather (a) and one for words (b), showing states and
 transitions. A start distribution π is required; setting π = [0.1, 0.7, 0.2] for (a) would mean a
 probability 0.7 of starting in state 2 (cold), probability 0.1 of starting in state 1 (hot), etc.

 More formally, consider a sequence of state variables q1 , q2 , ..., qi . A Markov
 Markov
 assumption model embodies the Markov assumption on the probabilities of this sequence: that
 when predicting the future, the past doesn’t matter, only the present.

 Markov Assumption: P(qi = a|q1 ...qi−1 ) = P(qi = a|qi−1 ) (A.1)

 Figure A.1a shows a Markov chain for assigning a probability to a sequence of
 weather events, for which the vocabulary consists of HOT, COLD, and WARM. The
 states are represented as nodes in the graph, and the transitions, with their probabilities, as edges. The transitions are probabilities: the values of arcs leaving a given
2 A PPENDIX A • H IDDEN M ARKOV M ODELS

 state must sum to 1. Figure A.1b shows a Markov chain for assigning a probability to a sequence of words w1 ...wn . This Markov chain should be familiar; in fact,
 it represents a bigram language model, with each edge expressing the probability
 p(wi |w j )! Given the two models in Fig. A.1, we can assign a probability to any
 sequence from our vocabulary.
 Formally, a Markov chain is specified by the following components:
 Q = q1 q2 . . . qN a set of N states
 A = a11 a12 . . . aN1 . . . aNN a transition probability matrix A, each ai j representing the probability of moving from state i to state j, s.t.
 P n
 j=1 ai j = 1 ∀i
 π = π1 , π2 , ..., πN an initial probability distribution over states. πi is the
 probability that the Markov chain will start in state i.
 Some states j may have π j = 0, meaning that they cannot
 be initial states. Also, Ni=1 πi = 1
 P

 Before you go on, use the sample probabilities in Fig. A.1a (with π = [.1, .7., 2])
 to compute the probability of each of the following sequences:
 (A.2) hot hot hot hot
 (A.3) cold hot cold hot
 What does the difference in these probabilities tell you about a real-world weather
 fact encoded in Fig. A.1a?

A.2 The Hidden Markov Model
 A Markov chain is useful when we need to compute a probability for a sequence
 of observable events. In many cases, however, the events we are interested in are
 hidden hidden: we don’t observe them directly. For example we don’t normally observe
 part-of-speech tags in a text. Rather, we see words, and must infer the tags from the
 word sequence. We call the tags hidden because they are not observed.
 Hidden A hidden Markov model (HMM) allows us to talk about both observed events
 Markov model
 (like words that we see in the input) and hidden events (like part-of-speech tags) that
 we think of as causal factors in our probabilistic model. An HMM is specified by
 the following components:
 Q = q1 q2 . . . qN a set of N states
 A = a11 . . . ai j . . . aNN a transition probability matrix A, each ai j representing the probability
 of moving from state i to state j, s.t. Nj=1 ai j = 1 ∀i
 P

 B = bi (ot ) a sequence of observation likelihoods, also called emission probabilities, each expressing the probability of an observation ot (drawn from a
 vocabulary V = v1 , v2 , ..., vV ) being generated from a state qi
 π = π1 , π2 , ..., πN an initial probability distribution over states. πi is the probability that
 the Markov chain will start in state i. Some states P j may have π j = 0,
 meaning that they cannot be initial states. Also, ni=1 πi = 1

 The HMM is given as input O = o1 o2 . . . oT : a sequence of T observations, each
 one drawn from the vocabulary V .
 A first-order hidden Markov model instantiates two simplifying assumptions.
 First, as with a first-order Markov chain, the probability of a particular state depends
 A.2 • T HE H IDDEN M ARKOV M ODEL 3

only on the previous state:

 Markov Assumption: P(qi |q1 ...qi−1 ) = P(qi |qi−1 ) (A.4)
 Second, the probability of an output observation oi depends only on the state that
produced the observation qi and not on any other states or any other observations:

 Output Independence: P(oi |q1 . . . qi , . . . , qT , o1 , . . . , oi , . . . , oT ) = P(oi |qi ) (A.5)

 To exemplify these models, we’ll use a task invented by Jason Eisner (2002).
Imagine that you are a climatologist in the year 2799 studying the history of global
warming. You cannot find any records of the weather in Baltimore, Maryland, for
the summer of 2020, but you do find Jason Eisner’s diary, which lists how many ice
creams Jason ate every day that summer. Our goal is to use these observations to
estimate the temperature every day. We’ll simplify this weather task by assuming
there are only two kinds of days: cold (C) and hot (H). So the Eisner task is as
follows:
 Given a sequence of observations O (each an integer representing the
 number of ice creams eaten on a given day) find the ‘hidden’ sequence
 Q of weather states (H or C) which caused Jason to eat the ice cream.
 Figure A.2 shows a sample HMM for the ice cream task. The two hidden states
(H and C) correspond to hot and cold weather, and the observations (drawn from the
alphabet O = {1, 2, 3}) correspond to the number of ice creams eaten by Jason on a
given day.

 .5 .6

 .5
 COLD1 HOT2
 .4
 B1 B2
 P(1 | COLD) .5 P(1 | HOT) .2
 P(2 | COLD) = .4 P(2 | HOT) = .4
 P(3 | COLD) .1 P(3 | HOT) .4
 π = [.2,.8]

Figure A.2 A hidden Markov model for relating numbers of ice creams eaten by Jason (the
observations) to the weather (H or C, the hidden variables).

 An influential tutorial by Rabiner (1989), based on tutorials by Jack Ferguson in
the 1960s, introduced the idea that hidden Markov models should be characterized
by three fundamental problems:

 Problem 1 (Likelihood): Given an HMM λ = (A, B) and an observation sequence O, determine the likelihood P(O|λ ).
 Problem 2 (Decoding): Given an observation sequence O and an HMM λ =
 (A, B), discover the best hidden state sequence Q.
 Problem 3 (Learning): Given an observation sequence O and the set of states
 in the HMM, learn the HMM parameters A and B.

 We already saw an example of Problem 2 in Chapter 17. In the next two sections
we introduce the Forward and Forward-Backward algorithms to solve Problems 1
and 3 and give more information on Problem 2
4 A PPENDIX A • H IDDEN M ARKOV M ODELS

A.3 Likelihood Computation: The Forward Algorithm
 Our first problem is to compute the likelihood of a particular observation sequence.
 For example, given the ice-cream eating HMM in Fig. A.2, what is the probability
 of the sequence 3 1 3? More formally:
 Computing Likelihood: Given an HMM λ = (A, B) and an observation sequence O, determine the likelihood P(O|λ ).
 For a Markov chain, where the surface observations are the same as the hidden
 events, we could compute the probability of 3 1 3 just by following the states labeled
 3 1 3 and multiplying the probabilities along the arcs. For a hidden Markov model,
 things are not so simple. We want to determine the probability of an ice-cream
 observation sequence like 3 1 3, but we don’t know what the hidden state sequence
 is!
 Let’s start with a slightly simpler situation. Suppose we already knew the weather
 and wanted to predict how much ice cream Jason would eat. This is a useful part
 of many HMM tasks. For a given hidden state sequence (e.g., hot hot cold), we can
 easily compute the output likelihood of 3 1 3.
 Let’s see how. First, recall that for hidden Markov models, each hidden state
 produces only a single observation. Thus, the sequence of hidden states and the
 sequence of observations have the same length. 1
 Given this one-to-one mapping and the Markov assumptions expressed in Eq. A.4,
 for a particular hidden state sequence Q = q1 , q2 , ..., qT and an observation sequence
 O = o1 , o2 , ..., oT , the likelihood of the observation sequence is
 T
 Y
 P(O|Q) = P(oi |qi ) (A.6)
 i=1

 The computation of the forward probability for our ice-cream observation 3 1 3 from
 one possible hidden state sequence hot hot cold is shown in Eq. A.7. Figure A.3
 shows a graphic representation of this computation.

 P(3 1 3|hot hot cold) = P(3|hot) × P(1|hot) × P(3|cold) (A.7)

 hot hot cold
 .4 .2 .1

 3 1 3
 Figure A.3 The computation of the observation likelihood for the ice-cream events 3 1 3
 given the hidden state sequence hot hot cold.

 But of course, we don’t actually know what the hidden state (weather) sequence
 was. We’ll need to compute the probability of ice-cream events 3 1 3 instead by
 1 In a variant of HMMs called segmental HMMs (in speech recognition) or semi-HMMs (in text processing) this one-to-one mapping between the length of the hidden state sequence and the length of the
 observation sequence does not hold.
 A.3 • L IKELIHOOD C OMPUTATION : T HE F ORWARD A LGORITHM 5

 summing over all possible weather sequences, weighted by their probability. First,
 let’s compute the joint probability of being in a particular weather sequence Q and
 generating a particular sequence O of ice-cream events. In general, this is
 T
 Y T
 Y
 P(O, Q) = P(O|Q) × P(Q) = P(oi |qi ) × P(qi |qi−1 ) (A.8)
 i=1 i=1

 The computation of the joint probability of our ice-cream observation 3 1 3 and one
 possible hidden state sequence hot hot cold is shown in Eq. A.9. Figure A.4 shows
 a graphic representation of this computation.

 P(3 1 3, hot hot cold) = P(hot|start) × P(hot|hot) × P(cold|hot)
 ×P(3|hot) × P(1|hot) × P(3|cold) (A.9)

 .6 .4
 hot hot cold
 .4 .2 .1

 3 1 3
 Figure A.4 The computation of the joint probability of the ice-cream events 3 1 3 and the
 hidden state sequence hot hot cold.

 Now that we know how to compute the joint probability of the observations
 with a particular hidden state sequence, we can compute the total probability of the
 observations just by summing over all possible hidden state sequences:
 X X
 P(O) = P(O, Q) = P(O|Q)P(Q) (A.10)
 Q Q

 For our particular case, we would sum over the eight 3-event sequences cold cold
 cold, cold cold hot, that is,

 P(3 1 3) = P(3 1 3, cold cold cold) + P(3 1 3, cold cold hot) + P(3 1 3, hot hot cold) + ...

 For an HMM with N hidden states and an observation sequence of T observations, there are N T possible hidden sequences. For real tasks, where N and T are
 both large, N T is a very large number, so we cannot compute the total observation
 likelihood by computing a separate observation likelihood for each hidden state sequence and then summing them.
 Instead of using such an extremely exponential algorithm, we use an efficient
 forward
algorithm O(N 2 T ) algorithm called the forward algorithm. The forward algorithm is a kind
 of dynamic programming algorithm, that is, an algorithm that uses a table to store
 intermediate values as it builds up the probability of the observation sequence. The
 forward algorithm computes the observation probability by summing over the probabilities of all possible hidden state paths that could generate the observation sequence, but it does so efficiently by implicitly folding each of these paths into a
 single forward trellis.
 Figure A.5 shows an example of the forward trellis for computing the likelihood
 of 3 1 3 given the hidden state sequence hot hot cold.
6 A PPENDIX A • H IDDEN M ARKOV M ODELS

 α1(2)=.32 α2(2)= .32*.12 + .02*.1 = .0404

 P(H|H) * P(1|H)
 q2 H H P(C .6 * .2 H H
 |H)
 .4 * P(1|C
 H) .5 )

 |H) α2(1) = .32*.2 + .02*.25 = .069
 * . (3|

 α1(1) = .02 P(1
 )* 2
 .8 t)*P

 |C .

 P(H .5 *
 r
 sta

 q1 P(C|C) * P(1|C)
 C C C C
 H|

 ) .5 * .5
 P(

 3 |C
 *P(
 rt) .1
 |sta .2 *
 C
 P( 1 3
 π
 o1 o2 o3

 t
Figure A.5 The forward trellis for computing the total observation likelihood for the ice-cream events 3 1 3.
Hidden states are in circles, observations in squares. The figure shows the P computation of αt ( j) for two states at
two time steps. The computation in each cell follows Eq. A.12: αt ( j) = N i=1 αt−1 (i)ai j b j (ot ). The resulting
probability expressed in each cell is Eq. A.11: αt ( j) = P(o1 , o2 . . . ot , qt = j|λ ).

 Each cell of the forward algorithm trellis αt ( j) represents the probability of being in state j after seeing the first t observations, given the automaton λ . The value
 of each cell αt ( j) is computed by summing over the probabilities of every path that
 could lead us to this cell. Formally, each cell expresses the following probability:

 αt ( j) = P(o1 , o2 . . . ot , qt = j|λ ) (A.11)

 Here, qt = j means “the t th state in the sequence of states is state j”. We compute
 this probability αt ( j) by summing over the extensions of all the paths that lead to
 the current cell. For a given state q j at time t, the value αt ( j) is computed as
 N
 X
 αt ( j) = αt−1 (i)ai j b j (ot ) (A.12)
 i=1

 The three factors that are multiplied in Eq. A.12 in extending the previous paths
 to compute the forward probability at time t are

 αt−1 (i) the previous forward path probability from the previous time step
 ai j the transition probability from previous state qi to current state q j
 b j (ot ) the state observation likelihood of the observation symbol ot given
 the current state j

 Consider the computation in Fig. A.5 of α2 (2), the forward probability of being
 at time step 2 in state 2 having generated the partial observation 3 1. We compute by
 extending the α probabilities from time step 1, via two paths, each extension consisting of the three factors above: α1 (1) × P(H|C) × P(1|H) and α1 (2) × P(H|H) ×
 P(1|H).
 Figure A.6 shows another visualization of this induction step for computing the
 value in one new cell of the trellis.
 We give two formal definitions of the forward algorithm: the pseudocode in
 Fig. A.7 and a statement of the definitional recursion here.
 A.3 • L IKELIHOOD C OMPUTATION : T HE F ORWARD A LGORITHM 7

 αt-2(N) αt-1(N)

 qN qN qN
 aNj αt(j)= Σi αt-1(i) aij bj(ot)

 qj

 αt-2(3) αt-1(3) a3j
 q3 q3 q3
 a2j
 αt-2(2) αt-1(2)
 bj(ot)
 q2 q2 a1j q2 q2

 αt-2(1) αt-1(1)

 q1 q1 q1 q1

 ot-2 ot-1 ot ot+1

Figure A.6 Visualizing the computation of a single element αt (i) in the trellis by summing
all the previous values αt−1 , weighted by their transition probabilities a, and multiplying by
the observation probability bi (ot ). For many applications of HMMs, many of the transition
probabilities are 0, so not all previous states will contribute to the forward probability of the
current state. Hidden states are in circles, observations in squares. Shaded nodes are included
in the probability computation for αt (i).

 function F ORWARD(observations of len T, state-graph of len N) returns forward-prob

 create a probability matrix forward[N,T]
 for each state s from 1 to N do ; initialization step
 forward[s,1] ← πs ∗ bs (o1 )
 for each time step t from 2 to T do ; recursion step
 for each state s from 1 to N do
 N
 forward[s0 ,t − 1] ∗ as0 ,s ∗ bs (ot )
 X
 forward[s,t] ←
 s0 =1
 N
 X
 forwardprob ← forward[s, T ] ; termination step
 s=1
 return forwardprob

Figure A.7 The forward algorithm, where forward[s,t] represents αt (s).

 1. Initialization:

 α1 ( j) = π j b j (o1 ) 1 ≤ j ≤ N

 2. Recursion:
 N
 X
 αt ( j) = αt−1 (i)ai j b j (ot ); 1 ≤ j ≤ N, 1 < t ≤ T
 i=1

 3. Termination:
 N
 X
 P(O|λ ) = αT (i)
 i=1
8 A PPENDIX A • H IDDEN M ARKOV M ODELS

A.4 Decoding: The Viterbi Algorithm
 For any model, such as an HMM, that contains hidden variables, the task of determining which sequence of variables is the underlying source of some sequence of
 decoding observations is called the decoding task. In the ice-cream domain, given a sequence
 of ice-cream observations 3 1 3 and an HMM, the task of the decoder is to find the
 best hidden weather sequence (H H H). More formally,
 Decoding: Given as input an HMM λ = (A, B) and a sequence of observations O = o1 , o2 , ..., oT , find the most probable sequence of states
 Q = q1 q2 q3 . . . qT .
 We might propose to find the best sequence as follows: For each possible hidden state sequence (HHH, HHC, HCH, etc.), we could run the forward algorithm
 and compute the likelihood of the observation sequence given that hidden state sequence. Then we could choose the hidden state sequence with the maximum observation likelihood. It should be clear from the previous section that we cannot do this
 because there are an exponentially large number of state sequences.
 Instead, the most common decoding algorithms for HMMs is the Viterbi algo-
Viterbi
 algorithm rithm. Like the forward algorithm, Viterbi is a kind of dynamic programming
 that makes uses of a dynamic programming trellis. Viterbi also strongly resembles
 another dynamic programming variant, the minimum edit distance algorithm of
 Chapter 2.

 v1(2)=.32 v2(2)= max(.32*.12, .02*.10) = .038

 P(H|H) * P(1|H)
 q2 H H P(C .6 * .2 H H
 |H)
 .4 * P(1|C
 .5 )
 H)

 |H) v2(1) = max(.32*.20, .02*.25) = .064
 * . (3|

 v1(1) = .02 P(1
 )* 2
 .8 t)*P

 |C .

 P(H .5 *
 r
 sta

 q1 P(C|C) * P(1|C)
 C C C C
 H|

 ) .5 * .5
 P(

 3 |C
 P(*
 t)
 ar .1
 C |st .2 *
 P (

 π 3 1 3

 o1 o2 o3

 t
Figure A.8 The Viterbi trellis for computing the best path through the hidden state space for the ice-cream
eating events 3 1 3. Hidden states are in circles, observations in squares. White (unfilled) circles indicate illegal
transitions. The figure shows the computation of vt ( j) for two states at two time steps. The computation in each
cell follows Eq. A.14: vt ( j) = max1≤i≤N−1 vt−1 (i) ai j b j (ot ). The resulting probability expressed in each cell
is Eq. A.13: vt ( j) = P(q0 , q1 , . . . , qt−1 , o1 , o2 , . . . , ot , qt = j|λ ).

 Figure A.8 shows an example of the Viterbi trellis for computing the best hidden
 state sequence for the observation sequence 3 1 3. The idea is to process the observation sequence left to right, filling out the trellis. Each cell of the trellis, vt ( j),
 represents the probability that the HMM is in state j after seeing the first t observations and passing through the most probable state sequence q1 , ..., qt−1 , given the
 A.4 • D ECODING : T HE V ITERBI A LGORITHM 9

 automaton λ . The value of each cell vt ( j) is computed by recursively taking the
 most probable path that could lead us to this cell. Formally, each cell expresses the
 probability

 vt ( j) = max P(q1 ...qt−1 , o1 , o2 . . . ot , qt = j|λ ) (A.13)
 q1 ,...,qt−1

 Note that we represent the most probable path by taking the maximum over all
 possible previous state sequences max . Like other dynamic programming algoq1 ,...,qt−1
 rithms, Viterbi fills each cell recursively. Given that we had already computed the
 probability of being in every state at time t − 1, we compute the Viterbi probability
 by taking the most probable of the extensions of the paths that lead to the current
 cell. For a given state q j at time t, the value vt ( j) is computed as
 N
 vt ( j) = max vt−1 (i) ai j b j (ot ) (A.14)
 i=1

 The three factors that are multiplied in Eq. A.14 for extending the previous paths to
 compute the Viterbi probability at time t are

 vt−1 (i) the previous Viterbi path probability from the previous time step
 ai j the transition probability from previous state qi to current state q j
 b j (ot ) the state observation likelihood of the observation symbol ot given
 the current state j

 function V ITERBI(observations of len T,state-graph of len N) returns best-path, path-prob

 create a path probability matrix viterbi[N,T]
 for each state s from 1 to N do ; initialization step
 viterbi[s,1] ← πs ∗ bs (o1 )
 backpointer[s,1] ← 0
 for each time step t from 2 to T do ; recursion step
 for each state s from 1 to N do
 N
 viterbi[s,t] ← max
 viterbi[s0 ,t − 1] ∗ as0 ,s ∗ bs (ot )
 s =1
 N
 backpointer[s,t] ← argmax viterbi[s0 ,t − 1] ∗ as0 ,s ∗ bs (ot )
 s0 =1
 N
 bestpathprob ← max viterbi[s, T ] ; termination step
 s=1
 N
 bestpathpointer ← argmax viterbi[s, T ] ; termination step
 s=1
 bestpath ← the path starting at state bestpathpointer, that follows backpointer[] to states back in time
 return bestpath, bestpathprob

Figure A.9 Viterbi algorithm for finding optimal sequence of hidden states. Given an observation sequence
and an HMM λ = (A, B), the algorithm returns the state path through the HMM that assigns maximum likelihood
to the observation sequence.

 Figure A.9 shows pseudocode for the Viterbi algorithm. Note that the Viterbi
 algorithm is identical to the forward algorithm except that it takes the max over the
 previous path probabilities whereas the forward algorithm takes the sum. Note also
 that the Viterbi algorithm has one component that the forward algorithm doesn’t
10 A PPENDIX A • H IDDEN M ARKOV M ODELS

 have: backpointers. The reason is that while the forward algorithm needs to produce an observation likelihood, the Viterbi algorithm must produce a probability and
 also the most likely state sequence. We compute this best state sequence by keeping
 track of the path of hidden states that led to each state, as suggested in Fig. A.10, and
 Viterbi then at the end backtracing the best path to the beginning (the Viterbi backtrace).
 backtrace

 v1(2)=.32 v2(2)= max(.32*.12, .02*.10) = .038

 P(H|H) * P(1|H)
 q2 H H P(C .6 * .2
 H H
 |H)
 .4 * P(1|C
 .5 )
 )
 )

 (1|H
 * .4 3|H

 *P v2(1) = max(.32*.20, .02*.25) = .064
 )
 .8 t)*P(

 v1(1) = .02 |C .2
 P(H .5 *
 r
 sta

 q1 P(C|C) * P(1|C)
 C C C C
 H|

 ) .5 * .5
 P(

 3 |C
 * P(
 rt) .1
 |sta .2 *
 C
 P(
 3 1 3
 π
 o1 o2 o3

 t
Figure A.10 The Viterbi backtrace. As we extend each path to a new state account for the next observation,
we keep a backpointer (shown with broken lines) to the best path that led us to this state.

 Finally, we can give a formal definition of the Viterbi recursion as follows:

 1. Initialization:

 v1 ( j) = π j b j (o1 ) 1≤ j≤N
 bt1 ( j) = 0 1≤ j≤N

 2. Recursion
 N
 vt ( j) = max vt−1 (i) ai j b j (ot ); 1 ≤ j ≤ N, 1 < t ≤ T
 i=1
 N
 btt ( j) = argmax vt−1 (i) ai j b j (ot ); 1 ≤ j ≤ N, 1 < t ≤ T
 i=1

 3. Termination:
 N
 The best score: P∗ = max vT (i)
 i=1
 N
 The start of backtrace: qT ∗ = argmax vT (i)
 i=1

A.5 HMM Training: The Forward-Backward Algorithm
 We turn to the third problem for HMMs: learning the parameters of an HMM, that
 is, the A and B matrices. Formally,
 Learning: Given an observation sequence O and the set of possible
 states in the HMM, learn the HMM parameters A and B.
 A.5 • HMM T RAINING : T HE F ORWARD -BACKWARD A LGORITHM 11

 The input to such a learning algorithm would be an unlabeled sequence of observations O and a vocabulary of potential hidden states Q. Thus, for the ice cream
 task, we would start with a sequence of observations O = {1, 3, 2, ..., } and the set of
 hidden states H and C.
 Forward- The standard algorithm for HMM training is the forward-backward, or Baumbackward
Baum-Welch Welch algorithm (Baum, 1972), a special case of the Expectation-Maximization
 EM or EM algorithm (Dempster et al., 1977). The algorithm will let us train both the
 transition probabilities A and the emission probabilities B of the HMM. EM is an
 iterative algorithm, computing an initial estimate for the probabilities, then using
 those estimates to compute a better estimate, and so on, iteratively improving the
 probabilities that it learns.
 Let us begin by considering the much simpler case of training a fully visible
 Markov model, where we know both the temperature and the ice cream count for
 every day. That is, imagine we see the following set of input observations and magically knew the aligned hidden state sequences:
 3 3 2 1 1 2 1 2 3
 hot hot cold cold cold cold cold hot hot
 This would easily allow us to compute the HMM parameters just by maximum
 likelihood estimation from the training data. First, we can compute π from the count
 of the 3 initial hidden states:

 πh = 1/3 πc = 2/3

 Next we can directly compute the A matrix from the transitions, ignoring the final
 hidden states:

 p(hot|hot) = 2/3 p(cold|hot) = 1/3
 p(cold|cold) = 2/3 p(hot|cold) = 1/3

 and the B matrix:

 P(1|hot) = 0/4 = 0 p(1|cold) = 3/5 = .6
 P(2|hot) = 1/4 = .25 p(2|cold = 2/5 = .4
 P(3|hot) = 3/4 = .75 p(3|cold) = 0

 For a real HMM, we cannot compute these counts directly from an observation
 sequence since we don’t know which path of states was taken through the machine
 for a given input. For example, suppose I didn’t tell you the temperature on day 2,
 and you had to guess it, but you (magically) had the above probabilities, and the
 temperatures on the other days. You could do some Bayesian arithmetic with all the
 other probabilities to get estimates of the likely temperature on that missing day, and
 use those to get expected counts for the temperatures for day 2.
 But the real problem is even harder: we don’t know the counts of being in any
 of the hidden states!! The Baum-Welch algorithm solves this by iteratively estimating the counts. We will start with an estimate for the transition and observation
 probabilities and then use these estimated probabilities to derive better and better
 probabilities. And we’re going to do this by computing the forward probability for
 an observation and then dividing that probability mass among all the different paths
 that contributed to this forward probability.
 To understand the algorithm, we need to define a useful probability related to the
 backward
 probability forward probability and called the backward probability. The backward probabil-
12 A PPENDIX A • H IDDEN M ARKOV M ODELS

 ity β is the probability of seeing the observations from time t + 1 to the end, given
 that we are in state i at time t (and given the automaton λ ):

 βt (i) = P(ot+1 , ot+2 . . . oT |qt = i, λ ) (A.15)
 It is computed inductively in a similar manner to the forward algorithm.

 1. Initialization:

 βT (i) = 1, 1 ≤ i ≤ N

 2. Recursion
 N
 X
 βt (i) = ai j b j (ot+1 ) βt+1 ( j), 1 ≤ i ≤ N, 1 ≤ t < T
 j=1

 3. Termination:
 N
 X
 P(O|λ ) = π j b j (o1 ) β1 ( j)
 j=1

 Figure A.11 illustrates the backward induction step.

 βt+1(N)
 qN βt(i)= Σj βt+1(j) aij bj(ot+1) qN
 aiN

 qi

 ai3 βt+1(3)
 q3 q3
 ai2 bN(ot+1)
 βt+1(2)
 b3(ot+1)
 q2 q2 ai1 q2 b2(ot+1)
 βt+1(1)
 b1(ot+1)
 q1 q1 q1

 ot-1 ot ot+1

 Figure A.11 The computation of βt (i) by summing all the successive values βt+1 ( j)
 weighted by their transition probabilities ai j and their observation probabilities b j (ot+1 ).

 We are now ready to see how the forward and backward probabilities can help
 compute the transition probability ai j and observation probability bi (ot ) from an observation sequence, even though the actual path taken through the model is hidden.
 Let’s begin by seeing how to estimate âi j by a variant of simple maximum likelihood estimation:
 expected number of transitions from state i to state j
 âi j = (A.16)
 expected number of transitions from state i
 How do we compute the numerator? Here’s the intuition. Assume we had some
 estimate of the probability that a given transition i → j was taken at a particular
 point in time t in the observation sequence. If we knew this probability for each
 particular time t, we could sum over all times t to estimate the total count for the
 transition i → j.
 A.5 • HMM T RAINING : T HE F ORWARD -BACKWARD A LGORITHM 13

 More formally, let’s define the probability ξt as the probability of being in state
i at time t and state j at time t + 1, given the observation sequence and of course the
model:

 ξt (i, j) = P(qt = i, qt+1 = j|O, λ ) (A.17)

To compute ξt , we first compute a probability which is similar to ξt , but differs in
including the probability of the observation; note the different conditioning of O
from Eq. A.17:

 not-quite-ξt (i, j) = P(qt = i, qt+1 = j, O|λ ) (A.18)

 si sj

 aijbj(ot+1)

 αt(i) βt+1(j)

 ot-1 ot ot+1 ot+2

Figure A.12 Computation of the joint probability of being in state i at time t and state j at
time t + 1. The figure shows the various probabilities that need to be combined to produce
P(qt = i, qt+1 = j, O|λ ): the α and β probabilities, the transition probability ai j and the
observation probability b j (ot+1 ). After Rabiner (1989) which is ©1989 IEEE.

 Figure A.12 shows the various probabilities that go into computing not-quite-ξt :
the transition probability for the arc in question, the α probability before the arc, the
β probability after the arc, and the observation probability for the symbol just after
the arc. These four are multiplied together to produce not-quite-ξt as follows:

 not-quite-ξt (i, j) = αt (i) ai j b j (ot+1 )βt+1 ( j) (A.19)

To compute ξt from not-quite-ξt , we follow the laws of probability and divide by
P(O|λ ), since

 P(X,Y |Z)
 P(X|Y, Z) = (A.20)
 P(Y |Z)
 The probability of the observation given the model is simply the forward probability of the whole utterance (or alternatively, the backward probability of the whole
utterance):
 X N
 P(O|λ ) = αt ( j)βt ( j) (A.21)
 j=1
14 A PPENDIX A • H IDDEN M ARKOV M ODELS

 So, the final equation for ξt is
 αt (i) ai j b j (ot+1 )βt+1 ( j)
 ξt (i, j) = PN (A.22)
 j=1 αt ( j)βt ( j)

 The expected number of transitions from state i to state j is then the sum over all
 t of ξ . For our estimate of ai j in Eq. A.16, we just need one more thing: the total
 expected number of transitions from state i. We can get this by summing over all
 transitions out of state i. Here’s the final formula for âi j :
 PT −1
 ξt (i, j)
 âi j = PT −1t=1
 PN (A.23)
 t=1 k=1 ξt (i, k)

 We also need a formula for recomputing the observation probability. This is the
 probability of a given symbol vk from the observation vocabulary V , given a state j:
 b̂ j (vk ). We will do this by trying to compute

 expected number of times in state j and observing symbol vk
 b̂ j (vk ) = (A.24)
 expected number of times in state j
 For this, we will need to know the probability of being in state j at time t, which
 we will call γt ( j):

 γt ( j) = P(qt = j|O, λ ) (A.25)
 Once again, we will compute this by including the observation sequence in the
 probability:
 P(qt = j, O|λ )
 γt ( j) = (A.26)
 P(O|λ )

 sj

 αt(j) βt(j)

 ot-1 ot ot+1

 Figure A.13 The computation of γt ( j), the probability of being in state j at time t. Note
 that γ is really a degenerate case of ξ and hence this figure is like a version of Fig. A.12 with
 state i collapsed with state j. After Rabiner (1989) which is ©1989 IEEE.

 As Fig. A.13 shows, the numerator of Eq. A.26 is just the product of the forward
 probability and the backward probability:
 αt ( j)βt ( j)
 γt ( j) = (A.27)
 P(O|λ )
 A.5 • HMM T RAINING : T HE F ORWARD -BACKWARD A LGORITHM 15

 We are ready to compute b. For the numerator, we sum γt ( j) for all time steps
 t in which the observation ot is the symbol vk that we are interested in. For the
 denominator, we sum γt ( j) over all time steps t. The result is the
 PTpercentage of the
 times that we were in state j and saw symbol vk (the notation t=1 s.t.Ot =vk means
 “sum over all t for which the observation at time t was vk ”):
 PT
 t=1 s.t.Ot =vk γt ( j)
 b̂ j (vk ) = PT (A.28)
 t=1 γt ( j)

 We now have ways in Eq. A.23 and Eq. A.28 to re-estimate the transition A and observation B probabilities from an observation sequence O, assuming that we already
 have a previous estimate of A and B.
 These re-estimations form the core of the iterative forward-backward algorithm.
 The forward-backward algorithm (Fig. A.14) starts with some initial estimate of the
 HMM parameters λ = (A, B). We then iteratively run two steps. Like other cases of
 the EM (expectation-maximization) algorithm, the forward-backward algorithm has
E-step two steps: the expectation step, or E-step, and the maximization step, or M-step.
M-step In the E-step, we compute the expected state occupancy count γ and the expected
 state transition count ξ from the earlier A and B probabilities. In the M-step, we use
 γ and ξ to recompute new A and B probabilities.

 function F ORWARD -BACKWARD(observations of len T, output vocabulary V, hidden
 state set Q) returns HMM=(A,B)

 initialize A and B
 iterate until convergence
 E-step
 α ( j)βt ( j)
 γt ( j) = t ∀ t and j
 αT (qF )
 α (i) ai j b j (ot+1 )βt+1 ( j)
 ξt (i, j) = t ∀ t, i, and j
 αT (qF )
 M-step
 T
 X −1
 ξt (i, j)
 t=1
 âi j = T −1 X N
 X
 ξt (i, k)
 t=1 k=1
 XT
 γt ( j)
 t=1s.t. Ot =vk
 b̂ j (vk ) = T
 X
 γt ( j)
 t=1
 return A, B

 Figure A.14 The forward-backward algorithm.

 Although in principle the forward-backward algorithm can do completely unsupervised learning of the A and B parameters, in practice the initial conditions are
 very important. For this reason the algorithm is often given extra information. For
 example, for HMM-based speech recognition, the HMM structure is often set by
 hand, and only the emission (B) and (non-zero) A transition probabilities are trained
 from a set of observation sequences O.
16 A PPENDIX A • H IDDEN M ARKOV M ODELS

A.6 Summary
 This chapter introduced the hidden Markov model for probabilistic sequence classification.
 • Hidden Markov models (HMMs) are a way of relating a sequence of observations to a sequence of hidden classes or hidden states that explain the
 observations.
 • The process of discovering the sequence of hidden states, given the sequence
 of observations, is known as decoding or inference. The Viterbi algorithm is
 commonly used for decoding.
 • The parameters of an HMM are the A transition probability matrix and the B
 observation likelihood matrix. Both can be trained with the Baum-Welch or
 forward-backward algorithm.

Historical Notes
 As we discussed in Chapter 17, Markov chains were first used by Markov (1913)
 (translation Markov 2006), to predict whether an upcoming letter in Pushkin’s Eugene Onegin would be a vowel or a consonant. The hidden Markov model was developed by Baum and colleagues at the Institute for Defense Analyses in Princeton
 (Baum and Petrie 1966, Baum and Eagon 1967).
 The Viterbi algorithm was first applied to speech and language processing in the
 context of speech recognition by Vintsyuk (1968) but has what Kruskal (1983) calls
 a “remarkable history of multiple independent discovery and publication”. Kruskal
 and others give at least the following independently-discovered variants of the algorithm published in four separate fields:

 Citation Field
 Viterbi (1967) information theory
 Vintsyuk (1968) speech processing
 Needleman and Wunsch (1970) molecular biology
 Sakoe and Chiba (1971) speech processing
 Sankoff (1972) molecular biology
 Reichert et al. (1973) molecular biology
 Wagner and Fischer (1974) computer science

 The use of the term Viterbi is now standard for the application of dynamic programming to any kind of probabilistic maximization problem in speech and language
 processing. For non-probabilistic problems (such as for minimum edit distance), the
 plain term dynamic programming is often used. Forney, Jr. (1973) wrote an early
 survey paper that explores the origin of the Viterbi algorithm in the context of information and communications theory.
 Our presentation of the idea that hidden Markov models should be characterized
 by three fundamental problems was modeled after an influential tutorial by Rabiner
 (1989), which was itself based on tutorials by Jack Ferguson of IDA in the 1960s.
 Jelinek (1997) and Rabiner and Juang (1993) give very complete descriptions of the
 forward-backward algorithm as applied to the speech recognition problem. Jelinek
 (1997) also shows the relationship between forward-backward and EM.
 Historical Notes 17

Baum, L. E. 1972. An inequality and associated maximiza- Vintsyuk, T. K. 1968. Speech discrimination by dynamic
 tion technique in statistical estimation for probabilistic programming. Cybernetics, 4(1):52–57. Original Rusfunctions of Markov processes. Inequalities III: Pro- sian: Kibernetika 4(1):81-88. 1968.
 ceedings of the 3rd Symposium on Inequalities. Academic Viterbi, A. J. 1967. Error bounds for convolutional codes and
 Press. an asymptotically optimum decoding algorithm. IEEE
Baum, L. E. and J. A. Eagon. 1967. An inequality with appli- Transactions on Information Theory, IT-13(2):260–269.
 cations to statistical estimation for probabilistic functions Wagner, R. A. and M. J. Fischer. 1974. The string-to-string
 of Markov processes and to a model for ecology. Bulletin correction problem. Journal of the ACM, 21:168–173.
 of the American Mathematical Society, 73(3):360–363.
Baum, L. E. and T. Petrie. 1966. Statistical inference for
 probabilistic functions of finite-state Markov chains. Annals of Mathematical Statistics, 37(6):1554–1563.
Dempster, A. P., N. M. Laird, and D. B. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, 39(1):1–
 21.
Eisner, J. 2002. An interactive spreadsheet for teaching
 the forward-backward algorithm. Proceedings of the
 ACL Workshop on Effective Tools and Methodologies for
 Teaching NLP and CL.
Forney, Jr., G. D. 1973. The Viterbi algorithm. Proceedings
 of the IEEE, 61(3):268–278.
Jelinek, F. 1997. Statistical Methods for Speech Recognition.
 MIT Press.
Kruskal, J. B. 1983. An overview of sequence comparison.
 In D. Sankoff and J. B. Kruskal, eds, Time Warps, String
 Edits, and Macromolecules: The Theory and Practice of
 Sequence Comparison, 1–44. Addison-Wesley.
Markov, A. A. 1913. Essai d’une recherche statistique sur
 le texte du roman “Eugene Onegin” illustrant la liaison
 des epreuve en chain (‘Example of a statistical investigation of the text of “Eugene Onegin” illustrating the dependence between samples in chain’). Izvistia Imperatorskoi Akademii Nauk (Bulletin de l’Académie Impériale
 des Sciences de St.-Pétersbourg), 7:153–162.
Markov, A. A. 2006. Classical text in translation: A. A.
 Markov, an example of statistical investigation of the text
 Eugene Onegin concerning the connection of samples in
 chains. Science in Context, 19(4):591–600. Translated by
 David Link.
Needleman, S. B. and C. D. Wunsch. 1970. A general
 method applicable to the search for similarities in the
 amino-acid sequence of two proteins. Journal of Molecular Biology, 48:443–453.
Rabiner, L. R. 1989. A tutorial on hidden Markov models
 and selected applications in speech recognition. Proceedings of the IEEE, 77(2):257–286.
Rabiner, L. R. and B. H. Juang. 1993. Fundamentals of
 Speech Recognition. Prentice Hall.
Reichert, T. A., D. N. Cohen, and A. K. C. Wong. 1973.
 An application of information theory to genetic mutations
 and the matching of polypeptide sequences. Journal of
 Theoretical Biology, 42:245–261.
Sakoe, H. and S. Chiba. 1971. A dynamic programming
 approach to continuous speech recognition. Proceedings
 of the Seventh International Congress on Acoustics, volume 3. Akadémiai Kiadó.
Sankoff, D. 1972. Matching sequences under deletioninsertion constraints. Proceedings of the National
 Academy of Sciences, 69:4–6.
