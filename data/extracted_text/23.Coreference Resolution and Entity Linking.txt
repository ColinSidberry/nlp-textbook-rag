rights reserved. Draft of August 24, 2025.

CHAPTER

 Coreference Resolution and
23 Entity Linking
 and even Stigand, the patriotic archbishop of Canterbury, found it advisable‚Äì‚Äù‚Äô
 ‚ÄòFound WHAT?‚Äô said the Duck.
 ‚ÄòFound IT,‚Äô the Mouse replied rather crossly: ‚Äòof course you know what ‚Äúit‚Äùmeans.‚Äô
 ‚ÄòI know what ‚Äúit‚Äùmeans well enough, when I find a thing,‚Äô said the Duck: ‚Äòit‚Äôs generally a frog or a worm. The question is, what did the archbishop find?‚Äô

 Lewis Carroll, Alice in Wonderland

 An important component of language processing is knowing who is being talked
 about in a text. Consider the following passage:
 (23.1) Victoria Chen, CFO of Megabucks Banking, saw her pay jump to $2.3
 million, as the 38-year-old became the company‚Äôs president. It is widely
 known that she came to Megabucks from rival Lotsabucks.
 Each of the underlined phrases in this passage is used by the writer to refer to
 a person named Victoria Chen. We call linguistic expressions like her or Victoria
 mention Chen mentions or referring expressions, and the discourse entity that is referred
 referent to (Victoria Chen) the referent. (To distinguish between referring expressions and
 their referents, we italicize the former.)1 Two or more referring expressions that are
 corefer used to refer to the same discourse entity are said to corefer; thus, Victoria Chen
 and she corefer in (23.1).
 Coreference is an important component of natural language processing. A dialogue system that has just told the user ‚ÄúThere is a 2pm flight on United and a 4pm
 one on Cathay Pacific‚Äù must know which flight the user means by ‚ÄúI‚Äôll take the second one‚Äù. A question answering system that uses Wikipedia to answer a question
 about Marie Curie must know who she was in the sentence ‚ÄúShe was born in Warsaw‚Äù. And a machine translation system translating from a language like Spanish, in
 which pronouns can be dropped, must use coreference from the previous sentence to
 decide whether the Spanish sentence ‚Äò‚ÄúMe encanta el conocimiento‚Äù, dice.‚Äô should
 be translated as ‚Äò‚ÄúI love knowledge‚Äù, he says‚Äô, or ‚Äò‚ÄúI love knowledge‚Äù, she says‚Äô.
 Indeed, this example comes from an actual news article in El Paƒ±ÃÅs about a female
 professor and was mistranslated as ‚Äúhe‚Äù in machine translation because of inaccurate
 coreference resolution (Schiebinger, 2013).
 Natural language processing systems (and humans) interpret linguistic expresdiscourse sions with respect to a discourse model (Karttunen, 1969). A discourse model
 model
 (Fig. 23.1) is a mental model that the understander builds incrementally when interpreting a text, containing representations of the entities referred to in the text,
 as well as properties of the entities and relations among them. When a referent is
 evoked first mentioned in a discourse, we say that a representation for it is evoked into the
 accessed model. Upon subsequent mention, this representation is accessed from the model.
 1 As a convenient shorthand, we sometimes speak of a referring expression referring to a referent, e.g.,
 saying that she refers to Victoria Chen. However, the reader should keep in mind that what we really
 mean is that the speaker is performing the act of referring to Victoria Chen by uttering she.
2 C HAPTER 23 ‚Ä¢ C OREFERENCE R ESOLUTION AND E NTITY L INKING

 Discourse Model

 Lotsabucks
 V
 Megabucks
 $ pay refer (access)
 refer (evoke)
 ‚ÄúVictoria‚Äù corefer ‚Äúshe‚Äù

 Reference in a text to an entity that has been previously introduced into the
 anaphora discourse is called anaphora, and the referring expression used is said to be an
 anaphor anaphor, or anaphoric.2 In passage (23.1), the pronouns she and her and the definite NP the 38-year-old are therefore anaphoric. The anaphor corefers with a prior
 antecedent mention (in this case Victoria Chen) that is called the antecedent. Not every referring expression is an antecedent. An entity that has only a single mention in a text
 singleton (like Lotsabucks in (23.1)) is called a singleton.
 coreference In this chapter we focus on the task of coreference resolution. Coreference
 resolution
 resolution is the task of determining whether two mentions corefer, by which we
 mean they refer to the same entity in the discourse model (the same discourse entity).
 coreference The set of coreferring expressions is often called a coreference chain or a cluster.
 chain
 cluster For example, in processing (23.1), a coreference resolution algorithm would need
 to find at least four coreference chains, corresponding to the four entities in the
 discourse model in Fig. 23.1.
 1. {Victoria Chen, her, the 38-year-old, She}
 2. {Megabucks Banking, the company, Megabucks}
 3. {her pay}
 4. {Lotsabucks}
 Note that mentions can be nested; for example the mention her is syntactically
 part of another mention, her pay, referring to a completely different discourse entity.
 Coreference resolution thus comprises two tasks (although they are often performed jointly): (1) identifying the mentions, and (2) clustering them into coreference chains/discourse entities.
 We said that two mentions corefered if they are associated with the same discourse entity. But often we‚Äôd like to go further, deciding which real world entity is
 associated with this discourse entity. For example, the mention Washington might
 refer to the US state, or the capital city, or the person George Washington; the interpretation of the sentence will of course be very different for each of these. The task
 entity linking of entity linking (Ji and Grishman, 2011) or entity resolution is the task of mapping
 a discourse entity to some real-world individual.3 We usually operationalize entity
 2 We will follow the common NLP usage of anaphor to mean any mention that has an antecedent, rather
 than the more narrow usage to mean only mentions (like pronouns) whose interpretation depends on the
 antecedent (under the narrower interpretation, repeated names are not anaphors).
 3 Computational linguistics/NLP thus differs in its use of the term reference from the field of formal
 semantics, which uses the words reference and coreference to describe the relation between a mention
 and a real-world entity. By contrast, we follow the functional linguistics tradition in which a mention
 refers to a discourse entity (Webber, 1978) and the relation between a discourse entity and the real world
 individual requires an additional step of linking.

 linking or resolution by mapping to an ontology: a list of entities in the world, like
 a gazeteer (Appendix F). Perhaps the most common ontology used for this task is
 Wikipedia; each Wikipedia page acts as the unique id for a particular entity. Thus
 the entity linking task of wikification (Mihalcea and Csomai, 2007) is the task of deciding which Wikipedia page corresponding to an individual is being referred to by
 a mention. But entity linking can be done with any ontology; for example if we have
 an ontology of genes, we can link mentions of genes in text to the disambiguated
 gene name in the ontology.
 In the next sections we introduce the task of coreference resolution in more detail, and survey a variety of architectures for resolution. We also introduce two
 architectures for the task of entity linking.
 Before turning to algorithms, however, we mention some important tasks we
 will only touch on briefly at the end of this chapter. First are the famous Winograd
 Schema problems (so-called because they were first pointed out by Terry Winograd
 in his dissertation). These entity coreference resolution problems are designed to be
 too difficult to be solved by the resolution methods we describe in this chapter, and
 the kind of real-world knowledge they require has made them a kind of challenge
 task for natural language processing. For example, consider the task of determining
 the correct antecedent of the pronoun they in the following example:
 (23.2) The city council denied the demonstrators a permit because
 a. they feared violence.
 b. they advocated violence.
 Determining the correct antecedent for the pronoun they requires understanding
 that the second clause is intended as an explanation of the first clause, and also
 that city councils are perhaps more likely than demonstrators to fear violence and
 that demonstrators might be more likely to advocate violence. Solving Winograd
 Schema problems requires finding way to represent or discover the necessary real
 world knowledge.
 A problem we won‚Äôt discuss in this chapter is the related task of event coreferevent ence, deciding whether two event mentions (such as the buy and the acquisition in
 coreference
 these two sentences from the ECB+ corpus) refer to the same event:
 (23.3) AMD agreed to [buy] Markham, Ontario-based ATI for around $5.4 billion
 in cash and stock, the companies announced Monday.
 (23.4) The [acquisition] would turn AMD into one of the world‚Äôs largest providers
 of graphics chips.
 Event mentions are much harder to detect than entity mentions, since they can be verbal as well as nominal. Once detected, the same mention-pair and mention-ranking
 models used for entities are often applied to events.
discourse deixis An even more complex kind of coreference is discourse deixis (Webber, 1988),
 in which an anaphor refers back to a discourse segment, which can be quite hard to
 delimit or categorize, like the examples in (23.5) adapted from Webber (1991):
 (23.5) According to Soleil, Beau just opened a restaurant
 a. But that turned out to be a lie.
 b. But that was false.
 c. That struck me as a funny way to describe the situation.
 The referent of that is a speech act (see Chapter 25) in (23.5a), a proposition in
 (23.5b), and a manner of description in (23.5c). We don‚Äôt give algorithms in this
 chapter for these difficult types of non-nominal antecedents, but see Kolhatkar
 et al. (2018) for a survey.
4 C HAPTER 23 ‚Ä¢ C OREFERENCE R ESOLUTION AND E NTITY L INKING

 We now offer some linguistic background on reference phenomena. We introduce
 the four types of referring expressions (definite and indefinite NPs, pronouns, and
 names), describe how these are used to evoke and access entities in the discourse
 model, and talk about linguistic features of the anaphor/antecedent relation (like
 number/gender agreement, or properties of verb semantics).

 23.1.1 Types of Referring Expressions
 Indefinite Noun Phrases: The most common form of indefinite reference in English is marked with the determiner a (or an), but it can also be marked by a quantifier such as some or even the determiner this. Indefinite reference generally introduces into the discourse context entities that are new to the hearer.
 (23.6) a. Mrs. Martin was so very kind as to send Mrs. Goddard a beautiful goose.
 b. He had gone round one day to bring her some walnuts.
 c. I saw this beautiful cauliflower today.
 Definite Noun Phrases: Definite reference, such as via NPs that use the English
 article the, refers to an entity that is identifiable to the hearer. An entity can be
 identifiable to the hearer because it has been mentioned previously in the text and
 thus is already represented in the discourse model:
 (23.7) It concerns a white stallion which I have sold to an officer. But the pedigree
 of the white stallion was not fully established.
 Alternatively, an entity can be identifiable because it is contained in the hearer‚Äôs
 set of beliefs about the world, or the uniqueness of the object is implied by the
 description itself, in which case it evokes a representation of the referent into the
 discourse model, as in (23.9):
 (23.8) I read about it in the New York Times.
 (23.9) Have you seen the car keys?
 These last uses are quite common; more than half of definite NPs in newswire
 texts are non-anaphoric, often because they are the first time an entity is mentioned
 (Poesio and Vieira 1998, Bean and Riloff 1999).
 Pronouns: Another form of definite reference is pronominalization, used for entities that are extremely salient in the discourse, (as we discuss below):
 (23.10) Emma smiled and chatted as cheerfully as she could,
 cataphora Pronouns can also participate in cataphora, in which they are mentioned before
 their referents are, as in (23.11).
 (23.11) Even before she saw it, Dorothy had been thinking about the Emerald City
 every day.
 Here, the pronouns she and it both occur before their referents are introduced.
 Pronouns also appear in quantified contexts in which they are considered to be
 bound bound, as in (23.12).
 (23.12) Every dancer brought her left arm forward.
 Under the relevant reading, her does not refer to some woman in context, but instead
 behaves like a variable bound to the quantified expression every dancer. We are not
 concerned with the bound interpretation of pronouns in this chapter.
 23.1 ‚Ä¢ C OREFERENCE P HENOMENA : L INGUISTIC BACKGROUND 5

 In some languages, pronouns can appear as clitics attached to a word, like lo
 (‚Äòit‚Äô) in this Spanish example from AnCora (Recasens and Martƒ±ÃÅ, 2010):
 (23.13) La intencioÃÅn es reconocer el gran prestigio que tiene la maratoÃÅn y unirlo
 con esta gran carrera.
 ‚ÄòThe aim is to recognize the great prestige that the Marathon has and join|it
 with this great race.‚Äù
 Demonstrative Pronouns: Demonstrative pronouns this and that can appear either alone or as determiners, for instance, this ingredient, that spice:
 (23.14) I just bought a copy of Thoreau‚Äôs Walden. I had bought one five years ago.
 That one had been very tattered; this one was in much better condition.
 Note that this NP is ambiguous; in colloquial spoken English, it can be indefinite,
 as in (23.6), or definite, as in (23.14).
 Zero Anaphora: Instead of using a pronoun, in some languages (including Chinese, Japanese, and Italian) it is possible to have an anaphor that has no lexical
zero anaphor realization at all, called a zero anaphor or zero pronoun, as in the following Italian
 and Japanese examples from Poesio et al. (2016):
 (23.15) EN [John]i went to visit some friends. On the way [he]i bought some
 wine.
 IT [Giovanni]i andoÃÄ a far visita a degli amici. Per via œÜi comproÃÄ del vino.
 JA [John]i -wa yujin-o houmon-sita. Tochu-de œÜi wain-o ka-tta.
 or this Chinese example:
 (23.16) [Êàë] Ââç‰∏Ä‰ºöÁ≤æÁ•û‰∏äÂ§™Á¥ßÂº†„ÄÇ[0] Áé∞Âú®ÊØîËæÉÂπ≥Èùô‰∫Ü
 [I] was too nervous a while ago. ... [0] am now calmer.
 Zero anaphors complicate the task of mention detection in these languages.
 Names: Names (such as of people, locations, or organizations) can be used to refer
 to both new and old entities in the discourse:
 (23.17) a. Miss Woodhouse certainly had not done him justice.
 b. International Business Machines sought patent compensation
 from Amazon; IBM had previously sued other companies.

 23.1.2 Information Status
 The way referring expressions are used to evoke new referents into the discourse
 (introducing new information), or access old entities from the model (old informainformation tion), is called their information status or information structure. Entities can be
 status
discourse-new discourse-new or discourse-old, and indeed it is common to distinguish at least
discourse-old three kinds of entities informationally (Prince, 1981):
 new NPs:
 brand new NPs: these introduce entities that are discourse-new and hearernew like a fruit or some walnuts.
 unused NPs: these introduce entities that are discourse-new but hearer-old
 (like Hong Kong, Marie Curie, or the New York Times.
 old NPs: also called evoked NPs, these introduce entities that already in the discourse model, hence are both discourse-old and hearer-old, like it in ‚ÄúI went
 to a new restaurant. It was...‚Äù.
6 C HAPTER 23 ‚Ä¢ C OREFERENCE R ESOLUTION AND E NTITY L INKING

 inferrables: these introduce entities that are neither hearer-old nor discourse-old,
 but the hearer can infer their existence by reasoning based on other entities
 that are in the discourse. Consider the following examples:
 (23.18) I went to a superb restaurant yesterday. The chef had just opened it.
 (23.19) Mix flour, butter and water. Knead the dough until shiny.
 Neither the chef nor the dough were in the discourse model based on the first
 bridging sentence of either example, but the reader can make a bridging inference
 inference
 that these entities should be added to the discourse model and associated with
 the restaurant and the ingredients, based on world knowledge that restaurants
 have chefs and dough is the result of mixing flour and liquid (Haviland and
 Clark 1974, Webber and Baldwin 1992, Nissim et al. 2004, Hou et al. 2018).
 The form of an NP gives strong clues to its information status. We often talk
 given-new about an entity‚Äôs position on the given-new dimension, the extent to which the referent is given (salient in the discourse, easier for the hearer to call to mind, predictable
 by the hearer), versus new (non-salient in the discourse, unpredictable) (Chafe 1976,
 accessible Prince 1981, Gundel et al. 1993). A referent that is very accessible (Ariel, 2001)
 i.e., very salient in the hearer‚Äôs mind or easy to call to mind, can be referred to with
 less linguistic material. For example pronouns are used only when the referent has
 salience a high degree of activation or salience in the discourse model.4 By contrast, less
 salient entities, like a new referent being introduced to the discourse, will need to be
 introduced with a longer and more explicit referring expression to help the hearer
 recover the referent.
 Thus when an entity is first introduced into a discourse its mentions are likely
 to have full names, titles or roles, or appositive or restrictive relative clauses, as in
 the introduction of our protagonist in (23.1): Victoria Chen, CFO of Megabucks
 Banking. As an entity is discussed over a discourse, it becomes more salient to the
 hearer and its mentions on average typically becomes shorter and less informative,
 for example with a shortened name (for example Ms. Chen), a definite description
 (the 38-year-old), or a pronoun (she or her) (Hawkins 1978). However, this change
 in length is not monotonic, and is sensitive to discourse structure (Grosz 1977, Reichman 1985, Fox 1993).

 23.1.3 Complications: Non-Referring Expressions
 Many noun phrases or other nominals are not referring expressions, although they
 may bear a confusing superficial resemblance. For example in some of the earliest
 computational work on reference resolution, Karttunen (1969) pointed out that the
 NP a car in the following example does not create a discourse referent:
 (23.20) Janet doesn‚Äôt have a car.
 and cannot be referred back to by anaphoric it or the car:
 (23.21) *It is a Toyota.
 (23.22) *The car is red.
 We summarize here four common types of structures that are not counted as mentions in coreference tasks and hence complicate the task of mention-detection:
 4 Pronouns also usually (but not always) refer to entities that were introduced no further than one or two
 sentences back in the ongoing discourse, whereas definite noun phrases can often refer further back.
 23.1 ‚Ä¢ C OREFERENCE P HENOMENA : L INGUISTIC BACKGROUND 7

 Appositives: An appositional structure is a noun phrase that appears next to a
 head noun phrase, describing the head. In English they often appear in commas, like
 ‚Äúa unit of UAL‚Äù appearing in apposition to the NP United, or CFO of Megabucks
 Banking in apposition to Victoria Chen.
 (23.23) Victoria Chen, CFO of Megabucks Banking, saw ...
 (23.24) United, a unit of UAL, matched the fares.
 Appositional NPs are not referring expressions, instead functioning as a kind of
 supplementary parenthetical description of the head NP. Nonetheless, sometimes it
 is useful to link these phrases to an entity they describe, and so some datasets like
 OntoNotes mark appositional relationships.
 Predicative and Prenominal NPs: Predicative or attributive NPs describe properties of the head noun. In United is a unit of UAL, the NP a unit of UAL describes
 a property of United, rather than referring to a distinct entity. Thus they are not
 marked as mentions in coreference tasks; in our example the NPs $2.3 million and
 the company‚Äôs president, are attributive, describing properties of her pay and the
 38-year-old; Example (23.27) shows a Chinese example in which the predicate NP
 (‰∏≠ÂõΩÊúÄÂ§ßÁöÑÂüéÂ∏Ç; China‚Äôs biggest city) is not a mention.
 (23.25) her pay jumped to $2.3 million
 (23.26) the 38-year-old became the company‚Äôs president
 (23.27) ‰∏äÊµ∑ÊòØ[‰∏≠ÂõΩÊúÄÂ§ßÁöÑÂüéÂ∏Ç] [Shanghai is China‚Äôs biggest city]
 Expletives: Many uses of pronouns like it in English and corresponding pronouns
expletive in other languages are not referential. Such expletive or pleonastic cases include
 clefts it is raining, in idioms like hit it off, or in particular syntactic situations like clefts
 (23.28a) or extraposition (23.28b):
 (23.28) a. It was Emma Goldman who founded Mother Earth
 b. It surprised me that there was a herring hanging on her wall.
 Generics: Another kind of expression that does not refer back to an entity explicitly evoked in the text is generic reference. Consider (23.29).
 (23.29) I love mangos. They are very tasty.
 Here, they refers, not to a particular mango or set of mangos, but instead to the class
 of mangos in general. The pronoun you can also be used generically:
 (23.30) In July in San Francisco you have to wear a jacket.

 23.1.4 Linguistic Properties of the Coreference Relation
 Now that we have seen the linguistic properties of individual referring expressions
 we turn to properties of the antecedent/anaphor pair. Understanding these properties
 is helpful both in designing novel features and performing error analyses.
 Number Agreement: Referring expressions and their referents must generally
 agree in number; English she/her/he/him/his/it are singular, we/us/they/them are plural, and you is unspecified for number. So a plural antecedent like the chefs cannot
 generally corefer with a singular anaphor like she. However, algorithms cannot
 enforce number agreement too strictly. First, semantically plural entities can be referred to by either it or they:
 (23.31) IBM announced a new machine translation product yesterday. They have
 been working on it for 20 years.
8 C HAPTER 23 ‚Ä¢ C OREFERENCE R ESOLUTION AND E NTITY L INKING

 singular they Second, singular they has become much more common, in which they is used to
 describe singular individuals, often useful because they is gender neutral. Although
 recently increasing, singular they is quite old, part of English for many centuries.5

 Person Agreement: English distinguishes between first, second, and third person,
 and a pronoun‚Äôs antecedent must agree with the pronoun in person. Thus a third
 person pronoun (he, she, they, him, her, them, his, her, their) must have a third person
 antecedent (one of the above or any other noun phrase). However, phenomena like
 quotation can cause exceptions; in this example I, my, and she are coreferent:
 (23.32) ‚ÄúI voted for Nader because he was most aligned with my values,‚Äù she said.

 Gender or Noun Class Agreement: In many languages, all nouns have grammatical gender or noun class6 and pronouns generally agree with the grammatical gender
 of their antecedent. In English this occurs only with third-person singular pronouns,
 which distinguish between male (he, him, his), female (she, her), and nonpersonal
 (it) grammatical genders. Non-binary pronouns like ze or hir may also occur in more
 recent texts. Knowing which gender to associate with a name in text can be complex,
 and may require world knowledge about the individual. Some examples:
 (23.33) Maryam has a theorem. She is exciting. (she=Maryam, not the theorem)
 (23.34) Maryam has a theorem. It is exciting. (it=the theorem, not Maryam)

 Binding Theory Constraints: The binding theory is a name for syntactic constraints on the relations between a mention and an antecedent in the same sentence
 reflexive (Chomsky, 1981). Oversimplifying a bit, reflexive pronouns like himself and herself corefer with the subject of the most immediate clause that contains them (23.35),
 whereas nonreflexives cannot corefer with this subject (23.36).
 (23.35) Janet bought herself a bottle of fish sauce. [herself=Janet]
 (23.36) Janet bought her a bottle of fish sauce. [her6=Janet]

 Recency: Entities introduced in recent utterances tend to be more salient than
 those introduced from utterances further back. Thus, in (23.37), the pronoun it is
 more likely to refer to Jim‚Äôs map than the doctor‚Äôs map.
 (23.37) The doctor found an old map in the captain‚Äôs chest. Jim found an even
 older map hidden on the shelf. It described an island.

 Grammatical Role: Entities mentioned in subject position are more salient than
 those in object position, which are in turn more salient than those mentioned in
 oblique positions. Thus although the first sentence in (23.38) and (23.39) expresses
 roughly the same propositional content, the preferred referent for the pronoun he
 varies with the subject‚ÄîBilly Bones in (23.38) and Jim Hawkins in (23.39).
 (23.38) Billy Bones went to the bar with Jim Hawkins. He called for a glass of
 rum. [ he = Billy ]
 (23.39) Jim Hawkins went to the bar with Billy Bones. He called for a glass of
 rum. [ he = Jim ]
 5 Here‚Äôs a bound pronoun example from Shakespeare‚Äôs Comedy of Errors: There‚Äôs not a man I meet but
 doth salute me As if I were their well-acquainted friend
 6 The word ‚Äúgender‚Äù is generally only used for languages with 2 or 3 noun classes, like most Indo-
European languages; many languages, like the Bantu languages or Chinese, have a much larger number
 of noun classes.
 23.2 ‚Ä¢ C OREFERENCE TASKS AND DATASETS 9

 Verb Semantics: Some verbs semantically emphasize one of their arguments, biasing the interpretation of subsequent pronouns. Compare (23.40) and (23.41).
 (23.40) John telephoned Bill. He lost the laptop.
 (23.41) John criticized Bill. He lost the laptop.
 These examples differ only in the verb used in the first sentence, yet ‚Äúhe‚Äù in (23.40)
 is typically resolved to John, whereas ‚Äúhe‚Äù in (23.41) is resolved to Bill. This may
 be partly due to the link between implicit causality and saliency: the implicit cause
 of a ‚Äúcriticizing‚Äù event is its object, whereas the implicit cause of a ‚Äútelephoning‚Äù
 event is its subject. In such verbs, the entity which is the implicit cause may be more
 salient.
 Selectional Restrictions: Many other kinds of semantic knowledge can play a role
 in referent preference. For example, the selectional restrictions that a verb places on
 its arguments (Chapter 21) can help eliminate referents, as in (23.42).
 (23.42) I ate the soup in my new bowl after cooking it for hours
 There are two possible referents for it, the soup and the bowl. The verb eat, however,
 requires that its direct object denote something edible, and this constraint can rule
 out bowl as a possible referent.

 We can formulate the task of coreference resolution as follows: Given a text T , find
 all entities and the coreference links between them. We evaluate our task by comparing the links our system creates with those in human-created gold coreference
 annotations on T .
 Let‚Äôs return to our coreference example, now using superscript numbers for each
 coreference chain (cluster), and subscript letters for individual mentions in the cluster:
 (23.43) [Victoria Chen]1a , CFO of [Megabucks Banking]2a , saw [[her]1b pay]3a jump
 to $2.3 million, as [the 38-year-old]1c also became [[the company]2b ‚Äôs
 president. It is widely known that [she]1d came to [Megabucks]2c from rival
 [Lotsabucks]4a .
 Assuming example (23.43) was the entirety of the article, the chains for her pay and
 Lotsabucks are singleton mentions:
 1. {Victoria Chen, her, the 38-year-old, She}
 2. {Megabucks Banking, the company, Megabucks}
 3. { her pay}
 4. { Lotsabucks}
 For most coreference evaluation campaigns, the input to the system is the raw
 text of articles, and systems must detect mentions and then link them into clusters.
 Solving this task requires dealing with pronominal anaphora (figuring out that her
 refers to Victoria Chen), filtering out non-referential pronouns like the pleonastic It
 in It has been ten years), dealing with definite noun phrases to figure out that the
 38-year-old is coreferent with Victoria Chen, and that the company is the same as
 Megabucks. And we need to deal with names, to realize that Megabucks is the same
 as Megabucks Banking.
10 C HAPTER 23 ‚Ä¢ C OREFERENCE R ESOLUTION AND E NTITY L INKING

 Exactly what counts as a mention and what links are annotated differs from task
 to task and dataset to dataset. For example some coreference datasets do not label
 singletons, making the task much simpler. Resolvers can achieve much higher scores
 on corpora without singletons, since singletons constitute the majority of mentions in
 running text, and they are often hard to distinguish from non-referential NPs. Some
 tasks use gold mention-detection (i.e. the system is given human-labeled mention
 boundaries and the task is just to cluster these gold mentions), which eliminates the
 need to detect and segment mentions from running text.
 Coreference is usually evaluated by the CoNLL F1 score, which combines three
 metrics: MUC, B3 , and CEAFe ; Section 23.8 gives the details.
 Let‚Äôs mention a few characteristics of one popular coreference dataset, OntoNotes
 (Pradhan et al. 2007b, Pradhan et al. 2007a), and the CoNLL 2012 Shared Task
 based on it (Pradhan et al., 2012a). OntoNotes contains hand-annotated Chinese
 and English coreference datasets of roughly one million words each, consisting of
 newswire, magazine articles, broadcast news, broadcast conversations, web data and
 conversational speech data, as well as about 300,000 words of annotated Arabic
 newswire. The most important distinguishing characteristic of OntoNotes is that
 it does not label singletons, simplifying the coreference task, since singletons represent 60%-70% of all entities. In other ways, it is similar to other coreference
 datasets. Referring expression NPs that are coreferent are marked as mentions, but
 generics and pleonastic pronouns are not marked. Appositive clauses are not marked
 as separate mentions, but they are included in the mention. Thus in the NP, ‚ÄúRichard
 Godown, president of the Industrial Biotechnology Association‚Äù the mention is the
 entire phrase. Prenominal modifiers are annotated as separate entities only if they
 are proper nouns. Thus wheat is not an entity in wheat fields, but UN is an entity in
 UN policy (but not adjectives like American in American policy).
 A number of corpora mark richer discourse phenomena. The ISNotes corpus
 annotates a portion of OntoNotes for information status, include bridging examples
 (Hou et al., 2018). The LitBank coreference corpus (Bamman et al., 2020) contains
 coreference annotations for 210,532 tokens from 100 different literary novels, including singletons and quantified and negated noun phrases. The AnCora-CO coreference corpus (Recasens and Martƒ±ÃÅ, 2010) contains 400,000 words each of Spanish
 (AnCora-CO-Es) and Catalan (AnCora-CO-Ca) news data, and includes labels for
 complex phenomena like discourse deixis in both languages. The ARRAU corpus
 (Uryupina et al., 2020) contains 350,000 words of English marking all NPs, which
 means singleton clusters are available. ARRAU includes diverse genres like dialog
 (the TRAINS data) and fiction (the Pear Stories), and has labels for bridging references, discourse deixis, generics, and ambiguous anaphoric relations.

 mention The first stage of coreference is mention detection: finding the spans of text that
 detection
 constitute each mention. Mention detection algorithms are usually very liberal in
 proposing candidate mentions (i.e., emphasizing recall), and only filtering later. For
 example many systems run parsers and named entity taggers on the text and extract
 every span that is either an NP, a possessive pronoun, or a named entity.
 Doing so from our sample text repeated in (23.44):
 (23.44) Victoria Chen, CFO of Megabucks Banking, saw her pay jump to $2.3
 23.3 ‚Ä¢ M ENTION D ETECTION 11

 million, as the 38-year-old also became the company‚Äôs president. It is
 widely known that she came to Megabucks from rival Lotsabucks.
 might result in the following list of 13 potential mentions:
 Victoria Chen $2.3 million she
 CFO of Megabucks Banking the 38-year-old Megabucks
 Megabucks Banking the company Lotsabucks
 her the company‚Äôs president
 her pay It
 More recent mention detection systems are even more generous; the span-based
 algorithm we will describe in Section 23.6 first extracts literally all n-gram spans
 of words up to N=10. Of course recall from Section 23.1.3 that many NPs‚Äîand
 the overwhelming majority of random n-gram spans‚Äîare not referring expressions.
 Therefore all such mention detection systems need to eventually filter out pleonastic/expletive pronouns like It above, appositives like CFO of Megabucks Banking
 Inc, or predicate nominals like the company‚Äôs president or $2.3 million.
 Some of this filtering can be done by rules. Early rule-based systems designed
 regular expressions to deal with pleonastic it, like the following rules from Lappin
 and Leass (1994) that use dictionaries of cognitive verbs (e.g., believe, know, anticipate) to capture pleonastic it in ‚ÄúIt is thought that ketchup...‚Äù, or modal adjectives
 (e.g., necessary, possible, certain, important), for, e.g., ‚ÄúIt is likely that I...‚Äù. Such
 rules are sometimes used as part of modern systems:

 It is Modaladjective that S
 It is Modaladjective (for NP) to VP
 It is Cogv-ed that S
 It seems/appears/means/follows (that) S

 Mention-detection rules are sometimes designed specifically for particular evaluation campaigns. For OntoNotes, for example, mentions are not embedded within
 larger mentions, and while numeric quantities are annotated, they are rarely coreferential. Thus for OntoNotes tasks like CoNLL 2012 (Pradhan et al., 2012a), a
 common first pass rule-based mention detection algorithm (Lee et al., 2013) is:

 1. Take all NPs, possessive pronouns, and named entities.
 2. Remove numeric quantities (100 dollars, 8%), mentions embedded in
 larger mentions, adjectival forms of nations, and stop words (like there).
 3. Remove pleonastic it based on regular expression patterns.

 Rule-based systems, however, are generally insufficient to deal with mentiondetection, and so modern systems incorporate some sort of learned mention detection component, such as a referentiality classifier, an anaphoricity classifier‚Äî
 detecting whether an NP is an anaphor‚Äîor a discourse-new classifier‚Äî detecting
 whether a mention is discourse-new and a potential antecedent for a future anaphor.
anaphoricity An anaphoricity detector, for example, can draw its positive training examples
 detector
 from any span that is labeled as an anaphoric referring expression in hand-labeled
 datasets like OntoNotes, ARRAU, or AnCora. Any other NP or named entity can be
 marked as a negative training example. Anaphoricity classifiers use features of the
 candidate mention such as its head word, surrounding words, definiteness, animacy,
 length, position in the sentence/discourse, many of which were first proposed in
 early work by Ng and Cardie (2002a); see Section 23.5 for more on features.
12 C HAPTER 23 ‚Ä¢ C OREFERENCE R ESOLUTION AND E NTITY L INKING

 Referentiality or anaphoricity detectors can be run as filters, in which only mentions that are classified as anaphoric or referential are passed on to the coreference
 system. The end result of such a filtering mention detection system on our example
 above might be the following filtered set of 9 potential mentions:
 Victoria Chen her pay she
 Megabucks Bank the 38-year-old Megabucks
 her the company Lotsabucks
 It turns out, however, that hard filtering of mentions based on an anaphoricity
 or referentiality classifier leads to poor performance. If the anaphoricity classifier
 threshold is set too high, too many mentions are filtered out and recall suffers. If the
 classifier threshold is set too low, too many pleonastic or non-referential mentions
 are included and precision suffers.
 The modern approach is instead to perform mention detection, anaphoricity, and
 coreference jointly in a single end-to-end model (Ng 2005b, Denis and Baldridge
 2007, Rahman and Ng 2009). For example mention detection in the Lee et al.
 (2017b),2018 system is based on a single end-to-end neural network that computes
 a score for each mention being referential, a score for two mentions being coreference, and combines them to make a decision, training all these scores with a single
 end-to-end loss. We‚Äôll describe this method in detail in Section 23.6. 7
 Despite these advances, correctly detecting referential mentions seems to still be
 an unsolved problem, since systems incorrectly marking pleonastic pronouns like
 it and other non-referential NPs as coreferent is a large source of errors of modern
 coreference resolution systems (Kummerfeld and Klein 2013, Martschat and Strube
 2014, Martschat and Strube 2015, Wiseman et al. 2015, Lee et al. 2017a).
 Mention, referentiality, or anaphoricity detection is thus an important open area
 of investigation. Other sources of knowledge may turn out to be helpful, especially
 in combination with unsupervised and semisupervised algorithms, which also mitigate the expense of labeled datasets. In early work, for example Bean and Riloff
 (1999) learned patterns for characterizing anaphoric or non-anaphoric NPs; (by extracting and generalizing over the first NPs in a text, which are guaranteed to be
 non-anaphoric). Chang et al. (2012) look for head nouns that appear frequently in
 the training data but never appear as gold mentions to help find non-referential NPs.
 Bergsma et al. (2008) use web counts as a semisupervised way to augment standard
 features for anaphoricity detection for English it, an important task because it is both
 common and ambiguous; between a quarter and half it examples are non-anaphoric.
 Consider the following two examples:
 (23.45) You can make [it] in advance. [anaphoric]
 (23.46) You can make [it] in Hollywood. [non-anaphoric]
 The it in make it is non-anaphoric, part of the idiom make it. Bergsma et al. (2008)
 turn the context around each example into patterns, like ‚Äúmake * in advance‚Äù from
 (23.45), and ‚Äúmake * in Hollywood‚Äù from (23.46). They then use Google n-grams to
 enumerate all the words that can replace it in the patterns. Non-anaphoric contexts
 tend to only have it in the wildcard positions, while anaphoric contexts occur with
 many other NPs (for example make them in advance is just as frequent in their data
 7 Some systems try to avoid mention detection or anaphoricity detection altogether. For datasets like
 OntoNotes which don‚Äôt label singletons, an alternative to filtering out non-referential mentions is to run
 coreference resolution, and then simply delete any candidate mentions which were not corefered with
 another mention. This likely doesn‚Äôt work as well as explicitly modeling referentiality, and cannot solve
 the problem of detecting singletons, which is important for tasks like entity linking.
 23.4 ‚Ä¢ A RCHITECTURES FOR C OREFERENCE A LGORITHMS 13

 as make it in advance, but make them in Hollywood did not occur at all). These
 n-gram contexts can be used as features in a supervised anaphoricity classifier.

 Modern systems for coreference are based on supervised neural machine learning,
 supervised from hand-labeled datasets like OntoNotes. In this section we overview
 the various architecture of modern systems, using the categorization of Ng (2010),
 which distinguishes algorithms based on whether they make each coreference decision in a way that is entity-based‚Äîrepresenting each entity in the discourse model‚Äî
 or only mention-based‚Äîconsidering each mention independently, and whether they
 use ranking models to directly compare potential antecedents. Afterwards, we go
 into more detail on one state-of-the-art algorithm in Section 23.6.

 23.4.1 The Mention-Pair Architecture
 mention-pair We begin with the mention-pair architecture, the simplest and most influential
 coreference architecture, which introduces many of the features of more complex
 mention-pair algorithms, even though other architectures perform better. The mention-pair architecture is based around a classifier that‚Äî as its name suggests‚Äîis given a pair
 of mentions, a candidate anaphor and a candidate antecedent, and makes a binary
 classification decision: coreferring or not.
 Let‚Äôs consider the task of this classifier for the pronoun she in our example, and
 assume the slightly simplified set of potential antecedents in Fig. 23.2.

 p(coref|‚ÄùVictoria Chen‚Äù,‚Äùshe‚Äù)

 Victoria Chen Megabucks Banking her her pay the 37-year-old she

 p(coref|‚ÄùMegabucks Banking‚Äù,‚Äùshe‚Äù)

 Victoria Chen or her), the mention-pair classifier assigns a probability of a coreference link.

 For each prior mention (Victoria Chen, Megabucks Banking, her, etc.), the binary
 classifier computes a probability: whether or not the mention is the antecedent of
 she. We want this probability to be high for actual antecedents (Victoria Chen, her,
 the 38-year-old) and low for non-antecedents (Megabucks Banking, her pay).
 Early classifiers used hand-built features (Section 23.5); more recent classifiers
 use neural representation learning (Section 23.6)
 For training, we need a heuristic for selecting training samples; since most pairs
 of mentions in a document are not coreferent, selecting every pair would lead to
 a massive overabundance of negative samples. The most common heuristic, from
 (Soon et al., 2001), is to choose the closest antecedent as a positive example, and all
 pairs in between as the negative examples. More formally, for each anaphor mention
 mi we create
 ‚Ä¢ one positive instance (mi , m j ) where m j is the closest antecedent to mi , and
14 C HAPTER 23 ‚Ä¢ C OREFERENCE R ESOLUTION AND E NTITY L INKING

 ‚Ä¢ a negative instance (mi , mk ) for each mk between m j and mi
 Thus for the anaphor she, we would choose (she, her) as the positive example
 and no negative examples. Similarly, for the anaphor the company we would choose
 (the company, Megabucks) as the positive example and (the company, she) (the company, the 38-year-old) (the company, her pay) and (the company, her) as negative
 examples.
 Once the classifier is trained, it is applied to each test sentence in a clustering
 step. For each mention i in a document, the classifier considers each of the prior i ‚àí 1
 mentions. In closest-first clustering (Soon et al., 2001), the classifier is run right to
 left (from mention i ‚àí 1 down to mention 1) and the first antecedent with probability
 > .5 is linked to i. If no antecedent has probably > 0.5, no antecedent is selected for
 i. In best-first clustering, the classifier is run on all i ‚àí 1 antecedents and the most
 probable preceding mention is chosen as the antecedent for i. The transitive closure
 of the pairwise relation is taken as the cluster.
 While the mention-pair model has the advantage of simplicity, it has two main
 problems. First, the classifier doesn‚Äôt directly compare candidate antecedents to
 each other, so it‚Äôs not trained to decide, between two likely antecedents, which one
 is in fact better. Second, it ignores the discourse model, looking only at mentions,
 not entities. Each classifier decision is made completely locally to the pair, without
 being able to take into account other mentions of the same entity. The next two
 models each address one of these two flaws.

 23.4.2 The Mention-Rank Architecture
 The mention ranking model directly compares candidate antecedents to each other,
 choosing the highest-scoring antecedent for each anaphor.
 In early formulations, for mention i, the classifier decides which of the {1, ..., i ‚àí
 1} prior mentions is the antecedent (Denis and Baldridge, 2008). But suppose i is
 in fact not anaphoric, and none of the antecedents should be chosen? Such a model
 would need to run a separate anaphoricity classifier on i. Instead, it turns out to be
 better to jointly learn anaphoricity detection and coreference together with a single
 loss (Rahman and Ng, 2009).
 So in modern mention-ranking systems, for the ith mention (anaphor), we have
 an associated random variable yi ranging over the values Y (i) = {1, ..., i ‚àí 1, }. The
 value  is a special dummy mention meaning that i does not have an antecedent (i.e.,
 is either discourse-new and starts a new coref chain, or is non-anaphoric).

 p(‚ÄùVictoria Chen‚Äù|‚Äùshe‚Äù) p(‚Äùher‚Äù|she‚Äù) p(‚Äùthe 37-year-old‚Äù|she‚Äù)

 } One or more
 of these
 should be high

 }
 œµ Victoria Chen Megabucks Banking her her pay the 37-year-old she
 All of these
 should be low
 p(œµ|‚Äùshe‚Äù) p(‚ÄùMegabucks Banking‚Äù|she‚Äù) p(‚Äùher pay‚Äù|she‚Äù)

 At test time, for a given mention i the model computes one softmax over all the
 antecedents (plus ) giving a probability for each candidate antecedent (or none).
 23.5 ‚Ä¢ C LASSIFIERS USING HAND - BUILT FEATURES 15

 Fig. 23.3 shows an example of the computation for the single candidate anaphor
 she.
 Once the antecedent is classified for each anaphor, transitive closure can be run
 over the pairwise decisions to get a complete clustering.
 Training is trickier in the mention-ranking model than the mention-pair model,
 because for each anaphor we don‚Äôt know which of all the possible gold antecedents
 to use for training. Instead, the best antecedent for each mention is latent; that
 is, for each mention we have a whole cluster of legal gold antecedents to choose
 from. Early work used heuristics to choose an antecedent, for example choosing the
 closest antecedent as the gold antecedent and all non-antecedents in a window of
 two sentences as the negative examples (Denis and Baldridge, 2008). Various kinds
 of ways to model latent antecedents exist (Fernandes et al. 2012, Chang et al. 2013,
 Durrett and Klein 2013). The simplest way is to give credit to any legal antecedent
 by summing over all of them, with a loss function that optimizes the likelihood of
 all correct antecedents from the gold clustering (Lee et al., 2017b). We‚Äôll see the
 details in Section 23.6.
 Mention-ranking models can be implemented with hand-build features or with
 neural representation learning (which might also incorporate some hand-built features). we‚Äôll explore both directions in Section 23.5 and Section 23.6.

 23.4.3 Entity-based Models
 Both the mention-pair and mention-ranking models make their decisions about mentions. By contrast, entity-based models link each mention not to a previous mention
 but to a previous discourse entity (cluster of mentions).
 A mention-ranking model can be turned into an entity-ranking model simply
 by having the classifier make its decisions over clusters of mentions rather than
 individual mentions (Rahman and Ng, 2009).
 For traditional feature-based models, this can be done by extracting features over
 clusters. The size of a cluster is a useful feature, as is its ‚Äòshape‚Äô, which is the
 list of types of the mentions in the cluster i.e., sequences of the tokens (P)roper,
 (D)efinite, (I)ndefinite, (Pr)onoun, so that a cluster composed of {Victoria, her, the
 38-year-old} would have the shape P-Pr-D (BjoÃàrkelund and Kuhn, 2014). An entitybased model that includes a mention-pair classifier can use as features aggregates of
 mention-pair probabilities, for example computing the average probability of coreference over all mention-pairs in the two clusters (Clark and Manning 2015).
 Neural models can learn representations of clusters automatically, for example
 by using an RNN over the sequence of cluster mentions to encode a state corresponding to a cluster representation (Wiseman et al., 2016), or by learning distributed representations for pairs of clusters by pooling over learned representations of mention
 pairs (Clark and Manning, 2016b).
 However, although entity-based models are more expressive, the use of clusterlevel information in practice has not led to large gains in performance, so mentionranking models are still more commonly used.

 Feature-based classifiers, use hand-designed features in logistic regression, SVM,
 or random forest classifiers for coreference resolution. These classifiers don‚Äôt per-
16 C HAPTER 23 ‚Ä¢ C OREFERENCE R ESOLUTION AND E NTITY L INKING

 form as well as neural ones. Nonetheless, they are still sometimes useful to build
 lightweight systems when compute or data are sparse, and the features themselves
 are useful for error analysis even in neural systems.
 Given an anaphor mention and a potential antecedent mention, feature based
 classifiers make use of three types of features: (i) features of the anaphor, (ii) features
 of the candidate antecedent, and (iii) features of the relationship between the pair.
 Entity-based models can make additional use of two additional classes: (iv) feature
 of all mentions from the antecedent‚Äôs entity cluster, and (v) features of the relation
 between the anaphor and the mentions in the antecedent entity cluster.

 Features of the Anaphor or Antecedent Mention
 First (last) word Victoria/she First or last word (or embedding) of antecedent/anaphor
 Head word Victoria/she Head word (or head embedding) of antecedent/anaphor
 Attributes Sg-F-A-3-PER/ The number, gender, animacy, person, named entity type
 Sg-F-A-3-PER attributes of (antecedent/anaphor)
 Length 2/1 length in words of (antecedent/anaphor)
 Mention type P/Pr Type: (P)roper, (D)efinite, (I)ndefinite, (Pr)onoun) of antecedent/anaphor
 Features of the Antecedent Entity
 Entity shape P-Pr-D The ‚Äòshape‚Äô or list of types of the mentions in the
 antecedent entity (cluster), i.e., sequences of (P)roper,
 (D)efinite, (I)ndefinite, (Pr)onoun.
 Entity attributes Sg-F-A-3-PER The number, gender, animacy, person, named entity type
 attributes of the antecedent entity
 Ant. cluster size 3 Number of mentions in the antecedent cluster
 Features of the Pair of Mentions
 Sentence distance 1 The number of sentences between antecedent and anaphor
 Mention distance 4 The number of mentions between antecedent and anaphor
 i-within-i F Anaphor has i-within-i relation with antecedent
 Cosine Cosine between antecedent and anaphor embeddings
 Features of the Pair of Entities
 Exact String Match F True if the strings of any two mentions from the antecedent
 and anaphor clusters are identical.
 Head Word Match F True if any mentions from antecedent cluster has same
 headword as any mention in anaphor cluster
 Word Inclusion F All words in anaphor cluster included in antecedent cluster
‚ÄúVictoria Chen‚Äù.

 that would be computed for the potential anaphor ‚Äúshe‚Äù and potential antecedent
 ‚ÄúVictoria Chen‚Äù in our example sentence, repeated below:
 (23.47) Victoria Chen, CFO of Megabucks Banking, saw her pay jump to $2.3
 million, as the 38-year-old also became the company‚Äôs president. It is
 widely known that she came to Megabucks from rival Lotsabucks.
 Features that prior work has found to be particularly useful are exact string
 match, entity headword agreement, mention distance, as well as (for pronouns) exact
 attribute match and i-within-i, and (for nominals and proper names) word inclusion
 and cosine. For lexical features (like head words) it is common to only use words
 that appear enough times (>20 times).
 23.6 ‚Ä¢ A NEURAL MENTION - RANKING ALGORITHM 17

 It is crucial in feature-based systems to use conjunctions of features; one experiment suggested that moving from individual features in a classifier to conjunctions
 of multiple features increased F1 by 4 points (Lee et al., 2017a). Specific conjunctions can be designed by hand (Durrett and Klein, 2013), all pairs of features can be
 conjoined (Bengtson and Roth, 2008), or feature conjunctions can be learned using
 decision tree or random forest classifiers (Ng and Cardie 2002a, Lee et al. 2017a).
 Features can also be used in neural models as well. Neural systems use contextual word embeddings so don‚Äôt benefit from shallow features like string match or or
 mention types. However features like mention length, distance between mentions,
 or genre can complement neural contextual embedding models.

 In this section we describe the neural e2e-coref algorithms of Lee et al. (2017b)
 (simplified and extended a bit, drawing on Joshi et al. (2019) and others). This is
 a mention-ranking algorithm that considers all possible spans of text in the document, assigns a mention-score to each span, prunes the mentions based on this score,
 then assigns coreference links to the remaining mentions.
 More formally, given a document D with T words, the model considers all of
 the T (T2+1) text spans in D (unigrams, bigrams, trigrams, 4-grams, etc; in practice
 we only consider spans up a maximum length around 10). The task is to assign
 to each span i an antecedent yi , a random variable ranging over the values Y (i) =
 {1, ..., i ‚àí 1, }; each previous span and a special dummy token . Choosing the
 dummy token means that i does not have an antecedent, either because i is discoursenew and starts a new coreference chain, or because i is non-anaphoric.
 For each pair of spans i and j, the system assigns a score s(i, j) for the coreference link between span i and span j. The system then learns a distribution P(yi )
 over the antecedents for span i:
 exp(s(i, yi ))
 P(yi ) = P 0
 (23.48)
 y0 ‚ààY (i) exp(s(i, y ))

 This score s(i, j) includes three factors that we‚Äôll define below: m(i); whether span
 i is a mention; m( j); whether span j is a mention; and c(i, j); whether j is the
 antecedent of i:

 s(i, j) = m(i) + m( j) + c(i, j) (23.49)

 For the dummy antecedent , the score s(i, ) is fixed to 0. This way if any nondummy scores are positive, the model predicts the highest-scoring antecedent, but if
 all the scores are negative it abstains.

 23.6.1 Computing span representations
 To compute the two functions m(i) and c(i, j) which score a span i or a pair of spans
 (i, j), we‚Äôll need a way to represent a span. The e2e-coref family of algorithms
 represents each span by trying to capture 3 words/tokens: the first word, the last
 word, and the most important word. We first run each paragraph or subdocument
 through an encoder (like BERT) to generate embeddings hi for each token i. The
 span i is then represented by a vector gi that is a concatenation of the encoder output
18 C HAPTER 23 ‚Ä¢ C OREFERENCE R ESOLUTION AND E NTITY L INKING

 embedding for the first (start) token of the span, the encoder output for the last (end)
 token of the span, and a third vector which is an attention-based representation:

 gi = [hSTART(i) , hEND(i) , hATT(i) ] (23.50)

 The goal of the attention vector is to represent which word/token is the likely
 syntactic head-word of the span; we saw in the prior section that head-words are
 a useful feature; a matching head-word is a good indicator of coreference. The
 attention representation is computed as usual; the system learns a weight vector wŒ± ,
 and computes its dot product with the hidden state ht transformed by a FFN:

 Œ±t = wŒ± ¬∑ FFNŒ± (ht ) (23.51)

 The attention score is normalized into a distribution via a softmax:
 exp(Œ±t )
 ai,t = PEND(i) (23.52)
 k= START (i)
 exp(Œ±k )

 And then the attention distribution is used to create a vector hATT(i) which is an
 attention-weighted sum of the embeddings et of each of the words in span i:
 END
 X(i)
 hATT(i) = ai,t ¬∑ et (23.53)
 t= START (i)

 Fig. 23.5 shows the computation of the span representation and the mention
 score.

 General Electric Electric said the the Postal Service Service contacted the the company
 Mention score (m)

 Span representation (g)

 Span head (hATT) + + + + +

 Encodings (h)

 Encoder

 ‚Ä¶ General Electric said the Postal Service contacted the company

e2e-coref model (Lee et al. 2017b, Joshi et al. 2019). The model considers all spans up to a maximum width of
say 10; the figure shows a small subset of the bigram and trigram spans.

 23.6.2 Computing the mention and antecedent scores m and c
 Now that we know how to compute the vector gi for representing span i, we can
 see the details of the two scoring functions m(i) and c(i, j). Both are computed by
 feedforward networks:

 m(i) = wm ¬∑ FFNm (gi ) (23.54)
 c(i, j) = wc ¬∑ FFNc ([gi , g j , gi ‚ó¶ g j , ]) (23.55)

 At inference time, this mention score m is used as a filter to keep only the best few
 mentions.
 23.6 ‚Ä¢ A NEURAL MENTION - RANKING ALGORITHM 19

 We then compute the antecedent score for high-scoring mentions. The antecedent
score c(i, j) takes as input a representation of the spans i and j, but also the elementwise similarity of the two spans to each other gi ‚ó¶ g j (here ‚ó¶ is element-wise multiplication). Fig. 23.6 shows the computation of the score s for the three possible
antecedents of the company in the example sentence from Fig. 23.5.

 Given the set of mentions, the joint distribution of antecedents for each document is computed in a forward pass, and we can then do transitive closure on the
antecedents to create a final clustering for the document.
 Fig. 23.7 shows example predictions from the model, showing the attention
weights, which Lee et al. (2017b) find correlate with traditional semantic heads.
Note that the model gets the second example wrong, presumably because attendants
and pilot likely have nearby word embeddings.

example, showing one correct example and one mistake. Bold, parenthesized spans are mentions in the predicted cluster. The amount of red color on a word indicates the head-finding
attention weight ai,t in Eq. 23.52. Figure adapted from Lee et al. (2017b).

23.6.3 Learning
For training, we don‚Äôt have a single gold antecedent for each mention; instead the
coreference labeling only gives us each entire cluster of coreferent mentions; so a
mention only has a latent antecedent. We therefore use a loss function that maximizes the sum of the coreference probability of any of the legal antecedents. For a
given mention i with possible antecedents Y (i), let GOLD(i) be the set of mentions
in the gold cluster containing i. Since the set of mentions occurring before i is Y (i),
the set of mentions in that gold cluster that also occur before i is Y (i) ‚à© GOLD(i). We
20 C HAPTER 23 ‚Ä¢ C OREFERENCE R ESOLUTION AND E NTITY L INKING

 therefore want to maximize:
 X
 P(yÃÇ) (23.56)
 yÃÇ‚ààY (i)‚à© GOLD (i)

 If a mention i is not in a gold cluster GOLD(i) = .
 To turn this probability into a loss function, we‚Äôll use the cross-entropy loss
 function we defined in Eq. ?? in Chapter 4, by taking the ‚àí log of the probability. If
 we then sum over all mentions, we get the final loss function for training:
 N
 X X
 L= ‚àí log P(yÃÇ) (23.57)
 i=2 yÃÇ‚ààY (i)‚à© GOLD (i)

 entity linking Entity linking is the task of associating a mention in text with the representation of
 some real-world entity in an ontology or knowledge base (Ji and Grishman, 2011). It
 is the natural follow-on to coreference resolution; coreference resolution is the task
 of associating textual mentions that corefer to the same entity. Entity linking takes
 the further step of identifying who that entity is. It is especially important for any
 NLP task that links to a knowledge base.
 While there are all sorts of potential knowledge-bases, we‚Äôll focus in this section
 on Wikipedia, since it‚Äôs widely used as an ontology for NLP tasks. In this usage,
 each unique Wikipedia page acts as the unique id for a particular entity. This task of
 deciding which Wikipedia page corresponding to an individual is being referred to
 wikification by a text mention has its own name: wikification (Mihalcea and Csomai, 2007).
 Since the earliest systems (Mihalcea and Csomai 2007, Cucerzan 2007, Milne
 and Witten 2008), entity linking is done in (roughly) two stages: mention detection and mention disambiguation. We‚Äôll give two algorithms, one simple classic
 baseline that uses anchor dictionaries and information from the Wikipedia graph
 structure (Ferragina and Scaiella, 2011) and one modern neural algorithm (Li et al.,
 2020). We‚Äôll focus here mainly on the application of entity linking to questions,
 since a lot of the literature has been in that context.

 23.7.1 Linking based on Anchor Dictionaries and Web Graph
 As a simple baseline we introduce the TAGME linker (Ferragina and Scaiella, 2011)
 for Wikipedia, which itself draws on earlier algorithms (Mihalcea and Csomai 2007,
 Cucerzan 2007, Milne and Witten 2008). Wikification algorithms define the set of
 entities as the set of Wikipedia pages, so we‚Äôll refer to each Wikipedia page as a
 unique entity e. TAGME first creates a catalog of all entities (i.e. all Wikipedia
 pages, removing some disambiguation and other meta-pages) and indexes them in a
 standard IR engine like Lucene. For each page e, the algorithm computes an in-link
 count in(e): the total number of in-links from other Wikipedia pages that point to e.
 These counts can be derived from Wikipedia dumps.
 Finally, the algorithm requires an anchor dictionary. An anchor dictionary
 anchor texts lists for each Wikipedia page, its anchor texts: the hyperlinked spans of text on
 other pages that point to it. For example, the web page for Stanford University,
 http://www.stanford.edu, might be pointed to from another page using anchor
 texts like Stanford or Stanford University:
 23.7 ‚Ä¢ E NTITY L INKING 21

 <a href="http://www.stanford.edu">Stanford University</a>
 We compute a Wikipedia anchor dictionary by including, for each Wikipedia
page e, e‚Äôs title as well as all the anchor texts from all Wikipedia pages that point to e.
For each anchor string a we‚Äôll also compute its total frequency freq(a) in Wikipedia
(including non-anchor uses), the number of times a occurs as a link (which we‚Äôll call
link(a)), and its link probability linkprob(a) = link(a)/freq(a). Some cleanup of the
final anchor dictionary is required, for example removing anchor strings composed
only of numbers or single characters, that are very rare, or that are very unlikely to
be useful entities because they have a very low linkprob.
Mention Detection Given a question (or other text we are trying to link), TAGME
detects mentions by querying the anchor dictionary for each token sequence up to
6 words. This large set of sequences is pruned with some simple heuristics (for
example pruning substrings if they have small linkprobs). The question:
 When was Ada Lovelace born?
might give rise to the anchor Ada Lovelace and possibly Ada, but substrings spans
like Lovelace might be pruned as having too low a linkprob, and but spans like born
have such a low linkprob that they would not be in the anchor dictionary at all.
Mention Disambiguation If a mention span is unambiguous (points to only one
entity/Wikipedia page), we are done with entity linking! However, many spans are
ambiguous, matching anchors for multiple Wikipedia entities/pages. The TAGME
algorithm uses two factors for disambiguating ambiguous spans, which have been
referred to as prior probability and relatedness/coherence. The first factor is p(e|a),
the probability with which the span refers to a particular entity. For each page e ‚àà
E (a), the probability p(e|a) that anchor a points to e, is the ratio of the number of
links into e with anchor text a to the total number of occurrences of a as an anchor:
 count(a ‚Üí e)
 prior(a ‚Üí e) = p(e|a) = (23.58)
 link(a)
Let‚Äôs see how that factor works in linking entities in the following question:
 What Chinese Dynasty came before the Yuan?
The most common association for the span Yuan in the anchor dictionary is the name
of the Chinese currency, i.e., the probability p(Yuan currency| yuan) is very high.
Rarer Wikipedia associations for Yuan include the common Chinese last name, a
language spoken in Thailand, and the correct entity in this case, the name of the
Chinese dynasty. So if we chose based only on p(e|a) , we would make the wrong
disambiguation and miss the correct link, Yuan dynasty.
 To help in just this sort of case, TAGME uses a second factor, the relatedness of
this entity to other entities in the input question. In our example, the fact that the
question also contains the span Chinese Dynasty, which has a high probability link to
the page Dynasties in Chinese history, ought to help match Yuan dynasty.
 Let‚Äôs see how this works. Given a question q, for each candidate anchors span
a detected in q, we assign a relatedness score to each possible entity e ‚àà E (a) of a.
The relatedness score of the link a ‚Üí e is the weighted average relatedness between
e and all other entities in q. Two entities are considered related to the extent their
Wikipedia pages share many in-links. More formally, the relatedness between two
entities A and B is computed as
 log(max(|in(A)|, |in(B)|)) ‚àí log(|in(A) ‚à© in(B)|)
 rel(A, B) = (23.59)
 log(|W |) ‚àí log(min(|in(A)|, |in(B)|))
22 C HAPTER 23 ‚Ä¢ C OREFERENCE R ESOLUTION AND E NTITY L INKING

 where in(x) is the set of Wikipedia pages pointing to x and W is the set of all Wikipedia pages in the collection.
 The vote given by anchor b to the candidate annotation a ‚Üí X is the average,
 over all the possible entities of b, of their relatedness to X, weighted by their prior
 probability:

 1 X
 vote(b, X) = rel(X,Y )p(Y |b) (23.60)
 |E (b)|
 Y ‚ààE (b)

 The total relatedness score for a ‚Üí X is the sum of the votes of all the other anchors
 detected in q:
 X
 relatedness(a ‚Üí X) = vote(b, X) (23.61)
 b‚ààXq \a

 To score a ‚Üí X, we combine relatedness and prior by choosing the entity X
 that has the highest relatedness(a ‚Üí X), finding other entities within a small  of
 this value, and from this set, choosing the entity with the highest prior P(X|a). The
 result of this step is a single entity assigned to each span in q.
 The TAGME algorithm has one further step of pruning spurious anchor/entity
 pairs, assigning a score averaging link probability with the coherence.

 1 X
 coherence(a ‚Üí X) = rel(B, X)
 |S| ‚àí 1
 B‚ààS \X
 coherence(a ‚Üí X) + linkprob(a)
 score(a ‚Üí X) = (23.62)
 Finally, pairs are pruned if score(a ‚Üí X) < Œª , where the threshold Œª is set on a
 held-out set.

 23.7.2 Neural Graph-based linking
 More recent entity linking models are based on bi-encoders, encoding a candidate
 mention span, encoding an entity, and computing the dot product between the encodings. This allows embeddings for all the entities in the knowledge base to be
 precomputed and cached (Wu et al., 2020). Let‚Äôs sketch the ELQ linking algorithm
 of Li et al. (2020), which is given a question q and a set of candidate entities from
 Wikipedia with associated Wikipedia text, and outputs tuples (e, ms , me ) of entity id,
 mention start, and mention end. As Fig. 23.8 shows, it does this by encoding each
 Wikipedia entity using text from Wikipedia, encoding each mention span using text
 from the question, and computing their similarity, as we describe below.
 Entity Mention Detection To get an h-dimensional embedding for each question
 token, the algorithm runs the question through BERT in the normal way:

 [q1 ¬∑ ¬∑ ¬∑ qn ] = BERT([CLS]q1 ¬∑ ¬∑ ¬∑ qn [SEP]) (23.63)

 It then computes the likelihood of each span [i, j] in q being an entity mention, in
 a way similar to the span-based algorithm we saw for the reader above. First we
 compute the score for i/ j being the start/end of a mention:

 sstart (i) = wstart ¬∑ qi , send ( j) = wend ¬∑ q j , (23.64)
 23.7 ‚Ä¢ E NTITY L INKING 23

questions (Li et al., 2020). Each candidate question mention span and candidate entity are
separately encoded, and then scored by the entity/span dot product.

where wstart and wend are vectors learned during training. Next, another trainable
embedding, wmention is used to compute a score for each token being part of a mention:

 smention (t) = wmention ¬∑ qt (23.65)

Mention probabilities are then computed by combining these three scores:
 j
 !
 X
 p([i, j]) = œÉ sstart (i) + send ( j) + smention (t) (23.66)
 t=i

Entity Linking To link mentions to entities, we next compute embeddings for
each entity in the set E = e1 , ¬∑ ¬∑ ¬∑ , ei , ¬∑ ¬∑ ¬∑ , ew of all Wikipedia entities. For each entity ei we‚Äôll get text from the entity‚Äôs Wikipedia page, the title t(ei ) and the first
128 tokens of the Wikipedia page which we‚Äôll call the description d(ei ). This is
again run through BERT, taking the output of the CLS token BERT[CLS] as the entity
representation:

 xei = BERT[CLS] ([CLS]t(ei )[ENT]d(ei )[SEP]) (23.67)

Mention spans can be linked to entities by computing, for each entity e and span
[i, j], the dot product similarity between the span encoding (the average of the token
embeddings) and the entity encoding.
 j
 1 X
 yi, j = qt
 ( j ‚àí i + 1)
 t=i
 s(e, [i, j]) = x¬∑e yi, j (23.68)

Finally, we take a softmax to get a distribution over entities for each span:

 exp(s(e, [i, j]))
 p(e|[i, j]) = P 0
 (23.69)
 e0 ‚ààE exp(s(e , [i, j]))

Training The ELQ mention detection and entity linking algorithm is fully supervised. This means, unlike the anchor dictionary algorithms from Section 23.7.1,
24 C HAPTER 23 ‚Ä¢ C OREFERENCE R ESOLUTION AND E NTITY L INKING

 it requires datasets with entity boundaries marked and linked. Two such labeled
 datasets are WebQuestionsSP (Yih et al., 2016), an extension of the WebQuestions
 (Berant et al., 2013) dataset derived from Google search questions, and GraphQuestions (Su et al., 2016). Both have had entity spans in the questions marked and
 linked (Sorokin and Gurevych 2018, Li et al. 2020) resulting in entity-labeled versions WebQSPEL and GraphQEL (Li et al., 2020).
 Given a training set, the ELQ mention detection and entity linking phases are
 trained jointly, optimizing the sum of their losses. The mention detection loss is
 a binary cross-entropy loss, with L the length of the passage and N the number of
 candidates:
 1 X
 LMD = ‚àí
 
 y[i, j] log p([i, j]) + (1 ‚àí y[i, j] ) log(1 ‚àí p([i, j])) (23.70)
 N
 1‚â§i‚â§ j‚â§min(i+L‚àí1,n)

 with y[i, j] = 1 if [i, j] is a gold mention span, else 0. The entity linking loss is:
 LED = ‚àílogp(eg |[i, j]) (23.71)
 where eg is the gold entity for mention [i, j].

 We evaluate coreference algorithms model-theoretically, comparing a set of hypothesis chains or clusters H produced by the system against a set of gold or reference
 chains or clusters R from a human labeling, and reporting precision and recall.
 However, there are a wide variety of methods for doing this comparison. In fact,
 there are 5 common metrics used to evaluate coreference algorithms: the link based
 MUC (Vilain et al., 1995) and BLANC (Recasens and Hovy 2011, Luo et al. 2014)
 metrics, the mention based B3 metric (Bagga and Baldwin, 1998), the entity based
 CEAF metric (Luo, 2005), and the link based entity aware LEA metric (Moosavi and
 Strube, 2016).
 MUC Let‚Äôs just explore two of the metrics. The MUC F-measure (Vilain et al., 1995)
 F-measure
 is based on the number of coreference links (pairs of mentions) common to H and
 R. Precision is the number of common links divided by the number of links in H.
 Recall is the number of common links divided by the number of links in R; This
 makes MUC biased toward systems that produce large chains (and fewer entities),
 and it ignores singletons, since they don‚Äôt involve links.
 B
 3 B3 is mention-based rather than link-based. For each mention in the reference
 chain, we compute a precision and recall, and then we take a weighted sum over all
 N mentions in the document to compute a precision and recall for the entire task. For
 a given mention i, let R be the reference chain that includes i, and H the hypothesis
 chain that has i. The set of correct mentions in H is H ‚à© R. Precision for mention i
 is thus |H‚à©R| |H‚à©R|
 |H| , and recall for mention i thus |R| . The total precision is the weighted
 sum of the precision for mention i, weighted by a weight wi . The total recall is the
 weighted sum of the recall for mention i, weighted by a weight wi . Equivalently:
 N
 X # of correct mentions in hypothesis chain containing entityi
 Precision = wi
 # of mentions in hypothesis chain containing entityi
 i=1
 N
 X # of correct mentions in hypothesis chain containing entityi
 Recall = wi
 # of mentions in reference chain containing entityi
 i=1
 23.9 ‚Ä¢ W INOGRAD S CHEMA PROBLEMS 25

 The weight wi for each entity can be set to different values to produce different
 versions of the algorithm.
 Following a proposal from Denis and Baldridge (2009), the CoNLL coreference
 competitions were scored based on the average of MUC, CEAF-e, and B3 (Pradhan
 et al. 2011, Pradhan et al. 2012b), and so it is common in many evaluation campaigns
 to report an average of these 3 metrics. See Luo and Pradhan (2016) for a detailed
 description of the entire set of metrics; reference implementations of these should
 be used rather than attempting to reimplement from scratch (Pradhan et al., 2014).
 Alternative metrics have been proposed that deal with particular coreference domains or tasks. For example, consider the task of resolving mentions to named
 entities (persons, organizations, geopolitical entities), which might be useful for information extraction or knowledge base completion. A hypothesis chain that correctly contains all the pronouns referring to an entity, but has no version of the name
 itself, or is linked with a wrong name, is not useful for this task. We might instead
 want a metric that weights each mention by how informative it is (with names being
 most informative) (Chen and Ng, 2013) or a metric that considers a hypothesis to
 match a gold chain only if it contains at least one variant of a name (the NEC F1
 metric of Agarwal et al. (2019)).

 From early on in the field, researchers have noted that some cases of coreference
 are quite difficult, seeming to require world knowledge or sophisticated reasoning
 to solve. The problem was most famously pointed out by Winograd (1972) with the
 following example:
 (23.72) The city council denied the demonstrators a permit because
 a. they feared violence.
 b. they advocated violence.
 Winograd noticed that the antecedent that most readers preferred for the pronoun they in continuation (a) was the city council, but in (b) was the demonstrators.
 He suggested that this requires understanding that the second clause is intended
 as an explanation of the first clause, and also that our cultural frames suggest that
 city councils are perhaps more likely than demonstrators to fear violence and that
 demonstrators might be more likely to advocate violence.
 In an attempt to get the field of NLP to focus more on methods involving world
 knowledge and common-sense reasoning, Levesque (2011) proposed a challenge
 Winograd
 schema task called the Winograd Schema Challenge.8 The problems in the challenge task
 are coreference problems designed to be easily disambiguated by the human reader,
 but hopefully not solvable by simple techniques such as selectional restrictions, or
 other basic word association methods.
 The problems are framed as a pair of statements that differ in a single word or
 phrase, and a coreference question:
 (23.73) The trophy didn‚Äôt fit into the suitcase because it was too large.
 Question: What was too large? Answer: The trophy
 8 Levesque‚Äôs call was quickly followed up by Levesque et al. (2012) and Rahman and Ng (2012), a
 competition at the IJCAI conference (Davis et al., 2017), and a natural language inference version of the
 problem called WNLI (Wang et al., 2018).
26 C HAPTER 23 ‚Ä¢ C OREFERENCE R ESOLUTION AND E NTITY L INKING

 (23.74) The trophy didn‚Äôt fit into the suitcase because it was too small.
 Question: What was too small? Answer: The suitcase
 The problems have the following characteristics:
 1. The problems each have two parties
 2. A pronoun preferentially refers to one of the parties, but could grammatically
 also refer to the other
 3. A question asks which party the pronoun refers to
 4. If one word in the question is changed, the human-preferred answer changes
 to the other party
 The kind of world knowledge that might be needed to solve the problems can
 vary. In the trophy/suitcase example, it is knowledge about the physical world; that
 a bigger object cannot fit into a smaller object. In the original Winograd sentence,
 it is stereotypes about social actors like politicians and protesters. In examples like
 the following, it is knowledge about human actions like turn-taking or thanking.
 (23.75) Bill passed the gameboy to John because his turn was [over/next]. Whose
 turn was [over/next]? Answers: Bill/John
 (23.76) Joan made sure to thank Susan for all the help she had [given/received].
 Who had [given/received] help? Answers: Susan/Joan.
 Although the Winograd Schema was designed to require common-sense reasoning, a large percentage of the original set of problems can be solved by pretrained language models, fine-tuned on Winograd Schema sentences (Kocijan et al.,
 2019). Large pretrained language models encode an enormous amount of world or
 common-sense knowledge! The current trend is therefore to propose new datasets
 with increasingly difficult Winograd-like coreference resolution problems like K NOW R EF
 (Emami et al., 2019), with examples like:
 (23.77) Marcus is undoubtedly faster than Jarrett right now but in [his] prime the
 gap wasn‚Äôt all that big.
 In the end, it seems likely that some combination of language modeling and knowledge will prove fruitful; indeed, it seems that knowledge-based models overfit less
 to lexical idiosyncracies in Winograd Schema training sets (Trichelair et al., 2018),

 As with other aspects of language processing, coreference models exhibit gender and
 other biases (Zhao et al. 2018, Rudinger et al. 2018, Webster et al. 2018). For example the WinoBias dataset (Zhao et al., 2018) uses a variant of the Winograd Schema
 paradigm to test the extent to which coreference algorithms are biased toward linking gendered pronouns with antecedents consistent with cultural stereotypes. As we
 summarized in Chapter 5, embeddings replicate societal biases in their training test,
 such as associating men with historically sterotypical male occupations like doctors,
 and women with stereotypical female occupations like secretaries (Caliskan et al.
 2017, Garg et al. 2018).
 A WinoBias sentence contain two mentions corresponding to stereotypicallymale and stereotypically-female occupations and a gendered pronoun that must be
 linked to one of them. The sentence cannot be disambiguated by the gender of the
 pronoun, but a biased model might be distracted by this cue. Here is an example
 sentence:
 23.11 ‚Ä¢ S UMMARY 27

 (23.78) The secretary called the physiciani and told himi about a new patient
 [pro-stereotypical]
 (23.79) The secretary called the physiciani and told heri about a new patient
 [anti-stereotypical]
 Zhao et al. (2018) consider a coreference system to be biased if it is more accurate at linking pronouns consistent with gender stereotypical occupations (e.g., him
 with physician in (23.78)) than linking pronouns inconsistent with gender-stereotypical
 occupations (e.g., her with physician in (23.79)). They show that coreference systems of all architectures (rule-based, feature-based machine learned, and end-toend-neural) all show significant bias, performing on average 21 F1 points worse in
 the anti-stereotypical cases.
 One possible source of this bias is that female entities are significantly underrepresented in the OntoNotes dataset, used to train most coreference systems.
 Zhao et al. (2018) propose a way to overcome this bias: they generate a second
 gender-swapped dataset in which all male entities in OntoNotes are replaced with
 female ones and vice versa, and retrain coreference systems on the combined original and swapped OntoNotes data, also using debiased GloVE embeddings (Bolukbasi et al., 2016). The resulting coreference systems no longer exhibit bias on the
 WinoBias dataset, without significantly impacting OntoNotes coreference accuracy.
 In a follow-up paper, Zhao et al. (2019) show that the same biases exist in ELMo
 contextualized word vector representations and coref systems that use them. They
 showed that retraining ELMo with data augmentation again reduces or removes bias
 in coreference systems on WinoBias.
 Webster et al. (2018) introduces another dataset, GAP, and the task of Gendered
 Pronoun Resolution as a tool for developing improved coreference algorithms for
 gendered pronouns. GAP is a gender-balanced labeled corpus of 4,454 sentences
 with gendered ambiguous pronouns (by contrast, only 20% of the gendered pronouns in the English OntoNotes training data are feminine). The examples were
 created by drawing on naturally occurring sentences from Wikipedia pages to create
 hard to resolve cases with two named entities of the same gender and an ambiguous
 pronoun that may refer to either person (or neither), like the following:
 (23.80) In May, Fujisawa joined Mari Motohashi‚Äôs rink as the team‚Äôs skip, moving
 back from Karuizawa to Kitami where she had spent her junior days.
 Webster et al. (2018) show that modern coreference algorithms perform significantly worse on resolving feminine pronouns than masculine pronouns in GAP.
 Kurita et al. (2019) shows that a system based on BERT contextualized word representations shows similar bias.

 This chapter introduced the task of coreference resolution.
 ‚Ä¢ This is the task of linking together mentions in text which corefer, i.e. refer
 to the same discourse entity in the discourse model, resulting in a set of
 coreference chains (also called clusters or entities).
 ‚Ä¢ Mentions can be definite NPs or indefinite NPs, pronouns (including zero
 pronouns) or names.
28 C HAPTER 23 ‚Ä¢ C OREFERENCE R ESOLUTION AND E NTITY L INKING

 ‚Ä¢ The surface form of an entity mention is linked to its information status
 (new, old, or inferrable), and how accessible or salient the entity is.
 ‚Ä¢ Some NPs are not referring expressions, such as pleonastic it in It is raining.
 ‚Ä¢ Many corpora have human-labeled coreference annotations that can be used
 for supervised learning, including OntoNotes for English, Chinese, and Arabic, ARRAU for English, and AnCora for Spanish and Catalan.
 ‚Ä¢ Mention detection can start with all nouns and named entities and then use
 anaphoricity classifiers or referentiality classifiers to filter out non-mentions.
 ‚Ä¢ Three common architectures for coreference are mention-pair, mention-rank,
 and entity-based, each of which can make use of feature-based or neural classifiers.
 ‚Ä¢ Modern coreference systems tend to be end-to-end, performing mention detection and coreference in a single end-to-end architecture.
 ‚Ä¢ Algorithms learn representations for text spans and heads, and learn to compare anaphor spans with candidate antecedent spans.
 ‚Ä¢ Entity linking is the task of associating a mention in text with the representation of some real-world entity in an ontology .
 ‚Ä¢ Coreference systems are evaluated by comparing with gold entity labels using
 precision/recall metrics like MUC, B3 , CEAF, BLANC, or LEA.
 ‚Ä¢ The Winograd Schema Challenge problems are difficult coreference problems that seem to require world knowledge or sophisticated reasoning to solve.
 ‚Ä¢ Coreference systems exhibit gender bias which can be evaluated using datasets
 like Winobias and GAP.

Historical Notes
 Coreference has been part of natural language processing since the 1970s (Woods
 et al. 1972, Winograd 1972). The discourse model and the entity-centric foundation
 of coreference was formulated by Karttunen (1969) (at the 3rd COLING conference), playing a role also in linguistic semantics (Heim 1982, Kamp 1981). But
 it was Bonnie Webber‚Äôs 1978 dissertation and following work (Webber 1983) that
 explored the model‚Äôs computational aspects, providing fundamental insights into
 how entities are represented in the discourse model and the ways in which they can
 license subsequent reference. Many of the examples she provided continue to challenge theories of reference to this day.
 Hobbs
 algorithm The Hobbs algorithm9 is a tree-search algorithm that was the first in a long
 series of syntax-based methods for identifying reference robustly in naturally occurring text. The input to the Hobbs algorithm is a pronoun to be resolved, together
 with a syntactic (constituency) parse of the sentences up to and including the current sentence. The details of the algorithm depend on the grammar used, but can be
 understood from a simplified version due to Kehler et al. (2004) that just searches
 through the list of NPs in the current and prior sentences. This simplified Hobbs
 algorithm searches NPs in the following order: ‚Äú(i) in the current sentence from
 right-to-left, starting with the first NP to the left of the pronoun, (ii) in the previous
 sentence from left-to-right, (iii) in two sentences prior from left-to-right, and (iv) in
 9 The simpler of two algorithms presented originally in Hobbs (1978).
 H ISTORICAL N OTES 29

the current sentence from left-to-right, starting with the first noun group to the right
of the pronoun (for cataphora). The first noun group that agrees with the pronoun
with respect to number, gender, and person is chosen as the antecedent‚Äù (Kehler
et al., 2004).
 Lappin and Leass (1994) was an influential entity-based system that used weights
to combine syntactic and other features, extended soon after by Kennedy and Boguraev (1996) whose system avoids the need for full syntactic parses.
 Approximately contemporaneously centering (Grosz et al., 1995) was applied
to pronominal anaphora resolution by Brennan et al. (1987), and a wide variety of
work followed focused on centering‚Äôs use in coreference (Kameyama 1986, Di Eugenio 1990, Walker et al. 1994, Di Eugenio 1996, Strube and Hahn 1996, Kehler
1997, Tetreault 2001, Iida et al. 2003). Kehler and Rohde (2013) show how centering can be integrated with coherence-driven theories of pronoun interpretation. See
Chapter 24 for the use of centering in measuring discourse coherence.
 Coreference competitions as part of the US DARPA-sponsored MUC conferences provided early labeled coreference datasets (the 1995 MUC-6 and 1998 MUC-
7 corpora), and set the tone for much later work, choosing to focus exclusively
on the simplest cases of identity coreference (ignoring difficult cases like bridging,
metonymy, and part-whole) and drawing the community toward supervised machine
learning and metrics like the MUC metric (Vilain et al., 1995). The later ACE evaluations produced labeled coreference corpora in English, Chinese, and Arabic that
were widely used for model training and evaluation.
 This DARPA work influenced the community toward supervised learning beginning in the mid-90s (Connolly et al. 1994, Aone and Bennett 1995, McCarthy and
Lehnert 1995). Soon et al. (2001) laid out a set of basic features, extended by Ng and
Cardie (2002b), and a series of machine learning models followed over the next 15
years. These often focused separately on pronominal anaphora resolution (Kehler
et al. 2004, Bergsma and Lin 2006), full NP coreference (Cardie and Wagstaff 1999,
Ng and Cardie 2002b, Ng 2005a) and definite NP reference (Poesio and Vieira 1998,
Vieira and Poesio 2000), as well as separate anaphoricity detection (Bean and Riloff
1999, Bean and Riloff 2004, Ng and Cardie 2002a, Ng 2004), or singleton detection
(de Marneffe et al., 2015).
 The move from mention-pair to mention-ranking approaches was pioneered by
Yang et al. (2003) and Iida et al. (2003) who proposed pairwise ranking methods,
then extended by Denis and Baldridge (2008) who proposed to do ranking via a softmax over all prior mentions. The idea of doing mention detection, anaphoricity, and
coreference jointly in a single end-to-end model grew out of the early proposal of Ng
(2005b) to use a dummy antecedent for mention-ranking, allowing ‚Äònon-referential‚Äô
to be a choice for coreference classifiers, Denis and Baldridge‚Äôs 2007 joint system
combining anaphoricity classifier probabilities with coreference probabilities, the
Denis and Baldridge (2008) ranking model, and the Rahman and Ng (2009) proposal to train the two models jointly with a single objective.
 Simple rule-based systems for coreference returned to prominence in the 2010s,
partly because of their ability to encode entity-based features in a high-precision
way (Zhou et al. 2004, Haghighi and Klein 2009, Raghunathan et al. 2010, Lee et al.
2011, Lee et al. 2013, Hajishirzi et al. 2013) but in the end they suffered from an
inability to deal with the semantics necessary to correctly handle cases of common
noun coreference.
 A return to supervised learning led to a number of advances in mention-ranking
models which were also extended into neural architectures, for example using re-
30 C HAPTER 23 ‚Ä¢ C OREFERENCE R ESOLUTION AND E NTITY L INKING

 inforcement learning to directly optimize coreference evaluation models Clark and
 Manning (2016a), doing end-to-end coreference all the way from span extraction
 (Lee et al. 2017b, Zhang et al. 2018). Neural models also were designed to take
 advantage of global entity-level information (Clark and Manning 2016b, Wiseman
 et al. 2016, Lee et al. 2018).
 Coreference is also related to the task of entity linking discussed in Chapter 11.
 Coreference can help entity linking by giving more possible surface forms to help
 link to the right Wikipedia page, and conversely entity linking can help improve
 coreference resolution. Consider this example from Hajishirzi et al. (2013):
 (23.81) [Michael Eisner]1 and [Donald Tsang]2 announced the grand opening of
 [[Hong Kong]3 Disneyland]4 yesterday. [Eisner]1 thanked [the President]2
 and welcomed [fans]5 to [the park]4 .
 Integrating entity linking into coreference can help draw encyclopedic knowledge (like the fact that Donald Tsang is a president) to help disambiguate the mention the President. Ponzetto and Strube (2006) 2007 and Ratinov and Roth (2012)
 showed that such attributes extracted from Wikipedia pages could be used to build
 richer models of entity mentions in coreference. More recent research shows how to
 do linking and coreference jointly (Hajishirzi et al. 2013, Zheng et al. 2013) or even
 jointly with named entity tagging as well (Durrett and Klein 2014).
 The coreference task as we introduced it involves a simplifying assumption that
 the relationship between an anaphor and its antecedent is one of identity: the two
 coreferring mentions refer to the identical discourse referent. In real texts, the relationship can be more complex, where different aspects of a discourse referent can
 be neutralized or refocused. For example (23.82) (Recasens et al., 2011) shows an
 metonymy example of metonymy, in which the capital city Washington is used metonymically
 to refer to the US. (23.83-23.84) show other examples (Recasens et al., 2011):
 (23.82) a strict interpretation of a policy requires The U.S. to notify foreign
 dictators of certain coup plots ... Washington rejected the bid ...
 (23.83) I once crossed that border into Ashgh-Abad on Nowruz, the Persian New
 Year. In the South, everyone was celebrating New Year; to the North, it
 was a regular day.
 (23.84) In France, the president is elected for a term of seven years, while in the
 United States he is elected for a term of four years.
 For further linguistic discussions of these complications of coreference see Pustejovsky (1991), van Deemter and Kibble (2000), Poesio et al. (2006), Fauconnier and
 Turner (2008), Versley (2008), and Barker (2010).
 Ng (2017) offers a useful compact history of machine learning models in coreference resolution. There are three excellent book-length surveys of anaphora/coreference
 resolution, covering different time periods: Hirst (1981) (early work until about
 1981), Mitkov (2002) (1986-2001), and Poesio et al. (2016) (2001-2015).
 Andy Kehler wrote the Discourse chapter for the 2000 first edition of this textbook, which we used as the starting point for the second-edition chapter, and there
 are some remnants of Andy‚Äôs lovely prose still in this third-edition coreference chapter.

Exercises
 Exercises 31

Agarwal, O., S. Subramanian, A. Nenkova, and D. Roth. Chomsky, N. 1981. Lectures on Government and Binding.
 2019. Evaluation of named entity coreference. Work- Foris.
 shop on Computational Models of Reference, Anaphora Clark, K. and C. D. Manning. 2015. Entity-centric coreferand Coreference. ence resolution with model stacking. ACL.
Aone, C. and S. W. Bennett. 1995. Evaluating automated Clark, K. and C. D. Manning. 2016a. Deep reinforceand manual acquisition of anaphora resolution strategies. ment learning for mention-ranking coreference models.
 ACL. EMNLP.
Ariel, M. 2001. Accessibility theory: An overview. In Clark, K. and C. D. Manning. 2016b. Improving coreference
 T. Sanders, J. Schilperoord, and W. Spooren, eds, Text resolution by learning entity-level distributed representa-
Representation: Linguistic and Psycholinguistic Aspects, tions. ACL.
 29‚Äì87. Benjamins. Connolly, D., J. D. Burger, and D. S. Day. 1994. A machine
Bagga, A. and B. Baldwin. 1998. Algorithms for scoring learning approach to anaphoric reference. Proceedings
 coreference chains. LREC Workshop on Linguistic Coref- of the International Conference on New Methods in Lanerence. guage Processing (NeMLaP).
Bamman, D., O. Lewke, and A. Mansoor. 2020. An anno- Cucerzan, S. 2007. Large-scale named entity disambiguation
 tated dataset of coreference in English literature. LREC. based on Wikipedia data. EMNLP/CoNLL.
Barker, C. 2010. Nominals don‚Äôt provide criteria of iden- Davis, E., L. Morgenstern, and C. L. Ortiz. 2017. The first
 tity. In M. Rathert and A. Alexiadou, eds, The Semantics Winograd schema challenge at IJCAI-16. AI Magazine,
 of Nominalizations across Languages and Frameworks, 38(3):97‚Äì98.
 9‚Äì24. Mouton. Denis, P. and J. Baldridge. 2007. Joint determination
 of anaphoricity and coreference resolution using integer
Bean, D. and E. Riloff. 1999. Corpus-based identification of
 programming. NAACL-HLT.
 non-anaphoric noun phrases. ACL.
 Denis, P. and J. Baldridge. 2008. Specialized models and
Bean, D. and E. Riloff. 2004. Unsupervised learning of conranking for coreference resolution. EMNLP.
 textual role knowledge for coreference resolution. HLT-
NAACL. Denis, P. and J. Baldridge. 2009. Global joint models for
 coreference resolution and named entity classification.
Bengtson, E. and D. Roth. 2008. Understanding the value of Procesamiento del Lenguaje Natural, 42.
 features for coreference resolution. EMNLP.
 Di Eugenio, B. 1990. Centering theory and the Italian
Berant, J., A. Chou, R. Frostig, and P. Liang. 2013. Semantic pronominal system. COLING.
 parsing on freebase from question-answer pairs. EMNLP.
 Di Eugenio, B. 1996. The discourse functions of Italian sub-
Bergsma, S. and D. Lin. 2006. Bootstrapping path-based jects: A centering approach. COLING.
 pronoun resolution. COLING/ACL. Durrett, G. and D. Klein. 2013. Easy victories and uphill
Bergsma, S., D. Lin, and R. Goebel. 2008. Distributional battles in coreference resolution. EMNLP.
 identification of non-referential pronouns. ACL. Durrett, G. and D. Klein. 2014. A joint model for entity anal-
BjoÃàrkelund, A. and J. Kuhn. 2014. Learning structured ysis: Coreference, typing, and linking. TACL, 2:477‚Äì490.
 perceptrons for coreference resolution with latent an- Emami, A., P. Trichelair, A. Trischler, K. Suleman,
 tecedents and non-local features. ACL. H. Schulz, and J. C. K. Cheung. 2019. The KNOWREF
Bolukbasi, T., K.-W. Chang, J. Zou, V. Saligrama, and A. T. coreference corpus: Removing gender and number cues
 Kalai. 2016. Man is to computer programmer as woman for difficult pronominal anaphora resolution. ACL.
 is to homemaker? Debiasing word embeddings. NeurIPS. Fauconnier, G. and M. Turner. 2008. The way we think: Con-
Brennan, S. E., M. W. Friedman, and C. Pollard. 1987. A ceptual blending and the mind‚Äôs hidden complexities. Bacentering approach to pronouns. ACL. sic Books.
Caliskan, A., J. J. Bryson, and A. Narayanan. 2017. Seman- Fernandes, E. R., C. N. dos Santos, and R. L. MilidiuÃÅ. 2012.
 tics derived automatically from language corpora contain Latent structure perceptron with feature induction for unhuman-like biases. Science, 356(6334):183‚Äì186. restricted coreference resolution. CoNLL.
 Ferragina, P. and U. Scaiella. 2011. Fast and accurate anno-
Cardie, C. and K. Wagstaff. 1999. Noun phrase coreference
 tation of short texts with wikipedia pages. IEEE Software,
 as clustering. EMNLP/VLC.
 29(1):70‚Äì75.
Chafe, W. L. 1976. Givenness, contrastiveness, definiteness, Fox, B. A. 1993. Discourse Structure and Anaphora: Writsubjects, topics, and point of view. In C. N. Li, ed., Sub- ten and Conversational English. Cambridge.
 ject and Topic, 25‚Äì55. Academic Press.
 Garg, N., L. Schiebinger, D. Jurafsky, and J. Zou. 2018.
Chang, K.-W., R. Samdani, and D. Roth. 2013. A con- Word embeddings quantify 100 years of gender and ethstrained latent variable model for coreference resolution. nic stereotypes. Proceedings of the National Academy of
 EMNLP. Sciences, 115(16):E3635‚ÄìE3644.
Chang, K.-W., R. Samdani, A. Rozovskaya, M. Sammons, Grosz, B. J. 1977. The Representation and Use of Focus
 and D. Roth. 2012. Illinois-Coref: The UI system in the in Dialogue Understanding. Ph.D. thesis, University of
 CoNLL-2012 shared task. CoNLL. California, Berkeley.
Chen, C. and V. Ng. 2013. Linguistically aware coreference Grosz, B. J., A. K. Joshi, and S. Weinstein. 1995. Centerevaluation metrics. IJCNLP. ing: A framework for modeling the local coherence of
 discourse. Computational Linguistics, 21(2):203‚Äì225.
32 Chapter 23 ‚Ä¢ Coreference Resolution and Entity Linking

Gundel, J. K., N. Hedberg, and R. Zacharski. 1993. Cog- Kolhatkar, V., A. Roussel, S. Dipper, and H. Zinsmeister.
 nitive status and the form of referring expressions in dis- 2018. Anaphora with non-nominal antecedents in compucourse. Language, 69(2):274‚Äì307. tational linguistics: A survey. Computational Linguistics,
Haghighi, A. and D. Klein. 2009. Simple coreference reso- 44(3):547‚Äì612.
 lution with rich syntactic and semantic features. EMNLP. Kummerfeld, J. K. and D. Klein. 2013. Error-driven analysis
Hajishirzi, H., L. Zilles, D. S. Weld, and L. Zettlemoyer. of challenges in coreference resolution. EMNLP.
 2013. Joint coreference resolution and named-entity link- Kurita, K., N. Vyas, A. Pareek, A. W. Black, and
 ing with multi-pass sieves. EMNLP. Y. Tsvetkov. 2019. Quantifying social biases in contex-
Haviland, S. E. and H. H. Clark. 1974. What‚Äôs new? Acquir- tual word representations. 1st ACL Workshop on Gender
 ing new information as a process in comprehension. Jour- Bias for Natural Language Processing.
 nal of Verbal Learning and Verbal Behaviour, 13:512‚Äì Lappin, S. and H. Leass. 1994. An algorithm for pronom-
521. inal anaphora resolution. Computational Linguistics,
Hawkins, J. A. 1978. Definiteness and indefiniteness: a study 20(4):535‚Äì561.
 in reference and grammaticality prediction. Croom Helm Lee, H., A. Chang, Y. Peirsman, N. Chambers, M. Surdeanu,
 Ltd. and D. Jurafsky. 2013. Deterministic coreference resolu-
Heim, I. 1982. The semantics of definite and indefinite noun tion based on entity-centric, precision-ranked rules. Comphrases. Ph.D. thesis, University of Massachusetts at putational Linguistics, 39(4):885‚Äì916.
 Amherst. Lee, H., Y. Peirsman, A. Chang, N. Chambers, M. Surdeanu,
Hirst, G. 1981. Anaphora in Natural Language Understand- and D. Jurafsky. 2011. Stanford‚Äôs multi-pass sieve corefing: A survey. Number 119 in Lecture notes in computer erence resolution system at the CoNLL-2011 shared task.
 science. Springer-Verlag. CoNLL.
Hobbs, J. R. 1978. Resolving pronoun references. Lingua, Lee, H., M. Surdeanu, and D. Jurafsky. 2017a. A scaffolding
 44:311‚Äì338. approach to coreference resolution integrating statistical
 and rule-based models. Natural Language Engineering,
Hou, Y., K. Markert, and M. Strube. 2018. Unre- 23(5):733‚Äì762.
 stricted bridging resolution. Computational Linguistics,
 44(2):237‚Äì284. Lee, K., L. He, M. Lewis, and L. Zettlemoyer. 2017b. Endto-end neural coreference resolution. EMNLP.
Iida, R., K. Inui, H. Takamura, and Y. Matsumoto. 2003. Incorporating contextual cues in trainable models for coref- Lee, K., L. He, and L. Zettlemoyer. 2018. Highererence resolution. EACL Workshop on The Computa- order coreference resolution with coarse-to-fine infertional Treatment of Anaphora. ence. NAACL HLT.
Ji, H. and R. Grishman. 2011. Knowledge base population: Levesque, H. 2011. The Winograd Schema Challenge. Logi-
Successful approaches and challenges. ACL. cal Formalizations of Commonsense Reasoning ‚Äî Papers
 from the AAAI 2011 Spring Symposium (SS-11-06).
Joshi, M., O. Levy, D. S. Weld, and L. Zettlemoyer. 2019.
 BERT for coreference resolution: Baselines and analysis. Levesque, H., E. Davis, and L. Morgenstern. 2012. The
 EMNLP. Winograd Schema Challenge. KR-12.
Kameyama, M. 1986. A property-sharing constraint in cen- Li, B. Z., S. Min, S. Iyer, Y. Mehdad, and W.-t. Yih. 2020.
 tering. ACL. Efficient one-pass end-to-end entity linking for questions.
 EMNLP.
Kamp, H. 1981. A theory of truth and semantic representation. In J. Groenendijk, T. Janssen, and M. Stokhof, Luo, X. 2005. On coreference resolution performance meteds, Formal Methods in the Study of Language, 189‚Äì222. rics. EMNLP.
 Mathematical Centre, Amsterdam. Luo, X. and S. Pradhan. 2016. Evaluation metrics. In
Karttunen, L. 1969. Discourse referents. COLING. Preprint M. Poesio, R. Stuckardt, and Y. Versley, eds, Anaphora
 No. 70. resolution: Algorithms, resources, and applications,
 141‚Äì163. Springer.
Kehler, A. 1997. Current theories of centering for pronoun
 Luo, X., S. Pradhan, M. Recasens, and E. H. Hovy. 2014. An
 interpretation: A critical evaluation. Computational Linextension of BLANC to system mentions. ACL.
 guistics, 23(3):467‚Äì475.
 de Marneffe, M.-C., M. Recasens, and C. Potts. 2015. Mod-
Kehler, A., D. E. Appelt, L. Taylor, and A. Simma. 2004. The
 eling the lifespan of discourse entities with application to
 (non)utility of predicate-argument frequencies for procoreference resolution. JAIR, 52:445‚Äì475.
 noun interpretation. HLT-NAACL.
 Martschat, S. and M. Strube. 2014. Recall error analysis for
Kehler, A. and H. Rohde. 2013. A probabilistic reconcilcoreference resolution. EMNLP.
 iation of coherence-driven and centering-driven theories
 of pronoun interpretation. Theoretical Linguistics, 39(1- Martschat, S. and M. Strube. 2015. Latent structures for
 2):1‚Äì37. coreference resolution. TACL, 3:405‚Äì418.
Kennedy, C. and B. K. Boguraev. 1996. Anaphora for every- McCarthy, J. F. and W. G. Lehnert. 1995. Using decision
 one: Pronominal anaphora resolution without a parser. trees for coreference resolution. IJCAI-95.
 COLING. Mihalcea, R. and A. Csomai. 2007. Wikify!: Linking docu-
Kocijan, V., A.-M. Cretu, O.-M. Camburu, Y. Yordanov, and ments to encyclopedic knowledge. CIKM 2007.
 T. Lukasiewicz. 2019. A surprisingly robust trick for the Milne, D. and I. H. Witten. 2008. Learning to link with wiki-
Winograd Schema Challenge. ACL. pedia. CIKM 2008.
 Exercises 33

Mitkov, R. 2002. Anaphora Resolution. Longman. Pradhan, S., L. Ramshaw, R. Weischedel, J. MacBride, and
Moosavi, N. S. and M. Strube. 2016. Which coreference L. Micciulla. 2007b. Unrestricted coreference: Identievaluation metric do you trust? A proposal for a link- fying entities and events in OntoNotes. Proceedings of
 based entity aware metric. ACL. ICSC 2007.
Ng, V. 2004. Learning noun phrase anaphoricity to improve Prince, E. 1981. Toward a taxonomy of given-new inforcoreference resolution: Issues in representation and opti- mation. In P. Cole, ed., Radical Pragmatics, 223‚Äì255.
 mization. ACL. Academic Press.
Ng, V. 2005a. Machine learning for coreference resolution: Pustejovsky, J. 1991. The generative lexicon. Computational
 From local classification to global ranking. ACL. Linguistics, 17(4).
 Raghunathan, K., H. Lee, S. Rangarajan, N. Chambers,
Ng, V. 2005b. Supervised ranking for pronoun resolution:
 M. Surdeanu, D. Jurafsky, and C. D. Manning. 2010. A
 Some recent improvements. AAAI.
 multi-pass sieve for coreference resolution. EMNLP.
Ng, V. 2010. Supervised noun phrase coreference research:
 Rahman, A. and V. Ng. 2009. Supervised models for coref-
The first fifteen years. ACL.
 erence resolution. EMNLP.
Ng, V. 2017. Machine learning for entity coreference reso-
Rahman, A. and V. Ng. 2012. Resolving complex cases
 lution: A retrospective look at two decades of research.
 of definite pronouns: the Winograd Schema challenge.
 AAAI.
 EMNLP.
Ng, V. and C. Cardie. 2002a. Identifying anaphoric and non- Ratinov, L. and D. Roth. 2012. Learning-based multi-sieve
 anaphoric noun phrases to improve coreference resolu- co-reference resolution with knowledge. EMNLP.
 tion. COLING.
 Recasens, M. and E. H. Hovy. 2011. BLANC: Implement-
Ng, V. and C. Cardie. 2002b. Improving machine learning ing the Rand index for coreference evaluation. Natural
 approaches to coreference resolution. ACL. Language Engineering, 17(4):485‚Äì510.
Nissim, M., S. Dingare, J. Carletta, and M. Steedman. 2004. Recasens, M., E. H. Hovy, and M. A. Martƒ±ÃÅ. 2011. Identity,
 An annotation scheme for information status in dialogue. non-identity, and near-identity: Addressing the complex-
LREC. ity of coreference. Lingua, 121(6):1138‚Äì1152.
Poesio, M., R. Stuckardt, and Y. Versley. 2016. Anaphora Recasens, M. and M. A. Martƒ±ÃÅ. 2010. AnCora-CO: Coreferresolution: Algorithms, resources, and applications. entially annotated corpora for Spanish and Catalan. Lan-
Springer. guage Resources and Evaluation, 44(4):315‚Äì345.
Poesio, M., P. Sturt, R. Artstein, and R. Filik. 2006. Under- Reichman, R. 1985. Getting Computers to Talk Like You and
 specification and anaphora: Theoretical issues and pre- Me. MIT Press.
 liminary evidence. Discourse processes, 42(2):157‚Äì175.
 Rudinger, R., J. Naradowsky, B. Leonard, and
Poesio, M. and R. Vieira. 1998. A corpus-based investiga- B. Van Durme. 2018. Gender bias in coreference restion of definite description use. Computational Linguis- olution. NAACL HLT.
 tics, 24(2):183‚Äì216.
 Schiebinger, L. 2013. Machine translation: Analyzing gen-
Ponzetto, S. P. and M. Strube. 2006. Exploiting semantic role der. http://genderedinnovations.stanford.edu/
 labeling, WordNet and Wikipedia for coreference resolu- case-studies/nlp.html#tabs-2.
 tion. HLT-NAACL. Soon, W. M., H. T. Ng, and D. C. Y. Lim. 2001. A ma-
Ponzetto, S. P. and M. Strube. 2007. Knowledge de- chine learning approach to coreference resolution of noun
 rived from Wikipedia for computing semantic related- phrases. Computational Linguistics, 27(4):521‚Äì544.
 ness. JAIR, 30:181‚Äì212. Sorokin, D. and I. Gurevych. 2018. Mixing context granu-
Pradhan, S., E. H. Hovy, M. P. Marcus, M. Palmer, larities for improved entity linking on question answering
 L. Ramshaw, and R. Weischedel. 2007a. OntoNotes: A data across entity categories. *SEM.
 unified relational semantic representation. Proceedings of Strube, M. and U. Hahn. 1996. Functional centering. ACL.
 ICSC.
 Su, Y., H. Sun, B. Sadler, M. Srivatsa, I. GuÃàr, Z. Yan, and
Pradhan, S., X. Luo, M. Recasens, E. H. Hovy, V. Ng, and X. Yan. 2016. On generating characteristic-rich question
 M. Strube. 2014. Scoring coreference partitions of pre- sets for QA evaluation. EMNLP.
 dicted mentions: A reference implementation. ACL.
 Tetreault, J. R. 2001. A corpus-based evaluation of center-
Pradhan, S., A. Moschitti, N. Xue, O. Uryupina, and ing and pronoun resolution. Computational Linguistics,
 Y. Zhang. 2012a. CoNLL-2012 shared task: Model- 27(4):507‚Äì520.
 ing multilingual unrestricted coreference in OntoNotes.
 Trichelair, P., A. Emami, J. C. K. Cheung, A. Trischler,
 CoNLL.
 K. Suleman, and F. Diaz. 2018. On the evaluation of
Pradhan, S., A. Moschitti, N. Xue, O. Uryupina, and common-sense reasoning in natural language understand-
Y. Zhang. 2012b. Conll-2012 shared task: Modeling mul- ing. NeurIPS 2018 Workshop on Critiquing and Correcttilingual unrestricted coreference in OntoNotes. CoNLL. ing Trends in Machine Learning.
Pradhan, S., L. Ramshaw, M. P. Marcus, M. Palmer, Uryupina, O., R. Artstein, A. Bristot, F. Cavicchio, F. Del-
R. Weischedel, and N. Xue. 2011. CoNLL-2011 shared ogu, K. J. Rodriguez, and M. Poesio. 2020. Annotating
 task: Modeling unrestricted coreference in OntoNotes. a broad range of anaphoric phenomena, in a variety of
 CoNLL. genres: The ARRAU corpus. Natural Language Engineering, 26(1):1‚Äì34.
34 Chapter 23 ‚Ä¢ Coreference Resolution and Entity Linking

van Deemter, K. and R. Kibble. 2000. On coreferring: coref- Zhao, J., T. Wang, M. Yatskar, V. Ordonez, and K.-W.
 erence in MUC and related annotation schemes. Compu- Chang. 2018. Gender bias in coreference resolution:
 tational Linguistics, 26(4):629‚Äì637. Evaluation and debiasing methods. NAACL HLT.
Versley, Y. 2008. Vagueness and referential ambiguity in a Zheng, J., L. Vilnis, S. Singh, J. D. Choi, and A. McCallum.
 large-scale annotated corpus. Research on Language and 2013. Dynamic knowledge-base alignment for corefer-
Computation, 6(3-4):333‚Äì353. ence resolution. CoNLL.
Vieira, R. and M. Poesio. 2000. An empirically based sys- Zhou, L., M. Ticrea, and E. H. Hovy. 2004. Multi-document
 tem for processing definite descriptions. Computational biography summarization. EMNLP.
 Linguistics, 26(4):539‚Äì593.
Vilain, M., J. D. Burger, J. Aberdeen, D. Connolly, and
 L. Hirschman. 1995. A model-theoretic coreference scoring scheme. MUC-6.
Walker, M. A., M. Iida, and S. Cote. 1994. Japanese discourse and the process of centering. Computational Linguistics, 20(2):193‚Äì232.
Wang, A., A. Singh, J. Michael, F. Hill, O. Levy, and S. R.
 Bowman. 2018. Glue: A multi-task benchmark and analysis platform for natural language understanding. ICLR.
Webber, B. L. 1978. A Formal Approach to Discourse
 Anaphora. Ph.D. thesis, Harvard University.
Webber, B. L. 1983. So what can we talk about now? In
 M. Brady and R. C. Berwick, eds, Computational Models
 of Discourse, 331‚Äì371. The MIT Press.
Webber, B. L. 1991. Structure and ostension in the interpretation of discourse deixis. Language and Cognitive
 Processes, 6(2):107‚Äì135.
Webber, B. L. and B. Baldwin. 1992. Accommodating context change. ACL.
Webber, B. L. 1988. Discourse deixis: Reference to discourse segments. ACL.
Webster, K., M. Recasens, V. Axelrod, and J. Baldridge.
 2018. Mind the GAP: A balanced corpus of gendered
 ambiguous pronouns. TACL, 6:605‚Äì617.
Winograd, T. 1972. Understanding Natural Language. Academic Press.
Wiseman, S., A. M. Rush, and S. M. Shieber. 2016. Learning
 global features for coreference resolution. NAACL HLT.
Wiseman, S., A. M. Rush, S. M. Shieber, and J. Weston.
 2015. Learning anaphoricity and antecedent ranking features for coreference resolution. ACL.
Woods, W. A., R. M. Kaplan, and B. L. Nash-Webber. 1972.
 The lunar sciences natural language information system:
 Final report. Technical Report 2378, BBN.
Wu, L., F. Petroni, M. Josifoski, S. Riedel, and L. Zettlemoyer. 2020. Scalable zero-shot entity linking with dense
 entity retrieval. EMNLP.
Yang, X., G. Zhou, J. Su, and C. L. Tan. 2003. Coreference
 resolution using competition learning approach. ACL.
Yih, W.-t., M. Richardson, C. Meek, M.-W. Chang, and
 J. Suh. 2016. The value of semantic parse labeling for
 knowledge base question answering. ACL.
Zhang, R., C. N. dos Santos, M. Yasunaga, B. Xiang, and
 D. Radev. 2018. Neural coreference resolution with deep
 biaffine attention by joint mention detection and mention
 clustering. ACL.
Zhao, J., T. Wang, M. Yatskar, R. Cotterell, V. Ordonez, and
 K.-W. Chang. 2019. Gender bias in contextualized word
 embeddings. NAACL HLT.
